<html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252">
<title>
Rigorizing QED with "Rain of Bricks"
</title>
<link href="WarrenSmithQED131123_files/sbi.css" rel="stylesheet" type="text/css" id="sbi-style"></head>
<body style="font-family: Arial, sans-serif">

<h2>Rigorizing Quantum Electrodynamics and other Quantum Field Theories with "Rain of Bricks"
(and perhaps even creating "quantum gravity") </h2>

<p>
<small>By Warren D. Smith, <tt>warren.wds@gmail.com</tt>,
<br>
first draft October 2010; second draft April 2012; third draft August 2012;
fourth November 2013; now about 200 pages</small>
</p>
<p>
<b>Abstract.</b>
Book length (but has concise <a href="#resume">summary</a>).
We propose some simple ideas about the microscopic metrical structure of spacetime
(named "rain of bricks") which appear to
accomplish Dirac and Feynman's 60-80 year-old
dream of converting quantum electrodynamics (QED) into a
mathematically well-defined –
entirely-finite without need for either "renormalization,"
"UV cutoffs," or series-truncation –
falsifiable theory algorithmic to arbitrary accuracy.
Indeed, these ideas appear to work not only for QED, but also for a wide class of
infrared-finite renormalizable quantum field theories (IRFrQFT). 
I concentrate on QED merely because I understand it the best, and
it is the simplest, oldest, and most experimentally successful among the QFTs proposed
to explain physics.
The rain of bricks idea apparently can be implemented in many 
inequivalent ways, but there seems to be a largely 
<i>unique</i>
version distinguished from the herd by a combination of better properties,
greater symmetry, tractability, and/or simplicity.
<br>&nbsp;&nbsp;&nbsp;
The strongest rain of bricks versions seem able to convert power-law infinities 
&#8747;<sub>1&lt;z&lt;&#8734;</sub>z<sup>P</sup>dz lurking in old-style QFT physics
to finitude,
not only for any fixed degree P, but indeed even profusions of infinities with
unboundedly-great P can be handled if they have "bounded degree <i>per Feynman vertex</i>."
This looks enough even to regularize <i>non</i>renormalizable QFTs 
such as gravitons.  By setting rain of bricks in de Sitter rather than Minkowski space
(which we show yields "triply-special relativity"; and conversely we argue that
doubly- or triply-SR <i>logically forces</i> something like rain of bricks),
it looks like QFT <i>infrared</i> infinities also automatically are squelched. Also de Sitter
seems both necessary and sufficient to stop perturbative series from diverging.
Exact forms of
"unitarity," "causality," "gauge invariance," and "Lorentz invariance"
all still hold, but have to be stated/interpreted differently.
But rain of bricks also predicts some new effects such as now-imprecise conservation 
of angular momentum (but it becomes arbitrarily precise the larger
the radii and timespan of your observations),
a fixed "temperature floor" of &#8776;4×10<sup>-138</sup> Kelvin,
funny effects when attempts are made to accelerate particles to super-Planckian energies,
and the automatic presence of "dephasing/decoherence" in 
<a href="#screenscatprob">principle</a>
measurable even by observers who can only access energy scales far below 
(and length scales far above) Planckian.
Rain of bricks automatically causes "position-based decoherence"
thus resolving issues associated with "measurement" and "non-locality" in the
foundations of quantum mechanics.
Many new results are found (and old errors corrected) along the way, which should retain
their value even if the overall rain of bricks picture is doomed by some stupid flaw.
</p>
<hr>
<blockquote>
My own view is that all of the successful field theories of which we are so proud 
– 
electrodynamics, the electroweak theory, quantum chromodynamics, and even General Relativity 
–
are in truth effective field theories, only with a much larger characteristic energy, 
something like the Planck energy...
None of the renormalizable versions of these theories really 
describes nature at very high energy, where the non-renormalizable terms in the theory are
not suppressed...   The challenge for the future is to find the final underlying theory, 
to which the effective field theories of the standard model and General Relativity are 
low-energy approximations...
It is possible... that the ingredients of the [final] theory are not the quark,
lepton and gauge boson fields of the Standard Model, but something quite different, 
such as strings.
But there is another possibility. The underlying theory may be an 
ordinary quantum field theory, including fields for gravitation and the 
ingredients of the Standard Model. Of course, it could not be renormalizable 
in the Dyson sense...
<br>&nbsp;&nbsp;<b>–</b> 
Steven Weinberg 2009, describing his personal views/speculations.
<!-- http://arxiv.org/pdf/0903.0568v2 -->
</blockquote>
<blockquote>
If one applies the prescriptions of relativistic quantum mechanics to waves... one 
encounters infinities...
This contradiction does not really mean that quantum, relativity, or waves are false,
but rather shows that when they are combined a universal 
constant with the dimensions of length
must somehow be introduced.
<br>&nbsp;&nbsp;<b>–</b> 
Werner Heisenberg <small>[my free translation of a quote within 
Z.für Physik 101 (1936) 533-540.  This paper is now known to be silly,
but this particular quote is insightful.  Although Heisenberg thought of using the Planck length,
he seemed to prefer a length scale from meson theory.  Heisenberg
made an initial sketch of a "lattice universe"
theory in which spacetime had a discrete cell-structure, but 
within a few months abandoned it as obviously wrong – presumably the foremost
reason being that it destroys relativistic invariance.  That history partially recounted by
Carazza &amp; Kragh 1995.]</small>
</blockquote>
<hr>

<a name="toc"></a>
<h3> Table of Contents </h3>

<ol><li>
<!-- <a href="#toc"> Table of Contents </a></li><li> -->
<a href="#goal"> Goals of paper</a>, and <a href="#theplan">plan</a></li><li>
<a href="#notation"> Notation, Conventions, Units</a></li><li>
<a href="#physconsts"> Brief table of useful physical constants/units </a></li><li>
<a href="#whatswrong"> What is wrong with QED that needs fixing?</a></li><li>
<a href="#dyson"> Dyson 1952's generic-divergence argument – Review and new developments</a></li><li>
<a href="#ratediv"> What is the rate of divergence? (And some illuminating model functions) </a></li><li>
<!--
<a href="#newsum"> Some new series "summation" processes???</a></li><li>
-->
<a href="#whatisrigor"> What would a "rigorous QED" be?</a></li><li>
<a href="#exptlqed">Some large QED computations versus experiment, &amp; 
still more about series divergence 
</a></li><li>
<a href="#noisydists"> "Noisy distances": a preliminary idea to get you in the right frame of mind</a></li><li>
<a href="#doublesr"> Doubly- and triply-special relativity </a></li><li>
<a href="#quantgrav"> The quantum-gravitational basis for claiming the 
microscopic distance-structure of spacetime must differ from Minkowski</a></li><li>
<a href="#rainofbricks"> The "rain of bricks" metrical idea</a></li><li>
<a href="#brickshape"> What are D, E, and the parallelipiped shape &amp; size parameters?</a></li><li>
<a href="#dophys"> How to do physics in the rain of bricks</a></li><li>
<!--
<a href="#kkview"> Kaluza-Kleiny (almost)Equivalent Version of Rain of Bricks </a></li><li>
-->
<a href="#propformulas">Propagator Formulas (and their "brick smeared" forms)</a></li><li>
<a href="#classicalelstat">Brick-smeared classical electrostatics (an illustrative parallel)</a></li><li>
<a href="#nyquist">Nyquist's theorem &#8658; The rain of bricks naturally induces a Planck scale "UV cutoff"</a></li><li>
<a href="#casimirforce"> Casimir force</a></li><li>
<a href="#entropybds"> Entropy and Bounds on it</a></li><li>
<a href="#horizon">Low-tech explanation of Black Hole (and other) Horizon Entropy; and some numbers </a></li><li>
<a href="#bulkentropy">If surface entropy (of horizons) is so important, why 
 isn't <i>bulk</i> entropy even more important?</a></li><li>
<a href="#crudenumer"> Some crude numerical calculations of (now finite) values of the QED infinities</a></li><li>
<a href="#maxstrength"> How do we make it strong enough to handle graviton QFTs?</a></li><li>
<a href="#wasysafe"> S.Weinberg's "asymptotic safety" program and "reduced-dimensional gravity"</a></li><li>
<a href="#whataboutsym">Lorentz invariance? Probability-conservation? Energy &amp; Momentum conservation? Unitarity?</a>
</li><li>
<a href="#furrythm"> Charge Conjugation Invariance and Furry's Theorem</a></li><li>
<a href="#gaugeinvar"> Exact gauge invariance, and charge (or lepton number) conservation</a></li><li>
<a href="#wardident"> Ward-Takahashi Identity</a></li><li>
<a href="#causality"> Exact Causality</a></li><li>
<a href="#noangmom"><i>Non</i>conservation of angular momentum? </a></li><li>
<a href="#hamsand">Time-invariant "bound states"; Trotter product formula.
Hamiltonian? "Emergent" spacetime? </a>  </li><li>
<a href="#badsyms"> Other (non)symmetries: Scaling and Cunningham "inversions" </a></li><li>
    <a href="#moresyms">Overall <i>gain</i> of symmetry?</a></li><li>
<a href="#decoherence">A benefit of broken symmetry: quantum "decoherence" abolishes "weirdness"</a>
</li><li>
<a href="#desit">
De Sitter spacetime</a>
and its distance formula, propagators, and Poisson raindrop process
(and the nonexistence of massless fermions, and
the improved infrared behavior of the propagators)
</li><li>
<a href="#philosource"> Philosophical interlude: what was the source of all QED &amp; QCD's
infinities and nonanalyticities?</a></li><li>
<a href="#byeinfin"> What happens to these infinities with rain of bricks?</a> 
(First look – and first hint of a stunning salvage of analyticity by setting in de Sitter space)</li><li>
<!--
<a href="#???"> Brief review of "Borel summation"???</a></li><li>
-->
<a href="#convser"> The apparent convergence of rain of bricks QED power series
in de Sitter, and divergence in Minkowski, space</a></li><li>
<!--
<a href="#hawking">???Low-tech explanation of Bekenstein-Hawking "black hole entropy" and the exact value of &rho;<sub>rain</sub></a></li><li>
-->
<a href="#lacunae"> Lacunae</a></li><li>
<a href="#glossary"> Glossary</a></li><li>
<a href="#epitaph"> Comparative Summary</a></li><li>
<a href="#resume">CONCISE RESTATEMENT of my currently-most-favored version rain of bricks laws</a></li><li>
<a href="#openprobs">Problems for future workers</a></li><li>
<a href="#ackn"> Acknowledgments</a></li><li>
<a href="#refs"> References</a></li>
</ol>


<a name="tot"></a>
<h3> List of Tables </h3>
<ol><li>
<a href="#physconsts">Useful physical constants / Units arising naturally in QED</a>
</li><li>
<a href="#superheavy">Known &amp; unknown high-Z elements</a>
</li><li>
<a href="#tabpertdivrates">Conjectured (and sometimes proven) asymptotic behaviors for coefficients in
QED-like perturbation series</a>
</li><li>
<a href="#qedelectronmagmom">Electron magnetic moment QED calculation</a>
</li><li>
<a href="#tabemmdiagvalues">Values of diagrams arising in electron magnetic moment calculation</a>
</li><li>
<a href="#qeddatasummary">QED diagram-value datasets (statistical summary)</a>
</li><li>
<a href="#dipquadtab">dipole and quadrupole moments</a>
</li><li>
<a href="#scalinvar">Scaling Invariance of QED and QCD</a>
</li><li>
<a href="#tabcumnoise">D-dimensional cumulated noise</a> 
</li><li>
<a href="#tabclassicalgroup">Some "classical groups."</a>
</li><li>
<a href="#tabaxiomqft">Successful mathematical constructions of axiomatic quantum field
theories.</a>
</li><li>
<a href="#reportcard">ABCDF "report card" on physics.</a>
</li></ol>

<a name="tot"></a>
<h3> List of Figures </h3>
<ol><li>
<a href="#FIG1qedinf">
The fundamental QED infinities</a>
</li><li>
<a href="#FIG2hornskein">
Horn shaped singularity-free region</a>
</li><li>
<a href="#FIG3eichebco">
Chebyshev coefficients in model function</a>, 
demonstrating they behave the same way as is required
to get nowhere-analytic C<sub>&#8734;</sub>-smooth function
</li><li>
<a href="#FIGelectmm340">Feynman diagrams arising in computing electron magnetic moment</a>
</li><li>
<a href="#FIGqeddatasets">QED diagram datasets</a>
</li><li>
<a href="#FIGprotrad">Proton radius</a>
</li><li>
<a href="#runningcouplingevidence">Running coupling constants</a> 
</li><li>
<a href="#FIG4sheafview">Sheaf view</a>
</li><li>
<a href="#FIG5lautrupdiag">Lautrup's "renormalon" diagrams</a>
</li><li>
<a href="#FIG6badfeyn">
Construction of (what seem to be) divergent perturbation series in rain of bricks QED in 
Minkowski spacetime</a>
</li><li>
<a href="#FIG7ccvenn">Venn diagram of computational complexity classes</a>
</li><li>
<a href="#FIGtwoslit">Two slit diffraction thought-experiment relevant to deciding experimentally 
whether rain of bricks is true</a>
</li><li>
<a href="#gosperhilbpic">Points equispaced along Hilbert space-filling curve</a>
(picture supplied by R.W.Gosper).
</li></ol>

<a name="goal"></a>
<h3>1. Goals of paper, and <a href="#theplan">Plan</a> </h3>

<p>
QED's original goal was to be the correct theory of physics
if the only particles were electrons, positrons,
and photons (i.e. if gravity and the weak and strong forces did not exist).
QED can also readily handle certain other things such as 
additional kinds of oversimplified particles – e.g. non-decaying "muons"  and 
spinless point-charge "atomic nuclei" – and imposed conditions such as 
"an ellipsoidal cavity with perfectly reflecting walls containing a uniform 
magnetic field."   
</p><p>
For background in the area, take a few years to digest the books by 
Peskin &amp; Schroeder 1995, 
Mandl &amp; Shaw 1993,
Nash 1978,
Brown 1992,
Greiner &amp; Reinhardt 1994, 
Jauch &amp; Rohrlich 1976,
't Hooft 2010,
Dyson 2006,
Bjorken &amp; Drell 1964 &amp; 1965,
Collins 1986,
Smirnov 2006,
Veltman 1994,
and Zee 2003 
(and a "poetic" popular-science look
at QED is Feynman 1985) while trying not to be too shocked by the contradictions within and
among these.
The two best textbooks to learn from for me were
Peskin &amp; Schroeder and/or 
Greiner &amp; Reinhardt,
but they must be taken in combination with Collins.
A very compressed review is chapter 1 of Kreimer 2000.
In terms of clarity and honesty, the best book is
Folland 2008, but it is much less ambitious than most in scope.
Schweber 1961 is superior to others in the respect that it supplies more
information about old history.
</p><p>
Unfortunately QED as developed by Feynman, Dyson, et al
has (as they were aware) severe foundational problems.   
These problems seem at least as bad in other
QFTs such as electroweak and chromodynamics, and far worse, to the extent where we become
completely helpless, for "nonrenormalizable"
QFTs such as for "gravitons."
</p><p>
The first main goal of this paper is to <b>salvage perturbational QED</b>
by postulating that the metrical structure of 
spacetime differs from that of Minkowski (1+3)-space at small length scales.
We construct particular metrical structures, called "rain of bricks," which
resemble the Minkowski metric viewed at human length scales, but not small ones.
In particular rain of bricks spacetime is microscopically "lower dimensional."
QFTs in new improved universes of this sort do not exhibit infinities anymore at
any finite order in perturbation theory and
we do not need either "renormalization" or artificial "cutoffs" to get rid of them.
But we still enjoy Poincare/Lorentz invariance, 
gauge invariance, conservation laws, and unitarity (in suitable precisely-stated,
but somewhat weakened/different, senses).
</p><p>
At that point, we have saved <i>perturbational</i> QED
–
and probably also all the other QFTs in the "standard model,"
although I have not examined them as carefully –
and done so "algorithmically," i.e.
all the outputs of the theory are 
computable by algorithm to arbitrary (user-specifiable)
accuracy.
However, the perturbative power series that
QED outputs, still may diverge.
Indeed, we present new (and old)
divergence arguments strongly suggesting that generic QED and
QCD series (for old-style QCD, without rain of bricks) diverge
so severely that neither Borel summation – even in LeRoy's generalized form –
nor indeed <i>any</i>
series "summation" method based solely on the coefficients in the power series, can
rescue them, <i>and</i> even with rain of bricks we 
<a href="#minkdiv">still</a> have an
argument for convergence-radius zero in QED.
</p><p>
Our second main goal is to <b>save full QED.</b>
Astonishingly, it appears that setting up rain of bricks in a <i>de Sitter</i> curved
spacetime (rather than flat Minkowski spacetime) "background"
is <i>essential</i> to cause series convergence.
We then have arguments for convergence for all sufficiently small <nobr>|&#945;|,</nobr>
i.e. small but positive radius of convergence, and for analyticity in &#945;; hence
presumably one can "sum" the series by analytic continuation even for &#945; outside
the convergence disk.
Thus this <i>predicts</i> the experimental fact that the Einstein cosmical
constant &#923;<sub>ein</sub> is nonzero (also predicts/forces
|&#923;<sub>ein</sub>|&lt;&lt;1 in Planck units, thus partially saving us
from Weinberg 1989's "cosmical constant crisis") 
and we shall show by a new analysis that
in turn forces neutrino masses to be lower bounded by a 
positive number, confirming/explaining that experimental fact also.
Also there is some relation (I'm unsure how much, and have hardly explored this)
between our methods and those popularized by G.Scharf in his 1995 book
<i>Finite QED</i>.
<!--  our "bricks" are put in one by one in a time-ordered manner and
their effects on the time-evolution "S-matrix" then arise as an operator-product,
which for any finite cavity size is a <i>finite</i> product??? -->
It is currently unclear to me whether rain of bricks has 
any action-stationarization formulation (I suspect yes)
or can be viewed in a Hamiltonian manner (I suspect no), see 
<a href="#opprobaction">open problem 7</a> at the 
<a href="#openprobs">end</a>.
</p><p>
At that point we have constructed at least one plausible candidate for "rigorous QED."  
</p><p>
The <b>third goal</b> then is: what about other quantum field theories, including
QCD and <i>gravitons</i>?   Although we discuss these less, we explain why it is plausible 
that our ideas also can rigorize them, in which case one could get
for the first time
a fully rigorous combined QFT+gravity theory.
However, it currently is not clear how best to proceed and what happens to various key
questions when one tries to incorporate gravitons.  I think they probably
can be incorporated, but a systematic investigation of the possible ways to do so, and 
what happens in each, would be necessary and I have not done it.
</p><p>
<b>Warning:</b>
At present, I have not fully rigorously completed any of the three tasks, especially the third;
see lacunae summary <a href="#lacunae">§39</a>.
It looks like they can
be done, but reaching the present point (of plausible future success)
has already taken &#8776;200 pages.
In fact, this whole paper can be regarded as a "nonrigorous proof
that there exists a rigorous proof"!
</p><p>
Along the way, we shall discover numerous new results, e.g:
</p><ol>
<li>
The first (?) 
<a href="#correctdiracdesitt">correct</a>
derivation of Dirac's 
wave equations in de Sitter space – Dirac had done it wrong. The
approach suggested might also
be able to discover how to generalize other flatspace physics equations to 
more general curved spacetimes, see 
<a href="#curvdeqsprob">open problem 10</a> at 
<a href="#openprobs">end</a>.
</li><li>
That <a href="#neutmass">yields</a>
the conclusion that massless spin-1/2 fermions are impossible if
the universe is de Sitter space, thus explaining the experimental fact that none exist,
and neutrinos have positive mass. Related arguments also indicate the Einstein
cosmical constant must be nonzero, of the correct repulsive sign, and upper bounded
by a bound that is currently "only" 58 orders of magnitude too large, not 121.
</li><li>
<a href="#rovellirefut">Correction</a>
of erroneous conclusion about area quantization in Loop Quantum Gravity
by Rovelli 2004.
</li><li>
The multiD noise 
<a href="#tabcumnoise">theorem</a>.
</li><li>
First simple 
<a href="#doublesr">constructions </a>
of "doubly special relativity" (and "triply special").
</li><li>
Probably the first correct appreciation of the messy 
"<a href="#landaupole">Landau pole</a>"
puzzle which has dogged QED for 6 decades.
</li><li>
First <a href="#GIclassocuntI">demonstration</a>
the count of gauge-classes of QED Feynman diagrams, grows superexponentially,
and first explicit realization/evidence Cvitanovic 1977's "quenched QED convergence conjecture"
presumably is false.
</li><li>
First empirical statistical analysis of Feynman diagram values.
(Provides solid statistical support for Cvitanovic 1977's empirical observations.)
</li><li>
New "Dyson collapse" nonanalyticity arguments and new repairs of flaws in Dyson original argument.
</li><li>
New <a href="https://dl.dropboxusercontent.com/u/3507527/protradiuspuzzle">proposal</a> for solution of the "proton radius puzzle," 
currently the most severe disagreement of QED versus experiment, via asphericity of proton.
</li><li>
First 
<a href="#higgsstability">realization</a>
 the Higgs force cannot yield an 
"unstable matter"  collapse scenario, and this is due
to the Higgs self-interaction; 
so any force-carrying scalar boson without such 
a self-interaction hence presumably is impossible.
</li>
</ol>
<p>
These should retain value even if the overarching plan somehow self-destructs.
</p>

<a name="theplan"></a><h3>Plan of the work</h3>
<p>
<a href="#whatswrong">§4</a> 
describes what is wrong with old-style QED and related QFTs:
</p><ol type="a">
<li>
The need for "cutoffs" and "renormalization" 
– cannot be right since some physical theories <i>must</i> be nonrenormalizable;
also because the
"<a href="#landaupole">Landau pole</a>"
indicates perturbative QED is inherently self-contradictory, albeit we 
shall greatly <a href="#landauclarify">clarify</a> the Landau pole mess.
</li><li>
The need for "perturbative" approach via a nowhere-convergent series
which hopefully is asymptotically right only when &#945;&#8594;0+
(but the Landau pole 
damages or obliterates that hope, and so do my devastating extensions 
of everywhere-divergence arguments to indicate e.g. nonanalyticity at &#945;=0 and
probably everywhere, and nonsummability by Borel, Pade, or in fact by <i>any</i>
method)
</li><li>
Quantum "weirdness" conflicts with everyday classical experience (this is a problem with 
quantum mechanics more generally, not just QED) and facile attempts to claim that
"measurement" is really no problem, fail 
(we <a href="#delocthm1">exhibit</a> 
devastating new trivially simple theorems which obliterate fantasies
this was somehow ok)
</li><li>
Lack of "algorithmicity" – no algorithm exists to compute essentially any 
experimentally measurable quantity in
QED to arbitrary (or for that matter any) <i>guaranteed</i> accuracy,
hence whole theory currently is "nonfalsifiable" hence "not science" at all
</li><li>
It is commonly asserted by physicists that QED &#945;-power series, although divergent, still 
"converge for a long way before they start to diverge" (137 terms?) hence for most practical
purposes Feynman-Dyson style QED is a good-enough theory of nature.  
We point out that actually only is true for simple "toy"
QED problems involving only a few particles.
For realistically-high-complexity problems like a QED model of a virus 
<a href="#dnaexample">DNA</a> molecule, huge
divergence occurs starting after the <i>very first term</i> in essentially any series.  
Hence for most practical purposes QED is a theory of nature no more useful than pre-QED
theories.
</li><li>
Also in <a href="#quantgrav">§11</a> 
we discuss quantum gravity facts that clearly show in several ways
that current QFTs cannot be physically correct and their structure must change 
in a quite fundamental way.
</li></ol>
<p>
<a href="#dyson">§5</a> and
<a href="#ratediv">§6</a> 
review series-divergence arguments.  The first and still one of the nicest
such arguments was made for QED by Dyson 1952.  However there were lacunae and errors in Dyson's 
argument, some of which which we point out and repair here perhaps for the first time.
We also exhibit several other divergence arguments besides Dyson's, and for other QFTs 
beside QED, and which tell us other things besides mere divergence, such as how fast it 
diverges and what can be deduced about the analyticity properties (or lack thereof)
of the QED/QCD functions the series is trying to represent.  All these arguments are
nonrigorous but I consider their net impact nevertheless highly convincing and illuminating.
</p><p>
<a href="#whatisrigor">§7</a> 
says what a "rigorous QED" would be... it would solve all those problems.
</p><p>
<a href="#exptlqed">§8</a> , to be fair, discusses a few of the most impressive
accomplishments of QED and by QEDists, while also availing ourselves
of the opportunity to state a few new results, and to make a few down-to-earth
remarks about the current state of the art and with some statistical examination of
just what QED computers have found over the last 50 years. 
</p><p>
<a href="#noisydists">§9</a> 
discusses "noisy distances," which in
<a href="#doublesr">§10</a>  is shown to be
one way to implement "doubly special" (and "triply special") relativity;
rain of bricks is another.
Quantum gravity facts about the inexact measurabiity of distances, etc, are
derived in 
<a href="#quantgrav">§11</a> 
which also corrects numerous wrong papers in the literature.
</p><p>
In <a href="#rainofbricks">§12-14</a> 
we finally(!) begin explaining what our proposed cure – 
"rain of bricks" – is.
Rain of bricks depends on 3 or 4 new postulates about nature
(re-listed in abbreviated form for easy access in <a href="#resume">§42</a>).
These postulates say, roughly, that spacetime has a certain (specified) metrical structure
<i>different</i> than the classical (3+1)D "Minkowski spacetime"
continuum, and explain how to "do physics" within
that universe.
</p><p>
There are many possible inequivalent and equivalent
alternative forms for the postulates, but it turns out that the vast majority
of the attempts can be argued for one 
(sometimes quite nontrivial) reason or another 
to be <i>wrong</i>.
These wrong postulates and wrongness-arguments will consume
a good deal of space throughout the paper, but the net effect 
(which I had not expected when I began this project)
is that very few possibilities survive, and rain of bricks then is
remarkably unique or nearly.
My current recommendation is 
1-dimensional spatial-only circular bricks in de Sitter spacetime.
</p><p>
<a href="#propformulas">§15-31</a> 
discuss a large number of topics in rain of bricks physics including
conservation laws, unitarity, Lorentz invariance and other
symmetries, causality, energy cutoff effects, Casimir force,
and black hole entropy.  All key puzzles in those areas seem solved (or
largely so).  
One might naively have expected symmetries and conservation laws
with rain of bricks
no longer to be exactly obeyed, but
instead merely to be approximately obeyed and exact only in certain limits such as
&#961;<sub>rain</sub>&#8594;&#8734; and with all particle energies small in comparison to
Planck scale.  
But surprisingly they all can be formulated/interpreted in such a way that they can be 
regarded as <i>exact.</i>
In <a href="#moresyms">§32</a> 
it is argued that rain of bricks
actually is the same or <i>more</i> symmetric (hence presumably more attractive)
than old-style QFT physics, also contrary to
what one might have naively thought.
One further 
could claim that all the usual "Lorentz &amp; gauge" 
symmetry arguments for why physics "had to" have the 
"propagation" equations it does (Maxwell, Dirac, Klein-Gordon) still work in rain of bricks;
only "interaction" is altered.
In <a href="#decoherence">§33</a> 
we argue rain of bricks also solves
the "quantum weirdness"
problem, obsoleting an enormous raft of papers about "interpretation of quantum mechanics,"
the "philosphical meaning" of "measurement,"  its "nonlocality," etc.
(E.g. see Goldstein 1998 for three approaches, all of
which – he admits right at the end –
all totally fail as far as he knows, when relativity is introduced.)
</p><p>
<a href="#wasysafe">§24</a> was added well after most everything 
else in this paper was done, when I discovered that about 10 theoretical physicists
had been working along a rather different line of investigation:
S.Weinberg's "asymptotic safety" program for hopefully saving gravity as a QFT.
Their parallel investigation to mine led to some fascinating/provocative
similar tentative conclusions!
Namely that line of investigation (as well as several other independent ideas
discussed in
Carlip 2012) seems to force spacetime to "appear dimensionally
reduced at short length scales." This could be interpreted as further 
supporting evidence for the rain of bricks picture,
or evidence of the necessity of something like it.
</p><p>
<a href="#desit">§36-38</a> 
examine the convergence (or not) of the &#945;-power series generated by
perturbative rain of bricks QED, finding the remarkable result that 
there should be everywhere-divergence in Minkowski spacetime but convergence if
rain of bricks is set in de Sitter space.
The point of rain of bricks was to abolish "ultraviolet infinities" and
the need for "renormalization" and "cutoffs" thus saving perturbative QED;
the new point is with de Sitter we get series convergence thus also saving,
or actually for the first time creating,
<i>non</i>perturbative QED.
</p><p>
<a href="#lacunae">§39</a> 
highlights ways in which this paper's
arguments fall short of rigor and perfection. Hopefully we clarify what 
we <i>really</i> know after the investigations herein, versus what we have 
merely made very highly (or less) plausible.
(See also the "open problems" section <a href="#openprobs">§43</a>.)
I reiterate that this whole paper can be regarded as a "nonrigorous proof
that there exists a rigorous proof."  (To anybody who objects: I think this is closer
to full rigor than ever before, and hopefully will enable it after an 
additional 200 pages.)
<a href="#epitaph">§41</a>-<a href="#resume">§42</a> 
attempt to conclude/summarize/review the paper including comparing rain of
bricks versus superstrings and other rivals.
</p>

<a name="notation"></a>
<h3>2. Notation, Conventions, Units </h3>
<p>
We shall normally employ either <a href="#physconsts">Planck units</a> c=&#8463;=G=1,
(see <a href="#physconsts">§3</a>; 2&#960;&#8463;=h) or
SI units.   We use signature +---.
Our notations for special functions follow Abramowitz &amp; Stegun 1972
and the software system <tt>MAPLE</tt>;
for Gegenbauer functions (which are only lightly covered by A&amp;S and unavailable in
<tt>MAPLE</tt>)
see the papers by Durand
(also discussed by Szmytkowski 2007).
We let &#937;<sub>D</sub>=2&#960;<sup>D/2</sup>/&#915;(D/2)
be the surface (D-1)-area of
a unit-radius sphere in Euclidean D-space:
&#937;<sub>1</sub>=2,
&#937;<sub>2</sub>=2&#960;,
&#937;<sub>3</sub>=4&#960;,
&#937;<sub>4</sub>=2&#960;<sup>2</sup>.
The reader will need to know complex analysis and advanced matrix algebra.
</p>

<a name="physconsts"></a>
<h3>3. Brief table of useful physical constants 
(<a href="http://physics.nist.gov/cuu/Constants/Citations/Search.html">CODATA</a> 2006 &amp; 
<a href="http://arxiv.org/abs/1203.5425">2010</a>)
</h3>

<table>
<caption><b>Table 1:</b> Units arising naturally in QED:</caption>
<tbody><tr><td>Reduced Compton wavelength of electron</td><td>&#8463;/(m<sub>e</sub>c)</td><td>
3.8615926800(25)×10<sup>-13</sup> meter
 = 2.38923(12)×10<sup>22</sup>L<sub>pl</sub></td></tr>
<tr><td>QED time scale</td><td>&#8463;/(m<sub>e</sub>c<sup>2</sup>)</td><td>
1.2880886570(18)×10<sup>-21</sup> second
 = 2.38923(12)×10<sup>22</sup>t<sub>pl</sub></td></tr>
<tr><td>Mass of electron</td><td>m<sub>e</sub></td><td>
9.10938291(40)×10<sup>-31</sup> kg  <!--CODATA 2010-->
 = 4.18545(21)×10<sup>-23</sup>M<sub>pl</sub>   <!--still CODATA 2006-->
</td></tr>
<tr><td>Energy of electron</td><td>m<sub>e</sub>c<sup>2</sup></td><td>
8.18710506(36)×10<sup>-14</sup> Joule</td></tr>
<tr><td>Electron |Charge|</td><td>e</td><td>
<!-- 1.602176487(40)&times;10<sup>-19</sup> Coulomb was CODATA 2006 -->
1.602176565(35)×10<sup>-19</sup> Coulomb
 = 0.0854245429(29) q<sub>pl</sub>
</td></tr>
<tr><td>Fine structure constant</td><td>&#945;=e<sup>2</sup>/(2&#949;<sub>0</sub>hc)</td><td>
<!-- 1/137.035999679(94)  [dimensionless number]  this was CODATA 2006-->
0.0072973525698(24) = 1/137.035999074(44)  [dimensionless number]</td><td></td></tr> <!--this is CODATA 2010-->
<tr><td>Strong force coupling constant</td><td>&#945;<sub>S</sub> at M<sub>Z</sub></td>
<td>0.1184(7)=1/8.446(50)
[dimensionless number]</td><td></td></tr> 
<tr><td>Weak force coupling constant</td>
<td>&#945;<sub>W</sub>=&#945;/sin(&#952;<sub>W</sub>)<sup>2</sup> at M<sub>Z</sub></td>
<td>
0.031568(16) = 1/31.678(16)
[dimensionless number]</td><td></td></tr> 
<tr><td colspan="3" align="center">Units arising naturally in quantum gravity:</td></tr>
<tr><td>Planck length</td><td>L<sub>pl</sub>=(&#8463;G/c<sup>3</sup>)<sup>1/2</sup></td><td>
1.616199(97)×10<sup>-35</sup> meter</td></tr>
<tr><td>Planck time</td><td>t<sub>pl</sub>=(&#8463;G/c<sup>5</sup>)<sup>1/2</sup></td><td>
5.39106(32)×10<sup>-44</sup> second</td></tr>
<tr><td>Planck mass</td><td>M<sub>pl</sub>=(&#8463;c/G)<sup>1/2</sup></td><td>
2.17651(13)×10<sup>-8</sup> kg <!-- now corrected to use CODATA 2010--> 
= 
2.38952(12)×10<sup>22</sup>m<sub>e</sub></td></tr> <!--still CODATA 2006-->
<tr><td>Planck energy</td><td>M<sub>pl</sub>c<sup>2</sup>=(&#8463;c<sup>5</sup>/G)<sup>1/2</sup></td><td>
1.95609(10)×10<sup>9</sup> Joule = 1.220932(73)×10<sup>19</sup> GeV</td></tr>
<tr><td>Planck 4D number density</td><td>c<sup>7</sup>(&#8463;G)<sup>-2</sup></td><td>
4.39492(88)×10<sup>147</sup> (meter<sup>3</sup>second)<sup>-1</sup></td></tr>
<tr><td>Planck charge</td><td>q<sub>pl</sub>=(2&#949;<sub>0</sub>hc)<sup>1/2</sup></td><td>
1.875545870(47)×10<sup>-18</sup> Coulomb = 11.70623764(41) e</td></tr>
<tr><td colspan="3" align="center">Cosmology-related quantities:</td></tr>
<tr><td>Hubble "constant"</td><td>H 
<!--
</td><td>2.40(12)&times;10<sup>-18</sup>second<sup>-1</sup> &asymp; 1.30(7)&times;10<sup>-61</sup> (t<sub>Pl</sub>)<sup>-1</sup></td></tr>
-->
</td><td>2.32(10)×10<sup>-18</sup>second<sup>-1</sup> &#8776; 1.26(5)×10<sup>-61</sup> (t<sub>Pl</sub>)<sup>-1</sup></td></tr>
<tr><td>Einstein Cosmical constant</td><td>&#923;<sub>Ein</sub>
<!--
<a href="http://hyperphysics.phy-astr.gsu.edu/hbase/astro/denpar.html">this gives</a>
</td><td>1.62(09)&times;10<sup>-35</sup>second<sup>-2</sup> &asymp; 4.71(26)&times;10<sup>-122</sup> (t<sub>Pl</sub>)<sup>-2</sup></td></tr>
-->
</td><td>1.18(10)×10<sup>-35</sup>second<sup>-2</sup> &#8776; 3.43(29)×10<sup>-122</sup> (t<sub>Pl</sub>)<sup>-2</sup></td></tr>
<tr><td><a href="http://en.wikipedia.org/wiki/Age_of_the_universe">Age of universe</a></td><td>
(13.75±0.11) Gyr
</td><td>4.339(35)×10<sup>9</sup>second &#8776; 8.048(20)×10<sup>52</sup> t<sub>Pl</sub></td></tr>
</tbody></table>
<p>
<b>Notes:</b>
Most values are from CODATA 2006 but a few have been updated to CODATA 2010.
My sin(&#952;<sub>W</sub>)<sup>2</sup>=0.23116(12) and hence
cos(&#952;<sub>W</sub>)<sup>2</sup>=0.76884(12) 
values for the
Weinberg weak-mixing angle at M<sub>Z</sub>=91.1876(21) GeV/c<sup>2</sup>,
the mass of the Z boson,
and my &#945;<sub>S</sub>=0.1184(7) value, all are from
<a href="http://pdg.lbl.gov/2012/reviews/rpp2012-rev-phys-constants.pdf">Particle Data 
Group 2012</a>.
Other important particle masses:
M<sub>top quark</sub>=173.07(80) GeV/c<sup>2</sup>,
M<sub>W</sub>=80.385(15) GeV/c<sup>2</sup>,
M<sub>higgs</sub>=125.6(6) GeV/c<sup>2</sup>,
M<sub>muon</sub>=105.6583715(35) MeV/c<sup>2</sup>,
M<sub>tauon</sub>=1.77682(16) GeV/c<sup>2</sup>.
Other <a href="http://en.wikipedia.org/wiki/Quark">quarks</a>
are much lighter than the top, e.g. the down and up quarks, which
are the only stable ones, have masses 1.7-3.1 and 4.1-5.7 MeV/c<sup>2</sup>.
<!-- CODATA 2010 gives sin(thetaW)^2 = 0.2223(21) at unstated energy. -->
<!-- Other masses:
The Z-boson is neutral.  M[Zboson]=91.1876 GeV.
The W-boson is charged+-1 and has mass 80.385(15) GeV.
The Z boson mass arises from the Higgs mechanism.
M[HiggsBoson] = 125.7(6) GeV.
M[topQuark]=173.07(80) GeV.
The top quark Yukawa(via Higgs) coupling is far greater than for any other known
quark or lepton. It has a value of
  y[top] = sqrt(2) * m[top] / v =approx= 1
where v=246 GeV is the value of the Higgs vacuum expectation value.
-->
My value H=71.6±3.0 km/s/Mpc
was based on 5 experimental values 
{73.8±2.4,
67.0±3.2,
72.6±3.1,
71.0±2.5,
77±11.5}
published during 2006-2011 and mentioned in
the wikipedia article
<a href="http://en.wikipedia.org/wiki/Hubble_constant">Hubble constant</a>.
(I've put a somewhat mysterious 
combined error estimate which hopefully errs on the side of conservativism.)
I calculated the &#923; value from &#923;=3H<sup>2</sup>f,
where the fraction f of the universe's mass density attributable to
the cosmical constant is f=0.728(16) based on numbers
in the wikipedia article
<a href="http://en.wikipedia.org/wiki/Lambda-CDM_model">Lambda-CDM model</a>,
and I'm assuming the total mass density (including ordinary and dark matter, energy,
and cosmical constant) exactly equals the so-called "critical density"
(it does, experimentally, to within a factor 1.0023±0.0056).
The reader is warned that 
<a href="http://hyperphysics.phy-astr.gsu.edu/hbase/astro/denpar.html">this web site</a>
used essentially the same technique but based on older published experimental data to
get &#923;=1.62(09)×10<sup>-35</sup>second<sup>-2</sup> 
which is hugely outside our error bounds.  Many other estimates of
&#923; have been published which also lie well outside
our error bars, ranging from 0.6 to 6 times 
<nobr>10<sup>-35</sup>second<sup>-2</sup>.</nobr>
Such self-contradictions indicate to me that
the workers in this field cannot be (or until recently could not be) trusted.
The Einstein cosmical constant &#923; is often ssociated with an "effective mass-energy density 
of the vacuum" (in units of mass density) &#961;<sub>vac</sub>=&#923;/(8&#960;G).
This with our tabulated value of &#923; would be 
<nobr>
&#961;<sub>vac</sub>=7.035(596)×10<sup>-27</sup>kg/meter<sup>3</sup>.
</nobr>
</p>
<!--
LAM = sec^(-2)
G = meter^3 /sec^2 /kg
c = meter/sec
LAM / G = kg/meter^3

h reluncert  50 ppb  
e reluncert  25 ppb  
G reluncert 100 ppm
c, eps0 exact
<p>
H&asymp;71 km/second/megaparsec=2.3&times;10<sup>-18</sup>second<sup>-1</sup> is
COSMICAL CONSTANT???
rhoLambda = 10^(-8) erg/cm^3
10^(-120) in Planck units

Riess et al 2009 find H0 = 74.2 +- 3.6 km/sec/Mpc with 68% CL including both stat & sys errors
based on 240 low-z type Ia supernovae with z<0.1.
Freedman et al 2001 had earlier found H0=72+-8.

Riess, Adam G.; Lucas Macri, Stefano Casertano, Hubert Lampeitl, Henry C. Ferguson, Alexei V. Filippenko, Saurabh W. Jha, Weidong Li, Ryan Chornock (1 April 2011). "A 3% Solution: Determination of the Hubble Constant With the Hubble Space Telescope and Wide Field Camera". 
The Astrophysics Journal 730 (2): 119.
Hubble
73.8(2.4)  Riess2011
67.0(3.2)  Beutler2011
72.6(3.1)  Suyu2010
71.0(2.5)  WMAP2010
77(11.5)   Chandra2006
----------------------
71.6(2.5 and 4.4=5.0)
71.6(3.8)

Florian Beutler+8: 
The 6dF Galaxy Survey: baryon acoustic oscillations and the local Hubble constant,
Monthly Notices of the Royal Astronomical Society
Volume 416, Issue 4, pages 3017-3032, October 2011
http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2966.2011.19250.x/abstract;jsessionid=DCC196D79F60BD170126298BAC4EBD1D.d03t04

Adam G.Riess, Lucas Macri, &amp; 10 others:
A Redetermination of the Hubble Constant with the Hubble Space Telescope from a Differential 
Distance Ladder, 
Astrophysical Journal 699,1 (2009) 539-563.
A.G.Riess, Macri, L., Casertano, S., Sosey, M., Lampeitl, H., Ferguson, H. C., Filippenko, A. V., Jha, S. W., Li, W., Chornock, R., & Sarkar, D. 2009, ApJ, 699, 539

Freedman, W. L., Madore, B. F., Gibson, B. K., Ferrarese, L., Kelson, D. D., Sakai, S., Mould, J. R., Kennicutt, R. C., Ford, H. C., Graham, J. A., Huchra, J. P., Hughes, S. M. G., Illingworth, G. D., Macri, L. M., & Stetson, P. B. 2001, ApJ, 553, 47

This page actually gives error bars, hallelujah...:
   http://hyperphysics.phy-astr.gsu.edu/hbase/astro/denpar.html
finding total mass density of universe (including both normal matter, dark matter, 
normal energy, and cosmo. constant) is
   (9.66 +- 0.02) * 10^(-27) kg/m^3
of which (73+-4)% is due to cosmo.constant.
[Wikipedia  http://en.wikipedia.org/wiki/Lambda-CDM_model has
TotalDensity = 1.0023(56) * CriticalDensity
CosmoConstFraction=0.728(16)
CriticalDensity=(3*c^2*H0^2)/(8*pi*G)]

That in turn corresponds upon multiplying by 8*pi*G to 
   Lambda = (1.62 +- 0.09) *  10^(-35) / second^2
...an actual value, with an actual error bar!
Better data is here:

(0.6 to 6)&times;10<sup>-35</sup>second<sup>-2</sup> &asymp; (0.17 to 1.7)&times;10<sup>-121</sup> (t<sub>Pl</sub>)<sup>-2</sup></td></tr>
was what I;d found erlier from numrpus dubious literature numerical claims.
</p>
-->

<a name="whatswrong"></a>
<h3>4. What is wrong with QED that needs fixing? </h3>

<p>
<b>P.A.M.Dirac (1902-1984):</b>
<!-- "Physicists should neglect quantities because they are small,
not because they are infinite and we wish they were small." -->
"Sensible mathematics involves neglecting a quantity when it is small –
not neglecting it because it is infinitely great and you do not want it!"
<!-- quoted p.184 of Helge Kragh's 1990 biography QC16.D57K73. 
Fuller quote:
Most physicists are very satisfied with the situation. 
They say: 'Quantum electrodynamics is a good theory, and we do not have to worry about it 
any more.' I must say that I am very dissatisfied with the
situation, because this so-called 'good theory' involves neglecting infinities
which appear in its equations, neglecting them in an arbitrary way. This is just 
not sensible mathematics. Sensible mathematics involves neglecting a quantity when it is 
small -- not neglecting it just because it is infinitely great and you do not want it!"
-->
<br>
"[Renormalization in QED involves] a drastic departure from logic. It changes 
the whole character of the theory, from logical deductions to mere setting up of working rules."
<br>
"Some physicists may be happy to have a set of working rules leading
to results in agreement with observation. They may think this is the
goal of physics.  But it is not enough.  One wants to <i>understand how
nature works</i>."
<br>
Dirac even went so far as to opine that it was misguided to investigate
deeper theories of particle physics such as QCD until
the foundational difficulties with QED had been cleared up.
[<small>See Dirac's short papers of 1981 and 1990 for his final statements on the topic;
see pp.115-116 &amp; 291 
of S.Weinberg: <i>Dreams of a final theory</i> (Pantheon 1992) for a short 
account of
a debate between Weinberg and Dirac on this in the 1970s.  
Weinberg seems to think
he won, but then seems to think the <a href="#landaupole">Landau pole</a>
means he ultimately lost.  Indeed, Weinberg later (2009) proposed the 
"<a href="#wasysafe">asymptotic safety</a>"
idea as a way to perhaps allow QFTs to "live with infinities."
<!--e.g. somehow be made to survive despite the devastating Landau pole problem
or nonrenormaliability.
This idea remains speculative and largely uninvestigated at this time.
-->
It certainly <i>is</i> logically possible for a renormalizable QFT to feature
"infinitely large" quantities which genuinely have only small effects –
unphysical toy QFTs have been constructed for which this has been proven.
But the Landau pole indicates that that is not the case for QED, in which case Dirac's
view must be considered to be the winning one.</small>]
</p><p>
<b>R.P.<a href="http://users.physik.fu-berlin.de/%7Ekleinert/kleinert/?p=feynman&amp;page=3">Feynman</a>
(1918-1988):</b>
Always regarded QED as merely an "effective field theory."  
In his view "renormalization" was intended as a stopgap measure, a trick, intended 
to circumvent the fact that we did not know the true theory of physics.
Unrenormalized QED yields certain physically-insane
infinities, which cleverly contrived "renormalization" processes convert to correct-valued
finite quantities.  (For example, QED predicts the "mass of the electron" is "infinity"
but the renormalization manually erases all the terms responsible for that and
replaces them with new expressions which by design yield the experimental value.)
The unknown true theory of physics would simply have yielded the
correct finite values.  
The same view was held by QED's independent inventor <b>Julian Schwinger (1918-1994),</b> 
who, e.g, wrote
(Schwinger 1948 p.416 – the paper with historically the very first reasonably
complete understanding of the situation – describing the purpose and method
of renormalization, our emphasis added):
</p><blockquote>
Electrodynamics unquestionably requires revision at ultra-high
energies, but is presumably accurate at moderate relativistic
energies. It would be desirable, therefore, to isolate
those aspects of the current theory that essentially involve high
energies, and are subject
to modification by a <i>more satisfactory theory</i>, from aspects that
involve only moderate
energy and are thus relatively trustworthy.
This goal has been achieved by transforming the Hamiltonian of current electrodynamics to 
exhibit explicitly the logarithmically divergent self-energy of the free electron, which
arises from virtual emission and absorption of light quanta.  This electromagnetic 
self-energy... can be ascribed to an electromagnetic mass, which must be added 
to the mechanical mass of the free electron.  Indeed the only meaningful
statements of the theory involve this combination, which is the experimental
electron mass...
It is important to note that [this mass-adjustment] does not avoid all
divergences; the polarization of the vacuum produces [another].  However, it has
long been recognized [this was a reference to a 1934 lecture by Dirac]
that such a term is equivalent to altering the electron charge
by a constant factor, only the final value being identified with the experimental charge.
Thus the interaction between matter and radiation produces a renormalization of the 
electron charge and mass, all divergencies being contained in the renormalization factors.
</blockquote><p>
Feynman and Schwinger's surprising point was that by means of these tricks many of the (unknown)
deficiencies of QED versus the (unknown) correct theory, could be 
effectively corrected!  Feynman also said the following to his biographer Mehra only
1 month before he died (Mehra §15.8):
</p><blockquote>
I knew that the way I had cut off [the singularities] destroyed unitary...  Only the limit
was presumably OK.
I believed there was a way of cutting the theory off somehow which
wouldn't destroy anything, wasn't artificial, or just a mathematical trick.
It could be solved. With the finite cut-off the theory would be sound by itself. 
And the real theory [should be] the limit of <i>that</i>.
So I delayed [publishing]...
But thank God I did [publish] because it has never been straightened out to this day.
<br>&nbsp;&nbsp;&nbsp;
I thought perhaps Schwinger had straightened it out because he had a great reputation, 
but when I thought about what he did, I realized he had in his work the same problem 
in another form. [And so] I knew we both hadn't solved... quantum electrodynamics in the sense
of finding a sound theory the limit of which is electrodynamics; we couldn't prove that the 
electrodynamics we were writing was self consistent – [it's] possibly not unitary, 
possibly incorrect, very likely is so, but it's never been resolved.  It has never been 
proved one way or the other whether electrodynamics renormalizes a consistent theory.
</blockquote><p>
<b>My response to those Feynman quotes:</b>
First of all, which Feynman probably knew, "dimensional regularization" as introduced
by t'Hooft &amp; Veltman 1972 (see also Leibbrandt 1975, Veltman 1994),
is a way to "cut off singularities" in QED<sub>N</sub> 
(that is, QED at any particular perturbative order N)
which manifestly is Lorentz covariant, and which yields S-matrices which preserve
probability, i.e. respect "unitarity," and are "causal."
Also, this with summation over all diagrams at
a given order N
yields gauge-invariant results for QED
(albeit this is untrue for individual Feynman diagrams, and
in fancier "non-abelian gauge theories"
one must also add "ghost particles" to restore gauge invariant results).
But I think Feynman would contend that even QED<sub>N</sub> using
dimensional regularization to obtain a UV cutoff,
still "does not make sense by itself."
But: <a href="#rainofbricks">Rain of bricks</a>
seems to yield a "finitely cut-off" QED that <i>is</i> "sound by itself"
and if true does yield an "understanding of how nature works."

<!-- Veltman, Diagrammatica, opening of ch 9:
"[for quantum electrodynamics we] have a reasonably complete treatment, showing that 
the manifestly covariant Feynman rules as given for that theory define a 
Lorentz invariant S-matrix that also conserves probability."
-->
</p><p>
<b>Freeman Dyson (1923-&nbsp;):</b> 
Dyson was very discouraged by his 1952 discovery of a highly convincing simple
argument (but not a proof)
that essentially every QED perturbative series either 
diverges or fails to represent physics. 
Then Hurst 1952 incompletely
and Thirring 1953 completely
<i>proved</i>
divergence in an artificial unphysical (1+3)-dimensional
quantum field theory "&#955;&#966;<sup>3</sup> theory."
A perhaps more impressive result along the same lines is Jaffe 1965,
where a wide class of unphysically-low-dimensional boson QFTs are proved to 
have always-divergent perturbation series, albeit Jaffe also showed
that "renormalization" causes
each <i>individual term</i> in those series to be finite.
Then Lautrup 1977
proved a certain infinite explicit subsequence of QED "Feynman 
<a href="#lautrupdiaganal">diagrams</a>," one for each
order N&#8805;1, arising in 
computing the ratio of the magnetic moment of the electron divided by the Bohr magneton, had value 
</p><center>
(N-1)! (6&#960;)<sup>-N</sup> exp(-10/3) 3 [1 + O(N<sup>-1</sup>)]
</center><p>
for its coefficient of &#945;<sup>N</sup>.
(This is Lautrup's
starred equation on the right of his p.349, but replace his n by 
N-1 and multiply by &#960;<sup>-N</sup>.)
<!-- Hurst 2006 lied about Lautrup's formula. 
Lautrup's lower bound is tight in the weak sense that
one can show an upper bound of the
form N<sup>O(N)</sup> on the value of any (finite, renormalized) Nth-order diagram
by combining the integral finiteness-proof techniques of 
Zimmermann et al with the fact that determinants of N&times;N matrices
with |entries|&lt;B are bounded by B<sup>N</sup>N<sup>N/2</sup>,
the known behavior of formulas for the volumes of simplices and balls in N dimensions,
and the bound N!&lt;N<sup>N</sup> on the number of ways to permute N things.
(I claim this; Zimmermann himself gave no explicit bound in his papers.
NEED TO SKETCH THE PROOF???)
NO: actually Zimmermann yields an N!*K^N upper bound except only for Euclidean, for Minkowski
I am not seeing an explcit bound.   And he demands all mosses be positive, although
I am not seeing any explicit use of that within his proof.   I think Hahn & Zimmerman 1968 
use it though.
-->
Note the N!-style growth.
Upper bounds for size-N Feynman diagrams
roughly matching Lautrup's lower bound, i.e. proving Lautrup's examples
are optimal (or not very nonoptimal) were shown by
Feldman, Magnen, Rivasseau, Seneor 1985, except that these upper bounds were only shown in
certain restricted classes of rQFTs which do not include (3+1)-dimensional QED.
</p><p>
Dyson then abandoned the whole area, writing no more QED papers after 1952.
Meanwhile Thirring concluded "The chances for quantized fields to become a 
mathematically consistent theory are rather slender."
<!-- That dimness
remained unbrightened when Hurst reviewed the matter in 2006. -->
</p><p>
For a survey and exploration of ideas surrounding Dyson's divergence argument, see our 
<a href="#dyson">§5</a>
and 
<a href="#ratediv">6</a>.
</p><a name="kalleninf"></a><a><p>
<b>Gunnar Källen (1926-1968)</b> 
considered the possible escape hatch for QED that the divergencies pointed out by Dyson,
and/or the infinities that Feynman renormalized away,
might merely be a symptom of their
"perturbative approach" of expanding everything in a power series
in powers of the small parameter &#945;.  As he said (oversimplifying):
"All we really know [from perturbative QED]
is the electron self-energy is not an analytic 
function of &#945; at the origin."
Dyson's divergence argument left open the possibility that some other 
construction of QED, e.g. based on a power series in something else,
might be able to return clear results.   To investigate that question,
Källen in 1952 partially constructed a <i>non</i>perturbative QED.
This construction involved certain unknown functions.  
(Later, well after Källen's death, it became possible to compute those functions
approximately using lattice-QED methods.)
Despite the incompleteness of
our understanding of those functions, 
Källen 1953 was able to argue that if three particular renormalization constants
were all assumed to be finite, that would lead to a contradiction.  Hence, he
concluded, in <i>any</i> formulation of QED, at least one "infinite constant" 
(which physically, is insane) must arise.  This, if Källen's proof
is believed, slammed shut that escape hatch.
</p><blockquote><small>
But, Källen 1953 said on p.15, "[My proof] makes no pretense of being satisfactory from
a rigorous mathematical point of view.
It contains for example a large number of interchanges of order of integrations, 
limiting processes, and so on."  
Källen then noted that there were two possibilities: 
(a) his unknown functions were well-behaved enough that those
mathematical steps were justified (in which case, his proof was correct) or (b) not.
Källen considered (b) unlikely because if the QED functions were that badly behaved, then
it would be surprising that lower-order QED had been so successful in agreeing with experiment.
So apparently, Källen would have claimed that he fully-rigorously showed that <i>either</i>
at least one of those three renormalization constants had to be infinite <i>or</i>
at least one of his fundamental QED functions had to be badly behaved.
<br> &nbsp;&nbsp;&nbsp;
Further criticisms of Källen's "proof," some contradicting each other, 
were stated by 
Johnson 1958,
Gasiorowicz, Yennie &amp; Suura 1959,
Zumino 1960,
Fleischman 1963,
Aramaki 1978, and
Aramaki 1983.
The net result (Aramaki 1983)
seems to be that these critics do not believe Källen had a valid proof,
for reasons that go beyond those Källen himself pointed out.
Aramaki concludes that the question of whether a QED
renormalization constant must be infinite is a still
unresolved and difficult open question.  (All these critics agree Källen's result might
still be correct even though they dispute that he proved it; some of the critics apparently
still felt pretty confident he was right, while others hope he was wrong.)
Powers 1967 however rigorously proved a weakened version of Källen's
result and a related rigorous result is by Glimm &amp; Jaffe 1969.
</small></blockquote>
<p></p></a><p><a>
<b>Kenneth W. Johnson (1931-1999):</b> 
In a series of papers
with collaborators that included M.Baker, S.L.Adler, and R.S.Willey,
Johnson tried to explore "finite QED" –
the idea that a version of QED could be constructed with everything finite.
(But this has nothing to do with Scharf's 30-year-later book <i>Finite QED.</i>)
Their first paper (1963)
claimed to show that the only possible hope for that was if the bare mass of the electron was 
exactly <i>zero</i>, with, therefore, all electron mass arising "dynamically."
Unfortunately:  if for some reason the electron mass is somehow determined, why
is the <i>muon</i> 206.768 times heavier?  Another problem is the fact that without 
m<sub>e</sub> QED <i>does not contain</i> any quantity with the dimensions of "mass"!
So it seems clear that this kind of QED, if it is sensible at all,
is <i>necessarily incomplete</i> since the masses 
m<sub>e</sub> and m<sub>&#956;</sub> 
must arise from some
other laws of physics extraneous to QED itself.
Johnson, Baker, and Willey 1963 &amp; 1964 claimed 
they had succeeded in constructing a version of QED in which the 
bare electron mass was zero and the infinite renormalization constants 
&#916;m<sub>e</sub> and Z<sub>1</sub>=Z<sub>2</sub>
were both finite <i>provided</i> it was taken as known that
Z<sub>3</sub> also was finite.  This, they claimed, would be established in a subsequent paper.
However, that subsequent paper never came.  What did come was
Johnson, Baker, and Willey 1967 which argued that Z<sub>3</sub>
diverged to infinity "like a single
power of the logarithm of an ultraviolet cutoff in all orders of
perturbation theory."  Oops.
Thus, this entire hope/dream seemed to have fatally imploded.
And actually, even this still was wrong, see
de Rafael &amp; Rosner 1974,
Kinoshita, Kawai, Okamoto 1991,
and
Nigam 1999
(and our </a><a href="https://dl.dropboxusercontent.com/u/3507527/crudenumer">§22</a>)
for computations of 
Z<sub>3</sub> demonstrating
divergence to infinity like higher powers of
a logarithm.
Later, a paper by Adler generated hope
that somehow this sort of "finite QED" would be possible only with certain magic
values of the fine structure constant &#945;.
Acharya &amp;  Narayana Swamy 1997 claimed to have killed that hope too.
(According to Adler, both he in unplublished work, as well as
Baker &amp; Johnson, 
had by then also independently realized this hope was dead.)
</p><p>
<b>Kenneth G. Wilson (1936-):</b> 
Wilson offered a new approach to QED and especially QCD: "Lattice gauge theory" and
along with it a new, less ad hoc, scaling-based view of what "renormalization" is.
Essentially, one replaces the partial differential equations of physics with difference-based 
approximations, using a discrete point lattice instead of the spacetime continuum.
Certain physical "constants" (such as, in QED, the fine structure constant &#945;
encoding the "bare" electron mass and charge,
as well as certain renormalization-factors which are infinite in the continuum limit)
are now regarded as <i>functions</i> of the lattice spacing.   By choosing these functions
correctly (the "running of the coupling constants") Wilson argued the effect of renormalization
would be got.  Then a computer can be used (in principle, and after great effort in practice too)
to compute physical predictions.
(See also Lepage 1989 for an insightful re-examination of that.)
</p><a name="delamotte"></a><p>
Delamotte 2004 and Neumaier 2011 
offer elementary introductions to this kind of idea.
(In contrast, our rain of bricks theory will abandon renormalization and
propose that the bare electron charge and mass – which rain of bricks uses –
do <i>not</i> vary with scale, and instead are <i>fixed</i> at approximately the values
Wilson would claim appropriate for the Planck scale, see
<a href="#crudenumer">§22</a> for our numerical estimates.)
</p><blockquote>
Key quote (Delamotte):
We again emphasize that if [the coupling constant] is believed 
to be no more than a non-measurable parameter,
useful only in intermediate calculations, it is of no consequence that this quantity is
infinite when [the energy cutoff or reciprocated lattice spacing] goes to infinity.
</blockquote><p>
I object that the running quantities in QED (such as the electron's "bare mass")
certainly <i>seem</i> "physically real" 
(even if not directly measurable) – and it seems insane that the coupling constant, 
defined at a <i>point</i>, should "know" about some energy cutoff imposed by the whim of an 
experimenter not located at that point –
and hence this <i>is</i> of "consequence."  
Neumaier would counter that my words "physically real" have no clear meaning (dismisses it as
"the usual nonsense") hence my objection is not serious; I could then counter-counter that
Neumaier and Delamotte did not propose any clear notion of what is "measurable" versus what is 
not.  The electron's "bare mass" is typically regarded by people like them as unmeasureable,
which seems a reasonable contention within old-style QED.  However,
in therain of bricks framework the bare mass and charge <i>will</i> be "physically real" 
and in principle directly (anyhow not tremendously indirectly) measurable,
rendering this debate moot.
</p><blockquote>
Some Neumaier quotes:
The need for renormalization is not a defect of the theory
but a legitimate way to construct QFTs that are not easily constructed 
by giving an explicit Hamiltonian...  The running coupling constant plays a similar role
as a gauge field: one needs a valid description for it, but one can change its value
by applying a transformation from the renormalization [semi]group without altering the physics, 
just as one can change the value of a gauge field by applying a gauge transformation.
<br>
The <i>only</i> thing that is missing is to give the limit a 
mathematically well-defined meaning [and prove it exists]...
my conjecture is that [the S matrix of QED] has
some (yet unknown but) rigorous nonperturbative construction.
</blockquote><p>
Further serious objections are that (1) this whole approach cannot handle the power-law infinities 
in <i>non</i>renormalizable QFTs, and hence cannot be "the ultimate truth," 
and (2) see Landau's devastating objection below.
</p><p>
There are many possible ways to do lattice gauge theories.  There is 
virtually no rigor in the area and 
it is not at all clear (at least not to me) that the different ways are equivalent.
Nor is it clear to me that the process will indeed converge to unique and reasonable answers
in the limit of zero lattice spacing.   Nevertheless, these were hoped!
This all at least offered the <i>hope</i> (somewhat supported by numerical evidence)
that QED, QCD, etc could be turned into
rigorous and well-defined physical theories capable of
predicting everything they predict, to arbitrary user-specifiable accuracy.
</p>
<a name="landaupole"></a>
<p>
<b>Lev D. Landau (1908-1968) and
Isaak Ya. Pomeranchuk (1913-1966):</b>
An argument tracing to
Landau &amp; Pomeranchuk in 1955
claims that actually, <i>there is no such hope</i> (hence Delamotte's key 
<a href="#delamotte">quote</a> is wrong):   
The Wilsonian lattice gauge theory / scaling
approach is irredeemably mathematically self-inconsistent.
The reason is QED's running coupling constant &#945; goes infinite at
<i>finite</i> (though huge) UV cutoff energy &#923;:
</p><center>
&#945;(&#923;)
&#8776;
&#945; / [1+2&#945;(3&#960;)<sup>-1</sup>ln(m<sub>e</sub>/&#923;)]
<br>
&nbsp;
goes infinite at the "Landau energy"
&nbsp;
<br>
&#923;<sub>Landau</sub> 
= m<sub>e</sub> exp(&#945;<sup>-1</sup>3&#960;/2)
&#8776; 2.9×10<sup>280</sup>m<sub>e</sub> 
&#8776; 1.2×10<sup>258</sup>M<sub>pl</sub>.
</center><p><small>
See EQ 12.89 in ch.12 of Peskin &amp; Schroeder for the first formula, which they derived
by solving the Callan-Symanzik differential equation (their EQs 12.41, 12.83;
incidentally the "Callan-Symanik" equation was published much earlier 
by Ovsiannikov in Russian in 1956)
using only the leading term in
the beta function below; they mention
Landau's idea in a sentence on the next page "the coupling constant formally goes to 
infinity at a large but finite value of the momentum; thus it is not even clear that these
theories possess a nontrivial limit &#923;&#8594;&#8734;."
</small></p><p>
Hence Wilsonian scaling-based renormalization <i>cannot</i> continue forever
to arbitrarily small length scales.   Because 
&#923;<sub>Landau</sub> 
is so enormous (greater than the mass of the observable universe!), 
this is not a practical problem for current (or forseeable) computers;
but as a matter of mathematics, it is damning, and Landau had reasons to believe 
this infinity in &#945;(&#923;) was genuine
("<a href="http://en.wikipedia.org/wiki/Landau_pole">Landau pole</a>";
aka the "zero charge").
This belief was confirmed by <i>some</i> later theoretical and numerical developments.
For example I have also been able to exactly-solve the Callan-Symanzik equation 
(Peskin &amp; Schroeder ch.12) using the <i>two</i>-term expression for
the QED "beta function" 
(P&amp;S's EQ12.61, but now with one more term) whose asymptotic series is
</p><center>
&#946;(e) 
= 
&#960;<sup>-2</sup>e<sup>3</sup>/12 + 
&#960;<sup>-4</sup>e<sup>5</sup>/64 + 
...
<!-- EQ2.10 in http://arxiv.org/pdf/hep-th/0210240v1 gives two-loop version 
S.G.Gorishny, A.L.Kataev, S.Larin: Physics Letters B 194,3 (1987) 429-432 all wrong!
S.G.Gorishny, A.L.Kataev, S.Larin, L.Surguladze: 
The analytic four-loop corrections to the QED &beta;-function in the MS scheme and the QED
&psi;-function. Total reevaluation, Physics Letters B 256,1 (1991) 81-86.
gives 4 loops, but answer is scheme dependent starting at the 3rd term.
"It is found that the zeta(3), zeta(4), and zeta(5)-terms cancel in 
the ultimate result for the four-loop coefficient of the F1-function."
S.G. Gorishny, A.L. Kataev and S.A. Larin, Phys. Lett. B273 (1991) 141-??; Erratum B275 (1992) 512.

Solving the CS equation using the two-term beta gives something that goes infinite
at energy q such that Z=infinity is a root of
 -9*ln(Z)*Z+144*ln(Z)*Pi^2+144*Pi^2+8*C1*Z-128*C1*Pi^2-9*Z*ln(3)+
 Z*ln(-q^8*(-Z+16*Pi^2)^9)+144*Pi^2*ln(3)-16*Pi^2*ln(-q^8*(-Z+16*Pi^2)^9);
This requires  q=exp(9*ln(3)/8-C1)   and  C1=9*ln(3)/8-ln(q)
(which are the same requirement).  To make the solution reasonable at q=1 we also need
C1=(9*ln(16*Pi^2+3*ee0^2)-ln(ee0^18))/8 - 6*ee0^(-2)*Pi^2;
i.e. if ee0=sqrt(4*Pi/137.04) this is C1=-637.40=-58.45/ee0^2.
So we go infinite when q=exp(9*ln(3)/8+637.40)=exp(638.64)=2.3*10^(277) electron masses.
MAPLE:
 beta := (x) -> x^3 / (Pi^2*12) + x^5 /(Pi^4*64);
 diff( ee(q), q ) * q = beta(ee(q));
 dsolve(%, ee(q));
If only the first beta term is used we get ee(q)^2 = 6*Pi^2 / (6*C1*Pi^2 - ln(q))
and goes infinite when q=exp(6*C1*Pi^2).  To be reasonable at q=1 need C1=1/ee0^2=137/(4*pi).
This is q=exp(6*137.04*Pi/4)=exp(645.79)=2.9*10^(280).
R Acharya and B P Nigam
Verification of the four-loop corrections to QED: calculation of the fifth-order vacuum polarization coefficients
1992 J. Phys. G: Nucl. Part. Phys. 18 L137

DJ Broadhurst, AL Kataev, OV Tarasov: Analytical on shell QED results: 3-loop vacuum
polarization, 4-loop beta-function, and the muon anomaly,
CERN-TH-6602/92
Physics Letters B 298 (1993) 445-452.
http://arxiv.org/abs/hep-ph/9210255

EQ25: b1=2N/3, b2=1N/2, b3=-1N/16-11N*N/72, b4=-23N/63+(95/432-13(zeta3)/18)N*N-77*N*N*N/1944
where N=#types of lepton

EQ30 gives 4-term beta expansion, signs +,+,-,-, in new scheme.

EQ32 is yet another scheme, still +,+,-,-.
-->
</center><p>
and I find the &#945; arising from its solution now instead goes infinite when 
&#923;&#8776;2.3×10<sup>277</sup>m<sub>e</sub>.
</p>
<small><p>
<b>Mathematical details:</b>
The exact solution of the Callan-Symanzik differential equation
&nbsp; <nobr>q e'(q) = A e(q)<sup>3</sup>+ B e(q)<sup>5</sup></nobr> &nbsp;
where the right hand side is the two-term beta function of e(q)
when A and B are suitable positive constants, is as follows.  Let 
<nobr>X=ln(A+Be(q)<sup>2</sup>).</nobr>
Then I claim X exactly obeys this (non-differential) equation:
</p>
<center>
  XBA
- XBexp(X)
+ 2ln(q/K)A<sup>2</sup>exp(X)
- 2ln(q/K)A<sup>3</sup>
+ ln((exp(X)-A)/B)Bexp(X)
- ln((exp(X)-A)/B)BA
+ AB 
= 0
</center>
</small><p><small>
where K is a constant of integration which needs to be chosen so that e has
the right value at low energies.   When &#945;&#8594;&#8734; we have 
e&#8594;&#8734; and 
X&#8594;&#8734; whereupon 
<nobr>ln(q/K)&#8594;Bln(B)/(2A<sup>2</sup>)</nobr>
exactly locates the Landau pole for you.
In the special case B=0 of the 1-term beta function, the solution is simpler and
may be written fully explicitly:
<nobr>e(q)=[C-2Aln(q)]<sup>-1/2</sup></nobr> where C is another constant of integration.
Here when e&#8594;&#8734;
we instead have have ln(q)&#8594;C/(2A)
again exactly locating the Landau pole.
</small></p>
<p>
Further, if more terms of the beta function were known
(and they are, albeit there is some confusion
about them since they are "scheme dependent";
see e.g. 
Broadhurst, Kataev &amp; Tarasov 1993, 
Gorishny, Kataev et al 1991, and
Kataev &amp; Larin 2012; also we'll 
<a href="#runningcouplingevidence">discuss</a> beta-functions
more <a href="#cvitconj">later</a> in the other, 
&#945;-based, rather than e-based, notation)
then we still can solve the <b>Callan-Symanzik equation</b>
<nobr>qe'(q)=&#946;(e)</nobr>
for 
<nobr>e(q)<sup>2</sup>=4&#960;&#945;</nobr>
to find 
<nobr>&#8747;de/&#946;(e)=ln(q)+C</nobr>
where C is a constant of integration
which needs to be chosen so that we match the physical value of
&#945; at low (&#8776;1m<sub>e</sub>) energies.  
We then solve e(q)=&#8734; to find the energy q
(in units of the electron mass) of the Landau pole.
This procedure <b>proves</b> that a Landau pole at finite positive energy will 
<i>always</i> exist
provided that the beta function obeys 
</p><ol><li>
positivity:
&#946;(e)&gt;0 for e&gt;0,
and
</li><li>
fast-enough growth:
&#946;(e)&gt;(<i>e</i>)(ln<i>e</i>)(lnln<i>e</i>)<sup>1.001</sup> when e&#8594;&#8734;.
</li></ol>
<p>
In particular, it proves that at <i>any</i> fixed order in QED perturbation theory
[because then &#946;(e) is a polynomial in e] either (i) such a finite-energy
Landau pole occurs or (ii) the polynomial changes sign at some positive e.
[And for as long as (i) keeps happening as you increase the order of
perturbation theory, then the energy-location of the pole must keep
decreasing.]
Either way, this definitely seems severe enough to invalidate perturbative QED,
as argued in the following Cau-Schweber <a href="#CauSchweber">quote</a>:
</p><a name="CauSchweber"></a><blockquote>
The breaking of perturbation at ultra-relativistic energy, as shown
by Landau 1955 and by
Gell-Mann and Low 1954, indicates its inconsistency. That is, <i>assuming</i> an
expansion being perturbative entails its <i>negation</i>: at some energy
region the expansion becomes nonperturbative.
<br>&nbsp;&nbsp;&nbsp; –
Cau &amp; Schweber 
1993 (their note 14).  Note that <i>any</i> fixed order of perturbational QED is
seen by Cau &amp; Schweber's argument to be self-contradictory.
</blockquote><p>
Landau and Pomeranchuk believed (i) was exactly what happens.
However, if
&#946;(e)&gt;0 for e&gt;0
and we have <i>slow</i>-enough growth:
<nobr>&#946;(<i>e</i>)=O(<i>e</i>ln<i>e</i>lnln<i>e</i>)</nobr> when e&#8594;&#8734;,
then the "Landau pole" would be at infinite energy, i.e. the effective
QED squared coupling constant &#945; would <i>remain finite</i> 
at all positive energies, albeit still tending to &#8734; as the energy&#8594;&#8734;.
If that happened, then it would be less clear whether perturbative QED should be considered 
invalidated by Cau &amp; Schweber's argument;
the issue perhaps is arguable.  
For example, if you take whatever Nth-degree polynomial formula for &#946;(e) you
get from Nth order QED, then <i>multiply</i> it by
<nobr>1-exp(-exp(-e)/e)</nobr>, then you will get a new formula for &#946;(e)
that agrees with the old one when e&#8594;0+ to an arbitrarily large
order N in perturbation theory, but when e&#8594;&#8734; now goes to 0+ instead of
to ±&#8734;!
This new &#946; formula could no longer cause a Landau pole at any finite energy
(if the old &#946; caused that) and it only could cause &#945;&#8594;&#8734; at infinite energy.
[Landau "solved" that problem by simply denying the possibility of this-slow-growing
&#946;(e).]
</p><p>
But Krasnikov 1981 says that
"perturbation theory in QCD only has a meaning when &#945;&#8804;0.45."
A similar remark about QED is that it also should become meaningless when 
<nobr>&#945;&gt;3<sup>1/2</sup>/2&#8776;0.866</nobr> 
because
then the exact solution of Dirac equation for hydrogen atom shows
an electron would "fall in" to a point nucleus with infinite energy release.
</p>
<a name="superheavy"></a>
<blockquote>
<b>The hydrogenic atom with nuclear point charge Ze</b>
is discussed by both Greiner &amp; Reinhardt
example 5.6 page 278, and Folland page 76, with the conclusion "fall in" happens
if Z&#945;&gt;3<sup>1/2</sup>/2.
This presumably means the 119th atomic 
element cannot exist since 3<sup>1/2</sup>/(2&#945;)&#8776;118.677.
<small>Actually, there are escape hatches – the fact 
that atomic nuclei are not point particles and the possibility of partial
shielding by other electrons
might permit some elements&#8805;119 to exist.  
This question could presumably be answered by much more complicated theoretical considerations,
but my suspicion based on quick estimates is these escape hatches are insufficient
and any such atom should be expected to absorb one of its electrons in well under a microsecond.
(I therefore find it surprising the literature on superheavy elements seems not to mention the
Z&#8804;118 presumed upper bound, with instead, e.g,
the Dubna group arguing for the existence of "a second island of nuclear stability"
at the absurd value Z=164!)</small>
<!-- Uranium nuclear radius = 7.4 femtometers.
Hydrogen Bohr radius = 52918 femtometers.   
Bohr scales as 1/Z so BohrRad/118 = 448.5 fm.
-->
As far as the <i>mathematical consistency</i> of QED
is concerned, Z&#945;&gt;0.867 remains bad news;  readers who demand
<i>point</i> particles with no shielding could consider
an "atom" consisting of an electron orbiting an anti-muon.
The historical progress and non-progress as of 2013 in creating large-Z elements
supports this theory
(but be aware of Johnson 2002):
<table bgcolor="pink">
<caption><b>Table 2:</b> High-Z trans-uranic elements</caption>
<tbody><tr bgcolor="aqua">
<th>Z</th><th>IUPAC name</th><th>#atoms made so far</th><th>approximate 
halflives of known isotopes</th></tr>
<!--
105 Dubnium       34sec-29hours
106 Seaborgium    2min
107 Bohrium       millsec to hours
108 Hassium       10sec  over 100atoms made
109 Meitnerium    8sec
110 Darmstadtium  11sec
111 Roentgenium   26sec
112 Copernicium   29sec  about 75atoms detected
113 noname        millisec to 20sec  (created 2003?) IUPAC not yet accepted but probably will
http://en.wikipedia.org/wiki/Island_of_stability  has a nice table
Note for elements 108-118 except 115, the longest-lived isotope known
is always the heaviest or 2nd heaviest known, suggesting if we could put in more neutrons we'd
get more stable.
GEORGE JOHNSON:
At Lawrence Berkeley, Physicists Say a Colleague Took Them for a Ride,
New York Times 15 October 2002; about Victor Ninov fabricating several
element discoveries.
http://arxiv.org/abs/1207.5700   reviews, hopes 119 or 120 can be made before July 2014
-->
<tr><td>112</td><td>Copernicum, discovered 1996, IUPAC recognition in 2009</td>
<td>&#8776;75</td><td>&lt;1 millisecond to 420 seconds; but
undiscovered more-neutrons isotopes 
(Cn-291 and Cn-293) might last much longer 
</td></tr>
<tr><td>113</td><td>
Apparent discovery 2003-2005 as decay product of 115 &amp;
directly from Bi-83+Zn-30; but IUPAC 
found the evidence for discovery insufficient</td><td>6?</td><td>200&#956;sec-20 sec</td></tr>
<tr><td>114</td><td>Flerovium;
discovered 1998 from Ca-48+Pu-242, Ca-48+Pu-244, and Ca-48+Cf-249</td>
<td>&#8776;80</td>
<td>0.1 to 70 seconds; but
undiscovered more-neutrons isotopes 
might last much longer (months?);
supposedly Fl-298 would have "doubly magic" stability
but so far only Fl-285 to Fl-289 have been made.</td></tr>
<!-- Z=114 neut=184 total=298 is doubly magic; 285-289 is what has been made -->
<tr><td>115</td><td>Apparent discovery 2004-2005 from Ca-48+Am-243; but IUPAC 
finds the evidence for discovery insufficient</td><td>&#8776;50</td><td>16-173 millisec</td></tr>
<tr><td>116</td><td>Livermorium; discovered 2000 &amp; 2005 from Ca-48+Cm-248</td>
<td>&#8776;35</td><td>7-61 millisec</td></tr>
<tr><td>117</td><td>Apparent discovery 2009-2010 from Ca-48+Bk-249 not 
accepted by IUPAC</td><td>11</td><td>14-400 millisec</td></tr>
<tr><td>118</td><td>Apparent discovery 2002-2006 from Ca-48+Cf-249
not accepted by IUPAC</td><td>3</td><td>1 millisec</td></tr>
<tr><td>&#8805;119</td><td>No credible discovery claims as of year 2013.
119-synthesis attempts in 1985 and 2012, and 
several 120-attempts in 2007-2011,
all failed.</td><td>0</td><td>?</td></tr>
</tbody></table>
</blockquote><p>
Hence QED should self-invalidate even if &#945;&#8594;&#8734; 
happens <i>only</i> at unboundedly large energies.
</p><p>
Peskin &amp; Schroeder figure 12.4 gave a pictorial argument indicating
that &#945;&#8594;&#8734;
could be avoided (assuming a smooth 
beta function) <i>only</i>
if the beta function <i>changed sign</i> at some large positive energy; but supposedly
Källen's methods show that the QED beta function <i>must always be positive</i>
[<a href="http://en.wikipedia.org/wiki/Kallen-Lehmann_spectral_representation">Källen-Lehmann</a>
spectral representation; see Krasnikov 1980 &amp; 1981 
and pp.178-180 of Banks 2008].
</p><p>
Göckeler et al 1980
examined the question numerically using lattice QED methods and concluded a Landau pole exists
and thus
"spinor QED does not exist as an interacting theory,
similar to what Coleman &amp; Weinberg 1973 found for scalar QED";
the same conclusion was reached by Gies &amp; Jaeckel 2004 finding
&#923;<sub>Landau</sub>=10<sup>278±8</sup>GeV,
albeit these numerical studies speculated that enhancing QED by adding certain kinds
of new particles and/or chiral symmetry breaking plausibly could save it from Landau poles.
(I presume they were unaware that this
speculation seems to contradict the theorem of Coleman &amp; Gross 1973 
we shall 
<a href="#colemangross">soon</a> discuss.)
<!-- see also
S. Kim, John B. Kogut, Maria-Paola Lombardo :
Gauged Nambu-Jona-Lasinio studies of the triviality of quantum electrodynamics
Phys. Rev. D 65, 054015 (2002) 
http://arxiv.org/abs/hep-lat/0112009
-->
See also theorem 3.1 of
van Baalen, Kreimer, Uminsky, Yeats 2008 where it is argued nonperturbatively
that every global solution
of the Callan-Symanzik differential equation <i>except perhaps one</i> leads to a Landau pole
<i>under the assumption</i> that a certain function P(x) is twice-differentiable,
positive, and increasing for x&gt;0.
</p><p><small>
Van Baalen et al
actually optimistically speculate in their §1.3 that this one exception does exist and
is physical reality, but that seems to require a probability=0 "fine tuning"
scenario, and further the fact that
tons of non-QED but genuine physics (muons, electroweak, QCD, quantum gravity)
invalidate QED at energies far below the Landau scale would
seem to nullify any call to physics to help out with the mathematics
by somehow accomplishing that fine tuning.  I.e. pure QED, <i>but</i>
using the physical value &#945;&#8776;1/137.04 at &#923;&#8776;1m<sub>e</sub>
got from whatever the true laws
of physics (which go beyond QED) are, should have a Landau pole, even if those true laws 
somehow magically adjust &#945; to make <i>themselves</i> self-consistent.
</small></p><p>
Landau poles <i>also</i> suggest that there is no hope that any magic 
series-summing method ever will be able to save QED by somehow converting  the divergent
asymptotic power series in &#945; that it outputs, into unique finite physically-valid
answers.  But their main importance is showing that the whole
perturbative approach to QED is wrong, since  self-contradictory, just
as Cau &amp; Schweber pointed out above.
</p><p><small>
Glimm &amp; Jaffe 1981 (in their introduction) allude to this by noting 
the "speculation... that [QED may be logically] inconsistent" 
but might regain consistency if other quantum fields, e.g. quarks, are added to it.
Jaffe in 2007 went further, stating "Most physicists believe that QED on its
own is mathematically inconsistent."
</small></p><p>
However, another developmentfought back
against this tide of negativity – giving actual hope
for a reason ignored/unknown to Jaffe in that quote, that QED could be self-consistent,
or anyhow nearer to it.
This hope arises from Suslov 2008, who
(nonrigorously) claimed he understood the asymptotics of the QED beta function,
specifically he found &#946;(e) is asymptotically proportional to e
when e&#8594;&#8734;.
Furthermore,  Krasnikov 1980 &amp; 1981 claims actually to have a "rigorous proof"
based on the 
<a href="http://en.wikipedia.org/wiki/Kallen%C3%A2%E2%82%AC%E2%80%9CLehmann_spectral_representation">Källen-Lehmann</a>
spectral representation, that 
<nobr>
0&#8804;&#946;(e)/e&lt;C
</nobr>
for some positive constant C.
<!--
Suslov notation differs from ours=Peskin+Schroder.
Suslov claims in his notation beta=g asymptotically at large g.
At small g Suslov has beta=b2*g^2+b3*g^3+... where g=e^2.
So evidently 
beta[Peskin]=beta[Suslov]/e.
Hence Suslov is claiming beta[Peskin]=g/e=e asymptotically at large g,e.
--
<sup>&kappa;</sup>
when y&rarr;&infin; for some constant &kappa; with 0.9&lt;&kappa;&lt;1.1 &ndash;
which again means it stays positive.  Indeed he deduces &kappa;=1 exactly and 
-->
Either Suslov's result or Krasnikov's weaker claim
would imply that QED's squared 
coupling constant &#945; goes infinite <i>exactly</i> at zero length L, i.e. infinite
energy q, and according to Suslov's asymptotic it
would do so ultimately proportionally to 
<nobr>L<sup>-2</sup></nobr> i.e. to 
<nobr>q<sup>2</sup>.</nobr>
(This of course totally contradicts experiment which finds &#945;<sub>eff</sub>
agrees with low-order perturbative QED's predictions at
all experimentally accessible energies, but Suslov would claim that is no problem
since his effects only kick in a far greater energies.  More seriously, 
Suslov's asymptotic also would seem to contradict
the main claim of Johnson, Baker, and Willey 1967, at least at each perturbative order,
a problem Suslov "solves" by ignoring
and not citing JBW; but I think the correct solution is that JBW was wrong.)  
In that case we'd still be left with the problem that at sufficiently 
high finite &#945;, such as &#945;&gt;0.867, QED seems to break down, e.g. collapse of 
hydrogenic atoms into
points, with infinite energy release.
</p><p>
In that case perturbative 
pure-QED <i>still</i> seems destroyed or in deep trouble.
But perhaps there still may be a tiny chance 
Wilson's nonperturbative approach might be able to succeed, i.e. it might
be capable of one day rigorizing QED?
</p><p><small>
And indeed Jonathan Dimock 2002-2012 
has for the last 10 years
been attempting to do exactly that using techniques of T.Balaban to try to rigorize Wilson's
ideas, at least for (2+1)-dimensional QED. However, Dimock's manuscripts exhibit
no sign of awareness of either Krasnikov or Suslov, nor do I see any such sign of
awareness anywhere in the <a href="#cqft">constructive QFT</a> community.
</small></p><p>
But I think the answer to that must be "no" because, if Wilson's view were capable of thus
saving things, then Wilsonian QED clearly would be predicting very different behavior than
older QED formulations (e.g. about hydrogenic-atom collapse versus not) which in turn would
invalidate the whole speculation by the Wilsonians that their kind of lattice gauge approaches
have some sort of true and rigorous meaning...
</p><a name="landauclarify"></a><p>
<b>My conclusion about Landau poles:</b>
The Landau pole story has been a long and winding comedy of errors, 
and the true story is more subtle than any previous investigators realized.
But it appears we now have reached a conclusion.
If Landau was correct that the effective &#945; goes infinite at some positive
charge-separation, i.e. finite energy scale, then both perturbative QED and the
Wilsonian lattice-renormalization-flow idea, both seem irredeemably destroyed.
Historically, everybody who came into contact with it
seems to have been hugely impressed by the Landau pole argument.
If, however, Suslov and/or Krasnikov are correct, then Landau was partly wrong:
&#945; only goes infinite when we approach zero
charge distance-separation, i.e. infinite energy. 
Because of the known positivity of the beta function, 
one of these two alternatives seems certain. I checked his argument,
and believe in Krasnikov's upper bound on the beta
function – provided we accept his underlying assumptions –
hence the latter case is correct.  
Hence the "Landau pole" essentially does not really exist, i.e. only occurs
at infinite energy.
However, this still destroys QED because QED seems clearly to self-invalidate
once we merely reach sufficiently large <i>finite</i> &#945;.
</p><p>
I hope I've (a) shown that the history of the "Landau pole" idea has been extremely confused
but (b) the situation is now much clarified, and (c) with the conclusion still remaining
that pure QED still "kills itself" via an inherent self-contradiction, albeit this
self-immolation is less spectacular than Landau originally thought.
<!--This is perhaps because Krasnikov and Suslov were Russian and not publishing
in CMP?-->
</p><p>
<b>Gauge theorists (I'm not sure who first recognized this and when, but I'd guess it
was during the 1970s):</b>
characterized QED as the <i>unique</i> quantum field theory (apart
from trivial alterations such as changing the value of the electron mass and charge,
or adding more lepton flavors)
which
</p><ol>
<li>
lives in 1+3 dimensional Minkowski spacetime 
</li><li>
is Lorentz/Poincare invariant
</li><li>
is described by a real-valued lagrangian density
</li><li>
is renormalizable
</li><li>
is invariant under a U(1) gauge symmetry
</li><li>
is invariant under either time-reversal or parity (spatial reflection)
</li></ol>
See Peskin &amp; Schroeder §15.1 for a vague statement and proof sketch; another
is in §2.1.1 of Muta.
I am not aware of
a fully-clear theorem statement with genuine proof.
Originally, physicists after recognizing the gauge-symmetry of Maxwell's equations regarded
it as unimportant and annoying.   
However, there are an enormous set of gauge symmetries,
dwarfing the number of (e.g.) Lorentz/Poincare symmetries.  Thus, demanding 
a QFT be gauge-invariant is an enormous restriction, often enough to uniquely determine
the QFT.   Further, in the case of QED, U(1) gauge invariance of the lagrangian, 
via Noether's theorem
(Peskin &amp; Schroeder §2.2 &amp; 9.6), implies charge (or more precisely
lepton number) conservation.  Although 
I am unaware of any formal statement of a converse to Noether's theorem, i.e. of a claim
that <i>every</i> conserved quantity in QFTs must arise from a continuous symmetry of the 
lagrangian, I am unaware of any exception and
it seems difficult to see how one could come about in any other way.   E.g,
Feynman's "all histories" view indicates that in QED the quantum fields 
evolve in all possible ways,
including ways which violate lepton number conservation, <i>but</i>
thanks to the U(1) gauge symmetry every such evolution arises equiprobably for every 
complex phase angle &#952; with 0&#8804;&#952;&#8804;2&#960;, so that the summed probability
amplitude for a lepton-number-altering history is always exactly 0.
It is hard to see how, in the absence of such a symmetry, such a cancellation would always 
magically occur.  So it could plausibly be conjectured that 
gauge symmetry is <i>logically forced</i> by charge conservation and hence could be 
replaced by/dropped from our axiom list above.   In summary, gauge symmetry is
both extremely powerful and extremely natural and plausibly logically forced.
<p></p><a name="colemangross"></a><p>
The "Bjorken scaling" experimental behavior of high-energy electron scattering off hadrons
was shown (Callan &amp; Gross 1973) to only be possible in QFTs featuring
"UV asymptotic freedom."
But the theorem of Coleman &amp; Gross 1973
states that <i>no</i> QFT containing
any Abelian-gauged vector-boson, or 
consisting only of fermions and scalar bosons without any gauge fields,
can exhibit UV asymptotic freedom
in Minkowski 1+3-space. 
However, every 1+3D rQFT invariant under a
<i>nonAbelian</i> gauge group
(any such group necessarily involves at least 3 parameters)
is known to have
UV asymptotic freedom (provided not too many fermion flavors are included, and 
assuming the gauge group is semisimple, see Peskin &amp; Schroeder §16.5-16.7 or
Gross &amp; Wilczek 1973-4).
Thus we have the following implications
for a quantum field theory based on a real-valued lagrangian density in
1+3D spacetime:
</p><center>
Experiment 
&#8658; 
Bjorken scaling
&#8658; 
UV asymptotic freedom
&#8658; 
<br>
both nonAbelian Gauge symmetry (with <i>no</i> Abelian-gauged fields) and renormalizability.
</center><p>
The latter are exceedingly strong restrictions.   In particular, the simplest 
nonAbelian gauge group compatible
(since it is 8-dimensional) with the existence of 8 symmetrically-equivalent kinds of gluon
is SU(3), and that essentially uniquely yields QCD.
</p><!--
Muta &sect;2.1.2 derives qcd lagrangian from non-abelian local gauge invar, renorm;
then 6 kinds of quarks and 8 kinds of gluons forces QCD or perhaps some other gauge group possible.
[D.J.Gross &amp; F.Wilczek:  
<A href="http://www.aps.org/about/pressreleases/upload/Asymptotically_Free_Gauge_Theories_II.pdf">
Asymptotically Free Gauge Theories II</a>, Phys. Rev. D 9,4 (1974) 980-993??
Wilczek in his <a href="http://arxiv.org/abs/hep-ph/0502113">Nobel lecture</a>
and also in his Physics Today piece claims this paper proves 
QCD is the "essentially unique realization of asymptotic freedom in QFT" 
but actually I see no such proof in the paper... see Yndurain, Muta, Zee???
Wilczek also claims  http://arxiv.org/pdf/hep-ph/9907340
F.Wilczek: What QCD Tells Us About Nature - and Why We Should Listen
Nuclear Physics A, Volume 663 (2000) 3c-20c.
that AF can only be achieved in QFTs that have "simple matter content, 
nonAbelian gauge symmetry, and in which all interactions are renormalizable."
But again this claim simply is not proven in the paper he cites
D.J.Gross and F.Wilczek: Ultraviolet Behavior of non-Abelian Gauge Theories, 
Phys. Rev. Lett. 30,26 (1973) 1343-1346.
Sidney Coleman, David J.Gross: Price of asymptotic freedom, Phys.Rev.Letters 31,13 (1973) 851-854;
A.Zee: Study of the renormalization group for small coupling constants, 
Phys.Rev.D 7,12 (1973) 3630-3636;
--Zee: QED is not asymptly free nor are scalar+fermion QFTs involing 1 coupling constant--
Lee+Zinn-Justin PRD 5 (1972) 3121; 5 (1972) 3137;  5 (1972) 3155
[SU(2) was already taken for electroweak purposes and does not suffice to
explain QCD, e.g. there are 8 kinds of gluons and SU(3) is the only semisimple
Lie algebra of dimension 8 arising from a compact Lie group.]
--Coleman & Gross:
"No renormalizable field theory without non-Abelian gauge fields can be asymptotically free."
P+S p543 elaborate: "...in 4D spacetime."   Gross in "25 years" elaborates:
QFTs with arbitrary Yukawa, scalar, or Abelian gauge interactions only, cannot be AF.
SU(2) nonabelian is the simplest but would 
predict the u and d quarks had equal mass or is somehow excluded by experiment while QCD
is not.
-- 
Wilczek+Gross Asympt free QFTs I suggest QCD is probably unique provided we take above
list plus Bjorken scaling. Asymptly free QFTs II starts by saying Bjorken scaling implies
asymptotic freedom [Callan+Gross PRD8 (1973) 4383], 
only nonabelian GTs can be asymptly free [Coleman+Gross], and many nonAbel GTs are
asymptly free [themselves, Politzer, and tHooft unpub].
-->
<p>
I'm not sure whether any of these
<b>uniqueness theorems</b> have been fully formally stated and proven.
It would be better if they were so that we could
know exactly what we are talking about.   But anyhow, to the extent they are valid,
they make it seem rather difficult to mend QED and QCD's problems – 
<b>there is no way to repair a unique theory!!</b>
</p><p>
<a href="#rainofbricks">Rain of bricks</a>
will aim to repair QED and QCD by discarding assumption (1), taking the 
attitude that the other 5 members of our list above are too heavily supported by experiment
(or too damaging) to discard.
</p>
<blockquote>
Each "effective theory" of physics breaks down as one goes to shorter
and shorter distance scales.  Each effective theory inputs parameters
(such as masses and charges) from effective theories at shorter
distances.  That input process is [part of]
the renormalization procedure.... As we go to shorter distances or
higher energies we <i>expect</i> each effective theory to need
renormalization; that is not a problem or unexpected failure of that
theory.  However, the <i>final</i> theory had better not need such
inputs, or renormalization, and should never output infinities.
<br>&nbsp;&nbsp;&nbsp;–
Gordon L. Kane. [From p.51-52 of his popular book <i>Supersymmetry</i>
Perseus 2000.] Kane also was a coauthor of the important technical
book <a href="http://arxiv.org/abs/hep-ph/9302272">The Higgs hunter's guide</a>
(Addison-Wesley 1990 plus later editions).  I agree with this quote except for the
implication the final theory need not have any input-parameters; it could.
</blockquote>
<p>
<b>My own views:</b>
I agree with the above.
"Renormalization" may be beautiful to some eyes but cannot be the fundamental physical truth.
To make real progress it must be abolished (which the present paper shall do, and
the quite different superstring theory also does).
I now also want to point out the quantum "measurement problem" and
to contribute a few more ways to look at QFTs, including 
from computer science and philosophy of science, beyond just physics and mathematics.
QED also has several fundamental problems from those points of view.
</p><p>
First of all, QED – despite its successful calculation of various experimental
quantities (Lamb shifts, magnetic moment of electron and muon, 
hydrogenic and helium spectra, scattering)
to impressive numbers of decimal places – see <a href="#exptlqed">§8</a> –
is <b>not a falsifiable theory</b>
and hence "not science" (susceptible to the scientific method) at all.
Suppose, for example, we measure the lifetime of positronium and find it disagrees with 
QED's prediction with over 10&#963; worth of confidence.  
This in fact happened for
ortho-positronium during the late 1980s.
Does this falsify QED?  Well, no.  At the time, the disagreement was ascribed
to the presumed presence of an amazingly unexpectedly
large term at the next (as yet uncalculated)
order in the QED series.   
Higher-order QED correction terms were then computed (and found <i>not</i> to be
unexpectedly large and
the previous theoretical calculations were merely reconfirmed).
Did <i>that</i> falsify QED? No.
The crucial experiment was
re-analysed uncovering new sources of error, and new experiments were built and performed,
then criticized, re-built, and re-re-done.  (See <a href="#exptlqed">§8</a> for discussion
of the fate of this and other "experimental refutations" of QED.)
In the end, the reassuring result was that the puzzle (20 years later)
finally appears to have been resolved in QED's favor.
</p><p>
While in some sense these experiment/theory discrepancies all ultimately were success stories
for QED, as a matter of Popperian philosophy, those good endings were irrelevant.   
The key point to notice here is that <i>no matter what disagreement with experiment
occurred and no matter how many terms in the QED series had been calculated</i> 
it would <i>never</i> be possible to conclude with confidence that
represented a falsification of QED.
QED advocates could <i>always</i> claim that the disagreement was due to unexpectedly large
terms at higher (as yet uncalculated) order in the QED series!   And indeed, due
to Dyson 1952's convincing argument these series diverge, terms large enough to compensate for any
disagreement undoubtably exist!
</p><p>
Some people may not find that terribly convincing because after some number
of decimals of correct predictions they – pragmatically –
lose their principles and just declare QED a success.
I suggest to those people that they:
</p><a name="dnaexample"></a>
<ol type="A">
<li>
Consider QCD rather than QED.
The difference is only a matter of degree, but QCD only manages to get 1 decimal.
</li><li>
The Vaccinia virus 
is an important system which would be practically useful to model.
The VACV-Duke strain (Li et al 1988)
has a double-stranded DNA molecule containing
199960 base pairs, 66.6% of them "AT," for a
molecular weight of about 1.24×10<sup>8</sup> daltons.
It therefore contains about
6.4×10<sup>7</sup> electrons and about 
1.2×10<sup>7</sup> atomic nuclei of 5 different kinds (C,H,O,N,P).
This system could be treated using QED 
(with the nuclei regarded as pseudo-positrons
with much larger masses than an ordinary positron and various charges).
The number of "one loop" Feynman diagrams with 7.6×10<sup>7</sup> vertices
exceeds the number of "tree" diagrams by a factor of order &#8805;10<sup>10</sup>.
Because 10<sup>10</sup>&gt;&gt;137, indeed 
10<sup>10</sup>&#8776;137<sup>5</sup>,
we conclude that the QED series
"starts to diverge immediately."
It is commonly claimed that perturbative
QED series "converge for about the first 137 powers of
&#945; before they
start diverging" or some such.  <b>This claim is massively false.</b> 
More precisely, such behavior only occurs for very small and simple systems such
as computing the magnetic moment of a <i>single</i> electron or computing
scattering cross sections for <i>two</i> electrons.
<b>QED series for <i>large,</i>
i.e. realistic, systems begin diverging after the very first term.</b>
Perturbative QED is therefore utterly useless for most practical purposes,
giving no advantage whatever versus the simplest old-style quantum mechanics (pre-QED)
theory.
</li></ol>
<p>
<b>I'm tired</b>
of physicists claiming that QED is a "great success," and "the most accurate physical
theory ever" and all complaints about it are mere piddling irrelevant-to-practice
whining by mathematicians sadly steeped in "rigor mortis," while meanwhile heroic
physicists who don't worry about
such silliness can proceed to calculate whatever they want to as many digits
accuracy as any practical person could ever want.
The <b>reality</b> is Feynman-Dyson style QED
is <i>utterly useless</i> for almost every practical purpose, both in practice, and in principle,
and irretrievably so, and the QED correction to the naive pre-QED calculation
not only is <i>usually</i> not feasible to determine accurately with
current computers, but also,
attaining even the most pathetic <i>guaranteed</i> error level for it that any practical
person could ever want, e.g. ±999% error level,
is unreachable even with <i>infinite</i> computational power.
And no physicist, no matter how
heroic, has ever done any such calculation for any large system, indeed
not even for a 20-particle system.
</p><p>
From the standpoint of a computer scientist, what is missing is "algorithmicity."
There currently is <b>no algorithm</b>
known (nor even known nonconstructively to <i>exist</i>)
which inputs all relevant fundamental constants of physics and
an arbitrary "error tolerance" &#949;&gt;0
and outputs "the QED prediction of (any of the above-mentioned 
experimental quantities) <i>guaranteed</i> to
be accurate to within ±&#949;."
Indeed, no such algorithm is known for <i>any</i> value of &#949;,
no matter how generously large.  (QED via lattice gauge theory is not, and thanks to the 
<a href="#landaupole">Landau pole</a> and related pathologies
plausibly will never be, such an algorithm.)
<i>If</i> such an algorithm existed and were known,
<i>then</i> QED would be experimentally falsifiable.
</p><p>
It is the goal of this paper to propose a new physical 
theory replacing QED, which if it is correct,
and if various mathematical results about it are true despite
only being incompletely proven here, <i>really is algorithmic</i> –
i.e, it really would be possible to compute any (perhaps probabilistic) physical 
prediction accurate to as many decimals
as desired, given enough computational power and accurate-enough physical constants.
However
</p><ol type="a">
<li>
The computation usually still would be huge, and 
</li><li>
I have no idea whether this is the true theory of physics –
but at least if it is not, then it is falsifiable
by a combination of enough computing and experiments.
</li></ol>
<p>
<b>Artificial physical model intended to dramatize algorithmicity issues:</b>
Construct the following 
laws of "physics."   
Let the "universe" be the D-dimensional hypercube
[0,1)<sup>D</sup> with periodic boundary conditions.
(The value of D will not matter.  The simplest choice is D=1.)
Within this box are any of 9 different kinds of scalar bosons
(continuous real-valued fields).
Now introduce the following interaction potential-energy density:
</p><center>
P(A<sup>2</sup>,B<sup>2</sup>,C<sup>2</sup>,...,I<sup>2</sup>)<sup>2</sup>
 + |&#8711;A|<sup>2</sup>
 + |&#8711;B|<sup>2</sup>
 + |&#8711;C|<sup>2</sup> + ...  + |&#8711;I|<sup>2</sup>
</center><p>
for some multivariate polynomial P(A,B,C,...,I) with integer coefficients, whose
9 arguments are the 9 kinds of boson fields, each squared.
</p><p>
<b>Undecidability Theorem:</b>
In the above "laws of physics"  
there exists no algorithm for determining the least-energy
vacuum state, or for determining whether energy&#8804;0 is achievable.
I.e. this is Turing-<i>undecidable</i>.
</p><p>
<b>Proof:</b>
Energy=0 is achievable only if every squared-field is constant and (hence) integer-valued and the
value of P at those integer arguments is 0.   Otherwise energy&gt;0.
The result now follows immediately from the undecidability of solution-existence for
polynomial diophantine equations in 9 variables
(Matiyasevich 1993).  
<b>Q.E.D.</b>
</p><p>
<b>Randomized-PSPACE Theorem:</b>
Nth order QED ("QED<sub>N</sub>"), i.e. the prediction of QED using only terms of order 
&#945;<sup>k</sup> with k&#8804;N in its output series, is a well-defined and
<i>algorithmic</i> theory of the physics of electrons, positrons, and photons, for any
given integer N&#8805;0.  
Indeed, the algorithm to compute the prediction accurate to within ±&#948;
with confidence&gt;1-&#954;
(where &#948; and &#954; are user-specified rational-number
parameters lying strictly between 0 and 1/3)
is in PSPACE redefined to apply to a Turing machine
equipped with a <i>random bit generator</i>.
<!--  and allowing output to be incorrect with 
probability&lt;1/3. -->
</p><p>
<b>Proof sketch (using a Turing machine equipped with a random-bit generator):</b>
Using standard renormalization and regularization schemes, QED<sub>N</sub>'s predictions 
are expressible as the value of certain (&#8804;4N)-dimensional integrals
with explicitly known rational integrands.  The domain of integration is a large ball,
say of radius R, and there are certain cutoff-inducing parameters (which I will collectively call 
&#949;) inside the integrand.   The limit as R&#8594;&#8734; and &#949;&#8594;0+
(say with &#949;R=1) needs
to be taken.   It is known that for any given
&#949;&gt;0 and finite R the integral always has 
finite value
and the integrand is everywhere bounded.
(In the limit the former remains true but the latter does not.)
It is easy to produce an explicit a priori upper bound on the |integrand|
as a function of &#949;.   This in turn gives us an upper bound on the variance in
Monte Carlo integration, which allows us, algorithmically, 
to compute the value of the integral (for any given finite R and &#949;&gt;0 and tolerance 
&#948;&gt;0)
with arbitrarily-high user-specifiable
confidence that the true value lies within ±&#948;.
Further, it is easy to 
produce an explicit a priori upper bound on the truncation error arising from
the finiteness of R, and on the error arising from the fact that &#949;&gt;0.
This allows us to keep doubling R and halving &#949; until these errors both
are guaranteed to be below &#948;/3, then running Monte-Carlo long enough to force
the statistical error to also (with arbitrarily-high user-specifiable confidence) 
be below &#948;/3.
<b>Q.E.D.</b>
</p><p>
<b>Improvement:</b> I claim that
it is not hard to improve this result from PSPACE to P<sup>#P</sup>.
(I.e. I claim I can prove the latter.)
</p><p>
<i>But</i> it is clear both experimentally for small N
and theoretically for arbitrary finite N
that <i>none</i> of the QED<sub>N</sub> are the correct theory of physics;
and I repeat that no algorithm is currently known, analogous to the above theorem,
to exist to compute QED<sub>&#8734;</sub> predictions, nor is it even clear what
QED<sub>&#8734;</sub> is, nor that it really exists at all.
</p><p>
Of course I consider the fact that, e.g,
QED<sub>5</sub> with a few tiny non-QED corrections
predicted the electron magnetic moment
&#956;<sub>e</sub> to almost 13 significant figures, 
to be very strong evidence that QED is highly related to some as-yet-unknown
correct and falsifiable and algorithmic
scientific theory – it is just that QED is not it.
<!-- Gunnar Kallen 1953:
there is little doubt that the mathematical framework of quantum electrodynamics contains 
something which corresponds closely to physical reality.
-->
This same attack could be made more solidly against
many other physical theories, the vast majority of
which are not presently known to be algorithmic.   
In particular, quantum chromodynamics (QCD)
is highly related to QED
but its divergent series appear to "diverge immediately" in contrast to QED where they
"appear to converge for a long way before they start diverging" in simple scenarios.   
QCD is clearly both a far less-useful and less-falsifiable theory than QED, but that
is a mere matter of degree.  There is mildly-strong evidence for QCD.
Going further, it appears utterly infeasible at
present (and as far as the eye can see into the future) to get
"superstring theory" to make anything resembling a prediction of any feasible-to-measure
physical quantity.
</p><p>
From the standpoint of a mathematician, if you don't have a clearly-defined algorithm 
for producing physical predictions – or at least <i>bounds</i> on them –
you don't have anything.  
</p><p>
From the standpoint of a physicist like Dirac,
there is clearly something crazy about the <b>"renormalization"</b> tricks
for systematically replacing naturally-arising
expressions which yield infinities, with expressions
that yield finite and physically reasonable values.
Feynman's view to me seems obviously right: these tricks, however ingenious, cannot be
"the true theory of physics" and merely are a patchwork fix to our present
wrong theory of physics to prevent it from utterly embarrassing itself.
(Also, we know graviton QFTs are nonrenormalizable but yet must have some truth to them.)
To make an analogy, suppose I had a theory of celestial mechanics which seemed logical,
but predicted that the planet Neptune would
fly out of the solar system reaching infinite speed within the next hour.
<i>But</i> then I announced that whenever my theory predicted such an infinity, the 
infinite value should be manually erased and replaced by the experimentally-observed
velocity of Neptune.  This would be my new "renormalized" theory.  That would be 
unsatisfying and people would react by conjecturing that I probably had failed to find the
true laws of celestial mechanics.  If I responded that at the minor cost of artificially
manually inputting Neptune's correct velocity my theory seemed able to predict numerous 
other things (such as Earth's moon) to high accuracy, then
perhaps I'd get more respect, but
still, it seems safe to say this celestial mechanics theory would continue to cause
a lot of unhappiness.
So for the purpose of providing an underlying
understanding of what is going on, the QED<sub>N</sub> are failures despite going a considerable
distance in that direction.
</p><p>
In QED, and in "renormalizable" QFTs generally (at least, all
those involved in the "standard model" of nongravitational physics) the great
accomplishment of Feynman, Dyson, Weinberg, Salam, t'Hooft, Veltman, etc
was to show the following:
</p><ol><li>
For any integer N&#8805;0 and any of a large class of experimentally-measurable quantities Q
(energies, 
differential scattering cross sections, lifetimes...) one might wish to predict, there
exists an algorithm involving (a) generating graphical "Feynman diagrams" 
[Sasaki 1976, Nogueira 1993]
and
(b) converting them via 
"<a href="http://bolvan.ph.utexas.edu/%7Evadim/classes/2009s.homeworks/QED.pdf">Feynman
rules</a>"
[e.g. Greiner &amp; Reinhardt §4.1, Folland pages 165 &amp; 225]
to write down a finite-length expression
constituting QED<sub>N</sub>'s prediction of Q.
This expression involves certain finite-dimensional integrals.
Some of these integrals will, in general, have infinite values.
For a wide subclass of such calculations (such as for predicting the magnetic moment of
the electron), the expression for Q takes the form of
a power series in powers of the fine-structure constant &#945;&#8776;1/137.035999,
truncated to involve powers&#8804;N only, and QED<sub>N</sub> can be regarded simply as
outputting expressions for each of the first N coefficients in that power series.
But for some other quantities (such as positronium decay rate) 
powers of log(&#945;) also are involved.
</li><li>
If certain (specified and algorithmic) rules are followed about representing those
expressions in standard forms, then 
there will be only a <i>finite</i> number of types
of infinity-yielding subexpressions inside Q.
(In QED, each type actually arises in an denumerably-infinite number of different ways,
but inside QED<sub>N</sub> for any finite N&#8805;0, each type only
arises in only a finite number of ways.)
</li><li>
They provided a <i>finitely describable and algorithmic</i> set of
erase-and-replace rules for these subexpressions
to replace them by certain inequivalent but specified expressions.
Also the "integrals" are replaced by "certain limiting processes applied to integrals."
</li><li>
They showed that these replacements
would always cause everything to become finite.
In particular, all the coefficients
in the power series output by the
QED<sub>N</sub> 
become finite.
(The series arising when N&#8594;&#8734; almost certainly still
diverges, but its individual terms are now finite.)
</li><li>
Furthermore, it was shown that
all the original infinite integrals had
only "logarithmic," i.e. fairly mild, singularities, analogous to 
&#8734;=&#8747;<sub>1&lt;z&lt;&#8734;</sub>dz/z.
<i>Powers</i> of logarithms also arise.
</li><li>
The replacement rules all can be designed in such a way that QED<sub>N</sub>
remains "Lorentz invariant,"  "gauge invariant," and "unitary"; 
and also so that several of the most obvious
quantities which would have been infinite, now come out finite and with their
exact experimental values (e.g. electron mass).
</li></ol>
<p><small>
<b>Important historical note correcting QED myths:</b>
It is often wrongly claimed in the physics literature that Dyson in 1949
put renormalized QED into its final form by 
(a) designing a renormalization scheme and proving it
works to finitize QED<sub>N</sub> for arbitrary N, and 
(b) showing
Feynman, Schwinger, and Tomonaga's QED approaches all could be regarded 
as equivalent.  
The truth is that (as Dyson admitted in his paper), there were still some
lacunae.
The first major problem was Dyson's inability to
disentangle "overlapping divergences."  
Highly relatedly, the program MINCER for generating QED<sub>N</sub> Feynman diagrams,
and their corresponding integral expressions in renormalized form, only works for N&#8804;3
(Gorishny, Larin, Surguladze, Tkachov 1989).
Dyson's second problem was a lack of formal proof of finiteness
of all the integrals (even without worrying about overlaps); instead Dyson relied on
heuristic "power counting" principles which could conceivably fail.  
(But it turns out they work.)
Ward 1951 then claimed to have resolved these problems, but that claim was false 
(it was later found Ward's method breaks down at 14th order).
Some claimed that Salam's two papers in 1951 saved the situation,
but few or no people understood them.
Dyson's difficulties were all overcome, <i>but it took over 25 more years</i>.
Papers with the necessary theorems about finiteness of integrals are
Weinberg 1960,
Zimmermann 1968, 
Hahn &amp; Zimmermann 1968,
Lowenstein 1976,
Lowenstein &amp; Zimmermann 1975,
and
Zimmermann 1976.
The theorems in Hahn &amp; Zimmerman and in Zimmermann 1968
are deficient in that they
only apply to Feynman integrals arising from particles of positive mass, 
i.e. not photons; but this barrier is overcome in the papers with Lowenstein,
which as a bonus considers both UV and IR infinities.
Note that this rigorous understanding of, and <i>definition</i> of, renormalized 
perturbative QED, came over 10 years <i>after</i> the Nobel prizes for inventing
QED were handed out to Feynman, Schwinger and Tomonaga in 1965.
</small></p><p><small>
The 
<a href="http://www.scholarpedia.org/article/Bogoliubov-Parasiuk-Hepp-Zimmermann_renormalization_scheme">BPHZL</a>
team of 5 authors 
(N.N.Bogoliubov, O.S.Parasiuk, K.Hepp, W.Zimmermann, J.H.Lowenstein, in chronological order)
showed how to design a renormalization scheme which (they could prove) <i>could</i>
handle overlapping divergences – proving renormalizability
actually for a wider class of quantum field theories than just QED –
even some nonAbelian gauge theories. <!-- such as QCD?? -->
Two of the later papers in the BPHZ series were Hepp 1966 and Zimmermann 1969.
This area is reviewed in Velo &amp; Wightman 1976 and Collins 1986.
The BPHZ "zero-momentum subtraction" scheme actually quite directly and conceptually simply
accomplishes Dirac's nightmare of "replacing infinities with zero": 
it simply subtracts off, from each Feynman-diagram integrand, each term in
its Taylor series expansion in the external momenta, below some degree.
Observe that no "cutoffs" or "regularization" are used and no direct attempt is made
to identify such "physical" quantities as "electron mass" or "electron charge" –
BPHZ simply remove these terms responsible for UV infinities!
Although the BPHZ scheme has several major practical disadvantages, it does work.
(There actually are an infinite number of different renormalization schemes.
One of the ones that has enjoyed the most practical usage is the 
"modified minimal subtractions" scheme in combination with t'Hooft-Veltman 
dimensional regularization.")
Then
Epstein &amp; Glaser 1973 established that the formal series output by BPHZ QED
satisfied "microcausality" and "unitarity."
Simpler renormalizability proofs, avoiding BPHZ complexities and
aimed just at QED (but sometimes these only were partial proofs), were then produced,
see Feldman et al 1988, 
<!-- Hurd 1989,  -->
Rosen &amp; Wright 1990 (using "dimensional regularization"),
and Keller &amp; Kopper 1996 (using renormalization "flow").
Austin 2006 claimed to have redone BPHZ directly in <i>position</i> (not momentum) space
in an 252-page manuscript which apparently nobody ever read.
The first claimed proof of the renormalizability to <i>all</i> orders
of the "standard model" which combines QED, QCD, and electroweak QFT,
was
by Kraus 1998 (&gt;100 pages)
using BPHZ methods plus some newer ideas such as 
<a href="http://www.scholarpedia.org/article/Becchi-Rouet-Stora-Tyutin_symmetry">BRST symmetry.</a>
This was over <i>20 years</i>
after electroweak theory
and the standard model both became 
widely accepted (the Nobel prizes for electroweak were handed out in 1979).
</small></p><p><small>
It should be noted that Andrew Wiles' proof of "Fermat's last theorem" took considerably less time
than the job of rigorizing Dyson, despite considerably less motivation and funding, fewer
researchers, and the fact that the Fermat proof was harder.
Why is that?  I suspect it is because of the physics community's policy of lying.
Specifically, it was very easy to get the impression from the vast number of lies in the
physics literature, that the problem was solved decades ago. Only a small number
of people were aware that that was a lie.  
</small></p><blockquote><small>
<b>Typical sample lie:</b>
With this [Dyson] result it is now finally proved that the program 
of renormalization can be carried through in quantum electrodynamics to any desired order of e.
[Jauch &amp; Rohrlich's page 223.  Quote also present in original 1955 edition.]
</small></blockquote><p><small>
Meanwhile everybody knew that Fermat's 
conjecture remained open.  Incredibly, the same story then was repeated for electroweak theory
– it was commonly falsely claimed that t'Hooft and Veltman had "proved renormalizability"!
This policy of lying by physicists about rigor, and perpetually papering over
massive underlying problems in their theories and pretending they aren't really problems,
has hurt their field tremendously.
</small></p>
<p><small>
<b>Another historical note about Weisskopf, Furry, and the electron-self energy:</b>
Weisskopf had originally in 1934 mistakenly found a quadratically divergent electron self-energy
in unrenormalized QED<sub>1</sub>,
but received a letter from Wendell H. Furry (who had done approximately the same
work independently) correcting his mistake.   [However, Weisskopf 
finds that quadratic divergence, like a<sup>-2</sup>,
still would happen with <i>bosonic</i> electrons.]
The Weisskopf-Furry corrected result was that the electron self-energy
is logarithmically infinite.  
Weisskopf 1939 improved this to find log-power divergence at every 
order of unrenormalized QED, not just the first – specifically in his final section VI,
Weisskopf found the term proportional to
&#945;<sup>N</sup> is divergent like the <i>K</i>th power of the logarithm of 
the UV cutoff energy, for some K with 0&#8804;K&#8804;N:
</small></p><center><small>
W<sub>N</sub> ·
(2<sup>-1</sup>&#960;<sup>-1</sup>&#945;)<sup>N</sup> 
log(&#955;<sub>e</sub>/a)<sup>K</sup>
</small></center><small>
where &#955;<sub>e</sub>=h/(m<sub>e</sub>c)&#8776;2.426×10<sup>-12</sup> meters
is the Compton wavelength of the electron and a is the "electron radius." 
For Weisskopf
<nobr>&#955;<sub>e</sub>/a</nobr>
where
<nobr>&#955;<sub>e</sub>=h/(m<sub>e</sub>c)</nobr>
serves essentially the same role as a UV energy cutoff &#923; measured in units of 
m<sub>e</sub>c<sup>2</sup>.  
Here the dimensionless constants W<sub>N</sub> 
are, says Weisskopf, difficult to compute (so he does not try)
and he said essentially nothing (and much of what he did say, was probably wrong) about
the growth rate when N&#8594;&#8734; of the W<sub>N</sub>.  
Specifically, Weisskopf first said he did not know whether 
the sum of all the terms (for N=0,1,2,3...) converged, even for finite positive a.
But he then claimed convergence was "highly probable"
if |log(&#955;<sub>e</sub>/a)|&#945;&lt;2&#960;, and that this 
suggests the "electron radius" (in whatever the true quantal electron theory is) should be of
order exp(-2&#960;/&#945;) times its Compton wavelength, i.e. 
<nobr>a<sub>true</sub>&#8776;3×10<sup>-386</sup></nobr> meters.
Actually, I think it likely that the |W<sub>N</sub>| grow superexponentially with N
in which case his series never converges no matter how 
small 
|log(&#955;<sub>e</sub>/a)|&#945;&gt;0 
is, and therefore no claim about the "electron radius" is deducible in this manner at all.  
Weisskopf's
calculation involved "states" of the electron-positron and photon
fields, and inter-state transition matrices.  At Nth order he takes a product of N
transition matrices yielding the start state back again, and sums all such
products.  The number of such products is obviously at least factorially growing,
which makes it seem likely |W<sub>N</sub>|'s ultimate growth behavior is like
N!<sup>P</sup> for some positive power P.
As far as I can tell Weisskopf 1939's work was never redone during
1939-2013 even though presumably it would be much easier and clearer with modern techniques.
Indeed, I doubt there is anyone alive today who understands Weisskopf's now-obsolete
techniques, which were quite sketchily and incompletely done.  (E.g. his section VI
actually only considers the "electrodynamic" part of this
self-energy, merely claiming the "electrostatic" and "mixed" parts "can be
computed along the same lines"; and even for the electrodynamic part many details are omitted.)
Frank 1951 redid the cases with 0&#8804;N&#8804;2 in detail using Feynman diagram techniques
and showed the electron self-energy diverges like the <i>square</i> of the
logarithm of the UV cutoff at order &#945;<sup>2</sup> in QED perturbation theory.
I believe that K=N is always attained, i.e. the divergency is always
like a constant (which can be of either sign!) times the Nth power of the
logarithm of the UV cutoff
at order &#945;<sup>N</sup> in QED perturbation theory;
this is because I believe that K&lt;N would only be 
possible as the result of a miraculous cancellation at that value of N.
Specifically, our <a href="#firstlautrup">unrenormalized analysis in §37</a>
of the Coquereaux-Kawai-Kinoshita-Okamoto diagram (that was for a photon, but the 
electron self-energy may be treated similarly) shows that a K=N power 
always arises from a "chain of bubbles" diagram, so that if K&lt;N that can only be
due to a miraculous cancellation.
And it indeed seems indirectly deducible from the QED series for 1/&#947;
given by Chetyrkin 1997 
that K=N for all 0&#8804;N&#8804;4
(also implicit in work of Laporta 2001 and 2002).
<!-- the signs seem to be - at alpha^even order and otherwise +, but this
could be a delusion. 
-->
</small><p></p>
<p>
These diagram-generating, diagram-to-math-expressions-converting, renormalization-replacement-rule,
and integral-evaluating (via Monte Carlo) algorithms are not at all trivial,
but they actually have all been programmed and used e.g. by Kinoshita and collaborators as
part of their gigantic project to compute QED<sub>N</sub>'s predictions of &#956;<sub>e</sub>,
to a precision of about 13 decimal places for a successful comparison with experiment.
(We'll review this work in <a href="#exptlqed">§8</a>.)
These programs were first written in the 1970s, albeit 
bugs still were being found and corrected in the late 2000-2010 decade.
<!-- some found in 2006 during their run of the program with N=4 -->
</p>
<a name="FIG1qedinf"><img src="WarrenSmithQED131123_files/QED7infTypes.png" alt="fig1" width="90%"></a>
<p>
In QED, it depends how you count/classify them (there are many
different competing canonization, regularization, and renormalization schemes), 
but the most common view nowadays are there are exactly
<i>three</i> fundamental types of infinities arising in computing the electron mass,
the photon mass, and the "vertex correction" in
scattering of an electron by a photon.  
(Actually the photon has
exactly zero total mass at each order in QED as a trivial-sounding, 
but actually not so trivial, consequence of gauge invariance,
see <a href="#wardident">§27-28</a>,
<!--cf. &sect;7.4 and 7.5 of Peskin &amp; Schroeder,  for bad discussion; they
blame it on gauge invariance-->
<i>but</i> individual diagrams of photon-mass type
can have infinite and gauge-dependent values, and even gauge-invariant subsets of
these diagrams lead to infinite renormalizations of the 
photon <i>wave-function</i> despite not affecting the
photon <i>mass</i>.)
Besides those three there also is a fourth type
– "disconnected vacuum diagram" infinities –
but those all are <i>irrelevant/ignorable</i> in the sense
that none of them affect any QED<sub>N</sub> output.
</p><p>
These <b>vacuum diagrams</b> would seem to predict that the vacuum should have an
infinite mass-density in any QED<sub>N</sub> with N&#8805;1 –
greatly contradicting the experimental
bounds, based on gravitational effects, which are 
<nobr>10<sup>-27</sup>&lt;|&#961;<sub>vac</sub>|&lt;10<sup>-26</sup>kg/meter<sup>3</sup>.</nobr>
Unfortunately QED's vacuum diagrams <i>would</i> plausibly seem to matter
in any future attempt to unify QED and <i>gravity</i>, 
and most or all of the infinities they spawn 
<!--seem unremovable by renormalization tricks (at least not by tricks anybody has yet devised)
and -->
are power-law analogous to 
&#8734;=&#8747;<sub>1&lt;z&lt;&#8734;</sub>z<sup>3</sup>dz,
i.e. much more severe than logarithmic.
</p>
<a name="casimir1"></a>
<p>
<small>
QED vacuum energy also matters in the calculation of the <b>Casimir force</b> 
causing attraction between 
two parallel-planar perfectly-conducting metal plates.  This force
can and has been measured, with results agreeing fairly well with QED theory
(Lamoreaux 1997-2000).
For parallel plates distance L apart idealized as perfect mirrors, this force is 
<nobr>&#8463;c&#960;<sup>2</sup>L<sup>-4</sup>/240</nobr>
per unit area (Brown &amp; Maclay 1969, calculation based on electromagnetic field).
It arises from the difference in energy densities for the
vacuums between versus outside the plates; this <i>difference</i> remains finite
in the limit of large UV cutoff, even though the two energy densities themselves both 
become infinite.  A finite UV cutoff actually exists in this problem because,
e.g, no known metal reflects Xrays; and the mathematics tells us that the precise
nature of this cutoff and the question of what metal it is, etc, is irrelevant in the limit.
<i>However</i>, when Deutsch &amp; Candelas 1979 and Candelas 1982 <!--and Graham et al 2004-->
considered the Casimir force for <i>general</i> smooth surface shapes, they found it
to be <i>power-law infinite</i>!  
(Finite Casimir force is an artifact of the precise symmetries exhibited
by parallel planes and certain other simple geometries such as perfect spheres, 
which cause miraculous cancellations
which do not happen in general; even tiny perturbations away from perfect spheres 
etc will yield infinities.
Also, note that depending on the geometry the force 
can be either attractive or repulsive; this was experimentally verified by Munday et al 2009.)
</small></p><p><small>
Apparently the problem is due to sharply defined boundaries. 
If the field vanishes on a sharply defined surface, its momentum (and hence energy density)
at all points on the surface is unbounded due to the uncertainty principle.
With a "flexible" boundary whose position can fluctuate (or if it is a finite-mass object), 
these infinities go away!
Ford &amp; Svaiter 1998 considered a flat plate with Gaussian position-based wavefunction
and found an expected 
energy density for a scalar field outside the plate which was bounded everywhere!
But if the plate is precisely localized the |energy| density in the field goes to infinity
like x<sup>-4</sup> as
the boundary is approached (x&#8594;0+), where note
this infinity is non-integrable. (For EM fields the energy density remains finite for a flat but
not a curved plate.)
</small></p><p><small>
Hence when calculating Casimir forces in general geometries,
the precise nature of the metal's high frequency response is
crucial; it loses importance only in special geometries.
<!--The infinite energy generally behaves like the third power of the UV cutoff for conformal
fields and second power for massive fields, times inverse 3rd or 2nd power of length?
Fulling table p113.  But it looks like trying to build an experiment to 
see a huge force will not work well due to unavoidable near-cancelations.
-->
</small>
</p><p>
In the other QFTs involved in the "standard model" that go beyond QED to
also describe the weak and strong forces, the same things happen, except that
far more than "three"
types of infinities occur – 
so many that t'Hooft and Veltman needed computer aid to 
enumerate them all –
but still only (according to their and their successors' computer-aided proofs) 
a <i>finite</i> number of types.
</p><small><p>
It has been commonly, but falsely,
stated that t'Hooft and Veltman "proved the electroweak/standard
model renormalizable." After cleanup and improvement by
Becchi, Rouet, and Stora 1974, this was convincing up to two-loop order for
the electroweak model in perturbation theory.
But it was <i>not</i> shown at higher order.  
The first claimed proof of the renormalizability to <i>all</i> orders
of the "standard model" which combines QED, QCD, and electroweak QFT,
was
by Kraus 1998 (&gt;100 pages)
using BPHZ methods plus some newer ideas such as the 
"<a href="http://www.scholarpedia.org/article/Becchi-Rouet-Stora-Tyutin_symmetry">BRST 
symmetry</a>."
Note that this was over <i>20 years</i>
after electroweak theory
and the standard model both became 
widely accepted (the Nobel prizes for electroweak were handed out in 1979)
and indeed happened at about the same time it finally became clear
(with the discoveries of neutrino mass and 
"<a href="http://en.wikipedia.org/wiki/Dark_matter">dark matter</a>") 
that the standard model is
an incorrect model of nongravitational physics!
Kraus's proof was redone simplifed to only 37 pages by
Grassi, Hurth, Steinhauser 2001.
</p><p>
It also is easy to get the impression that computer programs now exist that will
do any standard model finite-order perturbative calculation, e.g. enumerating
the Feynman diagrams, converting them into integrals, and doing the integrals numerically.
But I believe this impression is false and available programs are not valid above some small 
order.
</p><p>
Such computer programs <i>do</i> exist for QED, and were used to calculate
the electron magnetic moment to high order, but no comparable sanity check has
ever been performed for the full standard model.  It should in principle
be possible to use the standard model to predict several kinds of atomic clock frequencies
whose ratios could then be compared to experimental values at accuracies of 18 decimal places.
While that if accomplished would be <i>extremely</i> impressive, the computations 
required seem well beyond what forseeable computer hardware and software can accomplish,
and perhaps this accuracy goal is unattainable even with infinite computational power.
</p><p>
Before Kraus's proof, it clearly was impossible
to write such a program, because nobody knew how, and indeed it was not even known whether
such a program could <i>exist</i> even in principle.  Assuming Kraus's proof
is correct – which it presumably is since it was redone by 
Grassi, Hurth, Steinhauser 2001 –
it now <i>is</i> known in principle how to write such a program, but
in practice I do not believe it has happened:
Citation searches on Kraus's and the GHS paper
fail to reveal any papers about such a computer program, so I believe one still has never
been written as of the year 2012.
</p></small><p>
This contrasts with <b>unrenormalizable</b> QFTs (such as gravitons) which 
seem generically to feature an <i>infinite</i> number of types of naturally-arising
infinities, all of them "power-law," i.e. very severe.  
</p><a name="weinbergdivquote"></a>
<blockquote>
Quantum electrodynamics contains... infinities, but only in three or
four special places, where they can be dealt with by renormalization
of charge, mass, and wave functions. In contrast, the quantum theory
of gravitation contains an <i>infinite</i> variety of infinities, as can be
seen by an elementary dimensional argument: the gravitational constant
has dimensions h/m<sup>2</sup>, so a term in a dimensionless probability
amplitude of order G<sup>N</sup> [in perturbation theory]
will diverge like a momentum-space integral
&#8747;p<sup>2N-1</sup>dp.
<br>&nbsp;&nbsp;<b>–</b> 
S.Weinberg (§10.8 of his book <i>Gravitation and Cosmology</i>).
<small>
Weinberg is writing this as a 1-dimensional integral;
if it were a K-dimensional 
integral we'd have
&#8747;p<sup>2N-K</sup>d<sup>K</sup>p
whose UV divergence would amount to the same thing.
Weinberg's
"naive power counting" reasoning,
although usually correct, can conceivably be wrong.
But Goroff &amp;  Sagnotti 1986
eliminated that quibble by <i>proving</i> the existence of power-law divergence
by exactly calculating a graviton S-matrix at 2 loops.
</small>
</blockquote><p>
However, again, at Nth order for any particular finite N&#8805;0
only a finite number of kinds of infinities would occur
even in graviton QFT.
</p>
<p>
<b>Measurement problem:</b>
Why do we humans in everyday life, not experience "quantum weirdness"?
For example, nobody has ever experienced a quantum
superposition of living in Paris and Tokyo.
The planets, the furniture –
everything you see seems to reside in a quite definite position.
So far, attempts to explain this using the laws of quantum mechanics
have been inadequate.   The words "measurement,"
"<a href="http://en.wikipedia.org/wiki/Quantum_decoherence">decoherence</a>,"
and "dephasing" are
often used.  If somebody "measures" your position, it becomes definite.  Except:
who is allowed to measure, and who is not? And such a measurement operation,
though mathematically definable, violates relativity (happens instantly, faster than lightspeed).
For such reasons, most physicists agree there really are no such things as "measurement"
and "instantaneous wavefunction collapse" in quantum mechanics.  Those
"Copenhagen" ideas were oversimplified starter ideas for children.   
An essentially mathematically-equivalent effect arises (as was seen
via Von Neumann's "density matrix formulation" of 
quantum mechanics) from "position-based dephasing."
<a name="densdecoh"></a>
That is: If for some reason, 
parts of your wavefunction located in different places get multiplied by different complex
phase factors (the phase factors depend on spacetime position in a random-like manner),
i.e. more precisely if the
off-diagonal (in the position basis) entries in Von Neumann's "density matrix"
get multiplied by uniformly-random-angle complex phase factors,
that
produces a mathematically equivalent effect to position-measurement.   
</p><a name="optenvavg"></a><blockquote><small>
To be precise, if we average over the randomnessness in the phase angles (often
called "thermal averaging" or "averaging over environmental factors") all the off-diagonal
density-matrix elements become <i>zero</i>, which is exactly what Von Neumann's original
definition of "measurement" was.  This averaging would in 
<a href="#rainofbricks">rain of bricks</a>
correspond to taking the expectation over the randomized raindrop locations.
But then the argument is that there are so hugely many raindrops (of
order 10<sup>148</sup> in a 1-cubic meter box in 7 nanoseconds)
that it is extremely probable just <i>one</i>
raindrop configuration effectively <i>is</i> performing such an averaging, thus explaining
why rain of bricks physics yields Von Neumann measurement effects.
Incidentally, if necessary we could add an <b>additional "averaging postulate"</b>
to our <a href="#rainbricks">rain of bricks</a> laws-of-physics <i>demanding</i> that an outer
averaging over all
possible raindrop configuration "environments"
be performed, in which case no dispute at all could be possible.
</small></blockquote><p>
To produce an
effect like X-measurement for some X other than "position," one instead needs the phase factors
to depend in a random-like manner on the value of X.   That effect can be got by dephasing
the location of a "pointer" in an X-measurement apparatus, which means if we had a mechanism
for position-based dephasing, we would automatically have a way to get X-measurement
effects, for essentially any X reasonable enough that one could build an apparatus to 
convert X-value to pointer-position.
</p><p>
It often is claimed that such 
dephasing results from interactions between the "system" and a random-like "external environment"
– the complex phase-angle rotation
caused by an interaction potential energy &#916;E acting for a timespan &#916;t
is &#916;E·&#916;t/&#8463; 
radians.
</p><p>
This explanation of position-based dephasing works well to produce estimates
in many practical applications.
But from the point of view of fundamentals, it is inadequate.  What if 
there is no "external environment" since our "system" is the whole universe?
And what justifies the implicit assumption that the external environment has clear
position?  For example, the "external environment" could be "a cosmic ray passing through
Tokyo."  It interacts with the part of your wavefunction in Tokyo, 
<!--thus dephasing the Tokyo part of your wavefunction,-->
dephasing it and thus preventing you from being in a superposition of Tokyo
and Paris.  This whole explanation would fail if the cosmic ray <i>itself</i>
were delocalized.
It then would measure something else about you, other than your position.  Since 
quantum mechanics is symmetric under unitary transformations of
Hilbert space, <i>every basis is equivalent.</i>
It is impossible for the  position-basis to be "favored."    There is no reason the 
cosmic rays should be localized in position hence no reason they should localize you.
</p><p>
Just to make that point completely clear (since some muddled physicists think that somehow the
"many worlds interpretation" might cause everybody 
to happily perceive positional localization, or
some such):
</p><a name="delocthm1"></a><p>
<b>Delocalization Theorem:</b>
For
any Poincare/Lorentz-invariant unitarily-invariant deterministic theory of quantum physics,
there are an infinite set of initial conditions such that those laws will <i>never</i> 
achieve even the slightest spatial localization of even a single particle.
</p><p>
<b>Proof:</b> 
Any initial conditions such that every particle in Minkowski (1+3)-space is initially in
a <i>momentum eigenstate</i> will do.
Momentum eigenstates are
translation-invariant up to a multiplicative phase factor (unitary).  
A deterministic symmetric theory with symmetric
initial conditions must evolve symmetrically; 
there is no way to break the translation symmetry. <b>Q.E.D.</b>
</p><p>
<b>Remarks:</b>
</p><ol><li>
The proof could also be redone based on spherically-symmetric initial states instead of
translation-symmetric ones (for rotation-invariant physical laws).
Much more strongly,
I conjecture that there is a reasonable measure under which <i>almost every</i> 
possible initial condition will feature time-evolution that avoids positional localization 
at almost every future time!
</li><li>
A critic might carp that perhaps our translation-invariant states might somehow be "unstable"
and thus in practice tiny errors would get amplified
exponentially and the symmetry would get broken.  That criticism is wrong because
quantum mechanics performs unitary rotations in a Hilbert space, and such
rotations automatically preserve density everywhere on the Hilbert "unit sphere,"
hence cannot engender "attraction" or "repulsion" from anywhere, nor any kind of "exponential
amplification."
</li></ol>
<p></p><p>
If spacetime is exactly Poincare-Lorentz invariant, then <i>every position is equivalent</i>,
thus preventing any kind of automatic position-based dephasing.
But it seems empirically obvious that position <i>is</i> a favored basis and that some sort
of position-based dephasing <i>is</i> happening automatically all the time!
Therefore, something must be wrong or incomplete about
traditional quantum mechanics plus Poincare-Lorentz invariance.
</p>

<a name="dyson"></a>
<h3>5. Dyson 1952's generic-divergence argument – Review and new developments </h3>

<p>
F.J.Dyson (1952) gave an argument suggesting that
essentially every physically-meaningful
QED power series in &#945; should ultimately <i>diverge</i> for every
complex &#945;&#8800;0, including the actual &#945;&#8776;1/137.
Therefore, these series at best
could be regarded as mere <i>asymptotic series</i>, i.e. whose
truncations are only asymptotically valid 
(i.e. with error of the same order as the first omitted term)
in the <i>limit </i> &#945;&#8594;0+.  Fortunately, &#945;&#8776;1/137 happens to be
quite small in QED, and (&#945;/&#960;)&#8776;0.00232, which often seems a more natural expansion
parameter, is even smaller, so
these series in simple scenarios
usually deliver highly accurate and useful pseudo-convergent results before they
start diverging.  Unfortunately the quantum field theory of the strong force (QCD) involves
a much larger &#945;<sub>s</sub>&#8776;0.12
and so in QCD the divergence appears to "start immediately."
</p><p>
<b>Dyson's divergence argument</b> redone to clarify it a little:
Consider a modified universe in which &#945; is small and <i>negative</i>.
This would correspond to <i>imaginary</i> electron charge e, since
&#945;=e<sup>2</sup>/(2&#949;<sub>0</sub>hc), which would correspond to
<i>attracting</i> instead of repelling electrons.
It is known, e.g. from the exact solution of Dirac's equation for "hydrogenic atoms"
that if the hydrogen nucleus had 
<nobr>charge&#8805;3<sup>1/2</sup>/(2&#945;)&#8776;118.677</nobr> elementary units, then the
electron would "fall in" to a point-nucleus, releasing <i>infinite</i> energy.
So it is fortunate the hydrogen atom has nuclear charge 1 and all so-far-known atoms have
charge&#8804;118; for them the Fermi-compression energy for the electron is enough to
balance the attraction of the nucleus so that these atoms do not collapse.
It indeed has been proven by Lieb et al in various mathematical models approximating
quantum electrodynamics, 
that <i>matter is stable</i> for
0&lt;&#945;&lt;0.01
(see also Fefferman et al 1997),
but would be unstable and collapse if &#945; were made too large and positive,
say &#945;&gt;3. 
(But charged <i>bosonic</i> electrons would always yield a collapse instability
in some of Lieb's models, causing him to remark that it is a good thing that
no stable charged boson exists.)
So anyhow, it seems clear (although this is not rigorous)
that if &#945; were <i>negative</i> then any sufficiently great number
of mutually-attracting electrons 
would collapse with infinite energy release.  
As the crudest estimate,
presumably any number N&gt;|1/&#945;|&#8776;137 of electrons,
arranged on the surface of a radius-R shrinking sphere, ought to suffice in order to make the
Coulombic energy of order &#945;N<sup>2</sup>R<sup>-1</sup>
outweigh the compression and rest-mass energies of order (R<sup>-1</sup>+1)N
in the limit R&#8594;0+.
(We'll soon point out why this estimate by Dyson is too crude, and 
<a href="#dysoncorrect">repair</a> it.)
Furthermore the vacuum itself would be unstable, i.e. not a state of
lowest possible energy
(lots of e<sup>-</sup>e<sup>+</sup> pairs would spontaneously create, segregate,
and then fall into singularities).  
Therefore, Dyson concludes (nonrigorously), the series must either (1) diverge,
or (2) fail to represent physics, for all negative &#945;.
In case (1) we can 
<a href="http://en.wikipedia.org/wiki/Radius_of_convergence">deduce</a>
it also must diverge for all positive, and
indeed all nonzero complex &#945;.
On the other hand, in case (2) 
the proposition "the QED series converges and represents valid physics for all &#945;
with |&#945;|&lt;X for some positive radius of convergence X" is contradicted;
we could get a series with a nonzero radius of convergence but it would yield
physically-wrong answers –
and if the true physical answer is an analytic function of &#945;
then it would (by analytic continuation)
necessarily yield wrong answers essentially <i>always</i>.
</p><p>
<b>The top criticism</b> of Dyson's argument –
as well as one of the top obstacles preventing 
from being rigorous –
is that, arguably, QED 
simply is not a sensible physical theory
unless &#945; is nonnegative real.
For example, it no longer has a hermitian Hamiltonian H, because 
H=H<sub>free</sub>+H<sub>interaction</sub>
with all three H's hermitian and nontrivial,
is now replaced by
H=H<sub>free</sub>+iH<sub>interaction</sub>.
Therefore its time evolution is no longer unitary.
Therefore, it cannot be said to represent "physics" at all.
(Also, QED is not gauge-invariant according to
EQs 4.3 and 4.6 of Peskin &amp; Schroeder <!--p78-->
unless &#945; is positive real.)
Therefore, Dyson, in claiming via physical arguments that
"collapses" would occur in "QED with &#945;&lt;0"
was physically arguing about something that isn't physics!
</p><a name="mandlgauge"></a><p>
We now point out for the first time that <b>Dyson's argument can be rescued</b>
from those criticisms.  Two ideas are required.
First idea:
on p.78 of Mandl &amp; Shaw 1993 is a different definition of 
"gauge invariance"
than the too-restrictive one in Peskin &amp; Schroeder.
It allows the electron charge e to be
an arbitary complex number:
the effect of adding &#8706;<sub>&#956;</sub>f(x) to the 
Maxwell 4-potential A<sub>&#956;</sub>(x)
[where x denotes (t;x,y,z)] is  to multiply
the electron field 
&#936; by
exp(2&#960;i e f(x))
and the Dirac-adjoint of &#936; by
exp(-2&#960;i e f(x)).
</p><p>
For the second idea, the key is the 1998
realization by Carl Bender that quantum-mechanical 
Hamiltonians <i>can be nonHermitian.</i>
(Nobody had previously considered that possibility for more than one minute, although
in hindsight, numerous nonHermitian PT-symmetric quantum and classical scenarios had 
already been investigated without those investigators noticing the unifying theme.)
This was discovered by proving
that H=p<sup>2</sup>+ix<sup>3</sup>
(where p and x are the usual momentum and position operators in
nonrelativistic 1-dimensional quantum mechanics)
has an entirely real, positive, and discrete spectrum, and it has "unitary" 
time-evolution in the sense that there exists a suitable quadratic positive-definite
Hilbert-space norm (albeit not the usual one) which that time-evolution preserves.
Instead of hermiticity, this Hamiltonian obeys an alternative property
Bender calls <b>PT-symmetry</b>.
This sparked an intensive investigation, still continuing 12 years later
(largely by Bender with over 20 collaborators) of PT-symmetric quantum mechanics,
see Bender 2007 for a review and Dorey et al for the proof for the particular example
H above.
It turns out that PT-symmetric Hamiltonians always lead to unitary time-evolution,
and the appropriate norm needed to show that, is defined with the aid of
a new self-inverse operator called "C."   In old-style hermitian quantum theory P 
is the "Parity" (negating the sign of x,y,z) and C is the "Charge conjugation"
operator, which commute.  But in PT-symmetric quantum mechanics, "P" may
be inequivalent to the parity operator, and P and C do not commute.
This makes it yet more clear that this is something new, and it also could allow
the masses of particles and antiparticles to differ (which they cannot in 
ordinary hermitian QFTs) which might be useful for explaining the matter-antimatter 
asymmetry of the universe.
</p><p>
Bender, Cavero-Pelaez, Milton, and Shajesh
realized in 2005
that QED with negative &#945;, 
although not Hermitian, still is a PT-symmetric quantum 
field theory, with unitary time-evolution.  
(Their "P" operator is <i>not</i> the same as the Parity operator.)
I point out that this (after over 50 years without progress!), 
suddenly instantly <b>nullified</b> that top-Dyson criticism.
It incidentally also evaded the (Nobel prize winning and supposedly all-encompassing!) 
impossibility <a href="#colemangross">theorem</a> of Coleman &amp; Gross 1973.
</p><p>
<small>
In Folland 2008 EQ on page 297
(and also in Muta 1998)
QCD is formulated in a manner intended to look very much like QED
(compare with Folland's EQ6.27 on page 140).
This suggests to me that QCD with an imaginary color charge also would be PT-symmetric,
by parallel reasoning.  However, I have not carefully confirmed that.
</small>
</p><p>
The <b>next-biggest criticism</b> of Dyson's argument is the fact
that arguments of Dyson's kind can yield misleading/wrong conclusions.  
Simon 1982 gives two counterexamples on his pages 4-5,
and the appendix of Bender &amp; Milton 1999 
gives two more (which also are discussed in §2.3 of Bender 2007).
</p><p>
<b>I now describe a <i>second</i> simple and convincing
argument, quite <i>independent</i>
of Dyson's</b>, that generic QED power series must have convergence radius=0,
and it also is immune to the counterexamples by Simon and B&amp;M.
The key realization is 
that negative-&#945; QED is an <b>asymptotically free</b>
QFT, meaning, more precisely,
that as we go to higher energy 
scales &#923;
(equivalently shorter length scales), 
the effective 
value of |&#945;| <i>decreases</i> roughly like 1/log&#923;,
see EQ12.92 in Peskin &amp; Schroeder.
<!-- p425. The QED beta function to 2-loop order is
 beta(e) = e^3/(12pi^2) + e^5/(64pi^4) + O(e^7).
says EQ2.10 of http://arxiv.org/pdf/hep-th/0210240v1 
where note alpha=e^2/(4pi)  says P&S on page xxi.
and EQ2.5 says
 de/beta(e) = dt.  In terms of a=alpha these are EQ 2.12
 beta(a) = 2(3pi) * a^2 + 1/(2pi^2) * a^3 + O(a^4)
 and then
 da/beta(a) = dt.
-->
Quantum <i>chromo</i>dynamics (QCD) also has that property, as was famously
shown by D.Gross, D.Politzer, and F.Wilczek, 
see EQ17.14 and 17.17 in Peskin &amp; Schroeder.
<!-- In QCD the 2-loop beta function expressed as a function of a=alpha_s  is
  beta(a) = -(11-2Nf/3)/(4pi) a^2 - (102-38Nf/3)/(4pi)^2 a^3 - O(a^4)
and 
  da/beta(a) = dQ/Q
where Q is momentum transfer squared and Nf=6=#flavors.
Yndurain states at more loops.
-->
[Fear not: This log-factor weakening is not enough to hurt Dyson's original collapse argument,
although it would somewhat alter attempts to use it to generate precise estimates
about the divergence rate.]
</p><p>
Now apply the Wilsonian "renormalization via rescaling" view of what QFT "renormalization" is.
Wilson's point was that renormalizable QFTs at smaller length scales, equivalently 
higher energy scales &#923;, behave like the same QFT
with certain parameters altered, in particular &#945;.
The "running" of these parameters as a function of &#923;
is described by the "Callan-Szymanik equation."
This all is discussed in Peskin &amp; Schroeder and Greiner &amp; Reinhardt.
For example, 
the "Uehling potential" between two charges
is computed by Greiner &amp; Reinhardt in their solved exercise 5.3,
e.g. see their EQ 15 for an integral expression exact in first-order QED,
EQ 22 for its asymptotic form at small separations,
and EQ 25 for the form at large separations (which of course
reduces to just the classical Coulombic potential).
<!--
http://www.nd.edu/~johnson/Publications/uehling.pdf
cites
E. H. Wichmann and N. H. Kroll, Phys. Rev. 101, 843 (1956).
-->
From their short-separation form we have
</p><center>
&#945;<sub>effective</sub>(&#923;)
&nbsp;  &#8776; &nbsp;
&#945;  ·  [1 + (3&#960;)<sup>-1</sup>2&#945; (ln(&#923;/m<sub>e</sub>) - 1.41)]
</center><p>
when &#923;&#8594;&#8734;.
This effectively <i>strengthens</i> interactions of charges 
as the length scale shrinks, i.e. when &#923; increases –
<i>except</i> that in Dyson's alternate universe in which &#945; is <i>negative</i>,
interactions <i>weaken</i>, e.g. according to 
Peskin &amp; Schroeder's
EQ 7.96 in their §7.5
</p><center>
&#945;<sub>effective</sub>(&#923;)
&nbsp;  &#8776; &nbsp;
&#945;  / (1 - [3&#960;]<sup>-1</sup>2&#945; [ln(&#923;/m<sub>e</sub>) - 1.67])
</center><p>
at high energy scales &#923;.  (These two textbooks got disagreeing results in their calculations
of the constant 1.41&#8800;1.67, but the value of this constant is irrelevant for our purposes.)
In QCD, 
<!-- P&amp;S's EQ 17.14 in &sect;17.2 -->
the running of the strong-force coupling
constant is found (Yndurain EQ 14.4b) <!--also Greiner &amp; Sch&auml;fer &sect;3.4.2-->
</p><center>
(&#945;<sub>s</sub>)<sub>effective</sub>(&#923;)
 = 
6&#960; / [(11Z - 2N<sub>f</sub>) ln(&#923;/M<sub>QCD</sub>)]
</center><p>
where M<sub>QCD</sub>&#8776;217MeV <!-- plus or minus 25 MeV says wikipedia. -->
is a constant of physics called the "QCD renormalization mass scale,"
N<sub>f</sub>=6 is the number of fermion (i.e. quark) flavors
(which actually grows with energy, but it is thought to be 6 at all sufficiently large energies)
and Z=3 for QCD, but we have stated the expression for a general SU(Z)-gauged Yang-Mills
theory.
Again, this gets weaker as the length scale goes to zero, demonstrating
both QED(&#945;&lt;0) and QCD's "asymptotic freedom."  
(Incidentally, a rigorous proof of asymptotic freedom
supposedly is now available by Balaban 1989.)
These results are well-supported
by experiment, see P&amp;S §7.5 and §17.6.
</p><p>
So here's the crux of my argument.
Consider any QED prediction F(&#945;) that is an analytic function of &#945;.
Suppose this function, somewhere, has at least one
point of nonanalyticity, e.g. a branch point
or pole. 
</p><blockquote>
These indeed exist. 
To see that, consider, e.g,
the exact closed form 
solution by Darwin 1928 
(and Gordon 1928 independently in German;
more modern recapitulations:
Biedenharn 1962, 
Waldenstrøm 1979,
de Lange 1989,
Goodman &amp; Ignjatovic 1997)
of Dirac's equation for a hydrogenic atom with point spinless nucleus with
charge Z|e|. 
It yields the following exact energy eigenvalues
(see §16.1-2, especially EQ 16.59, of Landau 1996; I have redefined the quantum numbers
n and j in a way disagreeing with most or all authors and which makes them both integers; also note
Granovskii 2004 showed that the famous agreement between this formula and 
Sommerfeld's formula from relativistic "old quantum" theory, was due to 
a <i>mistake</i> by Sommerfeld that nobody spotted for 88 years):
<p></p><center>
E<sub>n, j</sub> 
&nbsp; = &nbsp; mc<sup>2</sup>
[1+Z<sup>2</sup>&#945;<sup>2</sup>
/ (n+[j<sup>2</sup>-Z<sup>2</sup>&#945;<sup>2</sup>]<sup>1/2</sup>)
]<sup>-1/2</sup>
</center><p>
where j=0,1,2,3,... and n=0,1,2,3,... are nonnegative integers, and j&#8804;n,
and Z is the nuclear charge
measured in units of |e|, and &#945; is the fine structure constant.
At the next order of QED, "Lamb shifts" of the eigenenergies 
by additive amounts of order Z<sup>4</sup>&#945;<sup>5</sup>mc<sup>2</sup> 
arise, see Greiner &amp; Reinhardt §5.4; and also the fact that actual atomic nuclei
have nonzero size and magnetic moment causes further small
corrections; but these do not matter
for our purposes here.  
We just wish to remark that
regarded as an analytic function of complex &#945;, this energy formula
has square-root-type branch-point singularities when
</p><center>
Z&#945; = ±j
&nbsp;&nbsp;&nbsp; or &nbsp;&nbsp;&nbsp;
Z&#945; = ±i·(n+j)(n-j)/(2n)
</center><p>
and also there are poles at the former points.
These are infinite sets of singularity-points lying on the real 
and imaginary axes respectively.
<!-- But they stay away from the origin if Z is bounded. -->
</p></blockquote><p>
<i>Then</i> upon rescaling our asymptotically-free QFT, the <i>same</i> kind of singularity
must <i>reoccur</i> at a different point of the &#945; complex plane with smaller 
|&#945;|.  And that in turn will breed another copy with still smaller
|&#945;|.  And so on, spawning an infinite number of singular points, approaching the origin
&#945;=0 as a limit.   This <i>forces</i>
F(&#945;)'s Maclaurin series expansion to have zero radius of convergence,
with the origin necessarily being an "essential singularity."
</p><p>
This argument predicts that both QED and QCD must generically output
series with radius of convergence zero.  And it is highly independent of Dyson's in the
senses that 
</p><ol>
<li>
My scaling argument does not care whether Dyson's "collapse" can or does happen, 
</li><li>
Dyson in 1952 did not know or care about K.G.Wilson's view of renormalization,
and did not know about "asymptotic freedom."
</li></ol>
<p></p><p>
I personally find <i>two</i> independent nonrigorous arguments, far more convincing than one!
So between this verification and the fact that the
top two criticisms have of Dyson have
both gone down in flames, I now am far more convinced, indeed almost certain,
that <b>Dyson 1952's conclusion was correct:
QED series generically diverge for every complex &#945;&#8800;0.</b>
</p>
<a name="cvitconj"></a>
<p>
<b>Detour to discuss the wrong road taken by Cvitanovic 1977:</b>
Cvitanovic conjectured that renormalized-QED perturbative
series <i>converge</i>, not diverge, <i>provided</i>
</p><ol type="a">
<li>
We group the Feynman
diagrams at each order into "gauge invariant subsets" and sum the subset-sums, and 
</li><li>
We <i>disallow</i> diagrams containing virtual electrons, aka "electron loops."
This demand
can be viewed as considering QED in a limit 
m<sub>e</sub>&#8594;&#8734; 
of infinite electron mass (although
Cvitanovic did not mention that view).  Its
exclusion of all diagrams that include virtual electrons, also has been called 
<b>"quenched QED."</b>
</li></ol>
Cvitanovic's conjecture was based on Lautrup's idea that (i) individual diagrams
are "meaningless" in the sense that gauge changes alter their values, hence only
gauge-invariant diagram sets "have meaning," plus
Cvitanovic's empirical observation that 
(ii) amazing cancellations seem to occur causing the |sums| of 
gauge-invariant diagram sets to be much smaller than might naively have been expected from
the values of their member diagrams.   E.g. it is common for single diagrams to
have |value| exceeding the diagram |sum|, see
<a href="#exptlqed">§8</a>.
<p></p><p>
<b>Cvitanovic 1977's quenched-QED convergence conjecture</b>
seems immune both
to Lautrup 1977's discovery
of "renormalons," and to Dyson's collapse argument, and to all arguments based on
"vacuum polarization" (e.g. anything about "Landau pole," "asymptotic freedom," 
or "shielding"), since those all
involve virtual electrons.  
That left open the possibility that Cvitanovic's conjecture still might be correct.
Let me, then, take a stance on this: <b>I believe 
Cvitanovic's quenched-QED convergence conjecture is false.</b>
Indeed a nonrigorous disproof was supplied by Bogomolny &amp; Kubyshin 1981.
Their argument predicts N!-style divergence of the quenched QED subseries.  
But since Bogomolny &amp; Kubyshin were unaware of Cvitanovic 1977, 
while Cvitanovic
was unaware of B&amp;K, this refutation remained unrecognized until now.
This refutation is enormous since the number of quenched QED diagrams is also known
to grow like N! [up to a polynomial(N) factor] so that actually the quenched-diagram-value
cancellations ultimately are extremely poor, far <i>weaker</i> than would be expected for 
independent random standard normal deviates – exactly the opposite of Cvitanovic's
empirical observations of cancellations far <i>stronger</i> than expected from that naive model!
(We will discuss B&amp;K next section.  The discrepancy is presumably because 
Cvitanovic's obervations were only at low orders of QED reachable by computer, while
B&amp;K's argument was intended for the regime when the order tends to infinity.)
But Cvitanovic's empirical observation of a remarkably large amount of cancellation within
sums of gauge-invariant quenched diagram sets (in QED<sub>N</sub> for N small enough
for exhaustive computer exploration), remains impressive as of year 2013,
see <a href="#exptlqed">§8</a>.
Indeed more strange numerical evidence arose after 1977
suggesting amazing cancellations happen
in further amazing ways in the land of quenched diagrams, 
for reasons still not really understood:
</p><ul><li>
The furthest computation of the QED "beta function" to date was by
Kataev &amp; Larin 2012 who found in the "modified minimal subtraction scheme"
(for discussion of this and other schemes see Collins 1986)
<center>
&#946; =
  (1/3) X<sup>2</sup>
+ (1/4) X<sup>3</sup>
- (31/288) X<sup>4</sup>  <!--4 loop-->
- [2785/31104 + 13&#950;(3)/36] X<sup>5</sup>
<br>
+ [-195067/497664 - 13&#950;(4)/96 - 25&#950;(3)/96 + 215&#950;(5)/96] X<sup>6</sup>
+ O(X<sup>7</sup>)
</center>
where X=<u>&#945;</u>/&#960; and &#950;(k)=&#8721;<sub>n&#8805;1</sub>n<sup>-k</sup>.
A previous computation by Gorishny et al 1991 
(see their EQ2.9 and use n=1 electron flavors;
this agrees with Kataev-Larin formula except for an overall factor of 4)
only did the X<sup>4</sup>
and lower-degree terms, and included the remark
"It is well known that beginning from the 3-loop level (meaning X<sup>4</sup> terms)
the coefficients of the beta functions of QFTs with one coupling constant
<i>depend</i> on the choice of renormalization scheme."
The <i>quenched</i>-QED subseries (which, in contrast, does <i>not</i> depend on
the renormalization scheme at any order) is
<center>
&#946;<sub>Quenched</sub> =
  (4/3) A
+ 4 A<sup>2</sup>
- 2 A<sup>3</sup>
- 46 A<sup>4</sup>
+ [4157/6 + 128&#950;(3)] A<sup>5</sup>  <!-- 5-loop -->
+ O(A<sup>6</sup>)
</center>
according to EQ9 of Baikov, Chetyrykin, Hühn 2010, where A=&#945;/(4&#960;)
and this &#945; differs from the <u>&#945;</u> in the unquenched beta function.
The amazing thing about the latter is that the coefficients of 
A<sup>2</sup>, A<sup>3</sup>, and A<sup>4</sup> each are <b>integers</b>.
This was examined in detail by
Broadhurst 1999 <!--built on 30-year-older work by Kenneth Johnson
(resting in turn on top of even older work by Freeman Dyson)-->
who computed the values of the 24 quenched diagrams involved inside the A<sup>4</sup> term.
Miraculously, when the diagrams are summed,
terms involving &#950;(3) and &#950;(5) <i>cancel</i>, leaving
an integer as the final answer.  We also get pure rational coefficients
at these orders in the unquenched beta function, at least in the MMS scheme.
Although pure rationality is lost at the
A<sup>5</sup> term,
even there Kataev &amp; Larin find astonishing cancellations,
namely terms proportional to rational multiples of
&#950;(4), &#950;(5), &#950;(7), and &#950;(3)<sup>2</sup> cancel;
and the latter two also cancel inside the
X<sup>6</sup> term.
Broadhurst Delbourgo Kreimer 1996 found 
an explanation for these amazing cancellations
using "knot theory" (see Kreimer's 2000 book) discovering that "renormalization"
can be "organized with the aid of a Hopf algebra."  
Amazing stuff, which few understand (in particular, not me),
but this in my view this provides a large part of the explanation for
Broadhurst 1999's cancellations plus partly explains the
impressive cancellations observed by Cvitanovic at low orders.
</li><li>
In the computation of QED's &#945;<sup>3</sup>-order contribution to
the electron magnetic moment, some of the exact formulas for diagram-values involve
1-dimensional definite integrals whose integrands contain polylogarithmic functions
(e.g. see the paper by Levine, Remiddi, Roskies inside Kinoshita 1990).
<!--  Also:
M.J. Levine, E. Remiddi and R. Roskies:
Analytic contributions to the g factor of the electron in sixth order,
 Phys. Rev. D 20,8 (1979) 2068-2076.
3 graphs are done "semianalytically."
-->
Amazingly, all of
those cancel out when the diagrams are summed, yielding an exact formula for
C<sub>3</sub> expressible with no definite integrals (see <a href="#exptlqed">§8</a>).
But eventually Laporta 1983 was able to evaluate those definite integrals in closed form,
using Dirichlet L functions,
(&#960;ln2)<sup>2</sup>, &#960;<sup>2</sup>, &#960;<sup>4</sup>, (ln2)<sup>4</sup>, zeta(3), 
&#960;<sup>2</sup>ln2,
&#950;(3)&#960;<sup>2</sup>,
&#950;(5), and plain rationals, all were involved.
When these diagrams are summed, the result is expressible using
rational linear combinations of &#950;(3)&#960;<sup>2</sup>
and &#950;(5) only.
</li></ul><p>
<i>But</i> there is absolutely no evidence or logical reason why
those observed amazing amounts of cancellation should be
anywhere near <i>enough</i> to force series <i>convergence</i>
(and Cvitanovic never gave any)
–
and indeed the explicit numerical predictions made by Cvitanovic in 1977
seem refuted by year-2010 numerical evidence  (see <a href="#exptlqed">§8</a>).
E.g. the quenched-diagram count Q(k) is 
<a href="http://oeis.org/A005416">known</a> to grow essentially factorially, 
while the |sum| S of the quenched diagrams within the coefficient of
<nobr>(&#945;/&#960;)<sup>k</sup></nobr>
in the perturbative QED expansion of the electron g/2
is approximated by the following (for odd k=1,3,5): 
<nobr>
S(k)=0.4·Q(k)<sup>0.35</sup>
</nobr>
(see Aoyama et al 2010 and our <a href="#exptlqed">§8</a>)
where Q(k) is the number of quenched diagrams.
Both 0.4 and 0.35 are impressively small, but
even if cancellation this impressive kept happening forever (which I doubt), we'd still
get divergence for every &#945;&#8800;0.
</p><p>
What actually happens?    The Bogomolny-Kuryshin analysis indicates divergence
like N! for generic QED <i>sub</i>series arising from diagrams with exactly k closed electron loops
each, for any fixed k.   This combined with Bogomolny's view (discussed next section)
that generic <i>full</i> 
QED series diverge approximately like (N/2)! – far more slowly –
indicates that there must be 
<i>tremendously impressive</i> cancellation between the diagram subsums with different k.
This is a kind of cancellation not observed by Cvitanovic 1977 and indeed which seems not
to have been observed by anybody even as of 2013 because computers have been unable to
explore far enough.  Meanwhile as we said, the kind of cancellation Cvitanovic 1977
<i>did</i> observe (e.g. within the quenched diagrams at given order)
is predicted by Bogomolny-Kuryshin 1981 to become 
asymptotically tremendously <i>un</i>impressive!  
<!--
Both the initial impressiveness and the ultimate unimpressiveness
of this kind of cancellation can be explained by
new empirically-driven ideas
by me about "fractal" structure, see <a href="#exptlqed">&sect;8</a> for that?
-->
</p><p>
<b>(End of Cvitanovic detour.)</b>
</p><a name="higgsstability"></a><p>
<b>Another detour: What does the "Higgs force" do to the stability of matter?</b>
In order for Dyson's collapse argument 
– destroying QED when &#945;&lt;0 – to have impact, we 
also need that QED is <i>not</i> destroyed when 0&lt;&#945;&lt;0.01!
Demonstrations of the "stability of matter" in various mathematical models approximating QED,
by Lieb et al and by Fefferman et al, have already been mentioned.  However (to now do 
something new) with
the recent discovery of the Higgs boson –
the last
remaining ingredient of the "standard model" –
at the LHC,
means that there is now a new kind of 
force, the "Higgs force."    And it would naively seem that the Higgs force should cause
Dyson collapse in our universe!  Of course, we cannot let this threat go unanswered.
We shall now show (we of course do not claim complete rigor) that this naive impression is
untrue.  The world will not collapse due to the Higgs force, and the crucial reason is the Higgs
boson's <b>self-interaction.</b>  If the Higgs did not self-interact, it (a) would be unable to
have a nonzero "vacuum expectation value" that
generates "effective mass" of other standard model particles such as the W and Z bosons
via the "Higgs mechanism" (this was already well known), and now (b) we point
out that the universe would be vulnerable to collapse-instabilities.  
For reason (b) it seems likely that no non-self-interacting scalar force-carrier can exist; and
this is a new realization.
</p><p>
We first explain the problem.  The Higgs boson, were it not self-interacting, would 
cause an <i>always-attractive</i> Yukawa-type
force between any two particles the Higgs can interact with.  Since the Yukawa force
is asymptotically, at small distances, the same as the more familiar Coulomb force,
we would expect any two particles to form a bound state analogous to "hydrogen," and if the Higgs
coupling constant were large enough for that kind of particle (and assuming other,
non-Higgs, forces did not interfere), then (analogously to the
well known "fall" of the Dirac electron into a point nucleus with too-large charge Z&#8805;119)
"fall in" should occur, yielding infinite energy release.
Furthermore, even if our two particles had Higgs coupling constants
small enough to avoid this catastrophe (analogously to ordinary hydrogen being stable), 
then still a Dyson-style many-particle-"cloud"
collapse would be expected to occur provided the number of particles initially was
sufficiently great and they were initially located in a ball of sufficiently small radius.
This is because the Coulomb-like attractive interaction energy for N particles would
grow like N<sup>2</sup>, eventually outweighing the repulsive Fermi-gas (or Bose gas) energy of
confinement of those N particles, which grow as smaller powers of N.
</p><p>
Next, we explain the cure.  
The lagrangian of the Higgs field &#966; is (perhaps up to an overall proportionally
factor such as ±2,
which shall not matter; and in units with 
&#8463;=c=1)
</p><center>
L =
- |d&#966;/dt|<sup>2</sup>
+ |&#8711;&#966;|<sup>2</sup>
+ M<sup>2</sup>|&#966;|<sup>2</sup>
- B|&#966;|<sup>4</sup>
</center><p>
where M and B are positive constants
(Peskin &amp; Schroeder EQs 2.6, 2.45, 20.111, and 20.130; 
Greiner &amp; Müller EQs 4.14 and 4.15 after correct their wrong signs and 
inappropriate squarings).
Here B&gt;0 represents the self-interaction of the Higgs field.
(Physically, B&#8776;0.1291.)
The fact that M&gt;0 is because the Higgs boson has positive mass M&#8776;125 GeV.
Now assume a static field so that
|d&#966;/dt|=0, assume spherical symmetry, 
and demand that the action (i.e. integral of L over all of 3-space)
be stationarized. 
This yields the following differential equation obeyed by a spherically-symmetric 
time-independent Higgs field &#966;
in vacuum:
</p><center>
 M<sup>2</sup>r&#966;(r) 
- 2Br&#966;(r)<sup>3</sup>
- 2&#966;'(r)
- r&#966;''(r)
= 0.
</center><p>
(This is just the 
<a href="http://en.wikipedia.org/wiki/Euler%C3%A2%E2%82%AC%E2%80%9CLagrange_equation">Euler-Lagrange equation</a>
divided by 2r, assuming radial dependence of &#966;.)
Three exact solutions of this equation are
<nobr>&#966;=0</nobr> and
<nobr>&#966;=±(2B)<sup>-1/2</sup>M,</nobr>
i.e. constant Higgs fields.  In the self-interacting case
B&gt;0, the nonzero solutions are the stable ones,
leading to the well known prediction that
a constant-value Higgs field pervades the universe,
with vacuum expectation value 
<nobr>&#966;&#8776;246 GeV</nobr>.
</p><p>
In the no-self-interaction case B=0, a nonconstant exact solution valid when r&gt;0
is
<nobr>&#966;(r)=Kexp(-Mr)/r</nobr>, where K is any constant;
this is a "Yukawa potential," which when r&#8594;0+ becomes asymptotically
the same as the Coulomb potential <nobr>K/r.</nobr>
</p><p>
Because when B&gt;0 the differential equation becomes nonlinear,
I am unable to write down a closed form
nonconstant solution in the self-interacting case.  But I am able to
obtain its asymptotics via the "method of dominant balance" in the regime r&#8594;0+.
<b>The answer</b> is
</p><center>
<nobr>&#966;(r) &#8764; Kr<sup>-1</sup>ln(r)<sup>-1/2</sup></nobr>
 &nbsp; where &nbsp;
<nobr>K=±(-4B)<sup>-1/2</sup>,</nobr>
</center><p>
which causes the sum of the last three terms on the left hand side
of the differential equation to cancel at the topmost
asymptotic order
<nobr>r<sup>-2</sup>ln(r)<sup>-1/2</sup></nobr>
for any K, and also at the next order
<nobr>r<sup>-2</sup>ln(r)<sup>-3/2</sup></nobr>
if
<nobr>K=±(-4B)<sup>-1/2</sup>,</nobr>
leaving a term of order
<nobr>r<sup>-2</sup>ln(r)<sup>-5/2</sup></nobr>
uncanceled.  (This has ignored the first
term in the differential equation because it is
relatively neglectible when r&#8594;0+.)
<!-- file higgsfield2 gives details of dominant balance -->
One may verify that no other nontrivial asymptotics of the form 
<nobr>Kr<sup>P</sup>ln(r)<sup>Q</sup>lnln(r)<sup>S</sup></nobr>
are permitted, for any other constants (P,Q,S).
<!--
Also, in the opposite regime r&rarr;&infin; the usual kind of exponentially vanishing
Yukawa solution should be valid, except unstable, the consant slution instead should be 
approached. -->
</p><p>
There are two key things to note about this.  First, the singularity when r&#8594;0+ is
slightly (but ultimately infinitely) <i>weaker</i> than a Coulomb singularity,
thanks to the log factor.
Second, the Higgs self-interaction, acting via the nonlinear term
in the differential equation, forces K to be a <i>unique</i> value.
This behavior is quite unlike Coulomb potential where a larger point charge yields a larger
proportionality constant K.  Here, no matter what the Higgs "charge" of a point particle
at the origin tries to be
(this "charge"
actually is proportional to the part of its rest mass that is attributable to the Higgs
mechanism), the asymptotics of the Higgs field when r&#8594;0+  
remain the <i>same</i> at leading order!
</p><p>
As an immediate consequence of this asymptotic solution,
we first see that 2-particle "hydrogen" with the interparticle 
attraction caused by the Higgs force, should be stable against collapse
no matter how great the "Higgs charges"
on each particle.
Second, we see that the Dysonian "cloud collapse" also is not a threat, because a
tiny ball of N equal "Higgs charges," in the limit of tinyness, presumably will exert
at leading order the same effect as just <i>one</i> such "charge" (no matter what the value of N).
Thus we do not need to worry about the "overwhelmingly large attraction energy" 
of order N<sup>2</sup>
of an N-particle cloud, since once the cloud has shrunk enough it
actually will at most be proportional to N.
</p><p>
<b>(End of Higgs detour.)</b>
</p><p>
The skeptical reader will point out that, while she's now convinced for QED,
we still only have supplied <i>one</i> nonrigorous
argument for divergence for quantum <i>chromo</i>dynamics.
And our same QCD argument has been considered in more detail by Khuri 1981 
(see his theorem 1) and t'Hooft 
in his lecture in Zichichi 1979; they
indeed deduced that it forced a "horn shaped" singularity-free 
region of the &#945;<sub>s</sub> complex plane, behaving near the origin of
that plane (say &#945;<sub>s</sub>=x+iy) qualitatively like
|y|&#8804;x<sup>2</sup> for x&#8805;0.   See figure <a href="https://dl.dropboxusercontent.com/u/3507527/FIG2hornskein">2a</a>. 
</p>
<a name="FIG2hornskein"><img src="WarrenSmithQED131123_files/HornAndSkein.png" alt="fig2" width="100%"></a>
<p>
We now satisfy such a reader by supplying second and third divergence arguments.
The second argument is merely to cite Khuri &amp; Ren 1989.
The third argument, which happily is highly independent of the other two and,
like Dyson's, very conceptually simple, is new.
It relies on the currently standard belief 
that at any separation above &#8776;10<sup>-15</sup> meters,
each quark strongly attracts the nearest anti-colored antiquark,
with a force that is believed to remain approximately constant 
(&#8776;10<sup>4</sup> newtons) <i>independent</i> 
of their separation.  This causes "color confinement" 
(Greensite &amp; Olejnik 2003, Nishijima 1996)
– 
it is impossible to macroscopically-isolate
color (e.g. any 
attempt to pull a green quark alone out of some object, will just cause the creation of
appropriate antiquarks nearby that neutralize 
color).  This in turn causes the existence of lots of strongly-bound
colorless multi-quark assemblies such as protons, pions, and iron nuclei.
The <i>reason</i> for this remarkable constant-force long-distance behavior, is
the presumed existence of a <b>"skein"</b> or <b>"rope"</b> 
of virtual gluons joining the two quarks,
see <a href="https://dl.dropboxusercontent.com/u/3507527/FIG2hornskein">figure 2b</a>.
This happens because in QCD the force-carriers (gluons) can interact, 
i.e. emit and absorb other gluons.  In contrast, in QED, photons are noninteracting,
and we get inverse-square force falloff at large distances; there is no skein, there
are just individual "strands" going in all directions  which get sparser the further
away we go.  Now if &#945;<sub>s</sub>  were negative, then
same-colored quarks would attract instead of repel, while red and antired pairs
would then repel instead of attract, and more generally every kind
of gluon-pair or quark-pair which used to attract/repel will now repel/attract.
In particular, the gluons emitted by some quark which used to mutually attract would
with negative &#945;<sub>s</sub> 
instead interact <i>repulsively</i>.
</p><p>
<small>
See Davies 1992 for layman-readable
discussion of the attraction and repulsion
laws for different color combinations. Like colors repel (green+green);
opposite colors attract (red+antired) and unlike colors (e.g. red+green)
can either attract or repel but on average attract.
A green quark could emit a green-antired gluon thus
converting into a red quark.  That could emit either a red-antigreen gluon 
(thus converting back into
a green quark) or a red-antiblue gluon (converting into a blue quark).
Either way, the two emitted gluons would then mutually attract.
This all is with the physical sign &#945;<sub>s</sub>&gt;0.
</small>
</p><a name="quarkfreeing"></a><p>
Thus changing the sign of
&#945;<sub>s</sub>
would get rid of skeins and instead cause a scenario more like in 
figure <a href="https://dl.dropboxusercontent.com/u/3507527/FIG2hornskein">figure 2c</a>.
There would no longer be color confinement, and opposite-color quarks instead
would exhibit Coulomb-like repulsion.   
Same-color quarks would attract, at least if close by.
This would yield drastically different physics, and the transition would be sudden.
That is, as we slowly lower &#945;<sub>s</sub> all bound multiquark
assemblies and their spectra of excited states all still would 
exist; they would merely get larger and decrease in mass.  And color confinement would
still hold, it is just that the size of the "prison cell" for color would increase.
But <i>suddenly</i>
as &#945;<sub>s</sub> crossed zero, all those bound states would <i>vanish</i>.
This indicates that their eigenenergies, as a function of &#945;<sub>s</sub>,
must be <i>nonanalytic</i> at &#945;<sub>s</sub>=0.  (The situation is analogous
to the quartic anharmonic oscillator analysed by Bender &amp; Wu, 
and discussed next section, when g crosses 0.)
Hence any Maclaurin series expansion of them, must diverge for every
&#945;<sub>s</sub>&#8800;0.  
</p><p>
And also, suddenly upon changing the sign of
&#945;<sub>s</sub> free quarks would become a possible state.
I have not seen these observations before, although
Khuri &amp; Ren did seem to agree that with changed-sign &#945;<sub>s</sub>
there would no longer be "asymptotic freedom," which is an observation of the same general 
flavor.
</p><p>
To remind the reader about how bad "essential singularities" are, we note 
<b>"Picard's Great Theorem"</b>
which states that in any neighborhood of any essential singularity of an analytic function F(z),
<i>every</i> complex value (with at most one exceptional "missing" value) 
occurs an infinite number of times
as an output of F(z).  That is exactly the sort of behavior we do not want from a
"useful predictive physical theory."  However, essential singularities can behave well
if we restrict the ways one is allowed to approach them.  For example exp(-z<sup>-2</sup>)
is well behaved on any line through the origin
with angle&lt;&#960;/4 to the real axis, despite its essential singularity at z=0.
Khuri's "horn shape" means a very severe restriction indeed is necessary; only on a 
<i>single</i> line,
the real axis, can we hope for good behavior.
</p><p>
This argument can, more speculatively, be extended further.
Consider perturbing &#945;<sub>s</sub> away
from its present (positive real) value 
by adding a small <i>imaginary</i> number.  In that case, every "strand" in the "skein"
in figure <a href="https://dl.dropboxusercontent.com/u/3507527/FIG2hornskein">figure 2b</a>,
would add some small amount to the complex phase angle of the wave function.
The net result, after a far-enough distance, would be 
complete rotatory-randomization of that phase angle, which would be equally likely to
cause it to be negated (causing a repulsive interquark force)
or left the same (attractive).  The net effect would be to greatly diminish
the long-range interquark force.  I think
the falloff rate of the force would be approximately inverse square,
which would be easily sufficient to permit free quarks to
exist.  That would be a drastic change in physics.   This argument
suggests the possibility that even moving
an <i>unboundedly small</i> amount away from the positive-real 
&#945;<sub>s</sub> axis
would be enough to allow free quarks, which would be a huge
continuum infinity of
new allowed states which simply would <i>vanish</i>
when &#945;<sub>s</sub> hit the real axis.
This if so would cause generic physical predictions in QCD
to be, as a function of
&#945;<sub>s</sub>,
<b>nonanalytic <i>everywhere</i> on the positive real axis</b>.
</p><p>
That's devastating. 
I consider this a plausible possibility, although it apparently has not been 
considered before by (perpetually optimistic) physicists.
</p><p>
In particular, an immediate corollary would be that it would
prevent generic QCD Taylor series predictions from being "LeRoy/Borel summable"
regardless of the basepoint &#945;<sub>s</sub> (provided said basepoint was real and
nonnegative).
And we <i>already know</i> from the "horn-shaped" claim alone, combined
with the "Borel polygon"
theorem (Hardy 1949), that QCD <i>Maclaurin</i>
series predictions (i.e. with basepoint &#945;<sub>s</sub>=0)
must generically diverge for all &#945;<sub>s</sub>&#8800;0 and with this divergence 
<b>un-remediable by LeRoy/Borel summation.</b> That is:
</p><p>
<b>"Horn-Shape&#8658;No LeRoy/Borel &#8721;" Lemma:</b>
If a function F(z) is defined and analytic within some region R of the complex z-plane,
whose boundary-curve &#8706;R features a zero-angle cusp at the origin z=0
("horn shaped"), and if in any neighborhood of z=0 on &#8706;R there are an 
infinite number of points
of nonanalyticity of F(z), then:
F(z) is not deducible from its asymptotic Maclaurin series via "LeRoy/Borel summation."
(<b>Proof sketch:</b> follows from the Borel polygon theorem in Hardy 1949.)
</p><p>
We now go further.  
I claim there is good reason to believe that, if the cusp has "power law"
shape asymptotically near z=0 and there are an infinite number of poles among those
singularities 
(the above-cited authors seem to be claiming both in
the case of QCD), then <b><i>no</i></b> possible series summation process
based solely on the coefficients of F's Maclaurin series, can hope to reconstitute F(z).
The reason is as follows.  Consider the partial-fraction expansion 
G(z)=&#8721;<sub>k</sub> (z-z<sub>k</sub>)<sup>-1</sup>
where the sum is taken over all F's pole-locations z<sub>k</sub>.
This sum has residue=1 at each pole, whereas F(z) probably does not.
Hence consider F(z)=G(z)H(z) where we shall assume H(z) is comparatively well-behaved.
If H(z)=1, then F(z) would exhibit wild oscillations as z&#8594;0 within R,
causing F(z) not to have a limit value F(0), nor a derivative value F'(0),
and not to have a Maclaurin series.
If |H(z)| falls toward 0 but not very quickly (not as quickly as some power of |z|)
then this will not be good enough to cause F(z) to have a Maclaurin series, although
it could be enough to allow a limit value F(0) and a finite number of derivatives of F at 0
to exist (for a suitable definition of these things as 1-sided limits).
If |H(z)| falls toward 0 more quickly than any power of |z| as z&#8594;0 within R,
then that problem is solved, and F(z) has a Maclaurin series.  However, this series will
be 
0+0z+0z<sup>2</sup>+0z<sup>3</sup>+...
If we now instead assume F(z)=G(z)H(z)+K(z)  for some well-behaved K(z)
then any attempt to reconstitute F from its Maclaurin series alone, would give 
K(z), not F(z).
This all has not been a theorem because the term "well behaved" was undefined.
It is possible to construct artificial H(z) which drop very rapidly as z&#8594;0 along &#8706;R but
not as z&#8594;0 along the centerline of the cusp (interior to R).  But such H(z)
seem very unnatural.   In view of this, <b>I consider it likely
that generic QCD predictions are functions of &#945;<sub>s</sub> <i>not deducible</i>
from their Maclaurin series</b>.
</p><p>
Concerning the above argument, the reader may enjoy considering, as a <b>concrete example,</b>
the 2-branched function
G(z)=tanh(x) where x=1±(1-z<sup>-1</sup>)<sup>1/2</sup>.
This function has an infinite number of poles located in any neighborhood of the origin z=0,
and all G's poles lie on a curve featuring an asymptotically-power-law-shaped cusp at z=0.
G(z) oscillates an infinite number of times
with unboundedly increasing amplitude as z&#8594;0 along the positive real axis
and therefore has no limit value.  But for F(z)=G(z)exp(x<sup>2</sup>), the 
oscillations get damped out, causing F(z) to have a limit value and indeed causing
every derivative of F(z) to have a limit value – except all of these limits are 0.
This F(z) is analytic throughout a horn-shaped region like in 
<a href="https://dl.dropboxusercontent.com/u/3507527/FIG2hornskein">figure 2a</a>, and
indeed is defined everywhere (albeit it has two branches) in the complex z-plane
if &#8734; is regarded as a legitimate complex number and if we agree F(0)=0.
It evidently is impossible for any series summation process to deduce F(z) from its
Maclaurin series alone [e.g. 2F(z) would be just as valid a deduction].
</p><p>
In a nutshell, it seems highly likely that either this argument, or the preceding
one about perturbing &#945;<sub>s</sub> by i&#949;, or both, are true
(although both arguments are nonrigorous). 
If so, the entire vague hope and dream of the last 60 or so years of physicists, that somehow,
some way, a divergent series summation process could be invented to define QFTs such as QCD
in terms of Feynman diagrams alone... is now <b>dead.</b>
</p><p>
<b>Conclusion:</b>
We've reviewed Dyson's argument, overcome the top two criticisms of it, and
supplied a new independent argument yielding the same conclusion.  We've also mentioned
several independent arguments within QCD instead of QED. 
(We've also pretty much destroyed Cvitanovic 1977's quenched-QED convergence
conjecture which had offered partial hope for an escape hatch from Dysonian divergence.)
While none of these arguments are rigorous,
their net effect is very convincing.  All conclude that
in both QFTs, generic physical predictions, when expanded as Maclaurin series
in the QED coupling constant &#945; or strong-force coupling constant  
&#945;<sub>s</sub>,
must yield series that diverge everywhere (except at the useless single point &#945;=0).
Furthermore, we've supplied new arguments which suggest that in QCD, the whole
positive-real
&#945;<sub>s</sub>
axis is a locus of nonanalyticity; and/or that <b>no</b> possible series-summation process
can hope to reconstitute QCD predictions from their Maclaurin series in &#945;<sub>s</sub>.
Although these new arguments are quite similar in spirit to Dyson's original, they
wreak a far greater amount of devastation.
</p>

<a name="ratediv"></a>
<h3>6. What is the rate of divergence? (And some illuminating model functions) </h3>
<blockquote>
The precise form of the divergence of [power series in] 
perturbation theory is controlled by the tunneling rate formula.
<br>&nbsp;&nbsp;<b>–</b> 
Hagen Kleinert, §17.10 of his book 
<i>Path Integrals in Quantum Mechanics...</i>, World Scientific 2009.
</blockquote>
<p>
This section provides evidence for the following conjectures (ordered in decreasing order of how 
much I believe them):
</p>
<ol>
<li>
QFTs such as the "standard model,"
QED, and QCD generically output power series expansions (in powers of &#945;, for QED)
which diverge for all complex &#945;&#8800;0 and such that the coefficient of &#945;<sup>N</sup>
grows (on an infinite subsequence of positive density in the positive integers N)
roughly like (JN)!S<sup>N</sup> for some nonzero constants J and S with 1/4&#8804;J&#8804;1
when N&#8594;&#8734;.
</li><li>
These divergent series often <i>cannot</i>
be summed via the Pade or LeRoy/Borel methods (and/or those
sums will provide incorrect answers).
</li><li>
Indeed, often the physically-true functions these series attempt to approximate, are 
nonanalytic everywhere on the real axis, in which case any Taylor series expansion based anywhere,
will fail to converge to the true function value anywhere else.
</li>
</ol>
<a name="tabpertdivrates"></a>
<table><caption>
<b>Table 3:</b> Conjectured (and sometimes proven) asymptotic behaviors.  [K,A,S,P
represent non-negative constants 
independent of N but which may change from row to row of the table.]
</caption>
<tbody><tr bgcolor="pink">
<th width="25%">Quantity</th>
<th>Conjectured asymptotic behavior (N&#8594;&#8734;)</th>
<th>Who &amp; when</th>
<th width="28%">Reason why</th>
</tr><tr>
<td>Coefficient
C<sub>N</sub> of &#945;<sup>N</sup> in generic QED power series</td>
<td align="center">C<sub>N</sub>=<nobr>&#915;(N/2)</nobr><nobr>(-S)<sup>N</sup></nobr>N<sup>P</sup>A<nobr>[1+O(N<sup>-1</sup>)]</nobr></td>
<td>Balian, Itzykson, Zuber, Parisi 1978, building on work of S.Adler 1972-1974
claimed 
<nobr>0&lt;S&lt;&#960;<sup>-1</sup>&#8776;0.31831;</nobr>
Bogomolny &amp; Fateyev 1978
claimed
<nobr>S=3<sup>-3/4</sup>&#960;<sup>3/2</sup>2&#8776;4.88555</nobr>
and <nobr>P=0</nobr>; work incomplete 
and a larger paper with "details" was promised to be "published elsewhere" but apparently 
never was.
</td><td>
Complicated and details dubious; attempt to sum over all possible quantum-field configurations 
using "saddlepoint method" to estimate infinite-dimensional integral, plus several
conjectures to fill gaps in argument and to live with gauge-invariance. 
The "saddlepoint" is a semiclassical field solution (Instanton/antiInstanton).
Note contradiction about value of S.</td>
</tr><tr bgcolor="aqua">
<td>Coeff. C<sub>N</sub> of &#945;<sup>N</sup> in generic QED power series</td>
<td align="center">C<sub>N</sub>&#8776;&#915;(2N/3)(-S)<sup>N</sup></td>
<td>Me <a href="#dysoncorrect">here</a></td>
<td>Dysonian collapse argument corrected to apply to <i>fermions</i> (exclusion principle).
Note results contradict those from infinite-dimensional integration approach.
</td>
</tr><tr>
<td>Coeff. C<sub>N</sub> of &#945;<sup>N</sup> in generic <i>scalar</i>-QED power series
("electrons" now spin=0 charged bosons)</td>
<td align="center">
C<sub>N</sub>=&#915;(N)<nobr>(-S)<sup>N</sup></nobr>N<sup>P</sup>A<nobr>[1+O(N<sup>-1</sup>)]</nobr></td>
<td>Itzykson, Parisi, Zuber 1977 found S&#8776;0.0808; 
Buchvostov &amp; Lipatov 1977 argued IPZ's saddlepoint was subdominant hence their S
must be too small; B&amp;L's revised apparently genuinely maximal saddlepoint 
gives S&#8776;0.0878.</td>
<td>Attempt to sum over all possible quantum-field configurations 
using "saddlepoint method" to estimate infinite-dimensional integral.
Less dubious than for spin=1/2 QED and now does not contradict Dyson argument.</td>
</tr><tr bgcolor="aqua">
<td>Coeff. C<sub>N</sub> of &#945;<sup>N</sup> in generic <i>boson</i>-QED power series
("electrons" are spin=0 or spin=1 bosons; the W-boson actually exists 
with charge=±1 and spin=1 so this is physically relevant... except 
that electroweak QFT is not the same as a naive person would guess based on that,
and spin=0 "scalar QED" also differs from naive guesses, see Rohrlich 1950.)</td>
<td align="center">C<sub>N</sub>&#8776;&#915;(N)<nobr>(-S)<sup>N</sup></nobr></td>
<td>Dyson 1952</td>
<td>Dyson invoked a "collapse" argument for bosonic QED with <i>imaginary</i> charge to
argue the power series must diverge for all complex &#945;&#8800;0.  
Kleinert connects the
rate of growth of 
C<sub>N</sub> to estimates of the decay lifetime (tunneling rate) 
into this collapse process;
see also Jaffe 1965 for rigorous results in lower space-dimensions.
</td>
</tr><tr>
<td>Coeff. C<sub>N</sub>
of g<sup>N</sup> in perturbation expansion of ground state energy
of "1D power-law anharmonic oscillator" (Schrödinger equation with potential 
x<sup>2</sup>+g|x|<sup>P</sup>).
</td>
<td align="center">C<sub>N</sub>&#8776;[(P/2-1)N]!(-S)<sup>N</sup>N<sup>K</sup>A
</td>
<td> Bender &amp; Wu 1973 examined P=4; Bender &amp; Wu 1971
and Bender 1978 considered even-integer P&#8805;4
but I believe his techniques should enable treatment of any rational P&gt;2;
Dolgov &amp; Popov 1978 extended their results to arbitrary real P&gt;2.
They find:
<nobr>S=&#915;(2P/[P-2])<sup>P/2-1</sup>&#915;(P/[P-2])<sup>2-P</sup>/4</nobr>,
<nobr>K=-1/2.</nobr>
<!--In EQ2 of BenderWu 1976, change x to y where y=sqrt(2)x, then 2lambda=g, 2E[BW]=E[e],
and the problem becomes (-d^2/dy^2 + y^2 + g*y^(2K) = E[me]
then the coeffs A[n] in EQ4 get multiplied by overall constant factor which we ignore,
plus 2^n factor arises from the change lambda=g/2.
--
A.I.Vainshtein in a 1964 Russian report and independently
Alvarez considered P=3;
Jentschura, Surzhykov, Zinn-Justin 2009 considered arbitrary integer P>2
but all not using absolute value.
--
ss := (P) -> (1/4)*GAMMA(2*P/(P-2))^(P/2-1)*GAMMA(P/(P-2))^(2-P);
#ss(3)=sqrt(30)/4=1.36; ss(4)=3/2=1/5; ss(5)=1.57; ss(6)=16/Pi^2;=1.62; ...
#ss(100)=1.83; ss(1000)=1.84; ss(infinity)=exp(2)/4=1.84.
#ss(2+x) approaches 1 as x goes to 0+.
-->
</td>
<td>Bender has over the years (re)derived this using
WKB, diagrammatic, and difference-equation methods.
Some of his results were re-obtained using rigorous methods by
Barry Simon and collaborators.
</td>
</tr><tr bgcolor="aqua">
<td>Coeff. C<sub>N</sub>
of g<sup>N</sup> in perturbation expansion of ground state energy
of "1D exponential-law anharmonic oscillator" [Schrödinger equation with potential 
x<sup>2</sup>+g·exp(|x|<sup>P</sup>K).]
</td>
<td align="center">C<sub>N</sub>&#8776;exp(SN<sup>A</sup>).
<br>Note this grows faster than (JN)! for any fixed J&gt;0.
</td>
<td> 
Dolgov &amp; Popov 1978:
<nobr>A=2/(2-P)</nobr>,
<nobr>S=(A-1)<sup>A-1</sup>(K/A)<sup>A</sup></nobr>
for 0&lt;P&lt;2.  If P&#8805;2 then no expansion in powers of g is possible.
</td>
<td>Dolgov and Popov's approach was based on converting the
Schrödinger equation to a Riccati equation.
</td>
</tr><tr>
<td>
<a href="http://oeis.org/A005413">Number</a>
of order-N QED Feynman diagrams for computing C<sub>N</sub> for
magnetic moment of electron</td>
<td align="center">N!2<sup>N</sup>N<sup>1</sup><nobr>(4/&#960;)·</nobr>
<nobr>[1-5N<sup>-1</sup>/2-3N<sup>-2</sup>/8+...]</nobr></td>
<td>Cvitanovic, Lautrup, Pearson 1978; Riddell 1953</td>
<td>Combinatorics, generating functions, saddlepoint asymptotic analyses</td>
</tr><tr bgcolor="aqua">
<td>
<a href="http://oeis.org/A005416">Number</a>
of order-N <i>quenched</i>-QED Feynman diagrams for computing C<sub>N</sub> for
magnetic moment of electron</td>
<td align="center">
2<sup>N</sup> &#915;(N+3/2) &#960;<sup>-1/2</sup> ·
<br>
[1±O(N<sup>-1</sup>)]
</td>
<td>Cvitanovic 1977; Martin &amp; Kearney 2010;
Bogomolny &amp; Kubyshin 1981.</td>
<td>Combinatorics, generating functions, saddlepoint asymptotic analyses</td>
</tr><tr>
<td>Coeff. C<sub>N</sub> of &#945;<sup>N</sup> in generic <i>quenched</i>
QED power series</td>
<td align="center">
 A<sup>N</sup> N<sup>P</sup> N! D ·
<br>
[1±O(N<sup>-1</sup>)]
</td>
<td>Bogomolny &amp; Kubyshin 1981
where A,P,D are constants with AD&#8800;0.
This is claimed for QED where <i>only</i> diagrams with exactly k closed electron loops
are used, for <i>any</i> k&#8805;0 which stays fixed as N grows. (k=0 is the "quenched" case.
A,P,D depend on k.)
</td>
<td>Extension of the techniques
in the top line of this table (whose underlying idea is attributed to Lev Lipatov)
to allow consideration of the fixed-k sub-series.</td>
</tr><tr bgcolor="aqua">
<td>Coeff. C<sub>N</sub> in generic power series
for  "&#955;&#966;<sup>3</sup> theory"</td>
<td align="center">|C<sub>N</sub>|&#8805;S<sup>N</sup>N<sup>N/2</sup></td>
<td>Hurst and Thirring proved lower bound in early 1950s.
Houghton, Reeve, Wallace 1978 conjectured true behavior was
<nobr>
C<sub>N</sub>=N!(-S)<sup>N</sup>N<sup>P</sup>A[1+O(N<sup>-1</sup>)]
</nobr>
</td>
<td>Hurst and Thirring just counted Feynman diagrams,
proved bounds on their values, and argued for this 
particular QFT diagrams could not cancel since
values all positive.
</td>
</tr><tr>
<td>Number of disjoint nontrivial gauge-invariant classes of QED Feynman diagrams at
order N for generic QED series
</td>
<td align="center">
&#8805;
 exp(-15/4)96<sup>-N/4</sup>N! / <br>
[(N/2)!·(N/4)!]
<br>
for an infinite set of integer N&gt;0
</td>
<td>
Me, see <a href="#giclasscountbound">end</a> of <a href="#expleqd">§8</a>.
</td>
<td>
Combinatorics,
crude lower bounding techniques
(this bound is surely improvable, but
good enough to show superexponential growth)
</td>
</tr><tr bgcolor="aqua">
<td>Number of disjoint nontrivial gauge-invariant classes of QED Feynman diagrams at
order N for generic QED series
</td>
<td align="center">
&#8805;
X<sup>X</sup>
where <nobr>X&#8805;[1-o(1)]N/2</nobr>
<br>
for an infinite set of integer N&gt;0.
<nobr>
</nobr></td>
<td>
Me, see <a href="#giclasscountbound">end</a> of <a href="#expleqd">§8</a>.
</td>
<td>
Improvement of preceding line using cleverer combinatorics.
</td>
<!--
</tr><tr>
<td>Quantity</td>
<td>Asymptotic</td>
<td>WhoWhen</td>
<td>Reason</td>
</tr>
-->
</tr><tr>
<td>Gauge-invariant
value of a certain Nth-order renormalized-QED diagram arising when computing
electron magnetic moment
</td>
<td align="center">
(N-1)! (6&#960;)<sup>-N</sup> · <br>
exp(-10/3) 3 [1 + O(N<sup>-1</sup>)]
</td>
<td>Lautrup 1977
</td>
<td>
Expressed his Nth-order diagram as 1-dimensional definite
integral, then applied "saddlepoint method"
to determine large-N asymptotics
</td>
</tr>
</tbody></table>
<p>
Some simple systems
somewhat resembling QFTs 
– simple enough for full understanding of their perturbation theory –
are the quantum <i>anharmonic oscillators</i> (<b>AnHO</b>s).
If you additively perturb the usual simple harmonic oscillator potential x<sup>2</sup>
by gx<sup>4</sup>, where g is small, you get the quartic AnHO.
Other functions besides x<sup>4</sup> could also be considered.  
The point is that if g is negative, then the potential becomes
bottomless and no longer contains a spectrum of bound states.  If g is positive the spectrum still
is infinite, discrete, and all-positive.   This is just like Dyson's somewhat speculative
picture of QED when &#945; crosses 0.  By Dyson's same reasoning we would conclude that,
e.g, the AnHO ground state energy should be expressible as a power series in g, which <i>diverges</i>
for all g&#8800;0.  This is, in fact, true.  
</p><p>
<b>Optimism:</b>
Simon et al were able to prove that the series expansion of each energy level in the quartic
AnHO was "LeRoy/Borel summable" (see p.147 of Hardy) provided 
analytic continuation is employed to define the Borel integrand.
They also proved it "Pade summable," i.e. the diagonal sequence of Pade approximants
to the power series, converges to the correct energy level.
</p><p>
Another interesting parallel between the AnHOs and QED is the fact that the AnHO
with the potential x<sup>2</sup>+g(ix)<sup>P</sup> (for real P&#8805;2)
– which is nonreal complex unless P is an even integer –
is a PT-symmetric (generally nonHermitian) quantum hamiltonian.
It appears (numerically) to have an infinite discrete spectrum of all-real energy eigenvalues!
[For some rigorous proofs of "infinite discrete spectrum of all-real energy eigenvalues"
for related problems, see Dorey et al, and some of the papers cited by 
Grecchi, Maioli, Martinez 2009.]
In the <i>cubic</i> case P=3
this has been proven by Grecchi, Maioli, Martinez 2009. 
They also proved Pade summability (and cited proof by Caliceti et al of Borel
summability) for the perturbation series. 
This series had been found by Alvarez 1988.
</p><p>
<b>Pessimism:</b>
Graffi &amp; Grecchi 1978 showed that the AnHO with potential 
x<sup>2</sup>+g|x|<sup>P</sup> for each even integer P&#8805;6,
yields perturbative series for the ground state energy
whose Pade approximants do <i>not</i> converge to the energy eigenvalues;
and the Dolgov-Popov exponential AnHO in the table (obviously) has perturbation series immune
to LeRoy/Borel summation.
</p><p>
<b>Grounds for suspecting pessimism wins:</b>
Of those, I regard QED's "Dyson collapse" as 
more-analogous to the "bad" Borel and Pade non-summable
AnHOs, than it is to the "good" ones.  That is because the Dyson collapse,
after an initially improbable event (spontaneous generation of a large number of 
nearby electron-positron pairs) only requires them to travel a finite distance to get an infinite
energy release.   In the "good" quartic and cubic AnHOs, the travel distance is infinite.
In the "bad" exponential AnHO, it also is infinite, but a good deal less so.
</p><p>
<b>Grounds for suspecting optimism wins:</b>
Kleinert's connection (quote at start of this section)
between tunneling rate and growth-rate of the perturbation series |coefficients|
suggests that what matters is not the <i>distance</i> a particle needs to travel
to fall infinitely far, but rather the <i>rate</i> at which AnHO particles
initially trapped in the quadratic potential well will tunnel through
the barrier (with negative g) to reach lower energies.
</p><p>
<b>The tunneling rate / divergence rate connection</b> 
Kleinert alluded to has a simple explanation.
Consider (for concreteness) the quartic AnHO
with potential x<sup>2</sup>+gx<sup>4</sup>.   If g is made slightly negative, then
there is a nice potential well near x=0 in which many bound states will live... except
that the well does not extend infinitely far.   At a large value 
|x|&#8776;|g|<sup>-1/2</sup> the potential "wall" peaks, then drops toward negative
infinity.   There will thus be a small rate of "tunnelling" through the wall.
This physically can be described by assigning to each bound state, instead of its
spectral frequency as usual, a slightly <i>complex</i> frequency.  That is
the real frequency, corresponding to a real energy, of a bound state is
perturbed by the addition of a complex number with small real <i>and imaginary</i> parts.
This imaginary part describes the exponential-in-time <i>disappearance</i> of the bound state
(in a way that does <i>not</i> preserve probability) into the infinite pit –
in other words, the "tunneling rate."   Now due to the usual 
<a href="http://en.wikipedia.org/wiki/WKB_approximation">WKB</a>
analysis of
tunneling for the nonrelativistic Schrödinger equation, this tunneling rate declines
<i>exponentially</i> as a function of the product WH<sup>1/2</sup>
of the barrier width W and the square root 
of its height H (times a constant depending on the barrier shape) in the limit where 
this product is large.   
This dependence, note, is an essential singularity in
the complex g plane.   
Equivalently the imaginary part of the bound state energy is proportional to
WH<sup>1/2</sup>.   (The exact proportionality constant as well as lower order terms 
can be worked out using the WKB method; we have only described the crudest possible form of that
method.)
If now g instead is made small and <i>positive</i>
then the barrier and infinite pit both vanish
and we just have an all-real infinite discrete spectrum, but my point is that the same 
perturbation series applies to describe the small perturbations to the harmonic oscillator spectral
energies.   This explains the connection.
</p><p>
For <i>relativistic</i> tunneling (i.e. for which the energy barrier heights are larger
than the mass of the particle), the Schrödinger equation needs to be replaced 
by other equations; all the reader will need to know is that the tunneling rate now declines
exponentially as a function of WH, not WH<sup>1/2</sup>, asymptotically when the former is large.
</p><p>
The original (bosonic electron) Dyson collapse argument involves an energy barrier of height 
of order &#945;<sup>-1</sup>, when |&#945;|&#8594;0, to create a number of
this order of electron-positron pairs.
The travel distance of these particles could be argued to be
of order 1 in QED units, i.e. order 1 electron Compton wavelengths, <i>or</i>
arguably we should instead use the
Bohr radius (which is to say, order &#945;<sup>-1</sup> QED length units)
to rerrange themselves into a configuration suitable for collapse.
This suggests that
the breakdown of bosonic-QED when &#945; is small and negative 
involves a tunneling rate whose logarithm
presumably is of rough order 
<nobr>-|&#945;|<sup>-1</sup></nobr>
or 
<nobr>-|&#945;|<sup>-2</sup>.</nobr>
</p><p>
In the P-power Schrödinger-equation AnHO model, according to the WKB approximation
considering the distance we need to tunnel is of order
|g|<sup>-1/(P-2)</sup>
and the barrier height is of order
|g|<sup>-2/(P-2)</sup>
should yield a tunneling rate whose  logarithm
presumably is of rough order <nobr>-|g|<sup>-2/(P-2)</sup></nobr>,
in the limit |g|&#8594;0.
</p><p>
The power-P AnHO tunneling rate matches that postulated for bosonic QED if
P=4 or P=3 respectively.  That would predict QED with boson electrons would 
feature series divergence like N! or (N/2)! respectively for the &#945;<sup>N</sup>
term in the series.
</p><p>
<i>Repaired</i> Dyson for fermionic-QED (we shall 
<a href="https://dl.dropboxusercontent.com/u/3507527/dysoncorrect">soon</a> discuss that) 
has energy barrier height of order |&#945;|<sup>-3/2</sup>,
hence a tunneling rate whose logarithm
presumably is of rough order  -|&#945;|<sup>-3/2</sup>.
or -|&#945;|<sup>-5/2</sup>.
This matches the tunneling rate for the P-power AnHO when P=10/3 and P=14/5.
That would predict QED with fermion (spinor) electrons should 
feature series divergence like (2N/3)! or (2N/5)! 
respectively for the &#945;<sup>N</sup>
term in the series.
</p><p>
At least some of these matched AnHOs enjoy LeRoy/Borel and/or Pade-summable perturbation series.
</p><p>
<b>Tunneling Sanity check:</b>
Our crude width and height-based estimates of the logarithm of the tunneling rate 
also allow one to deduce (correctly) the
asymptotic behavior of the
Nth series |coefficient| for a P-power AnHO model: it grows roughly like
<nobr>[(P/2-1)N]!</nobr> up to factors simply-exponential in N.
(This not only is exactly right, it is derived with 100× less work than Bender et al
–
who perhaps did not explicitly understand the tunneling/divergence connection –
had to perform.)
</p><a name="dysoncorrect"></a><p>
<b>Corrected (to know about Fermions) version of Dyson's argument:</b>
Dyson's argument contained an oversight.
He forgot to account for the fact that electrons are <i>fermions</i>
which by the Pauli principle cannot co-occupy any state.  This error does not affect
Dyson's conclusion of divergence but does affect estimates of how severe that divergence is.
We can <b>repair</b> this by
noting that the Fermi confinement energy is
of order
<nobr>V<sup>-1/3</sup>N<sup>4/3</sup></nobr>
to trap N noninteracting 
fermions in a volume-V ball (in the
ultrarelativistic N&#8594;&#8734; limit).
</p><p>
Let us be more precise.
Assume at most two electrons (since an electron has spin-up or down)
can be confined per position-momentum "phase space" volume 
h<sup>3</sup> <!-- =(2&pi;&#8463;)<sup>3</sup> -->
and the usual ultra-relativistic formula that the energy
of a particle is c times its momentum.
Then confining N electrons in a volume-V ball forces the volume in momentum-space
to be at least 
<nobr>V<sup>-1</sup>Nh<sup>3</sup>/2,</nobr>
which forces the total confinement energy of the N electrons to be asymptotically 
</p><center>
E<sub>confin</sub> &#8764; 
(&#960;<sup>-1/3</sup>3<sup>4/3</sup>/8) 
V<sup>-1/3</sup>N<sup>4/3</sup> 
hc.
</center><p>
Meanwhile the Coulombic attraction energy (if &#945;&lt;0, and we revert to units with 
c=&#8463;=1) for a uniform ball of charge asymptotically obeys
</p><center>
E<sub>attract</sub> &#8804; 
(N-1)N &#945; V<sup>-1/3</sup> &#960;<sup>1/3</sup>6<sup>2/3</sup>/5 
<!-- This is 6/5 times what it would have been for a hollow sphere charge
of same radius,
Uniform ball of charge of radius=R:
Efield = k*Q * min( 1/r^2 , r/R^3 ).
(1/2) * Integrated Efield^2 =
   k^2*Q^2 * [2*Pi/(5*R) + Pi/R] = k^2*Q^2 * 12*Pi/(5*R) 
where k = 1/(4*Pi*eps0) and k=alpha if use QED units
the qty in [] is
2*Pi*int( (r/R^3)^2 * r^2, r=0..R ) + 2*Pi*int( 1/r^4 * r^2, r=R..infinity)
For a hollow charge energy = k*Q^2/r.
For a uniform charge we get a factor of exactly 1.2=6/5 times the self energy.
-->
</center><p>
(The self energy of a uniform ball of charge is 6/5 times the self energy of a hollow 
charged sphere.)
There are also other effects such as rest-mass energy Nm<sub>e</sub> and
so forth, but they are comparatively negligible when N&#8594;&#8734;.
Hence the total energy is negative if N exceeds about 
<nobr>|&#945;|<sup>-3/2</sup>10<sup>1/2</sup>15/(64&#960;)</nobr>
(which with the physical value of |&#945;| is &#8776;378.5).
Some nonuniform charge distribution might have lower energy, but our argument suffices 
to show an <i>upper bound</i> on the number of electrons needed for a collapse.
(With the optimum distribution a collapse might happen with fewer.)
With "running coupling constants" as in
the "Uehling potential"
(see Greiner &amp; Reinhardt's solved exercise 5.3)
E<sub>attract</sub> would be increased by a factor
depending logarithmically on V (for logarithmic running). This would not alter
the conclusion collapse occurs, but would alter the number of electrons needed:
it would now be proportional to |&#945;|<sup>-3/2</sup>|log(|&#945;|)|<sup>P</sup>
for some fixed power P.
<!--
This instead suggests that divergence ought to start at about the N=255 term,
consistent with a series behaving roughly like 
&sum;<sub>N&ge;0</sub>N!<sup>2/3</sup>(???&alpha;)<sup>N</sup>.
--
However, that estimate still was defective since electrons <i>do</i> interact, and
therefore calculating the Coulomb energy based on the approximation that the electrons are
uniformly distributed within the ball is not correct.
The correct electron density function &rho;(r), chosen to minimize total (Fermi+Coulomb)
energy and thus maximally-engender collapse, behaves proportionally to r<sup>-3</sup>?
Well no, that makes Fermi get infinite but Coulomb also infinite balanced in size
(both like 1/L where L is the minradius)... and the total electron count is
logthly infinite... you can pick rho=r<sup>-2.5</sup>
which causes Coulomb to be logarithmically infinite but Fermi to be powerlaw infinite.
If the power is below -3 than you get Coulomb more-infinite than Fermi.
--
Several workers
attempted to look more deeply
at Dyson's divergence argument, with the goal of determining the precise asymptotic
nature of the divergence of QED series.
Unfortunately to do so they had to employ methods substantially less rigorous, 
simple, and convincing than Dyson.
<ol><li>
A very crude analogy arises by
investigating the "1D quartic anharmonic oscillator," that is, the energy eigenvalues of 
a quantum mechanical particle (obeying the Schr&ouml;dinger equation on the real x-line)
in a potential x<sup>2</sup>+gx<sup>4</sup> with |g| small.  
When g is small and positive, we get an infinite spectrum of bound particle states.  However,
when g is negative by even the slightest amount, any bound particle can "tunnel" 
into a region with infinitely negative potential, so the system becomes unstable and
there are no bound states.  This suggests analogously to Dyson's argument 
(but far more simply, since Dyson's collapse is a many-particle phenomenon)
that the asymptotic expansion of the ground state energy as a function of g
cannot be analytic at g=0 and hence
must diverge for all g&ne;0.  <!--By using the "WKB method" and "saddlepoint"
--
Bender &amp; Wu 1973 (this is 4 times their EQ 1.8 with K=0)
were able to find the precise asymptotics of the
coefficient C<sub>N</sub> of g<sup>N</sup> in this expansion:
<center>
C<sub>N</sub> 
= 
-4 (6/&pi;<sup>3</sup>)<sup>1/2</sup> 
(-3)<sup>N</sup> 
&Gamma;(N+1/2)
[1+O(N<sup>-1</sup>)].
</center>
Unfortunately, Bender &amp; Wu's mathematical techniques, while inspired, range between mildly and
exceedingly nonrigorous.
But fortunately, most of their results were redone using rigorous
techniques by Barry Simon and collaborators (CITES???).
</li><li>
Simon et al were able to prove that the series expansion of each energy level
was "LeRoy/Borel summable" (see p.147 of Hardy) provided 
anaytic continuation is employed to define the Borel integrand.
They also proved it was "Pade summable," i.e. the diagonal sequence of Pade approximants
to the power series, converges to the correct energy level.
However, Graffi &amp; Grecchi 1978 showed that the <i>sextic</i> AnHO
and indeed the AnHO with potential 
x<sup>2</sup>+g|x|<sup>P</sup> for each even integer P&ge;6,
lead to series whose Pade approximants do <i>not</i> converge to the energy eigenvalues.
</li><li>

</li><li>
Hurst and Thirring's "&lambda;&phi;<sup>3</sup> theory" has power series which generically 
diverge with coefficients |C<sub>N</sub>| growing at least as quickly as
K<sup>N</sup>N<sup>N/2</sup>
where K is some positive constant.
-- both proved this lower bd. --
Note, these results are only lower bounds.
Houghton, Reeve, Wallace 1978 conjectured the true behavior was
C<sub>N</sub>=N!a<sup>N</sup>N<sup>b</sup>c[1+O(N<sup>-1</sup>]
for suitable real constants a,b,c, where they claim 
a is <i>negative</i> which perhaps means this divergent series is "Borel summable."
-- 
a=-2<sup>-9</sup>15&pi;<sup>3</sup>&asymp;-0.9084
in EQ48, a=-15/28=0.5357 in EQ53. 
--
A.Petermann also examined the 
&lambda;&phi;<sup>3</sup>,
and also
&lambda;&phi;<sup>4</sup>,
theory during 1952-1954 (reaching the same conclusions)
and made the additional point that, if instead of using a power series in &lambda; we
used one in the new variable 
<nobr>u=&lambda;(1+a&lambda;<sup>2</sup>+b&lambda;<sup>4</sup>+c&lambda;<sup>6</sup>+...)</nobr>
then we could cause the new series to have the same convergence-or-divergence behavior
as the old one, <i>but</i> with the first few coefficients changed to whatever we like!
This for a divergent series tends to diminish one's respect for the "meaning" of
those first few terms.
</li><li>
For generic QED series,
Balian, Itzykson, Zuber, Parisi 1978,
building on work of S.Adler 1972-1974, conjecturally
estimated 
<nobr>C<sub>N</sub>&sim;(-S)<sup>N</sup>&Gamma;(N/2)</nobr>
"up to powers of N and a constant factor,"
for some unknown constant S which they claimed to prove
had to obey 0&lt;S&le;&pi;<sup>-1</sup>&asymp;0.31831.
</li><li>
Bogomolny &amp; Fateyev 1978 found the same result but now, more precisely (a)
giving the exact value
<nobr>S=3<sup>-3/4</sup>&pi;<sup>3/2</sup>2&asymp;4.88555</nobr>
and
(b) claiming there is no "power of N" factor.
This work is incomplete and a larger paper with "details" was promised to be 
"published elsewhere" but apparently never was.
(Both the Bogomolny and Balian papers use complicated and dubious reasoning.
While Balian et al openly admit most of their results are not rigorous,
Bogomolny &amp; Fateyev falsely pretend theirs are.)
This would suggest that the divergence would start at about the
<nobr>N&asymp;(4.88555&alpha;)<sup>-2</sup>&asymp;787</nobr> 
term of the series.
</li></ol>
<p>
Estimates #4,5,6 all are approximately the same
[and also would match
the (P=3)-power 1D AnHO model]
and 
appear to have been reached 
somewhat independently, in the senses that
<ol type="a">
<li>
None of the three papers cite the others
</li><li> 
The three author-teams were geographically far apart
</li><li> 
The Bogomolny-Fateyev and Balian et al results <i>directly contradict each other</i>,
proving at least one of these papers is wrong.  This is not 
surprising considering the nonrigorous
techniques they employed, the complexity of their arguments,
and the amount of details they omitted.
</li></ol>
<p>
Estimates 4,5,6 and my repaired-Dyson estimate all
indicate a divergency considerably softer than what one might
have naively expected from either
the counts of Nth-order Feynman diagrams (which grow roughly like N!,
see Cvitanovic, Lautrup, Pearson 1978)
or from Lautrup's diagrams.  The only way these estimates can be true
is via an amazingly enormous amount of 
cancellation.  But even if all that cancellation indeed
happens, this still is severe enough to make QED series diverge for <i>every</i> &alpha;&ne;0.
</p><p>
One could <b>speculatively extend Dyson</b>'s argument to produce considerably more 
powerful conclusions (if one believes them, which I'm not sure one should).
Consider <i>complex non-real</i> &alpha;.   
REMOVE THIS DUPLICATE???:
It is a trivial matter to simply substitute that &alpha; into the known
exact closed form 
solutions of Dirac's equation for a hydrogenic atom, and this
known exact energy eigenvalue 
formula too:
</p><center>
E<sub>n, j</sub> 
&nbsp; = &nbsp; mc<sup>2</sup>
[1+Z<sup>2</sup>&alpha;<sup>2</sup>
(n+[(j+&frac12;)<sup>2</sup>-Z<sup>2</sup>&alpha;<sup>2</sup>]<sup>1/2</sup>)<sup>-1</sup>
]<sup>-1/2</sup>
</center><p>
where j=0,1,2,3,... and n=0,1,2,3,... are nonnegative integers, and j+&frac12;&le;n,
and Z is the nuclear charge
measured in units of |e|, and &alpha; is the fine structure constant.
Further "Lamb shifts" by amounts of order &alpha;<sup>5</sup>mc<sup>2</sup> 
(times log&alpha; terms?).
???MOVE THIS Regarded as an analytic function of complex &alpha;, this energy formula
has singularities (branch points and/or poles) when
&alpha;=&plusmn;(j+&frac12;)/Z
or
&alpha;=&plusmn;i&middot;(2nZ)<sup>-1</sup>(n+j+&frac12;)(n-j-&frac12;).
These points all lie on either the real or imaginary axis.
But they stay away from the origin if Z is bounded.
</p><p>
One then finds,
with generic non-real &alpha;,
that the energy eigenvlaues would be non-real and hence the
energy-eigenfunctions would no longer
<i>oscillate</i> (or more precisely, no longer 
orbit round a circle in the complex plane)
with time, 
but rather would exponentially shrink to 0 or grow to &infin;.
I.e. probability would no longer be conserved; we would have "non-unitarity."
(We could also remark that gauge invariance, EQ 4.6 of Peskin &amp; Schroeder, 
instantly vanishes as soon as the electron charge 
is made non-real...)
This would obviously be a vastly different universe, no matter how tiny the perturbation of 
&alpha; off the real line into the complex plane, and the idea of "probability=2" 
obviously is not physically or mathematically sensible. 
</p><p>
One <i>might</i> conclude from this that <i>any</i> perturbation of &alpha; into the complex plane
instantly destroys QED, in which case
presumably
<b>essentially every physically-meaningful
function of &alpha; in QED
must be <i>non-analytic</i> at <i>every real</i> &alpha;.</b> 
In particular, it would then follow that
if somehow somebody invented a way to do perturbative QED 
by expanding in a power series in the small perturbation of &alpha; away from
<i>anywhere</i> (not necessarily from &alpha;=0 anymore), <i>every such power series
would have radius of convergence zero</i>.
</p><p>
A counter-argument would be that, e.g, the exact hydrogenic-atom energy levels still 
exist, even though they would be complex with complex &alpha;, and hence such quantities
<i>are</i> well-defined analytic functions of &alpha;.
</p><p>
Anybody who buys the "analytic nowhere" extended-Dyson argument would, further,
find as an immediate consequence of the
the "Borel polygon" or "star shaped convergence region"
theorems (e.g. see theorem 129 of Hardy 1949) that
<b>essentially every physically-meaningful
function of &alpha; in QED
must be "Borel summable" <i>nowhere</i>
on the real &alpha; line (except at the base-point), 
regardless of the base-point on that line from which we expand.</b>
PIT LeRoy/Borel defn and results somewhere???
</p><p>
Indeed, even if we were to postulate that complex &alpha; were ok
because somehow only the real part was employed by physics,
then we'd still by Dyson's original argument 
be forced to conclude
the vacuum must be unstable for every &alpha; with re(&alpha;)&lt;0.
That in turn, by the Borel polygon theorem, would force nonconvergence of 
the Borel sum for every &alpha; not lying on the positive-real axis.
-->
</p><a name="dysonD"></a><p>
<b>Dyson argument redone in other-dimensional space and/or with various kinds of magic 
small-length "cutoffs":</b>
Dyson's argument is important enough that it is worth redoing in dimensions other than 
the physical dimension 1+3.
In 1+D dimensions (i.e. one time and D spatial dimensions, D&#8805;1)
the Fermi confinement energy is
of order 
<nobr>
V<sup>-1/D</sup>N<sup>1+1/D</sup> 
</nobr>
to trap N noninteracting  ultrarelativistic 
fermions in a volume-V ball (when N&#8594;&#8734;).
Meanwhile the Coulombic attraction energy (if &#945;&lt;0)
for a uniform ball of charge is of asymptotic order
</p><center>
(N-1)N &#945; V<sup>(2-D)/D</sup> &nbsp;&nbsp; if &nbsp;&nbsp; D&#8805;3,
<br>
(N-1)N &#945; |logV| &nbsp;&nbsp; if &nbsp;&nbsp; D=2,
</center><p>
and the rest mass of N electrons ("creation energy") if of order N.
In the N&#8594;&#8734; limit
the Coulombic attraction energy outweighs the other two, so that
we expect Dysonian collapse would still occur in every spatial-dimension D&#8805;2.
Therefore, we would expect QED redone in any dimension D&#8805;2 
still to exhibit &#945;-power series divergent for every &#945;&#8800;0.
But for D=1, we do not expect collapse, and <i>convergent</i>
&#945;-power series would be possible (at least as far as Dyson's argument is concerned).
</p><p>
If we now impose an <b>electron density upper bound</b> by magically
making electrons be "rigid balls" of some fixed but very tiny 
radius X&gt;0 to prevent electrons from getting too close, 
then provided X was tiny enough
we'd <i>still</i> expect Dysonian negative-&#945;
vacuum instability in any spatial dimension D&#8805;2
because even in the joint limit 
V&#8594;&#8734;, N&#8594;&#8734;, with N/V bounded between two positive constants,
the Coulombic attraction energy still would vastly outweigh the other two energies.
<!--
Fermi = 1
Coul = 1+2/D for D>=3 and 2+eps for D=2
Rest = 1
-->
</p><p>
Finally, if we magically imposed a <b>stronger kind of cutoff</b> by
forcing V to grow proportionally to N<sup>2</sup>, not N, when
N&#8594;&#8734;, then
(if the constant of proportionality was small enough)
<!--
Fermi = 1-1/D
Coul = 4/D for D>=3 and 2+eps for D=2
Rest = 1
-->
again we
still would expect Dysonian negative-&#945;
vacuum instability in any spatial dimension D&#8805;2.
Our point is that neither of those magically-imposed cutoffs would be enough medicine
to cure Dyson's disease.
</p><a name="dysonH"></a><p>
<b>Dyson argument redone in hyperbolic nonEuclidean geometry (we no longer collapse!):</b>
Interestingly, in <i>hyperbolic</i> D-dimensional geometry (D&#8805;2), 
a ball with volume initially proportional to N and containing
N mutually-attracting electrons, would <i>not</i> collapse (if the
fixed number-density were sufficiently small), no matter how large we
make N.  If the electrons were uniformly 
distributed in the ball, the classical
inward electric force on each electron would be uniformly bounded by
a <i>constant</i>,
which would not be enough to overcome a constant outward pressure
caused by the electron gas having some sufficiently large fixed
constant positive temperature (other sources of pressure could include a quantum
Fermi pseudotemperature, or the
effect of a repulsive Einstein cosmical constant).
</p><p><small>
These claims trace to the fact that in hyperbolic geometry the volume and surface of a ball
both grow exponentially and ultimately proportionally, for large radii.
The striking fact that some versions of Dyson's divergence argument fail
in hyperbolic geometry is new, and will
resurface <a href="#byeinfin">later</a>
<a href="#convser">as</a>
an underlying reason for why rain of bricks QED &#945;-power-series
are claimed to converge provided we set things up in de Sitter space.
</small></p><p>
We shall now <b>switch</b>
to examining several "model functions" as an exercise in complex analysis
to hopefully shed light on this from a different angle.
</p><p>
<b>First model function:</b> A useful class of functions of z to keep in mind is
</p><center>
F<sub>P</sub>(z) = &#8721;<sub>0&#8804;N</sub> R<sub>N</sub> exp(-N<sup>P</sup>) T<sub>N</sub>(z)
</center><p>
where P is an arbitrary constant with 0&lt;P&lt;1, for example P=1/2,
the R<sub>N</sub> are random ±1 signs chosens according to independent
coin tosses,
and the T<sub>N</sub>(z) are the Chebyshev polynomials, defined by
T<sub>0</sub>(z)=1,
T<sub>1</sub>(z)=z,
and 
<nobr>
T<sub>N+1</sub>(z)=2zT<sub>N</sub>(z)-T<sub>N-1</sub>(z)
</nobr>
for N&#8805;1.
<!-- the Chebyshev matrix
                                              [2x     -1]
                                         M := [         ]
                                              [ 0      1]
is not "normal" i.e. does not commute with Mtranspose. 
Its eigenvalues are 2x and 1.   So if you tried to make alpha be such a matrix you
would get a nonunitary quantum mechanics.
-->
This Chebyshev series converges <i>only</i> on the segment -1&#8804;z&#8804;1
of the real line
and for no other complex z.
[Proof sketch: 
Use the standard theorem (Boyd 2001)
that the convergence region for Chebyshev series is an ellipse with
foci ±1, the formula T<sub>N</sub>(z)=cos(N·arccos(z)), and the fact that 
cos(ix)=cosh(x)&#8776;exp(x)/2.]
</p><p>
<b>Inextensibility theorem:</b>
For any fixed P with 0&lt;P&lt;1,
with probability=1, as a function of z we have that
F<sub>P</sub>(z) is analytic  <i>nowhere</i>.
In other words, 
this F(z) cannot be extended by analytic continuation 
to any other point of the complex z-plane off the real interval [-1,1].
</p><p>
<b>Proof sketch:</b>
Let z=cos(&#952;) so that T<sub>N</sub>(z)=cos(N&#952;)
for 0&lt;&#952;&lt;&#960;.
The Kth derivative of F(z) with respect to &#952; is
</p><center>
D<sub>K</sub>(&#952;) = 
±&#8721;<sub>0&#8804;N</sub> R<sub>N</sub> exp(-N<sup>P</sup>) 
N<sup>K</sup> (cos or sin)(N&#952;).
</center><p>
One can use this to see that
|D<sub>K</sub>(&#952;)|
for any fixed &#952; will, with probability=1,
exceed (K/P)<sup>(K/P)</sup>C<sup>-K</sup>
for any fixed constant C&gt;1, for more than 90% of the positive integers K.
In view of the fact that 0&lt;P&lt;1, this means that the Taylor series in &#952;
for F(z) converges nowhere with 0&lt;&#952;&lt;&#960;, and
equivalently F(z) is non-analytic everywhere on the line segment -1&lt;z&lt;1.
Therefore, F(z) cannot be analytically continued to any other point of the complex plane, because
there is noplace analytic to start from.
<b>Q.E.D.</b>
</p><p>
<!--
Essentially the same proof as that
&sum;<sub>0&le;N</sub> R<sub>N</sub> exp(-N<sup>P</sup>) z<sup>N</sup>
has (with probability=1) a natural barrier at |z|=1 for any fixed P with 0&le;P&lt;1
???
See J-P. Kahane 1993???
R.E.A.C.Paley and A.Zygmund:
A note on analytic functions in the unit circle, Proc. Camb. Phil. Soc. 28 (1932) 266-272??
MATHEMATISCHE ZEITSCHRIFT Volume 37, Number 1, 647-668???
-->
</p><p>
Hence if expanded in a Taylor series in z, that series will always have radius
of convergence 0, regardless of the base-point, and thus can only be regarded
as an "asymptotic series."   
Despite all that, F(z) is quite well behaved throughout its domain of definition:
it is infinitely-differentiable everywhere, and its defining Chebyshev series 
converges quite rapidly, fast enough
to compute F(z) accurate to D digits of accuracy in polynomial(D) computation time.
</p><p>
<b>No-Go Series Theorem:</b>
Unlike for an analytic function,
local information about F(z) and its 
derivatives at any particular z <i>cannot</i> 
be used in <i>any</i> manner – I do not care how clever you are about inventing
ways to "sum" divergent series – to deduce global information about F(z);
and indeed cannot even be used to deduce the value of, nor even upper or lower bounds on,
F(x) for <i>any</i> x&#8800;z.
</p><p>
<b>Proof:</b>
Let K be a nonzero real.
F(z)+Kexp(-(z-y)<sup>-2</sup>)
has the same Taylor expansion as F(z) about the basepoint z=y, but
is a different function with a different Chebyshev series (which also converges
and is infinitely differentiable everywhere
in the real interval [-1,1] but nowhere else in the complex plane)
whose value is unequal to F(z) for every z&#8712;[-1,1] with z&#8800;y.
Since we can make K as large as we desire, not even upper or lower bounds of F(z)
are deducible.
<b>Q.E.D.</b>
</p><p>
<b>Easy extension:</b>
Even full
information about F and its derivatives at <i>any finite set</i> S 
of locations z, still tells you 
nothing about F(z) at any z not in S.
</p><p>
Functions like our model F(z) obviously are quite common in mathematics even though
not common in contemporary physics and math textbooks.  For example,
in place of the coefficients exp(-N<sup>P</sup>) in F's Chebyshev series,
one could employ <i>any</i> function of N whose absolute value decreases toward zero
as N&#8594;&#8734; asymptotically faster than every power N<sup>-c</sup> (with c&gt;0) of N, 
but slower than every exponential k<sup>N</sup> with 0&lt;k&lt;1.
The resulting F would still be infinitely differentiable everywhere, but analytic nowhere,
on [-1,1], and still would be completely insusceptible to analytic continuation.
</p><p>
<b>Second model function:</b> Consider 
</p><center>
G(z) = -z<sup>-1</sup> exp(z<sup>-1</sup>) Ei(-z<sup>-1</sup>)
= -z<sup>-1</sup> &#8747;<sub>0&lt;t&lt;&#8734;</sub> (1+t)<sup>-1</sup>exp(-t/z) dt
<br>
where Ei is the "exponential integral" 
&nbsp;&nbsp; 
Ei(x) = &#8747;<sub>-&#8734;&lt;t&lt;x</sub> exp(t)t<sup>-1</sup>dt.
</center><p>
This G(z) is analytic throughout the complex z-plane provided we slit it along
the negative real axis.
I chose this G(z) because when z&#8594;0+, it has the <b>asymptotic series</b>
</p><center>
G(z) &#8764; 1 - 1!z + 2!z<sup>2</sup> - 3!z<sup>3</sup> + 4!z<sup>4</sup> -...
&nbsp;&nbsp;
(diverges for every complex z&#8800;0)
</center><p>
Indeed by a theorem of Sokal 1980 (which he traced
back to G.N.Watson in 1912 and R.Nevanlinna in 1918)
this is the <i>unique</i>
function of z analytic
in any circle tangent to the imaginary axis at z=0
(and lying on the positive-real side of it)
and obeying an error bound saying that the Nth truncation of the asymptotic series
yields error asymptotic as z&#8594;0+ to something
below a constant, times an exponential function of N,
times the first neglected |term|.
</p><p>
Let the
Chebyshev series expansion of G(z), convergent for real z with 0&#8804;z&#8804;2, be
</p><center>
G(z) = &#8721;<sub>0&#8804;N</sub> C<sub>N</sub> T<sub>N</sub>(z-1).
</center><p>
I computed 
C<sub>0</sub>&#8776;0.65568,
C<sub>1</sub>&#8776;–0.24445, ...
<!-- C<sub>33</sub>&asymp;-7.6474&times;10<sup>-11</sup> 
C<sub>49</sub>&asymp;-4.7657&times;10<sup>-14</sup> -->
C<sub>50</sub>&#8776;3.09375×10<sup>-14</sup>, as well as
C<sub>N</sub> for N&#8712;{64, 128, 256, 512, 1024}
and
<nobr>C<sub>2048</sub>&#8776;5.80957×10<sup>-167</sup></nobr>,
<!--
C<sub>100</sub>&asymp;1.77649&times;10<sup>-22</sup>
and 
C<sub>200</sub>&asymp;1.46169&times;10<sup>-35</sup>,
-->
accurate to over 50 significant figures.
This investigation seems to show (see plot &amp; table) that  
for large m
</p><center>
ln |C<sub>m</sub>| &nbsp; &#8776; &nbsp; <b>–</b>2.37 m<sup>2/3</sup>.
</center><p>
<!--
I then found the approximate formula
</p><center>
|c<sub>m</sub>| &asymp; exp(0.518 - 2.199m<sup>0.6805</sup>)
</center><p>
was correct to within a multiplicative factor of 1.22
for every m=7, 8, 9..., 48, 49, 50, 100, 200:
</P>
<pre>
    m     ln|c<sub>m</sub>|,      approx,       difference
    7,     -7.558,      -7.748,      0.191
   10,     -9.881,     -10.019,      0.138
   15,    -13.297,     -13.367,      0.070
   20,    -16.351,     -16.370,      0.019
   25,    -19.160,     -19.139,     -0.021
   30,    -21.788,     -21.736,     -0.052
   35,    -24.273,     -24.197,     -0.076
   40,    -26.643,     -26.548,     -0.095
   45,    -28.916,     -28.807,     -0.109
   50,    -31.107,     -30.987,     -0.120
  100,    -50.082,     -49.974,     -0.108
  200,    -80.211,     -80.406,      0.195
</pre>
-->
<a name="FIG3eichebco"><img src="WarrenSmithQED131123_files/EiChebCoeffSize.png" alt="fig3" align="left" width="35%"></a>
</p><pre>              <b>log<sub>2</sub>m   log<sub>2</sub>|ln|c<sub>m</sub>||</b>
               0      0.4943985509
               1      1.443220331
               2      2.286328841
               3      3.063955965
               4      3.800335245
               5      4.510783522
               6      5.204958504
               7      5.888920430
               8      6.566467638
               9      7.239982407
              10      7.910960560
              11      8.580342259
</pre>
<!--
slope=0.66938 to 0.73509 based on differences;
plausibly limit slope is 0.666666.
crude extrapolation to infinity yielded limit slope=0.666722
-->
<br><br>
<p>
Thus, G(z) in [0,2], whose asymptotic series when z&#8594;0+ is similar to
the presumed behavior of QED series 
(crudest bosonic Dyson, also Bender &amp; Wu's quartic AnHO) when &#945;&#8594;0+,
indeed appears to behave similarly (as far as its Chebyshev series |coefficients| are
concerned) to how our first model function F<sub>2/3</sub>(z) behaves in [-1,1].
</p><p>
In addition to G(z)'s integral representation which constitutes the "Borel sum" of the
series, it is also possible to represent G(z) as a continued fraction
</p>
<pre>    1   z   z   2z  2z  3z  3z  4z  4z       
   --- --- --- --- --- --- --- --- ---       
    1+  1+  1+  1+  1+  1+  1+  1+  1+ ...   
</pre>
<p>
convergent for every complex z except negative reals.
This could be considered as the "Pade/Stieltjes sum"
of the divergent series.  
But these summability properties are an anomalous artifact of the
exact alternating-sign behavior of this particular function;
because our first model function is nonanalytic
everywhere, <i>it</i> cannot be summed by any such continued fraction trick yielding
an analytic function, and the Borel polygon theorem
(Hardy 1949) shows it to be non-Borel-summable for every basepoint.
</p><p>
For series of our present kind, the Borel sum's existence is dependent on the
exact alternation of the sign.
With all + signs the Borel sum would not exist.
With, say, exact sign alternation except perturbed from (-1)<sup>n</sup>
to (-1)<sup>n</sup>+10<sup>-100</sup>,
Borel would not exist.   With random coin flips as signs, Borel
will not exist with probability=1.
(And to see that this concern is very real, see
our remarks on the Coquereaux-Kawai-Kinoshita-Okamoto diagram in
<a href="#historylautrup">§37</a>.)
</p><p>
Stevenson 1984 introduced another interesting "summation"
procedure for series he called "sensitivity optimization."
Given a formal power series 
FPS(x)=&#8721;<sub>n&#8805;0</sub>b<sub>n</sub>x<sup>n</sup>, 
where in our case b<sub>n</sub>=(-1)<sup>n</sup>n!,
Stevenson considered 
re-expressing (via formal power series manipulation)
the series in powers of y rather than x, where y=x/(1+&#964;x)
and hence x=y/(1-&#964;y) and &#964;=y<sup>-1</sup>-x<sup>-1</sup>:
</p><center>
FPS(x)=&#8721;<sub>n&#8805;0</sub>c<sub>n</sub>y<sup>n</sup>
&nbsp;&nbsp;
where y and the c<sub>n</sub> depend on &#964;.
</center><p>
In our case this
still yields a series that diverges in n!-style, regardless of the
value of &#964;.   However, Stevenson then considered evaluating
the sequence of approximants
&#8721;<sub>0&#8804;n&lt;N</sub>c<sub>n</sub>y<sup>n</sup>
for N=1,2,3,...
using &#964; <i>depending</i> on N.
His idea was to select &#964;(N) in such a way that the approximant was least sensitive to 
small variations in &#964;, e.g. had zero &#964;-derivative.
Stevenson was able for our particular function G(z) to find an exact formula for &#964;(N)
– it turns out to be asymptotic to 0.278464542761...N – and showed that his sequence
of approximants then <i>converged</i> (for every fixed complex z
not a negative real), as N&#8594;&#8734; through even N, but <i>not</i> to G(z).
[There is convergence as 
N&#8594;&#8734;
for odd N too, but to a third function!
But if &#964; is allowed to be complex then 
Stevenson thinks the odd and even limits will
coincide.]
The difference &#916; between Stevenson's even-N convergent and G(z) is 
</p><center>
&#916;(z) = Ei<sub>1</sub>(4.59112.../z) exp(1/z)   &nbsp;&nbsp; if &nbsp; Re(z)&gt;0.
</center><p>
Stevenson computed 
&#916;(0.25)&#8776;2.99×10<sup>-8</sup> 
which does not seem too bad,
but 
&#916;(4)&#8776;0.221
and
&#916;(1+i)&#8776;0.027i-0.032
are less impressive.
</p><p>
Stevenson's particular choice y=x/(1+&#964;x) of the family of transformations
has certain advantages – the sensitivity optimization then becomes essentially the same as
"choosing &#964; to zero the last coefficient in the series-truncation" –
but one could also consider other
families.  Stevenson noted that his whole idea is very similar to the Wilsonian view
of "renormalization" and optimistically hoped that QCD perturbation power-series would 
become convergent if their argument were made to depend on 
the greatest power N in the series truncation in a manner guided by min-sensitivity principles
("optimized running coupling constant"). 
I'm dubious of that hope, and even Stevenson still believed this convergence
would, just as in our model-function example, be to a <i>wrong</i> answer,
i.e. one slightly disagreeing with physical reality.
</p><p>
Here is a devastating-seeming objection.
Let FPS(x)=&#8721;<sub>n</sub>q<sub>n</sub>f<sub>n</sub>x<sup>n</sup>
where the q<sub>n</sub> are independent "throin flips," that is {-1, 0, +1}
with probability 1/3 each, and the f<sub>n</sub> are your favorite interesting
function of n, such as f<sub>n</sub>=n!, 1, or whatever.
Then Stevenson's whole procedure will, 1/3 of the time, do nothing.
Therefore,
</p><p>
<b>Stevenson generic-failure Theorem:</b> Whenever the original series (of the FPS form in
the preceding paragraph ) diverges,
with probability=1 the Stevenson sequence will also diverge.
</p><p>
Given these results about Stevenson and Borel summation, it evidently is crucial (to hope
to save QED or QCD from series-divergence problems in this sort of manner) to prove
exact sign alternation, plus somehow obtain <i>very</i> exact understanding
of the asymptotic nature of the series coefficients.   Nobody
has yet been able to do that.

<!--
However, every series of form 
&sum;<sub>N&ge;0</sub>(NA)!log(N<sup>B</sup>)<sup>-CN</sup>
diverges for any set {A,B,C} of positive constants,
and the natural running of the QCD coupling constant would seem to depend on some power of N like
some power of a logarithm, so for that and other reasons I do not believe in Stevenson's hope.
-->

</p><p>
<b>Third model function:</b> Consider 
</p><center>
H(z) = z<sup>-1</sup> &#8747;<sub>0&lt;t&lt;&#8734;</sub> exp(t<sup>2</sup>-t/z) erfc(t) dt
</center><p>
This integral converges to a finite value for each complex z&#8800;0 with Re(z)&#8805;0, and
H(z) is analytic throughout that halfplane.
However it diverges for each complex z with Re(z)&lt;0.
</p><p>
<b>Natural Barrier Theorem:</b>
The imaginary axis is a "natural barrier"
preventing H(z)'s analytic continuation into the 
Re(z)&lt;0 halfplane.
</p><p>
<b>Proof sketch:</b>
The assertion of the theorem is equivalent to asserting that 
z<sup>-1</sup>H(z<sup>-1</sup>) has a natural barrier on the imaginary axis in the z-plane
(since reciprocating z maps the real and imaginary axes each to themselves).
The degree-N Taylor series coefficient of 
<nobr>z<sup>-1</sup>H(z<sup>-1</sup>)</nobr>
about
z=A+iB with 0&lt;A&lt;B
is
</p><center>
&#8747;<sub>0&lt;t&lt;&#8734;</sub> exp(t<sup>2</sup>-[A+iB]t) erfc(t) (-t)<sup>N</sup> dt/N!.
</center><p>
The theorem is equivalent to the assertion that the radius of convergence 
of every such Taylor series is &#8804;A.
To show the latter, we asymptotically analyse the Taylor series |coefficients| in the 
limit N&#8594;&#8734;
using the "saddlepoint method." 
By taking advantage of known facts such as
lim<sub>x&#8594;K&#8734;</sub>erfc(x)exp(x<sup>2</sup>)x=&#960;<sup>-1/2</sup>
for any fixed complex number K with re(K)&gt;0
we find that the location of the saddlepoint, i.e. the t
maximizing the the |integrand|
when N&#8594;&#8734;, is 
t=(N-1)/A±O(N<sup>-1</sup>).
This arises by maximizing (N-1)ln(t)-At 
by choice of t.
Now evaluating the integral at the saddlepoint with the aid of Stirling's formula
(and considering the "width" of the saddle is by comparison a very slowly changing function)
we find that the Nth |coefficient| is, for all sufficiently large N,
bounded between two constants times powers of N,
times A<sup>-N</sup>.
<br>
<b>Q.E.D.</b>
</p><p>
I chose this H(z) because when z&#8594;0+, it has the divergent <b>asymptotic series</b>
</p><center>
H(z) &#8764; &#8721;<sub>0&#8804;N</sub> (-z)<sup>N</sup> N! / (N/2)!
</center><p>
whose coefficients 
closely resemble those proposed by Balian et al 
and especially Bogomolny &amp; Fateyev for QED.   
Indeed by Sokal's theorem
this is the <i>unique</i> 
function of z analytic
in any circle tangent to the imaginary axis at z=0 (and lying on the positive-real side of it)
and obeying an error bound saying that the Nth truncation of the asymptotic series
yields error asymptotic as z&#8594;0+ to something
below (N/2)! D<sup>N</sup>C
times the first neglected |term|
(where C,D are any positive constants).
</p><p>
The convergence of the integral defining H(z) depends heavily on the 
alternating signs of the coefficients in the asymptotic series.
Both Balian et al
and Bogomolny &amp; Fateyev had conjectured such an alternation for QED;
but the Hurst-Thirring coefficients for &#955;&#966;<sup>3</sup> theory, 
which have roughly the same growth rate,
have all-positive signs.
If our signs instead had been all-positive,
then the corresponding integral now representing H(-z) would have been 
severely divergent for all complex z with |arg(z)|&lt;&#960;/4,
and if they had been random, then we would have gotten (with probability=1)
divergence for every complex z.
</p><p>
</p><p>
<b>Fourth model function:</b> Consider 
</p><center>
J(z) = z<sup>-1</sup> &#8747;<sub>0&lt;t&lt;&#8734;</sub> exp(-t<sup>3</sup>-t/z) dt
</center><p>
<tt>MAPLE</tt> is able to evaluate this integral with the aid of 
<a href="http://en.wikipedia.org/wiki/Anger_function">Anger</a>-J,
Weber-E,
<a href="http://en.wikipedia.org/wiki/Bessel_function">Bessel</a>-J, 
and Bessel-Y functions:
</p><center>
J(z) = 
9<sup>-1</sup> &#960; z<sup>-3/2</sup> [
  AngerJ<sub>1/3</sub>( 2 (3z)<sup>-3/2</sup>)
- BesselJ<sub>1/3</sub>( 2 (3z)<sup>-3/2</sup>)
- 3<sup>1/2</sup> BesselY<sub>1/3</sub>( 2 (3z)<sup>-3/2</sup>)
- 3<sup>1/2</sup> WeberE<sub>1/3</sub>( 2 (3z)<sup>-3/2</sup>)
]
</center><p>
(Functions of a similar ilk can also be constructed using 
<a href="http://en.wikipedia.org/wiki/Airy_function">Airy functions</a>.)

<!-- can be evaluated in closed form using WeberE, AngerJ, BesselJ, BesselY:
J(z) := (z) ->
1/27*Pi*3^(1/2)*(1/z*(1/z)^(1/2))^(1/3)*(3^(1/2)*AngerJ(1/3,2/9/z*(1/z)^(1/2)*3^(1/2))-3^(1/2)*
BesselJ(1/3,2/9/z*(1/z)^(1/2)*3^(1/2))-3*BesselY(1/3,2/9/z*(1/z)^(1/2)*3^(1/2))-3*WeberE(1/3,2/9/z
*(1/z)^(1/2)*3^(1/2)))/z;
-->
The integral converges to a finite value for each complex z&#8800;0 with 
|arg(z)|&#8804;&#960;/6, but diverges 
for each complex z&#8800;0 with &#960;/6&lt;|arg(z)|&#8804;&#960;/3.
But <tt>MAPLE</tt>'s close form evaluation, regarded as an analytic continuation,
shows that J(z) is analytic for every complex z, except z=0 where there is
an essential singularity.
</p><p>
I chose this J(z) because when z&#8594;0+, it has the divergent <b>asymptotic series</b>
</p><center>
J(z) &#8764; &#8721;<sub>0&#8804;N</sub> (-z)<sup>3N</sup> (3N)! / N!
</center><p>
whose coefficients resemble those arising from our fermionic revision/correction
of Dyson 1952's collapse argument.
</p><p>
The convergence of the integral defining J(z) depends heavily on the 
alternating signs of the coefficients in the asymptotic series.  
If these signs instead had been all-positive or random,
then the corresponding integral would have been 
severely divergent for all positive real z.
But even with all-positive signs, <tt>MAPLE</tt>'s formula (after replacing z by -z)
would still provide an function analytic everywhere with z&#8800;0.
J(z) on [0,2] appears to behave similarly (as far as its Chebyshev series |coefficients| are
concerned) to how our first model function F<sub>P</sub>(z) behaves in [-1,1],
for some P between 0.6 and 0.8.
</p><p>
<b>Conclusion:</b>
This has been a long and interesting trek, but it has not yielded 
definitive conclusions
about the nature of the divergent series in QED.
It seems almost certain that the Nth |coefficient| grows like (JN)!
up to factors simple- exponential in N,
for some J with 0.4&#8804;J&#8804;1.
We've raised the spectre, which I'm not 100% sure whether to believe, that
generic QED and/or QCD functions are <b>nowhere-analytic</b> functions of &#945;,
and we have defined some simple smooth model functions which exhibit such behavior.
That is a <b>worst-case scenario.</b>
If it is true,
then the entire approach of the last 70 years of attempting
to deal with QED "perturbatively" is doomed to failure:  <i>No process</i>
whose input is solely the Taylor series coefficients of some 
infinitely differentiable function F(&#945;)
expanded about some basepoint &#945;=b – even if the entire 
infinite set of them is known
and all are finite –
can be guaranteed to output any correct value of F(&#945;) for <i>any</i>
&#945;&#8800;b,
nor even <i>any bound</i> on that value.
</p><p>
On the other hand, the <b>best</b> realistically-hopeable
scenario is that QED series always are LeRoy/Borel-summable
(or some other such summation process works)
and always are analytic functions of &#945; within some neighborhood of the segment (0, 1/100]
of the real axis.   I and previous authors have presented great reason to doubt
this best-case scenario occurs.
</p><p>
Personally, I think the known results and our models above indicate that
both these scenarios have a nonzero chance of being true, but suspect 
the situation is much closer to the "worst" than "best" scenario. 
The clearest evidence for the failure of perturbative QED is the "Landau pole"
which strongly suggests generic QED predictions are functions of &#945;
that are nowhere-analytic on some segment [0,x] of the positive-real  &#945; axis.
</p>


<a name="newsum"></a>
<!--
<h3>X. Some new series "summation" processes ??? </h3>

<p>
This section will argue that it is possible to
repair the series-divergence problem of QED, QCD, etc pretty much by
dictum.   That is, we will present new summation method for 
divergent power series.   An infinite number
of inequivalent such summifiers can be constructed; the one, or few, that I focus on
is/are not at all unique.  One could then, with no justification, <i>declare</i> that the 
True Laws of Physics are: if you apply one particular selected one of these summifiers
to the QED series, you always get the right answer.
</p><p>
Let me be clear.
I do not believe such a procedure really constitutes the true laws of physics &ndash;
and will later
propose other ideas which seem far more plausible physically to me.
I am simply pointing out that it <i>could</i>; and these summifiers are powerful enough that
experimentalists would have a hell of a time
trying to disprove them.
</p><p>
The reader may object that any particular summifier chosen
from among mine is rather arbitrary.  
My reply is: valid objection!
One of the joys of creating one's own new theory of QED is: 
the theory-creator gets to make whatever ridiculous rules
he wants, then declare them (tentatively) to be <i>the</i> laws of physics, then
defy any experimentalist to prove him wrong.  The object of this section is to demonstrate that
<i>some</i> set of (perhaps ridiculously ugly) 
physical laws can be stated that create a rigorously-defined QED.
I make no pretense that the particular laws invented in this section are the only possibilities.
I merely claim some exist.
</p><p>
It is easy
to choose parameters in our summifiers so that
its results are consistent with every QED-testing physical experiment tried so far &ndash;
indeed the "default" expressions given in the theorems below, without any further 
attempt to "tune" them, should work.
Far greater  &ndash; probably unreachably high &ndash; experimental precision
would be required to refute them.
Personally I find it very unlikely that any particular
such arbitrary choice I make would be <i>the</i> correct theory of physics &ndash;
<i>but</i> it may be centuries before
any experimentalist will be able to refute it!
</p><p>
Here's the method.  Consider some power series
</p><center>
S<sub>divgt</sub>(z) = &sum;<sub>0&le;N</sub> c<sub>N</sub> z<sup>N</sup>
</center><p>
which might be divergent for every z&ne;0.
Suppose we have some a priori bound on the coefficients of the form
|c<sub>N</sub>|&lt;N<sup>kN</sup>
(for some real constant k&gt;0) valid for all sufficiently large N.
--Our other repairs to QED, discussed later in this paper,
will enable us to prove such a priori bounds?--
Then:
</p><p>
<b>First Summifier Theorem:</b>
The following new series converges for all real z&ge;0, 
and outputs an infinitely-differentiable function S<sub>convt</sub>(z).
This function has formal Maclaurin series
which agrees term by term with the old series S<sub>divgt</sub>(z):
</p><center>
S<sub>convt</sub>(z) 
= 
&sum;<sub>0&le;N</sub> c<sub>N</sub> [U(N<sup>k+0.001</sup>z)&middot;z]<sup>N</sup>
</center><p>
where U(x)=1-exp(-x<sup>-1</sup>) for all x&gt;0, and U(0)=1.
</p><p>
<b>The proof</b> is trivial once you realize that U(z) is infinitely differentiable with
Maclaurin series
1+0z+0z<sup>2</sup>+0z<sup>3</sup>+...
and monotonically decreases as z grows from 0 to +&infin;, with asymptotic
behavior U(z)&sim;z<sup>-1</sup> when |z|&rarr;&infin;.
</p><p>
<b>Extensions:</b>
By using U(0.1x) instead of U(x) the same theorem would hold (or any positive constant
could be used instead of 0.1).   
That's an infinite set of inequivalent summifiers right there.
If you do not like the convergence being
on the <i>positive</i> real axis only, then by instead employing
<nobr>U(x)=1-exp(-x<sup>-2</sup>)</nobr> for all x&ne;0 and N<sup>(k+0.001)/2</sup>
instead of N<sup>k+0.001</sup>, we will instead get convergence for all real z.
Also, of course, pretty much any other well-behaved a priori bound on the coefficients
could also be handled by appropriately altering the same kind of summifier idea.
</p><p>
For QED purposes, the problem with the First Summifier Theorem is it actually is <i>too good</i>:
It produces convergence throughout an infinite <i>wedge</i> 
in the complex z plane, centered on the positive real axis,
with apex at z=0, and with any angle below 180 degrees &ndash;
with S<sub>convt</sub>(z) analytic throughout that wedge.
</p><p>
To solve that "problem," one could employ the
</p><p>
<b>Second Summifier Theorem:</b>
The following new series converges for all real z with 0&le;z&le;B
(but diverges for every other complex number z)
and outputs an infinitely-differentiable function S<sub>convt</sub>(z).
This function has formal Maclaurin series
which agrees with the old series S<sub>divgt</sub>(z):
</p><center>
S<sub>convt</sub>(z) 
= 
&sum;<sub>0&le;N</sub> { c<sub>N</sub> [U(N<sup>k+0.001</sup>z)&middot;z]<sup>N</sup>
U(z<sup>N</sup>)
+ V(z)<sup>N</sup>N! }
</center><p>
where U(x)=1-exp(-x<sup>-1</sup>) for all x&gt;0, and U(0)=1,
and V(x)=exp(-(x-B)<sup>-1</sup>) for all x&gt;B, and V(x)=0 if x&le;B.
</p><p>
-->

<a name="whatisrigor"></a>
<h3>7. What would a "rigorous QED" be? </h3>

<p>
I don't have a definition, but think a <i>sufficient</i> condition would be
</p><p></p><ol>
<li>
All coefficients output by any QED<sub>N</sub> are finite, 
without need for any "renormalization" or "cutoffs."
All unphysical infinities (both UV and IR) go away.
</li><li>
When applied to any of the physical scenarios in which
the old QED<sub>N</sub>'s had been used to make physical predictions,
the new rigorous version of QED should output numbers very close to the old outputs –
indeed in some limit identical.
</li><li>
The new rigorous QED should enjoy unitary time-evolution and 
Lorentz- and gauge-invariance properties, or
something extremely similar to them.
</li><li>
The power series converges, or at least can be made to yield a <i>unique</i> answer,
calculable  <i>algorithmically</i> to arbitrary user-specifiable precision.
This may require something beyond
ordinary summation, analytic continuation, or divergent series "summation" methods
– indeed
it may require abandoning the whole series-based methodology –
but the important thing is that by some algorithmic process, a unique
"nonperturbative" arbitrary-precision-accurate answer
is obtainable.
</li></ol>
<p>
<a href="#rainofbricks">Rain of bricks</a>
appears to accomplish goals 1, 2, 3 of saving <i>perturbative</i> QED, 
while at the same time making
QED more physically realistic.
When set in de Sitter space it appears also to accomplish goal #4.
</p>

<a name="exptlqed"></a>
<h3>8. Some large QED computations versus experiment, &amp; 
more about series divergence rate </h3>

<blockquote>
In our formulation, the integrand generated
by SCHOONSCHIP, even when [it] is convergent, is
singular on some boundaries of the multidimensional integration
domain.  Furthermore, in UV- and/or IR-divergent
cases, the divergence of the integral as a whole
is avoided only [because of our] carefully tailored counter-terms.
They would pose no problem if registers could accommodate
arbitrarily large numbers and if each step of computation
were carried out with infinite precision. In reality
registers carry only a finite number of digits and we have
to perform calculations in finite precision. The intended
cancellation of divergences may fail occasionally because
canceling terms have no more than 12 or 13 significant
digits (in double precision) and their difference tends to
be dominated near a singularity by round-off errors causing
undesirable fluctuations...
<br>&nbsp;&nbsp;&nbsp;
A simple measure to alleviate this problem is to remove
small intervals (typically 10<sup>-8</sup> to 10<sup>-10</sup>) from both ends
of the domain (0,1) for each integration variable. The error
thus introduced must be estimated by varying the size
of removed intervals.  This method works well for many
integrals we had to deal with.  However, in a few diagrams
such as M12, M16, M18 which contain multiple
powers of IR logarithms, this approach produced unsatisfactory
results and we were forced to adopt quadruple
precision arithmetic. This slowed down the computation
substantially because quadruple precision arithmetic is
not yet implemented in hardware.
In practice, quadruple precision is needed
only in the neighborhood of some singularities. Thus one
may evaluate the bulk of the integral in double precision,
resorting to quadruple precision only where it is absolutely
needed. In this manner we have been able to reduce the
impact of quadruple precision requirement considerably...
[We use] a highly vectorized version of VEGAS...
[It is essential to run VEGAS with enough points to cause the
N<sup>-1/2</sup> error behavior to set in otherwise VEGAS's claimed errors
can be highly misleading...]
<br> &nbsp;
– T.Kinoshita &amp; W.B. Lindquist 1990.
</blockquote>
<p>
To be fair to pre-rain of bricks QED –
we don't want to give the impression it is completely valueless! –
this section discusses a few of the most impressive
accomplishments of QED and by QEDists. This also will enable us
to describe the current state of the art, as well as to consider some computational
evidence.
</p><p>
The most oft-cited "great success" of QED is its prediction of the 
magnetic moment &#956;<sub>e</sub> of the electron.
This is one of the most extensive theoretical physics calculations
ever undertaken, compared with one of the most accurate experiments ever performed.
Aside from the sheer monumentalness of it, it also is interesting simply as a
prototypical case study of what QED calculations are like.
</p>
<center>
&#956;<sub>e</sub> = (g/2) &#956;<sub>B</sub> 
&nbsp;&nbsp;&nbsp; where 
&nbsp;&nbsp;&nbsp; 
&#956;<sub>B</sub> = eh/(4&#960;m<sub>e</sub>) &#8776; 9.27400968(20)×10<sup>-24</sup> Joule/Tesla
</center>
<p>
is the "Bohr magneton" and g is a dimensionless number calculated in QED as a power series
</p>
<center>
g/2 = &#8721;<sub>k&#8805;0</sub> C<sub>k</sub> · (&#945;/&#960;)<sup>k</sup>.
</center>
<p>
Here &#945;&#8776;1/137.036 is the "fine structure constant" and 
the "/2" is because the electron has spin 1/2.  The mainstream belief since Dyson 1952 is
that the series almost certainly diverges, but
because 
<nobr>&#945;/&#960;&#8776;0.00232</nobr>
is so small, it "looks like it is converging"
for quite a long way.  (The presumed divergence only "starts to happen" at terms of degree k
greater than any calculator has yet gone.)
</p>
<a name="qedelectronmagmom"></a>

<table bgcolor="#FFFFCC">
<caption>
<b>Table 4:</b>  
The coefficients C<sub>k</sub> in the QED calculation of electron magnetic moment.
The polylogarithms are defined by
&nbsp;
<nobr>Li<sub>s</sub>(z)=&#8721;<sub>n&#8805;1</sub>z<sup>n</sup>/n<sup>s</sup>=(-1)<sup>s</sup>&#915;(s)<sup>-1</sup>&#8747;<sub>0&lt;t&lt;1</sub>(ln t)<sup>s-1</sup> dt / (t-1/z)
</nobr>
&nbsp;
and the Riemann zeta function by 
&nbsp;
<nobr>&#950;(s)=Li<sub>s</sub>(1).</nobr>
</caption>
<tbody><tr bgcolor="pink">
<th>k=#loops</th><th>C<sub>k</sub> (exact)</th>
<th>C<sub>k</sub> numerical</th>
<th>C<sub>k</sub> quenched only</th>
<th>greatest<br>|diagram|</th><th>mean<br>|diagram|</th>
<td><b>#diagrams=#<sub>0</sub>+#<sub>1</sub>+#<sub>2</sub>+...</b>
<br>
where #<sub>j</sub> counts only diagrams with j closed electron loops,
0&#8804;j&lt;k (#<sub>0</sub>="quenched")
</td>
<th>lower bound on #GI classes</th>
<th>who</th><th>when</th>
</tr><tr>
<td>0</td>
<td align="center">1</td><td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1=1</td>
<td align="center">1</td>
<td>P.A.M.Dirac [see also
Folland's §4.3, esp. p.76; Thaller; Greiner &amp; Reinhardt ex.7.6]
</td><td>1920s</td>
</tr><tr bgcolor="aqua">
<td>1</td>
<td align="center">1/2</td>
<td align="center">0.5</td><td align="center">0.5</td>
<td align="center">0.5</td>
<td align="center">0.5</td><td align="center">1=1</td>
<td align="center">1</td>
<td>J.Schwinger
(And §6.3 of Peskin &amp; Schroeder,
or Folland's §7.11, or Bjorken &amp; Drell 1964 §8.6)</td><td>1948</td>
</tr><tr>
<td>2</td><td>
197/144 
+ &#960;<sup>2</sup>/12
- &#960;<sup>2</sup>(ln2)/2
<nobr>+ 3&#950;(3)/4</nobr>
<!-- MAPLE:
my2 := 197/144 
+ Pi^2/12
- Pi^2*ln(2)/2
+ 3*polylog(3,1)/4;
-->
</td><td>
-0.32847 89655 79193 78458</td>
<td align="center">-0.34417 (Adkins) <br>
-0.312 (Petermann)</td>
<td align="center">-1.125 from Adkins' SE in Fried-Yennie gauge; 
<!--?0.344166 from ae--> 
Petermann 1957 (Feynman gauge) has 0.778 from IIa
</td>
<td align="center">0.257 (Adkins) but 0.383 (Petermann)</td>
<td align="center">7=6+1
<br>
(or 5=4+1
<br>
by Adkins' and Petermann's counts)</td>
<td align="center">2</td>  <!--actually 3 by Cvit1977 reckoning-->
<td>A.Petermann
&amp; C.M.Sommerfield independently; Adkins 1989 redid in Fried-Yennie gauge
</td><td>1957</td>
</tr><tr bgcolor="aqua">
<td>3</td><td>
83&#960;<sup>2</sup>&#950;(3)/72
- 215&#950;(5)/24
- 25&#960;<sup>2</sup>(ln2)<sup>2</sup>/18
+ 25(ln2)<sup>4</sup>/18
<nobr>+ 100Li<sub>4</sub>(1/2)/3</nobr>
- 239&#960;<sup>4</sup>/2160
+ 139&#950;(3)/18
- 298&#960;<sup>2</sup>(ln2)/9
+ 17101&#960;<sup>2</sup>/810
+ 28259/5184
<!-- MAPLE:
my3 := 83*Pi^2*polylog(3,1)/72
- 215*polylog(5,1)/24
- 25*Pi^2*ln(2)^2/18
+ 25*ln(2)^4/18
+ 100*polylog(4,1/2)/3
- 239*Pi^4/2160
+ 139*polylog(3,1)/18
- 298*Pi^2*ln(2)/9
+ 17101*Pi^2/810
+ 28259/5184;

lr3 := 83/72 * Pi^2 * polylog(3,1) - 215/24 * polylog(5,1) 
+ 100/3 * (( polylog(4,1/2) + 1/24 * ln(2)^4 ) - 1/24 * Pi^2 * ln(2)^2 ) 
- 239/2160 * Pi^4 + 139/18 * polylog(3,1) - 298/9 * Pi^2 * ln(2) 
+ 17101/810 * Pi^2 + 28259/5184;
#the two are identical
-->
</td><td>1.18124 14565 87200 00627</td>
<td align="center">0.904979</td>
<td align="center"> <!--0.371 from M6LL; 2.466 from 6G; ?3.015 from 6B; -->
6.55 from D3</td>
<td align="center">2.01 (28 quenched diagrams only); &#8776;1.50 (full set of 40)</td>
<td align="center">72=50+21+1
<br>
(or 40=28+11+1
<br>
by Levine count)</td>
<td align="center">6</td> 
<!-- Kinoshita has said 5, Aoyama et al 2012 PTEP says GI classes = 6 -->
<td>S.Laporta, E.Remiddi, M.J.Levine, R.Z.Roskies</td><td>1996</td>
</tr><tr>
<td>4</td><td>Numerical Monte-Carlo results were thrice shown wrong 
due to algorithm bugs; 2007 recalculation used 2 largely-independent programs. 

</td><td><nobr>
-1.9106(20)
</nobr></td>
<td align="center"><nobr>-2.17550(194)</nobr></td>
<td align="center"> <!--?17 for diag.M18--> 
Kinoshita &amp; Lindquist 1990's top3 amalgamated quenched diagrams:
18.1(M10), 12.4(M18),<nobr>-6.6</nobr>(M13).
Aoyama et al 2008 instead gave 15.9, 17, -0.42 for those, plus
now give M47=10.59 and M46=-7.93.
</td>
<td align="center">2.52? (quenched amalgamated diags only)</td>
<td align="center">891=518+327+45+1</td>
<!-- http://arxiv.org/abs/0712.2607 says the 518 are represented by 
only 47 independent diagrams-->
<td align="center">13</td>
<td>T.Aoyama, M.Hayakawa, T.Kinoshita, M.Nio</td><td>2007</td>
</tr><tr bgcolor="aqua">
<td>5</td><td>
Long-standing guess 0.0±3.8 by Kinoshita et al
<!--http://arxiv.org/pdf/0706.3496v2.pdf page 1 left column-->
shows that even world's top expert was unable to guess sign (&amp; realized it) and
also unable to guess magnitude (but didn't realize that)
</td><td>9.16(58)</td><td align="center">10.092(570)</td>
<td align="center"> <!--3.876 from X343?, ?5.19 from X076--> 7.02 from &#916;M<sub>B2</sub>
seems record for unquenched diagram; 
but final AHKN paper should beat that with
23.7 from amalgamated quenched diagram
X144.
</td>
<!--MB2 is in a table in http://arxiv.org/abs/1201.2461 -->
<td align="center">3.25? (quenched amalg diags only)</td>
<td align="center">12672=6354+5099<br>+1140+78+1</td>  
<!--The 6354 diagram values are computed in a paper "in preparation" as of May 2012-->
<td align="center">32</td>
<td>T.Aoyama, M.Hayakawa, T.Kinoshita, M.Nio</td>
<td>announced 2010; final paper expected 2014</td>
</tr><tr>
<td>6</td>
<td align="center">(This row almost entirely guesses, denoted "?")</td>
<td align="center">&#8776;-44?</td>
<td align="center">&#8776;-61?</td>
<td align="center">40?</td>
<td align="center">4?</td>
<td align="center">202770=89782+...+1</td>
<td align="center">72-85?</td>
<td align="center">?</td>
<td align="center">?</td>
</tr>
</tbody></table>

<!-- the putative maximum diagrams all are a single electron line with L loops
all caused by a photon joining the electron line at 2 points, L such photons in all.
This happens apparently both when L=4 diags M18 and M10, and L=3 diag 6G.

The muon moment exptl value (BNL 2008):
(g-2)/2 = 116 592 089 (63) * 10^(-11)   [0.5 ppm]
planned future experiments at PARC and fermilab intend to attain 0.1 ppm.
-->

<p>
The exact evaluation of C<sub>3</sub> was completed 
by Laporta and Remiddi in 1996, but the effort began 27 years prior with 
Mignaco &amp; Remiddi 1969, and in addition to the 4 main authors listed in the table,
the following 10 also participated:
R.Barbieri, D.Billi, M.Caffo, T.Engelman, L.L.DeRaad Jr, J.A.Mignaco, K.A.Milton, 
D.Oury, R.C.Perisho, W.Tsai, S.Turini.
This evaluation was one of the more impressive feats 
of symbolic integration up to the year 2000.  
Note that even though this is an exact result, no human readable proof is available
and it is not clear one even could be produced since
some parts of the result were obtained by custom symbolic manipulation
computer programs running for about 10000 hours.
Their results were confirmed by independent Monte
Carlo numerical evaluations, by e.g, Kinoshita 1995.
</p><p>
Feynman/Dyson diagram techniques in momentum space
have dominated all substantial computations of this
sort since the 1950s because they usually are 
about 10 times less work than (say) Schwinger's QED techniques as well
as more easily automated.
(Tomonaga's techniques would be even more laborious still.)
However, the situation is not entirely 1-sided.
Occasionally one can replace some computation of a large set of
Feynman diagrams, with a single non-diagrammatic computation.
</p><p>
The inexact evaluation of C<sub>5</sub> by Aoyama et al 2010 is one
of the most impressive feats 
of numerical integration up to the year 2013 (albeit they promised at least one more paper in 
their series – about the 6354 quenched diagrams – which has not yet appeared
as of October 2013 hence they called their result quoted here "preliminary").
Note that historically, incorrect evaluations of 
C<sub>2</sub>,
C<sub>3</sub>
and C<sub>4</sub>
were published before the correct ones, suggesting the odds are below 50%
that the claimed C<sub>5</sub> calculation will be free from error.
<!-- partial account: http://arxiv.org/abs/hep-ph/0512288 http://arxiv.org/abs/0709.1568 -->
To give some idea of the magnitude of the C<sub>5</sub> task:
Each diagram corresponded to an integral over all of 13-dimensional space, whose
integrand typically was described by 80000 lines of FORTRAN code.
Typically each integrand was evaluated order 10<sup>11</sup> times using the adaptive
Monte Carlo routine VEGAS (Lepage 1978) to estimate both the integral and an error
estimate.  
The entire task took over 10 years of work by a 4-author team, 
an account of it was published distributed over
about 20 papers, and the computation was accomplished by programs with many millions of
lines of code in all (most of it automatically generated), running for over 500 computer-years
(feasible thanks to using about 100 computers).
</p><p>
It is interesting to consider this computation vis-a-vis the Dysonian question of whether
the series really does diverge, and really does so with superexponential(k) growth
(like a positive power of k!).  It looks as though
superexponential growth for the full series indeed
commences starting at the k=4 term (record 
term ratio |C<sub>5</sub>/C<sub>4</sub>|&#8776;4.79), while for the
"quenched QED" sub-series it appears to start at k=2 or k=3
(term |ratios| 2.6, 2.4, 4.6 then follow).
Of course since 
<nobr>&#945;/&#960;&#8776;0.00232</nobr>
this superexponential coefficient growth will not cause series divergence until 
k gets much larger.
Also note the alternating sign behavior when
k&#8805;1 for both quenched and unquenched series (does this continue forever?).
The table includes my rough guesses about C<sub>6</sub>
based on these trends.
</p><p>
The numbers DiagCount(k) of relevant Feynman diagrams with k loops were 
exactly counted by Cvitanovic et al 1978;  the 
<a href="http://oeis.org/A005413">sequence</a> 
is
</p><center>
DiagCount(1,2,3,...) = 
1, 7, 72, 891, 12672, 202770, 3602880, 70425747, 1503484416, 34845294582, ...
</center>
<p>
and they showed asymptotically 
</p>
<center>
DiagCount(k) = k! · 2<sup>k</sup>  · k  · (4/&#960;) ·
[1 - (5/2)k<sup>-1</sup> - (3/8)k<sup>-2</sup> + O(k<sup>-3</sup>)].
</center>
<p>
This <i>proves</i> superexponential growth, albeit for a different quantity (diagram count
rather than diagram value-sum).
However, counting is somewhat muddied by the fact
there are certain "generating function tricks" (and other tricks)
which can enable "combining 
the calculation of several diagrams into only one integral" to save work.  Thus
Adkins 1989's exact calculation of C<sub>2</sub> effectively required only <b>5</b>, 
not 7 diagrams
(and all of Adkins' 5 except his "VP" diagram are "quenched").
The reason for this discrepancy is that two of the five diagrams
have distinct mirror-image ("time reversed") diagrams.
With even more cleverness one could realize that only 4 "master integrals"
<i>really</i> were needed;
different "compression schemes" yield different counts.
Similarly Levine et al's fig.3 (page 165 of 
Kinoshita 1990; repeated below),
gives only <b>40</b> diagrams for C<sub>3</sub> (not 72),
of which 28 are quenched (not 50).
The exact calculation of C<sub>3</sub> completed by Laporta and Remiddi 1996 in retrospect
only required evaluating 17 parameterized "master integrals" 
(Laporta 2001)
in terms of which, the sum of all 72 diagrams could
be expressed.  
</p>
<a name="FIGelectmm340"></a>
<img src="WarrenSmithQED131123_files/ElectMagMom340.png" align="middle">
<p>
<b>The Feynman diagrams for computing electron magnetic moment at orders 
&#945;<sup>k</sup> for k=0,1,2,3.</b>
Note the letter "I" is intentionally not used to label any diagram.
The external photon representing "applied magnetic field" is assumed connected
to the topmost 2-valent vertex of each diagram, thus converting it to valency 3.
Photons are drawn with dashed and electrons/positrons with solid lines.
The 12 diagrams with k=3 of classes J,K,L,M,N are "unquenched"; the rest are quenched.
The unsymmetric diagrams labeled "×2" need to have their
values multiplied by 2 before computing the diagram sum (or ×4
in the case of diagram N2).  This is to account for left-right mirrored diagrams and/or
diagrams containing an electron loop whose electron-flow direction 
(clockwise/anticlockwise) can be reversed.
</p>
<a name="tabemmdiagvalues"></a>
<pre>k=0:
 Dirac       1
k=1:
 Schwinger   1/2
k=2:   Adkins' value (Fried-Yennie guage)             Petermann's value (Feynman gauge)
 C     -5/6*Ln2*pi^2+5/12*pi^2+5/4*Z3+7/12            -5/6*Ln2*pi^2+13/36*pi^2+5/4*Z3+1/6
 R     -3/16                                          1/18*pi^2+11/48
 SV    1/3*Ln2*pi^2-1/2*Z3-29/24                      1/3*Ln2*pi^2+1/18*pi^2-1/2*Z3-67/24
 SE    -9/8                                           -1/18*pi^2+11/24
 VP    -1/3*pi^2+119/36                               -1/3*pi^2+119/36
k=3:
 A1    -19/48*pi^2-1/8*Z3+51/32
 A2    139/216*pi^2-17/12*Z3+95/144
 A3    -143/432*pi^2-1/3*Z3+169/576
 B1    1/8*pi^2+33/32
 B2    11/324*pi^2-7/18*Z3-595/432
 B3    59/648*pi^2+7/18*Z3+733/1728
 C1    -5/18*Ln2^2*pi^2-5/144*pi^4-1/2*Ln2*pi^2+25/72*pi^2+71/24*Z3+20/3*b4-5/2
 C2    -19/18*Ln2^2*pi^2-5/216*pi^4+4/3*Ln2*pi^2-1429/864*pi^2+11/2*Z3+40/3*b4+235/288
 C3    11/6*Ln2^2*pi^2-37/864*pi^4-3*Ln2*pi^2+287/96*pi^2-17/3*Z3-14*b4-173/288
 D1    -4/9*Ln2^2*pi^2-11/432*pi^4+5/6*Ln2*pi^2+3/8*pi^2-23/12*Z3-4/3*b4-47/48
 D2    20/9*Ln2^2*pi^2-223/2160*pi^4-6*Ln2*pi^2+74/27*pi^2+89/12*Z3+20/3*b4-415/144
 D3    -16/9*Ln2^2*pi^2+127/1080*pi^4+38/9*Ln2*pi^2-239/144*pi^2-33/8*Z3-16/3*b4-349/288
 D4    4/3*Ln2^2*pi^2-11/80*pi^4-53/18*Ln2*pi^2+463/216*pi^2+25/8*Z3+4*b4-4
 D5    -1/90*pi^4-1/9*Ln2*pi^2+19/48*pi^2+5/12*Z3-763/288
 E1    -4/9*Ln2^2*pi^2+71/432*pi^4+5/6*Ln2*pi^2-493/432*pi^2-79/12*Z3-4/3*b4+155/144
 E2    8/9*Ln2^2*pi^2-191/1080*pi^4-37/18*Ln2*pi^2+221/144*pi^2+26/3*Z3+8/3*b4-7/144
 E3    2/9*Ln2^2*pi^2-11/4320*pi^4-17/18*Ln2*pi^2+313/432*pi^2-13/3*Z3+2/3*b4+53/36
 F1    -32/9*Ln2^2*pi^2+467/1080*pi^4-11/18*Ln2*pi^2-43/36*pi^2*Z3+311/144*pi^2-1213/24*Z3+235/6*Z5-32/3*b4+439/144
 F2    4/9*Ln2^2*pi^2-37/432*pi^4+1/3*Ln2*pi^2+85/36*pi^2*Z3-295/72*pi^2+629/12*Z3-140/3*Z5+4/3*b4-7/48
 F3    16/9*Ln2^2*pi^2-53/216*pi^4-11/9*Ln2*pi^2-17/36*pi^2*Z3+431/432*pi^2+6*Z3+5/3*Z5+16/3*b4+2929/576
 G1    2*Ln2^2*pi^2+47/720*pi^4+55/18*Ln2*pi^2+5/9*pi^2*Z3-2743/1296*pi^2-80/9*Z3-5/2*Z5-28*b4+2521/864
 G2    -113/54*Ln2^2*pi^2-31/540*pi^4-155/18*Ln2*pi^2-2/3*pi^2*Z3+3809/648*pi^2+199/24*Z3+25/6*Z5+200/9*b4-29/27
 G3    37/18*Ln2^2*pi^2-43/432*pi^4+83/18*Ln2*pi^2-3/8*pi^2*Z3-4777/2592*pi^2-635/72*Z3+95/24*Z5-28/3*b4+1835/864
 G4    -137/27*Ln2^2*pi^2+41/180*pi^4-101/18*Ln2*pi^2+95/72*pi^2*Z3+2401/2592*pi^2+69/4*Z3-215/24*Z5+160/9*b4-3017/864
 G5    1/9*Ln2^2*pi^2+199/2160*pi^4+25/18*Ln2*pi^2-43/72*pi^2*Z3+3/16*pi^2-107/12*Z3+95/24*Z5-44/3*b4+43/36
 H1    -1/3*Ln2^2*pi^2-53/1080*pi^4-7/3*Ln2*pi^2-2/3*pi^2*Z3+4165/1296*pi^2-1229/72*Z3+215/12*Z5+8*b4-515/432
 H2    -5/3*Ln2^2*pi^2+43/540*pi^4+70/9*Ln2*pi^2+29/18*pi^2*Z3-1951/324*pi^2+623/72*Z3-275/12*Z5-493/432
 H3    32/9*Ln2^2*pi^2-161/1080*pi^4+20/9*Ln2*pi^2-4/9*pi^2*Z3-1043/432*pi^2+97/12*Z3+5/12*Z5+8/3*b4-1/48
 J1    -4/9*Ln2^2*pi^2-7/270*pi^4-22/9*Ln2*pi^2+161/162*pi^2+49/18*Z3+32/3*b4+1145/432
 J2    2*Ln2*pi^2-3/2*pi^2-2*Z3+1547/432
 J3    -4/135*pi^2+8/3*Z3-943/324
 K1    14/9*Ln2*pi^2-25/36*pi^2-31/9*Z3+73/216
 K2    -5/9*Ln2*pi^2+5/18*pi^2+4/9*Z3+11/24
 L1    Ln2*pi^2-353/324*pi^2+403/144*Z3+133/216
 L2    19/162*pi^2+335/144*Z3-293/72
 M1    1/270*pi^4-77/27*Ln2*pi^2+427/243*pi^2-197/54*Z3+2005/324
 M2    1/270*pi^4-8/27*Ln2*pi^2-205/486*pi^2+97/54*Z3+599/162
 M3    13/270*pi^4-5/27*Ln2*pi^2+305/486*pi^2-251/54*Z3-2641/648
N1+N2  -2/3*Ln2^2*pi^2-41/540*pi^4-24*Ln2*pi^2-5/18*pi^2*Z3+931/54*pi^2-4/3*Z3+5/6*Z5+16*b4+5/9
</pre>
<p><b>Table 5: Values of Feynman diagrams for electron magnetic moment.</b> Given in units of 
<nobr>(&#945;/&#960;)<sup>k</sup>&#956;<sub>B</sub>.</nobr>  
Constants:  
Ln2=ln(2)&#8776;0.69314718.
pi=&#960;&#8776;3.14159265.  
Z3=&#950;(3)&#8776;1.20205690.
Z5=&#950;(5)&#8776;1.03692776.
<nobr>b4=Li<sub>4</sub>(1/2)+(ln2)<sup>4</sup>/24&#8776;0.52709719.</nobr>
"Renormalization constants" which cancel out after diagram summation, omitted (even if "infinite").
Symmetry factors (such as "×2" and "×4") already multiplied into tabulated values.
Note the values of N1 and N2 are not known; only their sum N1+N2 has been found.
</p><p>
<b>Sources:</b>
At order &#945;<sup>3</sup>, the 26 diagrams
A1, A2, A3, B1, B2, B3, C1, C2, C3, D1, D2, D3, D4, D5, 
E1, E2, E3, J1, J2, J3, K1, K2, M1, M2, M3, L1
are tabulated on pages 215-216 of Kinoshita 1990 
(article by  Roskies, Remiddi, Levine)
but do not use their L2 value and instead use
<nobr>
L2=19&#950;(2)/27+335&#950;(3)/144-293/72
</nobr>
from EQ5 of
Barbieri, Caffo, Remiddi 1972.
F1,F2,F3 are from EQ 9-11 of Laporta 1983 (with typo repaired for F1).
H1,H2,H3 were done in EQs 1-3 in Laporta &amp; Remiddi 1996.
N1+N2 was done in EQ1 of Laporta &amp; Remiddi 1991.
G1,G2,G3,G4,G5 were done in EQs 1-5 of Laporta 1995.
Another error was pointed out by Kinoshita and corrected by Sapirstein 
as mentioned in EQ20 of Kinoshita 1995.
</p><p>
The numerical calculation of 
C<sub>4</sub> by Kinoshita in 1974
effectively only required "about 120" diagrams, not 891,
and the recalculation by Aoyama et al 2008 
(following Kinoshita &amp; Lindquist 1990) handled the 518 quenched diagrams with 
only 47 "amalgamated diagrams"; the 6354 quenched C<sub>5</sub> diagrams similarly were
handled with only 389 amalgamated diagrams.
</p><a name="diagcrit" <="" a=""><p>
<b>Criticism:</b> In the old days, authors would actually compute 
and publish the values for every diagram.
This facilitated later checks by re-computors.  Nowadays, authors have taken to
using only "amalgamated diagrams" or (even worse) only stating the final result of the
computation while muttering about software they never publish.
If, say, those papers were transported back in time and
placed in front of Feynman, he would be utterly unable to verify or deny their validity.  
This situation is absurd and I consider it unacceptable.   I think anybody
doing this should publish their value for every single diagram.  I have no objection
to such tricks as "diagram amalgamation," but those who use such tricks should realize that
with a comparatively trivial amount of extra bookkeeping in their software (costing
virtually nothing) they
could still compute the un-amalgamated values.
<!-- About the 120, see
P.Cvitanovic &amp; T.Kinoshita:
Sixth-order magnetic moment of the electron,
Phys Rev D 10 (1974) 4007-4031; 
New approach to the separation of 
ultraviolet and infrared divergences of Feynman-parametric integrals,
Phys Rev D 10 (1974) 3991-4007;
Feynman-Dyson rules in parametric space,
Phys. Rev. D10,12 (1974) 3978-3991
-->
</p><p>
But anyway, because different calculators employed different such tricks,
the "greatest |diagram|," "mean |diagram|,"
and "diagram counts" columns
of our table should not be taken too seriously.
</p></a><p><a name="diagcrit" <="" a="">
Another reason not to take the former two
too seriously is the fact that the diagram values in general are
<i>gauge-dependent</i> and only the sums over certain "gauge invariant (GI) <i>sets</i>"
of diagrams yield gauge-invariant answers.  
This is because there are different expressions for the photon propagator
in different gauges. Commonly used named gauges include Feynman gauge,
<!--   - i g<sup>uv</sup> / (q<sup>2</sup> + i0)  -->
Landau gauge, 
<!-- 
L.D. Landau and I.M. Khalatnikov: Sov. Phys. JETP 2 (1956) 69-??.
- i [ g<sup>uv</sup> - q<sup>u</sup>q<sup>v</sup>/(q<sup>2</sup> + i0) ]/(q<sup>2</sup> + i0)
-->
Fried-Yennie gauge,
<!-- 
H.M. Fried and D.R. Yennie: New Techniques in the Lamb Shift, Phys. Rev.112 (1958) 1391-1404
-i q<sup>-2</sup> (g<sup>uv</sup>+2k<sup>u</sup>k<sup>v</sup>k<sup>-2</sup>)
-->
</a><a href="http://en.wikipedia.org/wiki/Light_cone_gauge">light-cone gauge</a>,
and Coulomb 
<!--
i Z_A P^{ij}(k) / (k^2 + i0)
-->
gauge.
For this reason different authors can (and do)
obtain different values for particular diagrams.
The concept of a "gauge invariant (sub)set of diagrams" was invented by Benny Lautrup then
explored further by Cvitanovic 1977.
For example, in the &#956;<sub>e</sub> problem at order k=2, the three GI-sets are {C,R}, {SE,SV}, {VP}, 
and since we have tabulated the diagram values in two gauges one may readily verify that
each of these three sets has the same-diagram sum regardless of gauge, at least for our two gauges.
Actually, we in our table copied "counts" from papers by Kinoshita et al
of the number of GI classes, 
but actually those counts are mere <i>lower bounds</i> 
on the true number of GI classes,
because as Cvitanovic 1977 points out, some of 
Kinoshita's classes can be split into subclasses
hence the counts we tabulated should be increased.  For example Aoyama et al 2010
regard all quenched diagrams as a single GI class but in fact when k=5
Cvitanovic showed this "single" class can be split into 9 subclasses.
Nevertheless the tabulated GI-class counts constitute valid lower
bounds, and the fact they always get 
multiplied by &#8805;2 as we go to the next table row (for k&#8805;1)
suggests growth which ultimately is at least exponential and already well underway.
For <i>quenched</i> QED, 
the number of GI classes, as counted by Cvitanovic 1977, grows only quadratically:
</p><center>
GI<sub>k</sub><sup>quenched</sup> = &#8970;(k+1)/2&#8971;·&#8968;(k+1)/2&#8969;
&nbsp;&nbsp; if &nbsp;&nbsp; k&#8805;1.
</center><p>
It was this counting formula that suggested the idea to Cvitanovic
that quenched QED might enjoy convergent perturbation series.
But I pointed out to Cvitanovic in email that his formula 
<i>also</i> perhaps is merely a lower bound
because he did not <i>prove</i>
that it was always impossible to split his GI classes even further.
(He did not respond, and I do not know the answer to this 
question; but as we just saw, Cvitanovic's
count is exact when 1&#8804;k&#8804;2.)
</p>
<blockquote>
On the basis of very skimpy numerical evidence, I conjectured that <i>gauge invariance</i>,
(which is known to imply cancellation of all the infinities in renormalized QED) might
also induce substantial cancellations within the <i>finite</i> terms –
indeed cancellations so dramatic that the growth rate of high order perturbation theory 
corrections to mass-shell gauge-invariant quantities is much slower than N!. 
<small>[Note: single diagrams in QED generally give results that <i>depend</i> on
the specific "gauge" used.   Gauge-<i>invariant</i> results are only obtained when
diagrams are <i>summed</i>. It is not necessary to sum all diagrams;
instead they may be partitioned into subsums, each over a gauge-invariant subset of
the diagrams. "Mass-shell" for Cvitanovic means the same as our word "quenched," i.e.
diagrams containing virtual leptons are forbidden.]</small>
In the case of the electron magnetic moment &#956;<sub>e</sub>, the smallest gauge invariant set 
contributing to (N=m+m'+j)th order consists of m photon "strands" attached to the incoming 
electron, m' photon "strands" attached to the outgoing electron, and j photon "strands" 
crossing the external photon vertex. Ignoring sets with electron loops and 
assuming that each gauge set gives a finite contribution leads to a guess 
that the perturbation series for the electron magnetic moment has Nth order coefficient of
(&#945;/&#960;)<sup>N</sup>, approximately of order N, <i>not</i> N!.
<br>&nbsp;&nbsp;&nbsp;
   It would be an incredible stroke of luck if my guess were anywhere close to the true 
asymptotics, but any growth rate slower than factorial (more precisely, exponential or slower)
would suffice for a convergent theory.
<br> &nbsp;
– <a href="http://www.cns.gatech.edu/%7Epredrag/papers/finitness.html">Recollections
</a> by Predrag Cvitanovic 
(during the 1990s)
about his 1977 quenched QED convergence conjecture.
</blockquote>
<p>
I prove a new theorem at the <a href="#giclasscountbound">end</a> of this section
showing the number GI<sub>k</sub> of GI classes in full <i>un</i>quenched order-k QED
grows ultimately superexponentially as k&#8594;&#8734;, i.e. faster than A<sup>k</sup>
for any fixed A&gt;1.  Thus even in this more meaningful sense (if one took the stance that
only diagram sums over GI classes "have meaning") one would expect superexponential series
divergence.   
My personal stance is individual 
diagram values may not have <i>physical</i> meaning but certainly have
<i>computational</i> meaning and it is 
interesting to study collections of them statistically.
We'll do the first (!) such study below.
Assuming Cvitanovic's quadratic-counting 
formula is exact, not merely a lower bound, then his
formula versus my theorem provides an impressive
contrast between quenched and unquenched QED.
</p><p>
Empirically, the diagrams with greatest absolute value seem always
(at least for the k&#8804;5 within the range of our table
– this may cease eventually!)
to consist of a <i>single</i> electron line with
k photon lines, each attached to the electron line at both endpoints.
(For the &#956;<sub>e</sub> problem
there always also is one external photon line representing an applied magnetic field.)
I.e. there are no "virtual electrons."
These have been called "quenched" diagrams.  It is easy to see that their count is
</p>
<center>
QuenchedCount(k) = 2<sup>-k</sup> (2k+1)! / k! 
= (2k+1)!! 
= 2<sup>k+1</sup> &#915;(k + 3/2) &#960;<sup>-1/2</sup>
</center>
<p>
This count formula can be regarded as exact <i>but</i>
we have not removed multiple counting of certain effectively identical 
or mergable diagrams.
The smaller quenched counts I tabulated 
(arising from counting mirror image pairs only once)
also are exactly known,
with
<a href="http://oeis.org/A005416">sequence</a> 
</p><center>
QuenchedCount(1,2,3,...) = 
1, 1, 6, 50, 518, 6354, 89782, 1435330, 25625910, 505785122, 10944711398, 257834384850, ...
</center><p>
These are asymptotically half as numerous.
Note that the quenched diagrams constitute a substantial 
subset, according to our two formulas 
asymptotically a fraction (2k/&#960;)<sup>-1/2</sup>, of all diagrams, and within
the tabulated range 0&#8804;k&#8804;5 over half the diagrams are quenched.
</p><p>
The best available theoretical prediction 
of &#956;<sub>e</sub> includes not only the above QED terms,
but also small corrections from non-QED effects
arising from (in decreasing order of importance)
</p>
<ol type="i">
<li>Muons.</li>
<li>Tauons. The muon &amp; tauon contributions can be computed by QED also,
using essentially the same formulas, 
provided the muon/electron and tauon/electron mass-ratios 
[206.7682843(52) and 3477.15(31) respectively according to CODATA 2010]
are regarded as known.
</li>
<li>Hadrons.</li>
<li>Contributions from weak forces.</li>
<li>Estimates bounding contributions arising from new, as-yet-unknown, particles.</li>
</ol>

<p> 
<!--
The resulting theoretical prediction of g would be accurate to about
&plusmn;2&times;10<sup>-11</sup>
if the fine structure constant &alpha;&asymp;1/137.035999679(94) were known exactly??
If the lepton mass ratios were also exactly known
this error would shrink to about 2.8&times;10<sup>-13</sup>.
-->
Accuracy is limited because &#945; is only
<a href="http://physics.nist.gov/cuu/Constants/index.html">known</a>
to 3 parts in 10<sup>10</sup>.
The best available experimental measurement of g/2 is
</p>
<center>
g/2 = 1.001 159 652 180 73 (28)  &nbsp;&nbsp;&nbsp; [Hanneke, Fogwell, Gabrielse 2008]
</center>
<p>
Note their claimed error is 0.28 parts per <i>trillion</i>, making this one of the most 
precise experimental measurements ever made.  Some extremely clever ideas were used
to attain this accuracy.  (<i>The</i> most precise measurements I know of are 
comparisons of different kinds of atomic clocks, which can be regarded as measuring
certain atomic spectral ratios accurate to about 1 part in 10<sup>17</sup>
or 10<sup>18</sup>.)
If this g-measurement is regarded as a measurement of 1/&#945; it is the most-precise
available and yields
</p>
<blockquote>
<pre>137.035 999 1736 (68)(46)(26)(331)     From g measurement plus QED theory
137.035 999 679 (94)     Previous CODATA 2006 value of 1/&#945;... oops.
137.035 999 049 (90)     CODATA 2010, mostly from Rb spectroscopy 
</pre>
</blockquote>
<p>
This (&#8805;9)-figure agreement between theory and experiment
is commonly described as "stunning"
and an amazing verification of QED theory, albeit with the old CODATA 2006
experimental 
value of 1/&#945; it actually would have been a <nobr>&gt;5&#963;</nobr> refutation of QED.
</p><p>
Contrasting the "mean |diagram|" and "greatest |diagram|" columns of
the table with the values of C<sub>k</sub>
confirms Cvitanovic 1977's observation that there is a tremendous amount of cancellation going on.
He <a href="#cvitconj">boldly</a> (but I believe wrongly) conjectured
that for "quenched QED" this cancellation would continue and grow so enormous that
quenched QED series would actually converge.
<!--
Cvitanovic argues diagrams cannot matter, only gauge sets of diagrams can mater.
Diagram count grows factorially.  However, he claims in EQ7 with no proof whatever that
gauge set count grows only polynomially and has the generating function
   x/((1+x)*(1-x)^3) =  x / ((1-x)^2 * (1-x^2)) =
  1*x+2*x^2+4*x^3+6*x^4+9*x^5+12*x^6+16*x^7+20*x^8+25*x^9+30*x^10+36*x^11+42*x^12
   +49*x^13+56*x^14+64*x^15+72*x^16+81*x^17+...
and exact formula for this sequence is floor(n2)*ceiling(n/2)
-->
</p><p>
I see no evidence for that conjecture.
Indeed Cvitanovic 1977's numerical predictions seem
refuted thanks to the Aoyama et al 2010 calculations.
Cvitanovic was particularly concerned with "quenched" QED where we
only consider diagrams without virtual pairs, i.e. with only the original electron plus 
photons.  Within that subset of renormalized 
QED (which seems to yield the greatest-valued diagrams,
at least at low orders) Cvitanovic's convergence conjecture 
survives various divergence arguments by Dyson 1952, Lautrup 1977, and me
(see <a href="#dyson">§5</a> and  <a href="#ratediv">§6</a>),
but remains directly contradicted by another divergence argument by 
Bogomolny &amp; Kubyshin 1981.
</p><p>
<b>Statistical analysis of QED diagram-value data (apparently the first!):</b>
After 50 years of work computing the values of Feynman diagrams, it is long past time
to have a look at the statistics of collections of them.  Unfortunately,
too many authors make that difficult (see above <a href="#diagcrit">criticism</a>).
My method: I gather diagram-value datasets, then, simply viewing them as
collections of numbers,  examine their statistics.  
The first such datasets are those tabulated above
for the &#956;<sub>e</sub> problem at order &#945;<sup>k</sup> with 0&#8804;k&#8804;3.
This picture below shows several such datasets; we then also tabulate 
various facts about the datasets with N&#8805;7 values, then give those datasets themselves.
</p><p>
<a name="FIGqeddatasets"></a>
<img src="WarrenSmithQED131123_files/QEDdatasets.png" align="middle" hspace="30">
<a name="qeddatasummary"></a>
</p><p>
<table bgcolor="pink">
<caption>
<b>Table 6:</b> Statistical summary of our QED diagram-value datasets.
</caption>
<tbody><tr bgcolor="yellow">
<th>N</th>
<th>mean</th>
<th>median</th>
<th>min</th>
<th>max</th>
<th>absdev</th>
<th>stddev</th>
<th>sum</th>
<th>kurtosis</th>
<th>skewness</th>
<th>|subsum55/sum|</th>
<th>confid(nonnormal)</th>
</tr>

<tr>
<td>7</td>
<td>0.0</td>
<td>0.0</td>
<td>-89.6987</td>
<td>48.0</td>
<td>26.771</td>
<td>40.378</td>
<td>0.0</td>
<td>3.7941</td>
<td>-1.2741</td>
<td>&#8734;</td>
<td>97.2%</td>
</tr>

<tr>
<td>23</td>
<td>-6.957</td>
<td>0.0</td>
<td>-358.8</td>
<td>455.0</td>
<td>118.2</td>
<td>169.7</td>
<td>-160.0</td>
<td>3.957</td>
<td>0.2369</td>
<td>2.1481</td>
<td>93.5%</td>
</tr>

<tr>
<td>28</td>
<td>0.032321</td>
<td><nobr>-0.10358</nobr></td>
<td>-4.194</td>
<td>6.547</td>
<td>2.025</td>
<td>2.6696</td>
<td>0.904988</td>
<td>3.13682</td>
<td>0.69711</td>
<td>6.40548</td>
<td>92.5%</td>
</tr>

<tr>
<td>40</td>
<td>1.3609</td>
<td>3.8273</td>
<td>-89.0322</td>
<td>74.6290</td>
<td>18.0025</td>
<td>29.0755</td>
<td>54.436</td>
<td>5.85199</td>
<td>-0.60221</td>
<td>1.44212</td>
<td><!--99.7%  using |skew|&kurt-->99.99%*</td>
</tr>

<tr>
<td>62</td>
<td>-0.01931</td>
<td>0.06897</td>
<td>-4.25725</td>
<td>1.83827</td>
<td>0.68363</td>
<td>1.0242</td>
<td>-1.1974</td>
<td>8.43593</td>
<td>-1.74575</td>
<td>2.76774</td>
<td>99.9995%</td>
</tr>

<tr>
<td>389 &nbsp;</td>
<td>0.01156</td>
<td>0.0092</td>
<td><nobr>-20.5911</nobr></td>
<td>23.7308</td>
<td>3.24977</td>
<td>5.28787</td>
<td>4.49645</td>
<td>6.76163</td>
<td>0.25610</td>
<td>9.24869</td>
<td>99.99999999999998%*</td>
</tr>
</tbody></table>

</p><p>
Explanation of some of the columns:
</p><ul>
<li>
Skewness=&#956;<sub>3</sub>/&#963;<sup>3</sup>
where &#963;=stddev=&#8730;&#956;<sub>2</sub>, &#956;=mean, 
and 
<nobr>
&#956;<sub>k</sub>=N<sup>-1</sup>&#8721;<sub>j</sub>(x<sub>j</sub>-&#956;)<sup>k</sup>.
</nobr>
Skewness is zero for any even-symmetric probability density, such as normal density.
</li><li>
absdev=N<sup>-1</sup>&#8721;<sub>j</sub>|x<sub>j</sub>-&#956;|.
</li><li>
Kurtosis=&#956;<sub>4</sub>/&#963;<sup>4</sup>.
For a normal density, kurtosis=3.
</li><li>
|subsum55/sum|: choose a random subset of &#8970;0.55N&#8971; data
items, compute their sum, divide by the sum of all N data items,
and take the absolute value.  The expected value of the resulting quantity, is tabulated.
This expectation would be below 1 for any set of independent samples from any probability density.
So it is remarkable that for our 6 datasets, it always exceeds 1.
</li><li>
Confidence the data are <i>not</i> samples from any normal density:
*=Shapiro-Wilk test of normality; if no asterisk then test instead based on
exact probability that N true-normal deviates simultaneously have 
<nobr>kurtosis&#8805;ours</nobr> 
and <nobr>|skewness|&#8805;|ours|.</nobr>
</li></ul>
<p></p><p>Data sources:</p><p></p><ul>
<li>
7 &amp; 23: Broadhurst 1999, all quenched diagram values for QED beta function 
&#945;<sup>3</sup> and
&#945;<sup>4</sup> terms.
(Exact.)
</li><li>
28: From a preceding table. All 
quenched diagram values for electron magnetic moment &#945;<sup>3</sup> term.
(Exact.)
</li><li>
40: Broadhurst 1999, all quenched diagram values for QED gamma function 
&#945;<sup>4</sup> term.
(Exact.)
</li><li>
62: There are 83 diagrams involved in the 
&#945;<sup>2</sup> correction
to the (ortho)positronium decay rate 
(Adkins, Fell, Sapirstein 2002)
which they partition into classes
{a6,b4,c2,d7,e9,f9,g9,h6,i6,j21,k6}
where the letter is A-F-S's class name and the number says how many diagrams lie in that class.
This is 85 diagrams, but the k class contains two pairs of diagrams with identical values
due to charge conjugation symmetry,
hence their count of 83.
Their values are tabulated in their table VII (page 174) through XV (page 185)
then summarized in table XVI in which all prior values are multiplied by 5.174.
However, they do not compute the 21 diagrams in class j individually, instead referring
to previous papers by Burichenko and Adkins in which the whole j-total was
evaluated by a non-diagrammatic method.  Also, there are 2 diagrams in class c
which they effectively amalgamate, only evaluating their total.  This brings us down to
62 diagrams by omitting classes c and j.  (The picture shows 64=62+2 points
by including the combined sum of all c-class and j-class diagrms as the two extra 
datapoints.  The rightmost plotted point is the j-sum.)
Also, A-F-S's tables gave <i>unrenormalized</i> values for the diagrams of
types a, b, f, whereupon A-F-S applied an
additive renormalization adjustment to the a-sum, b-sum, and f-sum bodily.
I have handled that by regarding these additive adjustments as divided into 6,4,9 equal parts
(respectively) for each diagram in that class,
specifically I added
<nobr>-0.462452,</nobr>
<nobr>-0.170814,</nobr>
and
0.0227550
to each diagram in classes a,b,f respectively.  
(Inexact values obtained by numerical integration.)
</li><li>
389: The amalgamated quenched diagrams contributing 
to the electron magnetic moment &#945;<sup>5</sup> term.
These come from Kinoshita 2010 and I suspect he would prefer that I label this 
dataset "preliminary"; a later and more accurate version hopefully will be published
by Aoyama, Hayakawa, Kinoshita, Nio in about 2014.
(Inexact values obtained by numerical integration.)
</li></ul>
<p>
The data itself:
</p>
<pre><b>7:</b>
  -89.69873134, -4.0, 0.0, 0.0, 21.69873134, 24.0, 48.0
<b>23:</b>
  -358.7949254, -284.060650, -259.842889, -181.461265, -160.0, -87.7290572, -59.000239, -16.541887,
  -14.397224, -4.501126, -0.210292, 0.0, 0.0, 0.0, 0.0, 48.0, 71.335567, 89.864621, 118.0569164,
  140.570384, 151.696904, 192.0, 455.0151627
<b>28:</b>
 -4.193950295, -3.968284687, -3.374304684, -2.751419537, -2.670554736, -2.463225523, -1.889707995,
 -1.757936344, -1.509702226, -1.286976760, -1.20637654, -0.334695087, -0.1523009747,
 -0.1229813083, -0.0841742776, -0.02679949333, -0.0071460755, 0.607752806, 0.617711789,
 0.7995910678, 0.8282765618, 1.790277777, 1.861907866, 2.264950550, 2.570137010, 5.308081480,
 5.50993365, 6.546895249
<b>40:</b>
  -89.03216785, -74.8965291, -59.10315630, -21.09557500, -12.4423833, -10.48739472, -10.0569018,
  -9.30258296, -8.78564900, -7.5961228, -7.1643156, -6.08792245, -6.03951089, -5.3219319,
  -4.2148276, -3.1251142, -0.5307823, 0.0, 1.0894554, 3.6589822, 3.99567133, 4.51925305,
  4.7252562, 5.2780677, 5.7664672, 7.0126765, 7.1451338, 7.6101899, 8.062711879, 8.55111138,
  13.18148787, 13.4980616, 14.9863187, 15.6608012, 15.81851213, 22.9160967, 41.673713,
  43.45352680, 66.48638993, 74.6289859
<b>62:</b>
  -4.25724499, -3.77986499, -1.41440000, -1.38345228, -1.31274499, -1.25105000,
  -1.04744200, -0.98073000, -0.88648000, -0.82448000, -0.68810000, -0.54947000,
  -0.42335228, -0.41710100, -0.36961000, -0.36691000, -0.33290000, -0.29325228,
  -0.20377000, -0.19432605, -0.19198000, -0.19198000, -0.14045000, -0.11320405,
  -0.06111405, -0.01829700, -0.01738500,          0.01216700, 0.02937500,
  0.04078100, 0.05764772, 0.08028900, 0.11178000, 0.13983400, 0.18474772,
  0.18862000, 0.22911501, 0.23238595, 0.23304600, 0.33738101, 0.39124000,
  0.40711000, 0.43885000, 0.44420000, 0.45678000, 0.47344000, 0.47344000,
  0.53500000, 0.57410000, 0.71500000, 0.77100000, 0.77184772, 0.77354000,
  0.81185501, 0.91155000, 0.94806000, 1.13363501, 1.28294501, 1.29571900,
  1.59280501, 1.59612500, 1.83827000
</pre>
<p>
<b>Conclusions:</b>
The rightmost column of the summary table shows
these datasets are <i>not</i> normally distributed.  Further, Cvitanovic's
observation of remarkably high level of "cancellation" proving these
data are <i>not</i> independent samples
from any distribution, is supported by our table's 
subsum55 column.  The "4 term relations" discussed by Kreimer 2000 are
one example of a statistical dependency 
(these should generate a large network of such inter-diagram dependencies), 
partially explaining that.
I also attempted to use tests like the 
Hartigan-Hartigan "dip test of unimodality" and simpler "small gap tests"
to assess whether these datasets were "clustered" i.e. inconsistent with arising from 
any <i>unimodal</i> 
distribution; but on that question I failed to reach a statistically significant 
conclusion. (Warnings: rounding data to 2 decimal places <i>then</i> applying
the Hartigans' test will often give erroneously large dip statistics!
Also, Hartigan's published "AS 217" algorithm is buggy.)
</p><p>
<b>Warning to reader:</b>
It is possible that the above statistical events are instances of the "law of small numbers," e.g,
QED at 100th order perhaps might have diagram values that look quite normal, due to non-normal
effects which go away at high orders.   We're only able to statistically analyse the data we have, 
not the data we wish we had.
</p>
<!--
<p>
<b>The "fractal hierarchical model."</b>
This is
a possible simplistic statistical model, by me,
which is capable of approximating both Cvitanovic's
observed cancellations and the Bogomolny prediction that ultimately the mean quenched diagram
value is quite nonzero (even though these initially seem opposed goals), 
plus it can also handle multimodality
and the possibility for C<sub>k</sub> growth 
(versus the root mean square diagram value),
proportional to a power
of the number of diagrams
<i>not</i> equal to 1/2 or 1. 
Imagine that N diagram values are generated by the following kind of random process.
Generate N<sub>1</sub> random numbers,
then generate 
N<sub>2</sub> random numbers from a distribution of smaller width,
then generate 
N<sub>3</sub> random numbers from a distribution of still smaller width,
etc, and then add the numbers from different groups in all possible ways, obtaining 
N=N<sub>1</sub>N<sub>2</sub>N<sub>3</sub>...N<sub>J</sub>
sums in all.
The resulting N numbers will look like N<sub>1</sub> values;
but each of those values will (upon closer inspection)
be seen to be a cluster of
N<sub>2</sub> nearby values, and so on.  
(In mathematics, a construction like this one generates "Cantor sets.")
If the N<sub>1</sub> numbers happened to have mean near 0,
then the N numbers will exhibit "great cancelation" far more than expected from
any more-naive statistical model such as "N independent samples from a normal distribution."
On the other hand, by making the N<sub>1</sub> numbers have quite-nonzero mean, we instead can 
cause this model to exhibit far lesser cancellation than the naive one.
Also, the distribution at level k+1 might depend on the values the levels&le;k
in a more general model.
I am not contending this model is correct or best; I merely am pointing out that
it can have some of the right qualitative behaviors unobtainable by 
any naive "just sample from some fixed unimodal density" model.
</p>
-->

<p>
<b>Other comparisons of QED with experiment:</b> 
</p>
<ol>
<li>
"Lamb shifts" in the spectrum of hydrogenic atoms (a nucleus orbited by only 1 electron);
also for "muonic atoms" where the electron is replaced by a muon;
</li><li>
Scattering cross sections for various kinds of particle collisions
</li><li>
The spectrum of the helium atom (which has 2 electrons); 
similarly one may consider
at helium-like atoms (still 2 electrons, but with other nuclei).
</li><li>
The magnetic moment of the positron (same as for the electron, theoretically) has also
been measured to high precision.  One can also predict and measure the magnetic moment of the muon.
</li>
</ol>
<p>
All of these seem to 
have been highly successful in the sense that they
confirmed QED predictions to high accuracy.   
Yerokhin &amp; Pachucki 2010
successfully predict,
for example, that the 
<nobr>2<sup>3</sup>P<sub>2</sub>-2<sup>3</sup>S<sub>1</sub></nobr>
transition in Helium-4 should correspond to a 276732186.1(2.9) cm<sup>-1</sup>
spectral line, versus the experimental value  276732186.593(15).
For Magnesium with 2 electrons, the same transition was predicted to be 
80122.3(4), versus the experimental 80121.53(64).
For hydrogen, the 
<nobr>1S<sub>1/2</sub>-2S<sub>1/2</sub></nobr>
transition frequency was <i>both</i> measured and predicted to be
2466061413187103(46) Hz (!),
while the 
<nobr>1S<sub>1/2</sub>-2S<sub>1/2</sub></nobr>
transition was predicted to be
770859252845100(2800) Hz
and measured as 
770859252849500(5900) Hz,
even though both these predictions ignored "hyperfine effects."
(These are from  Jentschura, Kotochigova et al 2005 using measurements they cite.)
</p><p>
However, in terms of agreeing
with experiment to within claimed
error bars, the success has sometimes been more debatable.
We'll now discuss several cases.
</p><p>
One of the larger disagreements at present,
2.4&#963;, is for the <b>magnetic moment of the muon</b>
(Bodenstein et al 2013, in a paper which should have been updated with latest QED calculations
from Aoyama et al).
This is still a tiny disagreement – they claim
experiment=116592089(63)
versus theory=116591897(42)(26)(2)
for a discrepancy of 1.6 parts per million.
And both the experiment and the theory 
were so difficult that it is hard
to be confident this disagreement is real and not the product of a mistake, plus
2.4&#963; with normal statistics represents 99.2% confidence of a disagreement, which is
not tremendously significant anyhow.
If, however, Bodenstein et al's (perhaps optimistic)
hadronic computation is spurned, previous computations
found 3.6&#963; disagreement, which would have been 99.98% confidence the standard model
was refuted.
This disagreement has optimistically been regarded by some
as evidence for new physics, such as "supersymmetry,"
beyond the standard model (or merely:
"there is something funny about the muon"!).
</p>
<a name="FIGprotrad"></a>
<img src="WarrenSmithQED131123_files/ProtonRadiusFig.jpg" align="right" hspace="25" width="33%">
<a name="protradiuspuzzle"></a><p>
Spectral quantities measured in muonic hydrogen, when compared with a QED prediction
(where this prediction, to attain high accuracy,
needs to know that the proton is not a point),
currently disagree with CODATA 2010 about the RMS charge radius of the proton by
about 4%, a discrepancy of 6&#963; to 8&#963; depending on the 
author making the claim.  The current situation is illustrated
in the picture (stolen from Margolis 2013).
This has been called the <b>"proton radius problem"</b> and it <i>is</i> a QED problem because
the same QED calculation for ordinary electronic hydrogen, and proton radius measurements
based on ep scattering, both agree with CODATA about
the proton radius.   Why should the proton shrink when orbited by a muon?!?
This is reviewed by Pohl et al 2013.
Robson 2013 has proposed what he claims is "a plausible solution of the
proton radius puzzle" although others have dismissed it as naive.
To shed light on this, more precise &#956;p and ep scattering experiments are planned.
</p><p>
I myself, in early November 2013 email to Pohl, Robson, and Margolis,
proposed my own <b>new idea for solving the proton radius puzzle</b>, which I now outline.  
It is that the proton's charge (and mass) distributions are
<b>not spherical</b> and would be better approximated by ellipsoids.   
As a spinning extended body, the proton of course would be expected to be aspherical.
Some quick <b>numerical sanity checks</b> suggest that this idea should cause the correct
order of magnitude effect to explain the 4% radius-discrepancy:
</p><ol><li>
Crudely model the proton as a classical spinning body with angular momentum 
&#8463;/2
where 
&#8463;&#8776;1.05×10^<sup>-34</sup> meter<sup>2</sup>kg/sec,
mass=M<sub>p</sub>&#8776;1.67×l10<sup>-27</sup> kg, and RMS radius 
r&#8776;0.88 femtometer.  If all the mass were concentrated
on the perimeter of a circle, then
that circle would be rotating with speed v&#8776;0.12c.
(This was obtained by solving M<sub>p</sub>rv=&#8463;/2 for v.)
The kinetic energy associated with this would be 
<nobr>M<sub>p</sub>v<sup>2</sup>/2&#8776;0.0072M<sub>p</sub>c<sup>2</sup>.</nobr>
But it seems more reasonable to model the proton as a solid ball of uniform density,
in which case 
<nobr>r<sub>RMS</sub>=(3/5)<sup>1/2</sup>r<sub>ball</sub></nobr>
and 
<nobr>r<sub>inertia</sub>=(2/5)<sup>1/2</sup>r<sub>ball</sub></nobr>
i.e.
<nobr>r<sub>inertia</sub>=(2/3)<sup>1/2</sup>r<sub>RMS</sub>.</nobr>
This would increase the kinetic energy 
to <nobr>0.01M<sub>p</sub>c<sup>2</sup>.</nobr>
This makes it seem reasonable that the asphericity should exceed 1%.
</li><li>
Buchmann &amp; Henley 2000 used manual QCD calculations to estimate
the "intrinsic quadrupole moment of the proton" as 0.113 or 0.565 femtometer<sup>2</sup>e
in two crude models.  Both of these since positive would indicate the proton is a prolate
spheroid – cigar shaped; note this contradicts my naive classical model above, which would
predict oblate, i.e. pancake, shape.  Buchmann &amp; Henley
noted these quadrupole moments would 
correspond to ellipsoidal proton with axis ratio 1.11 and 1.73 respectively.
They also cite certain experiments which they claim prove the proton is aspherical,
but the interpretation of exactly what those experimental 
results tell us about the precise shape of
the proton, is tricky.
</li><li>
<a name="dipquadtab"></a>
A third crude way to estimate the "intrinsic quadrupole moment of the proton"
is to extrapolate from measured quadrupole moments of other nuclei.
<table bgcolor="pink">
<caption>
<b>Table 7:</b> Some measured magnetic dipole and electric quadrupole moments.
</caption>
<tbody><tr bgcolor="aqua"><th>Nucleon</th><th>charge</th><th>mass</th><th>spin</th>
<th>Magnetic dipole</th><th>Electric quadrupole</th></tr>
<tr><td>proton</td><td>1</td><td>1</td><td>1/2</td><td>+2.793</td><td>?</td></tr>
<tr><td>neutron</td><td>0</td><td>1</td><td>1/2</td><td>-1.913</td><td>?</td></tr>
<tr><td> H-2</td><td>1</td><td>2</td><td>1</td><td>+0.857</td><td>+0.286</td></tr>
<!-- <tr><td>He-3</td><td>2</td><td>3</td><td>1/2</td><td>-2.128</td><td>?</td></tr> -->
<!--<tr><td>He-4</td><td>2</td><td>4</td><td>0</td><td>0</td><td>0</td></tr> -->
<tr><td>Li-7</td><td>3</td><td>7</td><td>3/2</td><td>+3.256</td><td>-4.06</td></tr>
<tr><td>C-12</td><td>6</td><td>12</td><td>2</td><td>0</td><td>+6±3</td></tr>
<tr><td>O-17</td><td>8</td><td>17</td><td>5/2</td><td>-1.894</td><td>-2.58</td></tr>
<tr><td>Na-22</td><td>11</td><td>22</td><td>3</td><td>+0.535</td><td>?</td></tr>
</tbody></table>
(Units for the last two columns: nuclear magnetons and femtometer<sup>2</sup>e.)
Observe that, starting from H-2, the mass-sequence 2,7,12,17,22 
is described by the formula 5k-3 for k=1,2,3,4,5.
The spin is (k+1)/2.
The charge is the nearest integer to (k+1)ln(k+1).
So if we extrapolated to k=0.8 then we'd find mass&#8776;1, spin=0.9, charge&#8776;1.
There does not seem to be any obvious extrapolated value of the quadrupole or 
magnetic dipole moments,  but as an order of magnitude estimate 
<nobr>±0.5femtometer<sup>2</sup>e</nobr>
seems reasonable for the proton, agreeing to that extent with Buchmann-Henley.
</li></ol>
<p>
<b>Naive objection to this resolution:</b> 
The "Wigner-Eckart theorem" in QED 
tells us (since the proton has spin=1/2)
that the proton's 
intrinsic quadrupole (or any higher) moment 
should cause an <i>actual</i> moment (as judged by the far electric fields
generated by a proton) that is <i>zero</i>.
This in turn should cause <i>zero</i> spectroscopic effects
in a muonic or ordinary electronic hydrogen atom.
Hence this could not be the cause of the problem.
(The Wigner-Eckart theorem, and the distinction between the "intrinsic" quadrupole
moment of a nuclear particle and its external far-field based, or spectroscopically-inferred,
quadrupole moment, are discussed on pp.343 &amp; 347 of the textbook by Scheck 2013,
and on pp.85-94 of the textbook by Rose.
The original paper is Eckart 1930.)
<!--Paul Roman: Advanced Quantum Theory (Addison-Wesley 1965), p.583.-->
</p><p>
<b>Refutation of that objection:</b>
That objection is not valid if the muon or electron is allowed to go <i>inside</i> the
non-point proton, since then it "sees" only part of the proton.  Since the proton
is made of 3 quarks, such a muon then might "see" only 2 of them, with spin=1, and
quadrupole moments of spin-1 nuclei are 
entirely capable of exerting spectrosopic effects (as has been experimentally verified
many times to high precision, see above table re the dueteron).  
Since the whole "proton radius puzzle" is
entirely <i>about</i> the possibility muons or electrons go inside the proton, trying
to dismiss this idea with Wigner-Eckart is absurd.   Further, if you really believed that
the electric quadrupole moment of a non-point composite nucleus 
with spin&#8804;1/2 really was mathematically
unable to affect atomic spectra (even when electron 
allowed to go inside nucleus) then you would be forced
to believe that the 
He<sup>+</sup> ion and (HD)<sup>-</sup> molecular ion
(using He-3, and modeling the nuclei as stationary point charges in both cases, which of course
is a good enough model for part-in-thousand accuracy) would necessarily have identical spectra
– similarly for
<sup>4</sup>He<sup>+</sup> ion and (DD)<sup>-</sup> –
with indeed the spectrum of the latter being <i>independent</i> of the separation of 
its two nuclei.
That belief would be experimentally hugely wrong.
</p><p>
Another molecular analogue is the bent tri-atomic molecule CH<sub>2</sub>
(<a href="http://en.wikipedia.org/wiki/Methylene_%28compound%29">methylene</a>,
a "colourless gas that fluoresces in the mid-infrared range, and only persists in dilution,
or as an adduct... usually detected only at very low pressures or very low temperatures").
If its atoms are C-13 and H-1, each of which has nuclear spin 1/2, and its three
nuclear spins are not all aligned, then its
three nuclei are analogous to the three spin=1/2 quarks (two "up" quarks
and one "down" quark; the up-quark mass is 2.3 and the down-quark mass is 4.8 MeV/c<sup>2</sup>) 
with spins not all aligned, constituting a proton.
Compare this 
CH<sub>2</sub> molecule with the O-15 atom.  The unstable oxygen-15 nucleus, which can be viewed
as a C-13 and two H-1 nuclei glued together, has spin-1/2 and halflife 122.24 seconds.
Equivalently, we can view the C-13 and two H-1 nuclei as "a version of the O-15 nucleus
which happens to be much larger so that electrons can go 'inside' it."
Do you believe that the spectroscopy of this CH<sub>2</sub>
molecule is identical to the spectroscopy of an O-15 atom?  
Or (even more ludicrously) do you believe the CH<sub>2</sub>'s spectroscopy 
is <i>independent</i> of its 3 internuclear spacings... so that at large spacing we'd
conclude the spectroscopy of one C-13 and two H-1 atoms, all isolated, must be identical
to the spectroscopy of an O-15 atom?  Not so.  
</p><p>
<b>Conclusion about proton radius puzzle:</b> 
observe that a muon would be expected to couple far more strongly than 
an electron to proton asphericity.  For example, 
with M<sub>muon</sub>&#8776;207M<sub>electron</sub>
its orbit should be 207 times smaller and velocity 207 times greater, producing
magnetic and electric fields at the nucleus both
207<sup>2</sup> times greater, and a |wavefunction|<sup>2</sup>
which near the nucleus is 207<sup>3</sup> times greater. Also note that "D" states
(highly aspherical)  of
muonic hydrogen were used in all problematic experiments so far,
which (one might conjecture pending further experimental work) is not a coincidence.
Given all this, <b>it is reasonable to hope that
the "proton radius puzzle" is largely explicable using the asphericity of the proton</b>.
Previous analysis reviewed by Pohl 2013 had not considered proton asphericity.
Unfortunately although 1000s of compute-years have been wasted by lattice-QCD runs determining
the masses of the proton, neutron, and other baryons (nowadays achieving about 3% accuracy), for
some inexplicable reason those computors never tried to determine the quadrupole moments
or other measures of the aspherical shape of the baryons, even though that would seem to be
a comparatively trivial amount of additional work.  Further, I have been unable to find any
experimental literature directly assessing this shape-question.
</p><p>
Another experiment which seemed for 20 years to <i>refute</i> QED was the 
precise measurement, in 1989
by a group at the University of Michigan in Ann Arbor, of the 
<b>(ortho)positronium mean lifetime</b>, or equivalently its "decay rate."
It differed from the theoretical prediction by over 10 standard deviations.
This sparked the following developments:
</p><ol><li>
New direct measurements by a group in Tokyo (Asai 1995) failed to confirm Michigan.
Another measurement by Hasbach et al (1987) also failed to see Michigan's
discrepancy.  However neither measurement was as precise as Michigan was claiming.
</li><li>
New theoretical calculations by Adkins, e.g. up through order &#945;<sup>2</sup>
(and some partial results at higher order have also been found)
both got more precision than, and
confirmed, earlier theoretical calculations, thus solidifying the discrepancy.
</li><li>
Experimental searches for exotic decay modes failed to find them.
</li><li>
Exotic nonQED theory attempts were made to explain the discrepancy (i.e.
accepting the view that QED had been refuted).
</li><li>
The Michigan group, in particular R.S.Vallery,
made new measurements with a new improved experiment (2003),
plus reanalysed their old experiment to find there were more sources of error than they'd realized;
the result was that they now agreed with QED theory. 
</li><li>
B.M.Levin &amp; B.A.Kotov (2004-2007) still attacked that "resolution" of the puzzle, and
the new Michigan experiment, as
inadequate and called for better experiments.
</li><li>
Kniehl, Kotikov, Veretin 2008 completed calculating the <i>exact</i> coefficient of
the &#945;<sup>1</sup> term in the decay-rate power series, in a closed form
(their long EQ 21; this calculation began 26 years earlier by M.A.Stroscio)
involving logarithms, polylogarithms, and certain interesting
new special functions 
(Smirnov 2006's appendix B calls them "generalized Nielsen polylogarithms")
defined in their EQ 20.  The lowest order
term was already exactly known (Ore &amp; Powell 1949) to be
<center>
&#915;<sub>0</sub> =
2 (&#960;<sup>2</sup>-9) &#945;<sup>6</sup>
(m<sub>e</sub>c<sup>2</sup>) / (9&#960; &#8463;)
&#8776; 7.2111670(1) / microsecond
</center>
and the full perturbative prediction is a series in powers of &#945;
and log(&#945;) times this:
<center>
&#915;<sub>0</sub> ·
[1 
+ (&#945;/&#960;)A 
+ &#945;<sup>2</sup>ln(&#945;)/3
+ (&#945;/&#960;)<sup>2</sup>B 
- 3&#945;<sup>3</sup>ln(&#945;)<sup>2</sup>/(2&#960;) 
+ &#945;<sup>3</sup>ln(&#945;)C/&#960;
+ (&#945;/&#960;)<sup>3</sup>D]
</center>
where
A&#8776;-10.286614808628262240150169210991,
B&#8776;45.06(26),
C=(A/3)-(229/30)+8ln2
and 
nobody has even tried yet to calculate D.
The coefficient B remains only available as an inexact numerical integration
result from Adkins, Fell, Sapirstein 2002.
A-F-S also recounts the history of this; in particular, the first published numerical
computation of A had been incorrect.
</li><li>
Y. Kataoka, S. Asai,  and T. Kobayashi 
from the Tokyo group perform (2008 &amp; 2009) a new record-precision
experiment finding the decay rate 
7.0401(7)/microsec,
which agrees excellently with one of the latest theoretical predictions
7.039979(11).
<!--
http://tabletop.icepp.s.u-tokyo.ac.jp/Tabletop_experiments/English_Home_files/Ps_asai_1.pdf
Y. Kataoka, S. Asai, and T. Kobayashi:
First test of O(alpha^2) correction of the orthopositronium decay rate
Phys. Lett. B 671(2009)219-
http://arxiv.org/0809.1594
-->
</li></ol>
<p>
So... although it took 20 years, QED survived the threat.  However, all that experimental attention
to positronium, created a new threat: <b>positronium's <i>hyperfine splitting</i>:</b>
</p><blockquote>
<pre>203.388 65(67) GHz     two most precise experiments, combined (Mills 1983 &amp; Ritter 1984)
203.391 69(41) GHz     QED prediction 
230.394 1(16)(11) GHz  new experiment by Ishida et al (year 2013)
</pre>
</blockquote>
the two most precise experiments yield a number differing from QED's prediction by 3.9&#963;.
However:
<p></p><ol type="A">
<li>
The analogous measurement of the <i>muonium</i> (muon &amp; positron "hydrogen")
hyperfine frequency agrees excellently with experiment (Liu et al 1999).
</li><li>
The new experiment of Ishida 2013 (this paper cites the previous works),
although it has worse claimed accuracy,
agrees better with QED's prediction than with the old experiments – making it
probable somebody was underestimating their error.
</li></ol><p>
So it seems likely this positronium hyperfine problem
ultimately also will be resolved favorably.
<b>Three other</b> famous QED-theory discrepancies versus experiment:
</p>
<ol type="i"><li>
Mott "double scattering of electrons off heavy nuclei" (see
Rose &amp; Bethe 1939 for a discussion of what at that time 
was regarded as a horrible discrepancy).
Ultimately it was realized that all experiments performed during the 1930s were done in
an inherently wrong way which precluded observing Mott's predicted polarization effects.
</li><li>
Karplus &amp; Kroll's original theoretical
prediction of the magnetic moment of the electron and muon at &#945;<sup>2</sup> order
(published in 1950) disagreed with experiments, inspiring
recalculations by Petermann and Sommerfield in 1957
which found and corrected arithmetic errors.
</li><li>
Both R.P.Feynman and J.Schwinger, using their (quite different) techniques,
computed (agreeing) values for the relativistic "Lamb shift" in hydrogen.  Amazingly,
they were both wrong since both independently made essentially the same mistake.  
Then Kroll &amp; Lamb 1949
and Weisskopf &amp; French 1949
computed the result correctly
– despite using much less powerful 
theoretical techniques – finding 1052 and 1051 MHz, respectively.
An experimental measurement by
Newton, Andrews, Unsworth in 1979 of
1057.862(20) MHz excellently matched a more refined 1975 theoretical
calculation by Mohr of 1057.864(14).
</li></ol>
<a name="giclasscountbound"></a>
<p>
<b>In summary</b>, 
QED either agrees with experiment, or in the few cases where there currently appears to be
a statistically-significant discrepancy,
that is likely caused by either experimental or theoretical/calculational errors.
</p>
<a name="GIclassocuntI"></a>
<p>
<b>Gauge-invariant class counting theorem:</b>
The number GI<sub>N</sub>
of GI classes of the Feynman diagrams describing any particular QED process
at &#945;<sup>N</sup> order, exceeds
</p><center>
GI<sub>N</sub> &#8805;  exp(-15/4) 96<sup>-N/4</sup> N! / [ (N/2)! (N/4)! ]
</center><p>
for an infinite set of positive integers N.
</p><p>
<b>Proof sketch:</b>
From Bender &amp; Canfield 1978 we find that the 
number of V-vertex 4-regular graphs with labeled vertices is asymptotic to
<nobr>
exp(-15/4)96<sup>-V</sup>(4V)!/(2V)!
</nobr>
when 
<nobr>
V&#8594;&#8734;.
</nobr>
Hence the number of 
isomorphism-distinct
V-vertex 4-regular graphs with <i>un</i>labeled vertices asymptotically is lower-bounded by
<nobr>
exp(-15/4)96<sup>-V</sup>(4V)!/[V!·(2V)!].
</nobr>
Now set up a map from (a subset of)
QED <i>Feynman diagrams</i> to edge-labeled <i>graphs</i> as follows.
If the Feynman diagram contains V virtual electron-loops,
then the graph has V vertices.  If loop A is joined to loop B by C photons
(absorbed by one and emitted by the other) then there is an arc between vertex A and 
vertex B in the graph labeled with the integer C&gt;0.  
Equivalently, we can make this 
edge-labeled graph instead be an unlabeled <i>multi</i>graph with C repeats of
a graph edge formerly labeled C.
It should be obvious (anyhow we shall assume it) that 
</p><p>
<i>Claim:</i>
The Feynman diagrams with any particular number of vertices may be
partitioned into GI classes, in such a way that
if two of the Feynman diagrams correspond as above to two
<i>non-isomorphic</i>
edge-labeled graphs, then they are <i>not</i> in the same gauge-invariant class.
</p><p>
Given the claim, by (e.g.) discarding all diagrams with any graph edge-label&gt;1,
and discarding all diagrams with any graph vertex of valency&#8800;4,
the graph-count clearly provides a <i>lower bound</i> on the number of GI classes of
Feynman diagrams.  This lower bound is 
clearly weak by at least an exponential(N) multiplicative factor,
because only Feynman diagrams in which each electron-loop
contains exactly 4 photon-vertices are counted.   Now these diagrams, if they correspond
to a V-vertex graph, have N=4V vertices as a Feynman diagram, plus a constant number of 
vertices arising from "external" photon and lepton lines.  But this constant
only affects the counts by a factor polynomial in N at most.  
<br>
<b>Q.E.D.</b>
</p><p>
It is possible to improve that theorem with the aid of the following 
</p><p>
<b>Graph-counting lemma:</b>
When E&#8594;&#8734; the number of (isomorphism classes of) 
<i>connected</i> graphs – indeed even
merely the count for <i>Eulerian</i> graphs – with E edges is X<sup>X</sup>
where [1-o(1)]E&lt;X&lt;[1+o(1)]E.
Also, all this is valid for multigraphs (meaning you are allowed to
have more than one edge joining two particular vertices) and/or if "self-loop" edges are
permitted.
</p><p>
<b>Proof:</b>
To prove the lower bound, we consider Eulerian graphs with E edges.
An Eulerian graph is a graph containing
an <a href="http://en.wikipedia.org/wiki/Eulerian_path">Euler circuit</a>,
traversing each edge once.
Equivalently (by a theorem of L.Euler 1736 and Carl Hierholzer 1873), 
the graph is connected and each vertex has even valence.
We'll actually prove the lower bound for multigraphs with self-loops permitted, but
it is easy to see this changes the count by an amount subsumed within our "o(1)."
The Euler-circuit insight is highly useful, because it enables us to count these graphs 
using "Polya necklaces."  I.e. take an E-cycle to be our Euler tour, and label its E vertices
with numbers from 1 to V.  Any two nodes with the same label will be the same vertex.  In
this way we count vertex-labeled Eulerian graphs with &#8804;V vertices by counting necklaces
with E beads of up to V different colors.  
If we want to forbid self-loop edges, then we need to demand that any two beads
adjacent on the necklace have different colors – which cuts the count
for an achored neckalce from V<sup>E</sup> to  
<nobr>(V-2)(V-1)<sup>(E-2)</sup>V</nobr> &amp;Ndash; and if we want to forbid
multiple edges we need to demand that if beads colored A and B are adjacent somewhere
on the necklace, then they are not adjacent anywhere else.  That would cut the count
further by a multiplicative factor of order 
<nobr>[1-O(V<sup>-2</sup>)]<sup>E</sup>.</nobr>
As a result we find that 
the number of <i>un</i>labeled E-edge Eulerian graph isomorphism types with up to V vertices 
is lower bounded by
</p><center>
  (2.0000001E/V)!<sup>-V</sup> [V-O(1)]<sup>E</sup> / (2E · V!)
</center><p>
where the division by V! is to provide safety about graph isomorphisms, 
and the division by (2.0000001E/V)!<sup>V</sup>
is to provide safety about the issue that many Euler tours might yield the same graph.
(This latter safety factor is justifiable after imposing the demand that
the numbers of beads in each color class all be equal to E/V to within a factor of 0.00000005;
this for all large-enough E and E/V will ultimately not be a demand at all in the sense it
discards an asymptotically negligible fraction of necklaces.  Of course imposing such
demands is allowed because the resulting discardings can only make our lower bound more true.)
<!--
lgstirl := (x) -> (x+1/2)*ln(x) -x + ln(2*Pi)/2;
lgstirl(2*E/V) * (-V)  + E*ln(V) - lgstirl(V);
subs(V = 3.03 * E/ln(E), %);
-->
We have the freedom to use any V with 0&#8804;V&#8804;E.  
Ignoring the "O(1)" for a minute, 
this lower bound is approximately maximized by choosing 
<nobr>V=&#954;E/lnE</nobr>
where computation shows that for 
<nobr>10<sup>8</sup> &#8804;E&#8804;3×10<sup>8</sup></nobr>
the best choice of &#954; is &#954;=3.03.
If we agree to use this choice of &#954; also for all 
E&#8805;3×10<sup>8</sup>  (then this &#954; will be smaller than optimal, but 
still sufficiently good for our purposes), 
then we may see analytically that 
the resulting lower bound then
has natural logarithm lower bounded (for those E) by
0.595ElnE.
More strongly, for truly enormous E, one can see that
the "0.595" actually should approach 1 from below (very slowly); essentially, the
V<sup>E</sup> ultimately wins in the sense that all other terms in the formula
have logarithms which when E&#8594;&#8734; ultimately become negligible compared to ElogV.
For example, I compute that 
when E&#8805;10<sup>100</sup> 
the lower bound has natural logarithm lower bounded by 0.93ElnE,
when E&#8805;10<sup>1000</sup> this may be replaced by 0.99ElnE,
and when E&#8805;10<sup>20000</sup> it may be replaced by 0.999ElnE.
This proves the
number of E-edge Eulerian graphs exceeds X<sup>X</sup> where 
<nobr>X&#8805;[1-o(1)]E.</nobr>
</p><p>
Next, to prove the upper bound for multigraphs with self-loops permitted,
simply  overcount the number of ways to fill in the (V+1)V/2 entries of the
non-strict upper triangle of the graph's V×V 
adjacency matrix with non-negative integers summing to E.
A suitable overcount is 
<nobr>(E+V<sup>2</sup>)<sup>E</sup>/E!.</nobr>
Then sum those counts for all V with 1&#8804;V&#8804;E+1.
Here we have used the fact that a connected graph with E edges has at most V+1 vertices.
<br>
<b>Q.E.D.</b>
</p><p>
I have not seen that lemma before, so it may be a new result in graph theory.
But, of course, it is quite easy.  
</p><p>
<b>Improved GI-class lower bound theorem:</b>
For an infinite set of N, the number of GI-classes of order-N Feynman diagrams obeys
GI<sub>N</sub> &#8805;  X<sup>X</sup>   where  
X=(1-&#949;)N/2  for any fixed &#949;&gt;0 
(albeit the decrease of the least permissible &#949; toward 0 as N rises, is extremely slow).
</p><p>
<b>Proof:</b>
Our preceding unimproved GI-counting 
proof actually shows stronger bounds than the old theorem.
In particular, it shows the number
of N-node Feynman diagram GI-classes exceeds (for an infinite set of N&#8805;2) 
the number of connected-multigraph isomorphism-types
with E=N/2-O(1) edges and all valencies even.
<!--
The number of connected graphs with N edges 
<a href="http://oeis.org/A002905">is</a>
</p>
<center>
1, 1, 1, 3, 5, 12, 30, 79, 227, 710, 2322, 8071, 29503, 112822, 450141, 1867871, ...
</center>
for N=0,1,2,...15.
-->
Now our lemma 
counting multigraphs with an 
<a href="http://en.wikipedia.org/wiki/Eulerian_path">Euler circuit</a>
immediately yields the theorem.  
<br><b>Q.E.D.</b>
</p>
<!--
The LambertW function is defined by 
<nobr>LambertW(x)&middot;exp(LambertW(x))=x</nobr>
and 
<nobr>LambertW(x)=x-x<sup>2</sup>+(3/2)x<sup>3</sup>-(8/3)x<sup>4</sup>+...</nobr>
for small |x|  
and 
<nobr>LambertW(x)&asymp;ln(x)-lnln(x)+...</nobr>
for <nobr>x&rarr;&infin;.</nobr>
-->
<p>
Perhaps this improved result is best possible.
</p>

<a name="noisydists"></a>
<h3>9. "Noisy distances": a preliminary idea to get you in the right frame of mind </h3>

<p>
"Noisy distances"
is different from, and was an intellectual progenitor of, our ultimate 
"<a href="#rainofbricks">rain of bricks</a>" idea.
It is simply this. (Employ units with c=1.)  Start with the usual formula
</p><center>
UsualDistance<sup>2</sup> 
=
(T-t)<sup>2</sup>
- (X-x)<sup>2</sup>
- (Y-y)<sup>2</sup>
- (Z-z)<sup>2</sup>
</center>
<p>
for the "distance" between two points 
(T;X,Y,Z) and (t;x,y,z).
We shall follow the traditional convention of QFT theorists 
(except Weinberg!)
that 
timelike distances are real and 
spacelike distances imaginary, arising by using
a negative sign on spatial and positive on time
quantities; this is an infernal conspiracy to confuse everybody since 
exactly the opposite convention is usually followed by General Relativists.
Replace this formula by
</p><center>
NoisyDistance<sup>2</sup> 
=
UsualDistance<sup>2</sup> 
+ sign(UsualDistance<sup>2</sup>) · dist(S,s)<sup>2</sup>
</center><p>
where S is a <b>"random function"</b> of (T;X,Y,Z) and s is the same random function of
(t;x,y,z), and this function takes values on some other space equipped with a distance.
There are many possible acceptable prescriptions for what this "random function" and
"other space" ought to be.
For example, a D-dimensional vector-valued
S could be got, for any particular (T;X,Y,Z), by independently sampling D
random variates from some fixed normal distribution, then using
the usual nonnegative-real-valued D-dimensional euclidean distance function for "dist."
Here D&#8805;1 is some fixed integer and
S and s are regarded as D-dimensional <b>"measurement noise."</b>
</p><p>
This would in some sense approximate the idea that we do not live in Minkowski flat space;
we actually live in a randomized "microscopically bumpy" version of it.  In the above
example if S were 1-dimensional, then we'd be postulating that our 3+1 dimensional Minkowski space
were really embedded in a 1-extra-dimensional space like a horizontal
sheet of rubber; and that each point of that sheet then got independently 
randomly vertically perturbed; and then finally interpoint distances should be measured
via a geodesic in the larger space, not
restricted to the rubber sheet.
</p><p>
This instantiates the notion that
for some fundamental reason (e.g. from quantum gravity) it is impossible to measure either
timelike or spacelike distances
in Planck units, with additive error below about 1/(1+distance).
(We'll discuss that in 
<a href="#quantgrav">§11</a>.)
Let us make a few remarks about this new kind of distance.
</p><p>
<b>1.</b>
If we restrict to a spatial hyperplane such as
t=T=0, 
then we still get (aside from multiplying by &#8730;-1) a "<i>metric space</i>" obeying 
dist(A,B)=dist(B,A),
dist(A,B)=0 if and only if A=B,
dist(A,B)&gt;0 otherwise, 
and obeying the "triangle inequality."
</p><p>
<b>2.</b>
If the old distance was spacelike, null, or timelike, then the new noisy distance also is.
And as far as anybody whose measurement abilities are much less precise than our 
noise-amplitude is concerned, the new and old distances appear, experimentally,
to be the same.
</p><p>
<b>3.</b>
The new distance no longer obeys "Poincare/Lorentz invariance" and indeed disobeys
all its important special-case symmetries too, such as time-reversal, mirror-reflection,
rotation, translation, and boost.  <i>However</i>, it still
obeys all those symmetries in this <i>strong statistical sense</i>: integrated times 
<i>any</i>
well behaved "test function," these symmetries remain valid.
In other words, the details of the random noise break Lorentz symmetry, but
when integrated to "average out the noise" all Lorentz symmetries remain valid.
(<i>But</i> the new and old distances are <i>inequivalent</i> even in this statistical sense!)
</p><p>
<b>4.</b>
By designing the nature of the "noise" right, one can accomplish various interesting 
goals.  For example, on a generic (old definition) sphere of radius R about a generic 
centerpoint X (and let us for simplicity restrict to a hyperplane t=T=0 and multiply
by <nobr>&#8730;-1</nobr> to make spatial distances real and positive)
we can by choice of D cause there to be either usually 
<i>no</i> points at distance&#8804;R from the centerpoint
(and always at most a finite number);
<i>or</i> we can force there generically to be
an infinite number of points, forming a dense set on the sphere,
with distance=R, albeit this set still has measure 0.
The former happens for large noise-dimension D and the latter for small D.
</p><p>
<b>5.</b>
If we make each Gaussian have variance D<sup>-1</sup> and take the limit D&#8594;&#8734;
of <i>infinite</i>-dimensional noise, then the net effect is simply to add either +1
or -1 to every squared distance.  Specifically, sign(OldDist<sup>2</sup>) 
is added to OldDist<sup>2</sup>.
This limit has the annoying effect of preventing any two unequal points from having 
|distance|&lt;1, whereas with any finite D, every point is unboundedly close to an infinite
set of others.  
But if we can get over our prejudice against that sort of behavior, we can appreciate its
unique advantages: this is by far the <i>simplest</i> to handle,
and is the only kind of noise which actually
is not random noise at all.
</p><p>
<b>6.</b>
Larger-dimensional noise has a stronger effect at getting rid
of "UV infinities."     For example, consider &#8747;&#8747;dist(x-y)<sup>K</sup>dxdy
integrated over all point-pairs in, say, some large fixed ball.
(This integration is 8-dimensional and K is a constant.)
Without noise, this integral has infinite value for every sufficiently-negative K.
With noise (thanks to note 4), 
the threshhold K moves and becomes more-negative, and more so the greater the value
of D.  The most strength comes in the limit D&#8594;&#8734;
where all these integrals (no matter what the exponent K) become finite!
</p><p>
<b>7.</b>
We note that physics in a noisy-distance universe 
presumably would be 
<i>nondeterministic</i> in the sense that there would be no way to predict future
noise values – whereas every commonly accepted previous physical theory so far has been 
deterministic.  (Well, quantum "measurement" is nondeterministic, but the nowadays-usual view of 
quantum mechanics has been that "measurement" does not really exist, so really everything is 
deterministic.)   
<a href="#rainofbricks">Rain of bricks</a>
physics (<a href="#rainofbricks">§12</a>) similarly will be nondeterministic (although
it would be deterministic if the locations of all the raindrops were regarded as somehow known).
</p><p>
Although gaussian-noisy distances 
seem like a simple and nice idea, unfortunately I find it very
unclear how to "do physics" in such a universe. 
For example, in such a universe, how could you solve a partial
differential equation (and what would that even mean)? 
(The only hope might be in the simplest case, infinite-dimensional noise as in note 5.)
The 
"<a href="#rainofbricks">rain of bricks</a>"
approach we shall adopt instead,
imposes a more complicated and arbitrarily-contrived-seeming,
but more carefully designed, easier-to-work-with, and 
better-behaved, metrical structure.  It
still yields "doubly-special relativity" (in a still-weaker statistical sense) and still will
allow plenty of "design freedom," but with the added advantage that it is fairly clear how to do 
physics.
</p>

<a name="doublesr"></a>
<h3>10. Doubly- and triply-special relativity </h3>

<p>
While the old Minkowski distance obeyed a "scaling" symmetry,
the new noisy distance formula does not.  Instead, the new one features a
<i>characteristic length scale</i> (e.g. 1 Planck length) set by the noise-amplitude.
Early in the history of physics, 
it naively seemed impossible for the speed of light <i>c</i> 
always to be
the same in the view of one (stationary) observer and any other (moving) observer.
But this was possible self-consistently,
as Einstein and Minkowski showed and every special relativity fan now knows, because
differently-moving observers have different notions of "length" and "time."
Later, it still seemed impossible for <i>both</i> special relativity to hold <i>and</i>
for there to be a special length scale in physics, agreed on by all observers –
because the standard meter-stick for me, is regarded as only 3 centimeters long
by (the rapidly moving) you.   Similarly it seemed impossible for any 
standard momentum scale to exist, since an object that I consider to have small momentum
is regarded by you as having large momentum.
</p><p>
However, noisy distances (and the construction of 
"<a href="#rainofbricks">rain of bricks</a>"
we shall 
present in 
<a href="#rainofbricks">§12</a>)
both make it clear that it <i>is</i> possible
to set up a self-consistent 
metrical structure featuring <i>both</i> a special constant speed c&gt;0 <i>and</i>
a special constant (microscopic) length L&gt;0 and hence
special constant (huge) momentum 
&#8463;/L.
The accomplishment of both these goals has been called 
<a href="http://en.wikipedia.org/wiki/Doubly-special_relativity">doubly-special relativity</a>;
and "noisy distances" is, as far as I know, a new and extremely simple way to do it.
</p><p>
Doubly-special relativity (DSR; the "doubly" refers to the fact there are <i>two</i> invariant
quantities c and L) schemes have been devised by other authors, some of the best papers
appearing to be 
Magueijo &amp; Smolin 2003 and Judes &amp; Visser 2003.   However, those other authors
accomplished it in ways far more complicated than ours (indeed, arguably, they didn't 
really accomplish it at all – writing transformation formulas is not really 
a construction, it merely is a constraint on a possible future construction?)
and yielding far less clear results.
Also, their approaches yield strange "dispersion relations" in which the
speed of light <i>depends</i> on the frequency of the photon (somewhat mooting
the whole design goal of making c invariant).  Our 
<a href="#rainofbricks">rain of bricks</a>
construction appears to be the first DSR in
which the speed c of light and L both <i>genuinely</i> are invariant
(see the 
<a href="#asymnosmear">asymptotic no-smearing theorem</a> of §15).
This invariance is a testable physical prediction.  The tests so far
(Schaefer 1999, Bolmont et al 2008) based on observations of 
gamma ray bursters at cosmological distances all support genuine invariance of c, and it is 
conceiveable that future tests of this ilk might achieve enough precision to rule out
either Magueijo-Smolin-style DSR or rain of bricks.
E.g. Schaefer 1999 claimed |&#916;c/c|&lt;6.3×10<sup>-21</sup> for 30 and 300 KeV photons
and I can imagine improving that by 2-5 orders of magnitude in time and/or energy.
</p><p>
I also want to point out that another kind of DSR had already been known for nearly 100 years:
"de Sitter space" (or, say, a 3-space×time universe postulated to have a metric of form
S<sup>3</sup>×R<sup>1</sup>)
but in these cases the
invariant length scale is large – the characteristic size of the universe – 
not microscopic.   (Anyhow, any observer who did regard it as small would
regard space as extremely highly curved!)   It would be possible (<a href="#desit">§35</a>)
– and indeed we ultimately shall recommend this – to set up
rain of bricks in de Sitter
rather than Minkowski space, in which case we'd have simultaneously
</p><ol>
<li>
A (genuinely) invariant (meaning observer-independent) speed of light 
c&#8776;3.0×10<sup>8</sup> meters/sec;
</li><li>
Invariant <i>microscopic</i>
length <nobr>L<sub>pl</sub>&#8776;1.6×10<sup>-35</sup></nobr> meters;
</li><li>
Invariant <i>huge</i> length scale of order 
<nobr>(3/&#923;<sub>Ein</sub>)<sup>1/2</sup>&#8776;1.5×10<sup>26</sup></nobr> meters, 
where &#923;<sub>Ein</sub> is
the Einstein cosmical constant;
</li></ol>
<p>
i.e. <b>triply</b>-special relativity.
</p><p>
<b>(Somewhat vague) "no smooth DSR" conjecture:</b> 
Doubly-special relativity is <i>not achievable</i> with
any conventional smooth spacetime continuum metrical structure provided it resembles flat space at
large distances and the invariant length is microscopic.
</p><p>
(One precise formulation of this would be to observe that no manifold arising from a Lie group
can do the job.  This seems immediate from the 
known full classification of low-dimensional Lie groups.)
Whereupon: if you believe that
the true laws of physics <i>must</i> be doubly- (or triply-) relativistic, then
something like noisy distances or the 
"<a href="#rainofbricks">rain of bricks</a>"
is <i>logically forced.</i>  
</p>

<a name="quantgrav"></a>
<h3>11. The quantum-gravitational basis for claiming the 
microscopic distance-structure of spacetime must differ from Minkowski </h3>

<!--
<p>
J.Bekenstein's "entropy bound" states that the number B of bits of information contained
within a physical system of gravitating-mass M enclosed in a sphere of circumference 2&pi;R
(with both M and R measured in Planck units), 
obeys
</p><center>
B &le; 2&pi;MR/ln2.
</center><p>
Another, simpler and weaker (but more-generally applicable!), 
-->
A bound first
stated independently by G.t'Hooft and L.Susskind in 1995
(building on a less-generally-applicable earlier bound by J.Bekenstein), 
and often called the 
"<b>holographic</b> principle,"
states that any physical system enclosed by a 
surface of area A (measured in Planck units), contains a number B of bits of 
information-entropy
bounded by
<p></p><center>
<b>B &#8804; A/(4ln2).</b>
</center><p>
This is 1.381×10<sup>69</sup> bits per square meter and
1/(4ln2)&#8776;0.3607 bits per Planck area unit.
Bekenstein in 1981 derived his bound 
B&#8804;2&#960;MR/ln2
(for a physical system of mass M enclosed in a ball of radius R)
by the following remarkable argument.   Suppose for
a contradiction that some physical system exists violating the bound.  
Slowly lower it into a large
Schwarzschild black hole.  Once the system has fallen into the hole, the Bekenstein-Hawking
entropy of that hole (it may be shown) will rise by at most 2&#960;RM/ln2 bits.  Hence
this process would decrease entropy, contradicting thermodynamics.
</p><p>
Bekenstein's bound is not really justified because
his argument is only valid in "flat space" where the usual formula 2&#960;R
for the circumference of a radius-R sphere holds.  For large enough M, that approximation
must be violated. 
It also is wrong since M can be negative for certain quantum systems
(trapped negative energy is responsible for the Casimir force attracting two parallel plates)
but B cannot be.
Bousso 2002 reviewed these and argued
that the holographic bound ought to be valid not just in (1+3=4) dimensions, but actually in any 
dimension D (using D-2 dimensional "area").
</p><p>
<b>My own Derivation of Holographic Info Bound:</b>
Since I was unsatisfied with any derivation of the holographic bound I'd seen,
I now present my own, new, extremely simple and general, one.
Suppose A is (what general relativists call) a "trapped surface."  By the Penrose-Hawking
<a href="http://en.wikipedia.org/wiki/Penrose%E2%80%93Hawking_singularity_theorems">theorem</a>
in classical general relativity 
(Hawking &amp; Ellis 1973 assuming certain "energy conditions"),
everything enclosed by this surface is doomed 
eventually to fall into a black hole singularity, and during this process,
</p><ol><li>
the area A of the surface  can only shrink,
</li><li>
The entropy B trapped within this surface, since it cannot escape, can only rise.
</li></ol>
Assume the original surface is isolated far enough away from any external stuff
that the collapse process cannot influence that external stuff.
Since the holographic bound is known to be an equality for black holes,
we conclude from the assumption that the total entropy of the universe cannot decrease,
that the holographic bound must hold for such a surface. 
Now: we argue that the fact that the surface is "trapped" and "isolated"
is not perceivable
by any local physical experiment.
Any observer sitting either slightly within or slightly outside of the surface,
"feels the same"; and there is "no way for him to know" that he is inside, outside, on, near,
or far away from a trapped surface.
(Also, incidentally, a surface which is not trapped, can be converted
into one that is, by moving a dense mass inside it.  This movement would not
affect the local physics or entropy on or near the surface, and could only
increase the entropy inside it.)
Therefore, if physics is local, then
the bound must hold for <i>all</i> surfaces, not merely
ones which happen to be trapped and isolated.
<b>Q.E.D.</b>
<p></p><p>
The assumptions that went into my above argument were (a) classical general relativity
plus whatever
"energy conditions" are required to make the Penrose-Hawking collapse theorem valid
[the latter nowadays are quite weak, e.g. the "averaged null energy condition," but the
situation is not fully satisfactory and hence in my opinion the holographic bound
still cannot be regarded as certain],
(b) Hawking-Bekenstein exact formula for entropy of a black hole with event-horizon area A,
(c) entropy of universe cannot decrease, (d) physics exhibits spacetime locality.
The bound is tight in the sense that a black hole event horizon meets it.
</p><p>
What does the holographic bound tell us about measurement-error for measuring lengths, times,
and areas?  
</p><p>
<b>The impossibility of measuring small areas:</b>
It is not possible for any 
measurement apparatus to distinguish a body of surface area
A&lt;4ln2&#8776;2.7726 Planck units, from a surface of area 0.
<br> 
More generally,
let 0&#8804;P&#8804;1 and let 
<nobr>H(P)=(-1/ln2)[Pln(P)+(1-P)ln(1-P)]</nobr>
be the "Shannon entropy function" where note 
<nobr>H(0)=H(1)=0&#8804;H(P)&#8804;1=H(1/2).</nobr>
Then: It is not possible for any randomized
measurement apparatus to distinguish a body of surface area
A&lt;H(P)4ln2 Planck units, from a surface of area 0,
with correctness probability&#8805;P&#8805;1/2.
</p><p>
<b>Derivation:</b>
Suppose such a measurement apparatus existed.
Then the body would 
contain at least 1 bit of entropy (i.e. extractible information), 
violating the holographic bound.
<b>Q.E.D.</b>
</p><p>
<b>Corollary: The impossibility of measuring the diameter of small balls:</b>
In flat (1+3)-spacetime, it is not possible for any 
measurement apparatus to exist, which can always
distinguish a ball of diameter
Q&lt;2(ln2)<sup>1/2</sup>&#960;<sup>-1/2</sup>&#8776;0.9394 Planck length units, from a point.
<br> 
More generally, it is not possible for any randomized
measurement apparatus to distinguish a body of diameter
<nobr>Q&#8804;H(P)<sup>1/2</sup>2(ln2)<sup>1/2</sup>&#960;<sup>-1/2</sup></nobr>
Planck units, from a point
with correctness probability&#8805;P&#8805;1/2.
</p><p>
<b>Corollary: The impossibility of measuring small distances:</b>
In flat (1+3)-spacetime, it is not possible for any 
measurement apparatus to exist, which can always
distinguish the situation "particles A and B are less than
0.9394 Planck length units apart" from not.
</p><p>
<b>Corollary: The impossibility of measuring small times:</b>
In flat (1+3)-spacetime, it is not possible for any 
measurement apparatus to exist, which can always
distinguish a time interval of duration
T&lt;2(ln2)<sup>1/2</sup>&#960;<sup>-1/2</sup>&#8776;0.9394 Planck time units, from an instant.
<br> 
More generally, it is not possible for any randomized
measurement apparatus to distinguish a duration
<nobr>T&#8804;H(P)<sup>1/2</sup>2(ln2)<sup>1/2</sup>&#960;<sup>-1/2</sup></nobr>
Planck time units, from an instant,
with correctness probability&#8805;P&#8805;1/2.
</p><a name="minvargauss"></a><p>
<b>Corollary: The minimum-variance Gaussian:</b>
Suppose a device exists for measuring the position of a point in Euclidean 3-space
that outputs a claimed location (x,y,z) and an error bar &#963; such that
the probability distribution for the true location of the point is then Gaussian with
mean (x,y,z) and standard deviation (in each coordinate) &#963;, in Planck length units.
Then: <b>&#963;&gt;0.141968.</b>
</p><p>
<b>Proof:</b>
If &#963;=0.141968 were possible, then a 
sphere of radius R=0.3999 centered at (x,y,z) would contain the 
point with probability P&#8805;0.9536.
This statement would represent at least 1-H(P) bits of entropy 
[1 bit for saying the
point lies within the sphere, minus H(P) bits for only having confidence P&lt;1],
i.e. &#8805;0.724820 bits.   However, the surface area A=4&#960;R<sup>2</sup> of this sphere obeys
A/(4ln2)&#8804;0.724816, so we have a contradiction with the holographic bound.
<!-- MAPLE:
 int( exp(-r^2/(2*s^2)) / (2*Pi*s^2)^(3/2) * 4*Pi*r*r, r=0.. infinity );  #=1
 P := (R) ->  int( exp(-r^2/(2*s^2)) / (2*Pi*s^2)^(3/2) * 4*Pi*r*r, r=0..R );  
 P := (R,s) -> erf(sqrt(1/2)/s*R) - sqrt(2/Pi)*(R/s) * exp(-1/2*R^2/s^2);
 NH := (x) -> x*ln(x) + (1-x)*ln(1-x);
 ed := (R,s) -> (NH(P(R,s)) + ln(2)) - Pi*R^2;
 #ed <= 0
 #gi := (R) -> fsolve( ed(R,s), s );
 smx := 0.0;
 for R from 0.395 to 0.40 by 0.0001 do
   for s from 0.1419 to 0.1420 by 0.000001 do
     pp := evalf(P(R,s));
     if(pp < 0.5) then break; fi;
     q := evalf(ed(R,s));
     #print( R, s, q, pp );
     if(q > 0.0) then print(R,s,q,pp,evalf(1+NH(pp)/ln(2)),evalf(Pi*R*R/ln(2)),"s too small"); 
       if(s >= smx) then smx := s; fi;
     fi;
   od;
 od;
 print(smx, "=smx");  
#result R=0.3999, s=0.141968, ed=0.41151*10^-6, pp=0.9526159318, en=0.724821, ar=0.724815, "s too small"
-->
<b>Q.E.D.</b>
</p><p>
Another result in this line of reasoning (which we shall not actually use, but we state
it because it may have independent interest) is:
</p><p>
<b>The smallest possible black hole:</b>
Any black hole obeying the classical general relativistic formulae for its
surface area, has 
mass&#8805;(4&#960;/ln2)<sup>-1/2</sup>M<sub>pl</sub>&#8776;0.234859M<sub>pl</sub>&#8776;5.112&#956;g.
</p><p>
<b>Derivation:</b>
Known formulas <!-- Astrophysical Formulae by Kenneth R. Lang -->
show that the Kerr-Newman family of 
general rotating charged black holes has maximum surface area
for given mass if and only if the hole is Schwarzschild, i.e. nonrotating and uncharged.
If we regard a black hole, by existing or not, as providing 1 bit of entropy,
then we conclude from the Bekenstein bound combined with 
the <!-- http://scienceworld.wolfram.com/physics/SchwarzschildRadius.html -->
standard Schwarzschild 
radius formula R=2Gc<sup>-2</sup>M that
the smallest possible mass M for a black hole obeys
M&#8805;(&#960;/ln2)<sup>-1/2</sup>/2
Planck mass units.
<b>Q.E.D.</b>
</p><p>
This result seems less impressive than the preceding in the sense that it depends on the
validity of classical area formulae, unlike the preceding results which  
only needed the area-based entropy upper bound.   
</p><p>
<b>The regime of validity of the reasoning underlying those claims:</b>
Classical and semiclassical physics of the sort used
to derive the Hawking/Bekenstein entropy formulae presumably should be accurate until near
when the black hole's Compton wavelength h/(Mc)
equals its gravitational radius 2GM/c<sup>2</sup>.
The crossover mass M is 
<nobr>M=2<sup>-1/2</sup>(hc/G)<sup>1/2</sup></nobr>
i.e, 
<nobr>M=&#960;<sup>1/2</sup>&#8776;1.772</nobr> planck masses, which is 38.6&#956;gram.
For black holes smaller than that,
classical physics presumably becomes inadequate and quantum gravity (whatever that is)
is needed.
Hence the <i>exact</i> numbers claimed in these results, 
especially this last one, cannot be taken
seriously, <i>but</i> they all should be expected 
to be approximately correct, i.e. to within about
one order of magnitude, because our reasoning arguably never needed to stray more
than a small constant factor over this rough
classical-quantum borderline.
</p><p>
<b>What about measuring the diameter of <i>large</i> balls?</b>
Before looking at this theoretically, let us first review what is probably the 
best (and certainly the most expensive at over $5×10<sup>8</sup>), 
length-measuring device yet constructed:
the USA NSF's <a href="http://www.ligo.caltech.edu/">LIGO</a> 
(Laser Interferometer Gravitational-Wave Observatory) project, built in 1999 and
located in Hanford WA and Livingston LA.
This consists of several laser interferometers (wavelength=1064nm)
operating in high-vacuum tubes
each over 1 meter in diameter and either 2km or 4km=2.5×10<sup>38</sup>L<sub>pl</sub>
long.  Its goal is to detect small relative fluctuations in the lengths of 
two perpendicular arms caused by "gravitational waves."
By November 2005, strain sensitivity had bettered the design goal of 1 part
in 10<sup>21</sup> over a 100Hz-wide bandwidth,
i.e. measuring the length of the 4km arm to 
RMS error&lt;4×10<sup>-18</sup>meter=2.5×10<sup>17</sup>L<sub>pl</sub>.
"Enhanced LIGO" is presently under construction and scheduled to be operational in 2014.
By extending all the arms to
4km, increasing the laser power to 200 watts, 
increasing end-mirror diameter to 31.4cm,
and making various detector, suspension, damping-servo, and other
improvements, Enhanced LIGO is expected to improve
strain sensitivity to 
3×10<sup>-23</sup> everywhere from 100 to 300 Hz.
This constitutes a length measurement accuracy of
<nobr>±1.2×10<sup>-19</sup>meter=7.5×10<sup>15</sup>L<sub>pl</sub>.</nobr>
Note that this accuracy is almost 13 orders of magnitude smaller than the wavelength
of the laser light they are employing as a measuring stick (!), 
9 orders of magnitude
smaller than the diameter of an atom,
and 4 orders of magnitude smaller than the diameter of
a proton.
</p><p>
I believe that had LIGO been supported by an amazingly advanced
extraterrestrial civilization instead of the NSF,
then they could have built it essentially the same way but
with all lengths and tube diameters scaled up
by a factor of 10<sup>4</sup> and with laser power 20 gigawatts, achieving
strain sensitivity 10<sup>-34</sup>, i.e. measuring a 
40000km=2.5×10<sup>42</sup>L<sub>pl</sub>-long arm accurate to
<nobr>±4×10<sup>-27</sup>meter=2.5×10<sup>8</sup>L<sub>pl</sub>.</nobr>
</p>
<!-- 
<blockquote><small>
Another impressive feat, incidentally, has been in the domain of precise <i>time</i> measurement.
Based on a 2009 NIST <A href="http://arxiv.org/abs/0911.4527">paper</a>
about their experiments comparing different atomic clocks,
it appears that measurement of arbitrarily long times
is now possible with accuracy of 1 part in 
8.6&times;10<sup>-18<sup>, 
and discrepancies between the times experienced by two clocks
after time T (for all T&ge;10 seconds)
can be measured with an absolute accuracy of about 
&Delta;T&asymp;5&times;10<sup>-17</sup>T<sup>1/2</sup> 
if T is measured in hours, i.e. about
&Delta;T&asymp;1.3&times;10<sup>7</sup>T<sup>1/2</sup> 
if T is measured in Planck time units.
Their clock is based on a 10<sup>15</sup>Hz 
transition in isolated Al<sup>+</sup> ions.
</small></blockquote>
-->
<p>
In blissful ignorance of all that, 
Y.X.Chen and Y.Xiao [Physics Letters B 666,4 (2008) 371-375]
brilliantly showed 
"by collecting both quantum and gravitational principles" 
the "universal validity" of their uncertainty principle
<nobr>
&#916;L&#8805;&#960;<sup>1/4</sup>L<sup>1/2</sup>(L<sub>pl</sub>)<sup>1/2</sup>
</nobr>
for measuring lengths L with error &#916;L.
In particular, with L=4km this "proved"
&#916;L&#8805;3.4×10<sup>-16</sup> meter.
</p><p>
Meanwhile Y.J.Ng and H.van Dam [Modern Physics Letters A 9,4 (1994) 335-340]
claimed to show
<nobr>
&#916;L&#8805;L<sup>1/3</sup>(L<sub>pl</sub>)<sup>2/3</sup>,
</nobr>
which, while having the advantage of not having 
already been refuted by available technology
at the time of publication, <i>would</i>
be refuted by our speculative "advanced alien" scaled-up LIGO.
</p><p>
There also
have been <b>many other invalid "derivations"</b> of such uncertainty principles.
All shared the same approach:
</p><ol>
<li>
Propose one particular method of measuring length (or time, or whatever).
</li><li>
Attempt to analyse its errors.
</li><li>
Assert (with no justification other than the implication that
they are the smartest possible entities in the universe) that
no possible measurement method could be more accurate
than theirs.
</li></ol>
<p>
This is not a valid way to establish a lower bound.  It is at best a way to
<i>conjecture</i> a lower bound, or a way to establish an <i>upper</i> bound.
We shall now refute approximately 10 papers of this ilk by constructing (on paper)
a way to measure long distances to very-subPlanck accuracy.
As far as I can tell, nothing in classical general relativity and quantum mechanics
prevents that.
</p><p>
<b>High-accuracy length measurement technique (thought experiment):</b>
I claim it is possible in principle for a LIGO-like device to compare
the lengths of two length-L arms (when L&#8594;&#8734; and we measure all lengths in Planck units)
accurate to order ±L<sup>-0.49</sup>;
neither general relativity nor quantum mechanics prevents this.
Furthermore, <i>perhaps</i> the same technique with altered parameters can
improve the accuracy to order &#916;L&#8776;L<sup>-0.995</sup>.
</p><p>
<b>Rough design:</b> 
Basically the same as LIGO's 
design – a 
<a href="http://hyperphysics.phy-astr.gsu.edu/Hbase/phyopt/michel.html">Michelson interferometer</a>
–
with the following parameters.
Each arm has length L and pipe- and beam-diameters of order L<sup>0.51</sup>.
The wavelength of the laser light is &#955;=L<sup>0.01</sup>.
The total energy of all the laser light in the beams is L<sup>0.99</sup> since
the beams contain a total of L<sup>1.00</sup> photons.
Note that if this energy were of order greater than L<sup>1.00</sup> then the 
system would gravitationally collapse since the Schwarzschild radius 
of that energy would be L, the device's actual physical size.  
However, by making the energy be only L<sup>0.99</sup>, gravitational
distortions away from flat space
are caused to be negligible in the L&#8594;&#8734; limit.
(Also note any sphere of diameter L<sup>0.51</sup> contains beam energy
of order L<sup>0.50</sup>, again causing negligible gravitational distortion
away from flat space.)
Also of course the device is built symmetrically so that
those distortions are the same on both arms.
Our beam-diameter choice in relation to &#955; and L
has been designed to make beam spreading be relatively negligible.
(We follow the original LIGO design in using
<a href="http://en.wikipedia.org/wiki/Gaussian_beam">Gaussian beams</a>
and spherical mirrors.)
</p><p>
The device will be assumed to float freely in outer space so that we do not actually need
pipes.  
If the end mirrors were held together by ropes to counteract the radiation pressure 
pushing them apart, the total mass of the ropes (assuming constant rope-diameter)
would grow proportionally to L, 
which would yield non-negligible gravitational distortion.
We want sublinear rope mass.   
We get the effect of sublinear-mass
ropes, without actually having any ropes, by using additional laser beams 
hitting each end-mass from the other side.  
Servos can adjust such things as reflectivity to make sure the 
forces balance accurately.
The end-mirrors can be made massive so that the accelerations due to 
remaining slight force unbalances
will be very small.
</p><p>
<!--
Experimental determination of numberâ€”phase uncertainty relations
M. Beck, D. T. Smithey, J. Cooper, and M. G. Raymer
Optics Letters, Vol. 18, Issue 15, pp. 1259-1261 (1993);
M. BECK, D. T. SMITHEY, and M. G. RAYMER, "Number-phase Uncertainty Relations," 
Optics & Photonics News 4(12), 40-41 (1993); 
PHYSICAL REVIEW AVOLUME 57, NUMBER 3 MARCH 1998
Number-phase uncertainty relations: Verification by balanced homodyne measurement
T. Opatrny,  M. Dakna1 and D.-G. Welsch
-->
The energy density inside the beams (we deduce) has order L<sup>-1.03</sup>.
These beams cause a total force (=&#8747;radiation pressure·darea)
and total energy flux on each end-mirror of order L<sup>-0.01</sup>.
All these are intentionally negligibly small in the L&#8594;&#8734; limit.
</p><p>
An arm-length discrepancy &#916;L&#8776;&#955;/4
would yield a change from full brightness to
no brightness 
in some interference fringe
(and vice versa in a different region of the interfence fringes), 
with smaller &#916;L yielding roughly
proportionally smaller brightness changes.
</p><p>
If our measurement accuracy is limited by "shot noise" (i.e. detecting N photons
with noise actually measures N±N<sup>1/2</sup>) this should yield
the ability to confidently detect length changes of order
<b>&#916;L&#8776;L<sup>-0.49</sup></b>
caused by some passing gravity wave of wavelength&#8776;4L.
</p><p>
This completes our derivation of the upper bound L<sup>-0.49</sup>.
We now derive a stronger, but speculative, upper bound.
</p><p>
Note that we chose &#955;&#8776;L<sup>0.01</sup>
to be much <i>greater</i> than 1 Planck length unit (indeed, tending to infinity), so
there was no obstacle posed by
anybody claiming "wavelengths smaller than a Planck length cannot exist,
or cannot be reflected by a mirror."   If however we were to <i>allow</i> 
subPlanckian photons with 
ultrashort wavelength 
&#955;&#8776;L<sup>-0.99</sup> and now using N&#8776;L<sup>0.01</sup>
such photons in all in the beams,
then we'd be able to get far better length-change detectability, of order
&#916;L&#8776;L<sup>-0.995</sup>.
Because each of these photons would be spatially very spread out (over a length of order L)
it is perhaps arguable that they would be allowed by general relativity both to exist 
and to be reflected.
</p><p>
(End of derivation of new upper bounds.)
</p><p>
<b>A lower bound:</b>
&#916;L&gt;2/L bounds the measurement error &#916;L on any length L&gt;10
(with all lengths measured in Planck units).
Note that this lower bound essentially <i>matches</i> 
our <i>speculative</i> improved LIGO-like
upper bound.
</p><p>
<b>Derivation:</b>
Let the mass of whatever we are measuring plus the mass of the
measurement apparatus (actually it is best to view these two things as
one merged entity) be M.   Its Schwarzschild radius then is 2M (we work in Planck units).
Let the length of this entity be L&gt;10.
So L&gt;4M is necessary to avoid gravitational collapse, which would prevent any
information extracted from the measurement from reaching any external recipient.
Presumably quantum mechanics prevents a length measurement accuracy 
&#916;L better than
half the reduced 
<a href="http://en.wikipedia.org/wiki/Compton_wavelength">Compton wavelength</a>, 
i.e. &#916;L&#8805;1/(2M).
Combining these (and minimizing over all possible choices of M), we deduce
&#916;L&gt;2/L.
<b>Q.E.D.</b>
</p>
<p>
<b>Comments:</b>
</p><p>
1. The 
"<a href="#rainofbricks">rain of bricks</a>"
metrical structure
we shall construct later meets our lower bounds.  
That is, the distance L between any two "bricks" has a built-in additive "error" or
"fuzziness" of
order ±1/(1+|L|) in Planck length units
(arising from the brick size).
Some microscopic metric structure resembling the rain of bricks, then, 
is <b>logically forced</b> upon anybody who takes these
measurement-error lower bounds seriously.
</p><a name="scaling"></a><p>
2. To contrast this with old-style QED: QED is <i>invariant</i> under the 
symmetry of (for any given real number s&#8800;0) <b>scaling</b> all times and lengths by
s while scaling all particle rest masses by 1/s.
</p><blockquote>
<a name="scalinvar"></a>
<table bgcolor="pink">
<caption> <b>Table 8: Scaling Invariance of QED and QCD:</b>
According to the formulas for the QED and QCD lagrangian densities given by Folland 2008
(pages 297 and 294 in §9.2 and 9.1 for QCD; and EQ 6.27 page 140 for QED)
and following his notation,
both QED and QCD obey the following scaling symmetry
(where s is the scaling factor):
 </caption>
<tbody><tr><td>multiply by s:</td><td> t,x,y,z   </td><td>coordinates for space &amp; time
</td></tr><tr><td>
divide by s:</td><td> A<sub>u</sub>, A<sup>u</sup>  </td><td>Maxwell and gluon 4-vector potentials
</td></tr><tr><td>
divide by s:</td><td> m </td><td>quark and electron masses
</td></tr><tr><td>
divide by s<sup>2</sup>:</td><td> F<sup>uv</sup>, F<sub>uv</sub> </td><td>EM &amp; gluon fields
</td></tr><tr><td>
divide by s<sup>3/2</sup>:</td><td>  &#968; and its Dirac-adjoint
</td><td>quark and electron spinor fields
</td></tr><tr><td>
divide by s<sup>4</sup>:</td><td> L  </td><td>lagrangian density
</td></tr></tbody></table>
</blockquote>
<p>
This scaling symmetry can be regarded as 
<b>clear proof that old-style QED and QCD are incorrect
descriptions of our universe</b>, 
because our universe features Planck's <i>invariant</i> length.
So if the masses of all electrons were to double, we really should
<i>not</i> expect everything to be the same except twice as fast and half the size.
Rain of bricks will correct this obvious failure of QED to include a fixed length scale.
</p><p>
3. If <b>area</b> were somehow reckoned to be unmeasurable to accuracy better 
than ±O(1) Planck
area units, this would be equivalent to forcing <i>lengths</i> L&gt;10
which were, e.g, the radii of discs
or the sidelengths of Euclidean squares, to be unmeasurable to ±O(1/L)
accuracy – which is exactly what will happen in the rain of bricks.
I mention this because "loop quantum gravity," at least according to some authors in some
papers, features exactly that sort of "graininess" of the notion of "area."
</p><a name="rovellirefut"></a><p><small>Later, Rovelli 2004 in his 
§8.2.4 explained why the early naive area=integer×quantum
"equi-spaced area eigenvalues"
idea in LQG allegedly must be discarded since it yields a very-wrong-behaving black hole entropy
formula (his EQ 8.55). 
Rovelli instead has that any area is a sum of square roots of (n+2)n, all times
a fixed area quantum constant, where the sum is over n that are members of an arbitrary multiset
of positive integers.  (The n's are "double the spin of edges 
of the spin network passing though the horizon surface.") I have rechecked Rovelli's non-naive 
entropy calculation in his §8.2.2 and agree with it, but
I <i>disagree</i> that the effect he worried about
rules out the naive equi-spacing
area formula, since it seems to me it
would yield &#916;entropy=O(area<sup>1/2</sup>) –
arising from the asymptotic formula 
<nobr>p(n)&#8764;(4n&#8730;3)<sup>-1</sup>exp([2n/3]<sup>1/2</sup>&#960;)</nobr>
for integer partitions (Abramowitz &amp; Stegun §24.2.1)
– and that does <i>not</i>
seem to me to be strong evidence against the naive area formula.
[Rovelli had confused himself into thinking it yielded
entropy=O(area<sup>1/2</sup>).]
I then found out that my conclusion disagreeing with Rovelli,
was reached independently by
Gour &amp; Suneeta 2004,
Sahlmann 2008, and Barbero G, Lewandowski, &amp; Villasenor 2009.
All agree LQG still yields entropy 
equal to area (in Planck units) times a constant of order 1,
up to lower order corrections.
Rovelli's prejudice against equi-spacing might still be correct,
but if so it is not because of black hole entropy, but for deeper reasons.
The naive equi-spacing formula causes area of anything to be a quantized discrete value
(i.e. an <i>integer</i> times a constant of order 1 in Planck area units)
whereas Rovelli's preferred formula leads to a more complicated, and
far finer-grained, area spectrum.
</small></p><p>
4. It is also of some interest (not for our purposes, but for other people's) to
consider what I call <b>non-destructive separable measurement.</b>
Its rules are (I hereby proclaim)
that you have to be able to move your measurement apparatus from far away into some physical
system, then use it to make the measurement, then move it back far away – and any 
distortions to the physical system caused by these movement, retraction, and measurement processes
<i>count</i> as "measurement errors."   Under the assumptions that 
</p><ol type="a">
<li>
Any measurement device with mass-energy M will gravitationally-cause additive
length-distortions &#916;L of
order M or greater
(in Planck units)
</li><li>
Any measurement device with mass-energy M will unavaoidably experience length measurement 
additive errors
of order half its reduced Compton wavelength, i.e. &#916;L&#8805;1/(2M)
</li></ol>
one can derive that the <b>minimum possible &#916;L would be of order 1 Planck length unit</b> 
<i>regardless</i>
of the value of L&gt;10.   This bound could be achieved only by making M be O(1)
in Planck mass units – which is entirely feasible since, e.g,
the Planck mass is the same as the mass of a magnesium ball
with diameter 0.62mm=3.8×10<sup>31</sup>L<sub>pl</sub>.
<p></p><p>
But this measurement length-error bound for 
non-destructive separable measurements can be criticized.  
I consider the criticisms strong enough that I'm currently unwilling to accept this bound.
Although the gravity-caused length distortion &#916;L&#8776;M
is <a href="#schwdist">true</a>
for the mass-M Schwarzschild metric,
for lengths L of order M or greater, the problem is that a non-Schwarzschild
device geometry could be designed whose first-order
distortion was exactly zero, in which case the contribution would be of lower order
–
and our present argument would be invalidated.  Further, for a mass=M device of
very low density in Planck units
(i.e. realistic devices!)
the gravitational distortion can be tiny, of order &#916;L&#8776;1/L.  
In such a case even non-destructive separable measurement might be comparably accurate
as our above lower bound for unrestricted measurement.
</p><p>
5. In the 
<a href="#rainofbricks">rain of bricks</a>, 
regions of spacetime with 4-volume of order 1 in Planck units, 
essentially will be
replaced by something else ("bricks") which actually have zero 4-volume, but do have some
nonzero lower-dimensional measure, which also is of order 1 measured in Planck units.
One might naively proclaim that alterations of this sort
are <i>undetectable</i> by any measurement apparatus.  So in some sense I, by inventing
the rain of bricks, am immune
to criticism – no experiment will ever be able to prove me wrong!?!
Well – sorry – that conclusion is naive.   The entire universe by
being changed into a rain of bricks universe, has total size of the alterations
far larger than Planck length
and this change in net causes very big and very observable changes
in physics, e.g, we shall 
argue, replacing
infinities with finite values.  However, I agree
any particular brick inside it (if small enough), 
could be moved, 
altered, or omitted with this change being undetectable.
</p>

<p>
<b>One final comment – the statistical nature of D-dimensional cumulated noise:</b>
A different short 
<a href="https://dl.dropboxusercontent.com/u/3507527/DistNoise.html">paper</a>
(Smith 2010) by me analysed the mathematical/probabilistic notion of
"D-dimensional noise."   Consider an S×S×...×S cubical grid in D dimensional
Euclidean space.
Suppose each grid point is distorted away from its nominal position by "random noise."
However, these noises are <i>not</i> independent because we demand that the distorted
grid cubes must still "fit together" to tile D-space.
The first question I analysed was: how do
the noise-caused RMS errors <b>&#916;</b>
in the distance between two grid points nominally
distance=S apart, depend on S?
The second question was: how do
the noise-caused RMS errors <b>&#949;</b>
in the distance between the opposite faces of the cube
(nominally distance=S apart)
<i>averaged</i> over all S<sup>D-1</sup> corresponding point-pairs on these two faces, 
depend on S?
The results (when S&#8594;&#8734; with D fixed) are
</p>
<a name="tabcumnoise"></a>
<pre> <!-- align="center" works per line, which sucks -->
<b>Table 9: D-dimensional cumulated noise</b>
<u>dimension D</u>    <u>&#916; order</u>    <u>&#949; order</u>
D=1            S<sup>1/2</sup>        S<sup>1/2</sup>
D=2          (logS)<sup>1/2</sup>     1
D=3              1        S<sup>-1/2</sup>
D=4              1        S<sup>-1</sup>
each D&gt;2:        1        S<sup>1-D/2</sup>.
</pre>
<p>
When D=1 these results were well known (1-dimensional "random walk with S hops"
generates RMS positional deviation of order S<sup>1/2</sup>).  But they seem to be new in
dimensions D&#8805;2 and are much smaller than some people expected.
If somebody postulated from quantum gravity that every Planck-scale cube of vacuum exhibits
"random noise" in its edge lengths, then these laws would
(presumably for either D=2, 3, or 4) govern the length-distortions that would result at
macroscopic scales.   The present paper's 
<a href="#rainofbricks">rain of bricks</a>
model is "compatible" with the smallest
(namely &#949; with D=4) of the growth laws in the table, as well as with our lower bounds 
on measurement uncertainty,
and hence in some sense is the 
"most conservative" possible way we could have gone, i.e. it represents
the least possible alteration of
the usual noise-free Minkowski (1+3)-space picture.
</p><p>
It might also be possible to repair QED not by using rain of bricks, 
but instead some other "more radical"
metrical structure and postulating larger uncertainties.  
(The present paper restricts itself merely to claiming rain of bricks works;
we do not necessarily claim it is the <i>only</i> QED-repair that works.)
So it is of some interest to explore the possibilities
corresponding to entries in the above table.
I point out that <b>only the two smallest</b> among the 8 table entries 
(i.e. &#949; with D=3 and D=4) could hope to yield an
<a href="#asymnosmear">asymptotic no-smearing theorem</a> 
as in 
<a href="#propformulas">§15</a>.
</p>


<a name="rainofbricks"></a>
<h3>12. The "rain of bricks" metrical idea </h3>

<a name="post1"></a><p>
<b>Postulate I (Randomized Arena):</b>
The usual Minkowskian coordinate 4-tuples (t;x,y,z) are <i>not</i>
the arena on which physics happens, although it seems that way to observers with 
imperfectly-acute vision.
The true arena is something called the "rain of bricks"
which we shall now construct.
</p><p>
First, within Minkowskian (1+3)-space "rain" 
<a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a>-random points 
down at some constant
number-density &#961;<sub>rain</sub>
(per Lorentz-invariant 4-volume unit).  This density is a fundamental physical constant,
presumably of order 1 point per unit 4-volume
measured in Planckian length and time units.
(In 
<a href="#horizon">§20</a> and
<a href="#openprobs">open problem</a> 11
we will suggest that it can be determined 
exactly via a connection to black hole entropy.)
Note that <i>every Lorentz observer agrees</i> on the value of &#961;<sub>rain</sub>
and the Poisson
statistical  nature of the "rain."   That is, in any spatial region of volume V comoving
with that observer, that observer expects to see &#961;<sub>rain</sub>V raindrops per Planck time.
</p>
<blockquote><small>
1. Actually, this expected-count
remains true for the full group of |determinant|=1 linear transformations,
not merely the (comparatively tiny) Poincare/Lorentz subgroups.
<br>
2. But note that although all Lorentz observers agree on the <i>statistical nature</i> of
the random rain, they generically <i>disagree</i> on the microscopic details of
the random points themselves, e.g. their time-ordering.  So rain of bricks physics will 
only retain Lorentz invariance in a weakened, statistical, sense. See 
<a href="#whataboutsym">§25</a>.
<br>
3. Note that it would be a <i>mistake</i> 
to try to use any periodic <i>lattice</i>, such as the integer<sup>4</sup>
lattice; it is essential to use Poisson <i>random</i> points.  That is because lattices are not 
Lorentz invariant, indeed are neither boost-invariant nor rotation-invariant;
they have "special directions."  (But
lattices <i>can</i> be invariant under an infinite nowhere-dense 
discrete subgroup of the Poincare/Lorentz group.)
Poisson randomness is.
<br> &nbsp;&nbsp;&nbsp;
Indeed, see 
Lifshitz &amp; Pitaevskii 1980
(chapter 6 "Electrons in a crystal lattice")
for discussion of how physics is modified in the presence of an imposed periodic
background structure such as a crystal lattice, with e.g, direction-dependent 
<a href="http://en.wikipedia.org/wiki/Effective_mass_%28solid-state_physics%29">effective masses</a>,
etc.  
One might naively imagine that a lattice
with cell-size of order of the Planck length would have effects undetectable 
at the low energies humans can access.  But one may plausibly
suspect the opposite:  large effects 
would be produced, e.g. different "UV cutoffs" in different directions, which would
make quantum field theories quite different than they experimentally are.
<br>
4. In Poisson randomness, every point is totally independent of every other,
the ultimate in "locality" of physical law.   With lattices, or with certain 
non-Poisson random processes I invented which also are Poincare/Lorentz invariant, knowing
one point immediately tells you information about other points, which would be
a nonlocal physical behavior.
<br>
5. Another mistaken approach would be to replace, e.g, 3-space with a 2-dimensional manifold.
For example, imagine a wireframe grid filling 3-space.  The surfaces of all the wires
(if we smooth out the joints) are a 2-dimensional manifold which "approximates" 3-space.
One can build such things which are locally "nice" everywhere, for example have
constant negative curvature. [Hint: Glue two right-angled hexagons together at
alternate edges to make a "pants" manifold (Thurston &amp; Levy 1997) then
use pants as your fundamental building block.]
Such an object would look to an observer with un-acute
vision like continuum 3-space, but at the Planck scale would be revealed to be 2-dimensional.
But there seems to be no way to design such a construct to obey doubly-special
relativity.  There will be some shortest closed geodesic in the wireframe
(simply pull a string taut).  Is that an invariant length?  No:
A particle flying round 
that orbit at very high speed would perceive it, due to Lorentz contraction,
as arbitrarily short.
<br>
6. Another mistake would be to get rid of the discrete
Poissonly-distributed bricks and (more simply)
just
use the whole of continuous (1+3)-dimensional Minkowski space <i>Cartesian producted</i> with
a fixed microscopic compact manifold.  
(This kind of idea has been called <b>Kaluza-Klein model.</b>
It also would introduce a length scale – the size of the microscopic manifold.)
The problem with Kaluza-Klein is that the microscopic dimensions would
need to be treated on the <i>same</i>
footing as the usual macroscopic ones, causing space to acquire <i>more dimensions.</i>
That's bad because it makes QED's infinities worse.  
In contrast, rain of bricks <i>diminishes</i> the dimensionality of
space.  (A different reason, besides infinities, that diminishing dimension is good, is 
that in high dimensions, there tend to be <i>many</i> choices.   String theorists wanted
<a href="http://en.wikipedia.org/wiki/Calabi%E2%80%93Yau_manifold">Calabi-Yau manifolds</a>
for their 6 microscopic dimensions, and there seem to be an enormous,
perhaps even infinite, number of them.   If you've only got 3 dimensions to mess around in,
there are far <i>fewer</i> nice manifolds to worry about.)
Another problem with 
Kaluza-Klein is that it would also allow ultrahigh energy particles 
"orbiting round the microscopic dimensions" that never 
could go away, apparently contradicting reality
since such particles have never been observed.   With rain of bricks nontrivial
Fourier modes within
bricks are postulated (see 
<a href="#postU">postulate IV</a> below) 
not to exist, and also are conjectured to go away
by themselves even without that postulate, 
since propagator convolution acts <i>non</i>unitarily on
those modes to diminish them exponentially.  This is possible because the dimensions
within the bricks are <i>different</i> than the macroscopic dimensions.
<br> &nbsp;&nbsp;&nbsp;
[<i>However</i> the <a href="#sheafview">sheaf view</a> gives
a way to re-interpret rain of bricks in
a somewhat Kaluza-Kleiny fashion, in which case KK fans could rejoice in the end...]
<!--
In the KK-interpretation of rain of bricks, the above counter-arguments do not hold.
???The KK way has problems, it seems to me to be totally equivalent to ordinary QED
with its infinities included -- we need inequivalence to get finiteness.
-->
<a name="thepoissonlaw"></a>
<br>
7. It is worthwhile to record <b>Poisson's law</b> about such processes:
a spacetime region expected to contain &#955; points, will actually contain N&#8805;0
points with probability &nbsp; prob(N)=&#955;<sup>N</sup>exp(-&#955;)/N!.
<br>
8. The information-theoretic <b>entropy</b> of this distribution is
<nobr>
ln(2&#960;e&#955;)/2-(12&#955;)<sup>-1</sup>±O(&#955;<sup>-2</sup>) 
</nobr>
nats
when &#955;&gt;&gt;1 
(Adell, Lekuona, Yu 2010;
note 
<nobr>1 bit=ln2 nats).</nobr>
An explicit upper bound on the additive 
error
<nobr>"O(&#955;<sup>-2</sup>)" </nobr>
is
<nobr>31&#955;<sup>-2</sup>/24+33&#955;<sup>-3</sup>/20.</nobr>
<!--Agrees with 
Evans, Boersma, Blachman, Jagers 1988:
log(2&pi;e&lambda;)/2-(12&lambda;)<sup>-1</sup>+O(&lambda;<sup>-2</sup>)  nats
-->
In the opposite limit &#955;&#8594;0+ the entropy is
<nobr>
-&#955;ln(&#955;)+&#955;+&#955;<sup>2</sup>ln(2)/2+O(&#955;<sup>3</sup>)
</nobr>
nats.
An explicit upper bound on the additive 
error
"O(&#955;<sup>3</sup>)" is
<nobr>
&#955;<sup>3</sup>/(20-3&#955;)
</nobr>
if 0&lt;&#955;&#8804;6.
<a name="pentahedra"></a>
<a name="tetlemma"></a>
</small>
</blockquote>
<p>
Second, associate with each raindrop point, a (D+E)-dimensional "brick." 
My original notion of what a "brick" should be was
a compact D-space- and E-time-dimensional 
flat space+time manifold consisting of a (D+E)-dimensional
<b>parallelipiped</b> with periodic boundary conditions.
All of these bricks are isometric. 
An alternative and simpler idea, which I call <b>"round bricks,"</b>
is to make the brick be a D-dimensional compact manifold of constant curvature,
e.g. most simply the D-dimensional surface of a sphere in a 
(D+1)-dimensional Euclidean space (but other options could also be considered, 
such as "elliptic geometry").
The size and shape parameters defining these parallelipipeds 
– or in the case of round bricks there is only one such parameter, the radius –
are again regarded as new
fundamental physical constants and presumably all are of order 1 in Planckian
length and time units. (Also D and E are integer-valued physical constants.)
</p><p>
Round bricks
have the advantage of having the most possible symmetry, simplicity, and "uniqueness";
they also lead to simpler mathematical formulas.  
</p><blockquote><small>
With parallelipiped bricks, the surface area and volume
of "a sphere of radius r" inside the
brick-manifold are complicated and ugly 
functions of r which need to be defined
piecewise.  These functions keep arising.  With round bricks, the corresponding functions
are simple.
</small></blockquote>
Therefore, pending evidence to the contrary,
I prefer round bricks.
<p></p><blockquote><small>
<b>Further possible extensions:</b><br>
1. Of course, I am aware that other choices besides "parallelipipeds" and 
"spheres" could also be considered.   See Wolf 1974 and Wang 1952 for complete
enumerations of certain classes of possibilities.
["Round" and "square" bricks actually are the same thing if we are 
speaking of 1-dimensional bricks.]
Both our choices have the highly desirable and useful
property that every point in a brick is equivalent under
a symmetry to every other, but round bricks based on either spherical or
elliptic geometry have the stronger property that 
every <i>pair</i> of points
within a brick is equivalent to every other point-pair having the same interpoint distance.
Compact hyperbolic (i.e. constant negative curvature) manifolds in D&#8805;2 dimensions 
cannot enjoy either of these symmetry properties.
<!--
The complex projective plane P<sup>2</sup>(<b>C</b>), is 4-real-dimensional;
complex projective line is 2-real dimensional and is the Riemann sphere. 
-->
<br>
2. Those unsatisfied with integer space dimensions
might even consider certain
"fractional dimensional" structures such as the "Koch snowflake";
and those bored by the propect of all-isometric bricks might be able to allow them 
to all have different shapes and sizes, generated by some further probabilistic process.
One could consider intermixing different brick dimensionalities, or making
the within-brick distances be "noisy."
There is a large amount of design freedom
even restricting to boring isometric flat parallelipiped bricks
<!-- (although we could lessen it by demanding the
parallelipipeds be rectangular??) -->
but far more if we remove those restrictions.
<br> <a name="hierbricks"></a>
3. Yet another horrifying (but perhaps more powerful) possibility 
would be "<b>hierarchical</b> rain of bricks" in which, for example,
the bricks are 2-dimensional, except that if you look inside them with a microscope, they
have a finer rain of bricks structure based this time on (e.g.) 1-dimensional bricks.
<br>
4. Even with all brick sidelengths equal (hypercube),
we could have each brick magically perform a rotation of its
contents using one of the rotation-isometries of the brick (or if
using round spheres, an infinite group of rotations is possible) according to a
magic random rotation associated with each raindrop.
Under 
<a href="#postU">postulate IV</a>
below though, this kind of change would have no effect.
</small></blockquote><p>
<b>Clarifying remark:</b>
Our bricks are <i>not</i> contained within the Minkowski (1+3)-space.  
They each are separate from it.  Hence there is no such thing as the "orientation" of
a brick, as viewed from Minkowski space.
</p><p>
<b>The true arena of physics</b> is now postulated to be the points within all those bricks.
Mathematically, the arena is the Cartesian product of:
the Poisson-random raindrop points,
with:
a single brick.
</p><a name="post2"></a><p>
<b>Postulate II (Squared Distance):</b>
The distance between two points A and B in that arena is given by the following formula:
</p><center>
Distance(A,B)<sup>2</sup> 
=
MinkowskiDistance(between the two raindrop-points respectively 
associated with A and B's bricks)<sup>2</sup>
<br>
+
GeodesicDistance(from A to B <i>within</i> the brick, using the fact A's and B's bricks are congruent
and hence can be mentally superimposed)<sup>2</sup>.
<!-- <br>
+
GeodesicDistance(from B's raindrop point to B within B's brick)<sup>2</sup> -->
</center><p>
<i>except</i> that if A&#8800;B and A and B lie within the <i>same</i> brick,
then we shall regard them as being in some sense infinitely far
apart, or anyhow treat them physically differently than the usual case where they 
lie in different bricks (see postulate <a href="#post3">III</a> below).
</p>
<p><b>Remark:</b>
There is a very good reason bricks must additively modify the distance <i>squared</i>
–
as opposed to the distance itself or some power other than 2 (or some nonpower
function of distance)
–
and there is a very good
reason the modification must be <i>additive</i>:
These seem <i>necessary</i> to make rain of bricks physics enjoy the 
<a href="#limitequivthm">limit-equivalence theorem</a>
in 
<a href="#dophys">§14</a>.
</p>
<p>
To end this section, since it had to be done somewhere, we now state and prove the 
important 
</p><p>
<b>"Tetrahedron lemma:"</b>
Note that the metrical structure of Minkowski (D+1)-space, for any D&#8805;1, is such that the 
raindrops form a <b>locally noncompact set</b>; more precisely:
for any nonempty real interval (A,B),
there are an <i>infinite</i> number of raindrops whose squared Minkowski 
pseudodistances to any given raindrop, lie in (A,B).
Consequently each raindrop "feels the same" as any other, if its
"feelings" are based solely on its Minkowski pseudodistances to all the other raindrops
(all measured to within error ±&#949; in the limit &#949;&#8594;0).
More generally, any <i>k-tuple</i> of raindrops 
in Minkowski (D+1)-space, for any k with 1&#8804;k&#8804;D, 
would "feel the same" if translated, rotated,
and Lorentz boosted to any other location, orientation, and velocity 
in Minkowski space, which means
the psuedodistance structure of Minkowski continuum-space is
duplicated by its countable raindrop subset much better than one might
have naively thought!
[In contrast, if we had been in <i>Euclidean</i> space, every raindrop would feel different
and there would only have been a <i>finite</i> set of raindrops lying within any real
distance-interval (A,B) from any given point.]
But there are limits to this duplication:
the statement becomes false when k&#8805;D+1.  
Then raindrop k-tuples generically "feel different" when moved, and there are only
a <b>finite</b> number of other raindrops which lie in k specified allowed
pseudodistance intervals
from k given raindrops.
</p><blockquote>
<b>Proof sketch:</b>
Demanding k pseudodistances is k equations
which need to be satisfied by D+1 coordinate unknowns.  When there are more
equations than unknowns, there generically is no solution, and when there are
exactly the same number,
there generically are only a finite number of solutions.
If we make the pseudodistance demands be
narrow <i>intervals</i> rather than single real numbers, then
the finite set of solution points
"thickens" into a region of finite Lorentz-invariant (D+1)-volume, i.e. containing at most a 
finite number of raindrops with probability=1.
When there are N&#8805;1 fewer equations than unknowns, there generically are a finite number 
of N-parameter continua of solutions, each in our case plainly admitting unboundedly
large coordinate values.
This for any N&#8805;1 "thickens" into a region
of <i>infinite</i> Lorentz-invariant (D+1)-volume.
You can see why by considering the N=1 case where 
we have D points all with spacelike separations.
Without loss of generality we can demand (by performing a Lorentz transformation and space 
and time translation) these points form a simplex with all of them having
time-coordinate t=0 and x-coordinate x=0.  
We can also wlog make all of them have distance approximately R to the origin by making
the circumenter of the simplex lie near the origin where the
circumradius is R. The word "approximate" could have been "exact" (both can be made true)
but was intentional because I want to make it clear this scenario is generic, i.e.
we can perturb these D distances slightly away from R in a set of ways having positive measure.
In the exact case any point (x,t) with all the D-1 other coordinates 0,
will have psedodistance=E to all D points if 
t<sup>2</sup>-x<sup>2</sup>-R<sup>2</sup>=E<sup>2</sup>,
which is a hyperbola in the (x,t) plane.   This curve "thickens" when E is made
a narrow interval rather than a number, so that it has thickness approximately 
proportional to 1/|t| when |t|&#8594;&#8734;.  
The total area of the thickened curve is then (logarithmically) infinite
since &#8747;<sub>1&lt;t&lt;&#8734;</sub>dt/t diverges.
Hence we have verified for this specific point configuration that the 
locus of points of Minkowski (D+1)-space at pseudodistances in (E, E+&#916;)
from each of our specific D points, has infinite 
Lorentz-invariant (D+1)-volume for D&#8805;1 and any &#916;&gt;0,
and hence with probability=1 contains
an infinite set of raindrops.  
In the alternative approximate case we can make the D pseudodistances to all
the simplex vertices be D different numbers each near E, but adjustable...
So this is plainly still true even if the allowed pseudodistance intervals are slightly
perturbed (for a nonzero measure subset of possible perturbations), i.e.
is generically true.
<b>Q.E.D.</b>
<br> &nbsp;&nbsp;&nbsp;
<b>Quantitative extension of Tetrahedron lemma:</b>
The Lorentz-invariant (D+1)-volume of the set of points at |distance|&#8804;R to
each of D+1 given points in general position in Minkowski (D+1)-space, is
O(R<sup>D+1</sup>)
when
R&#8594;&#8734;.
(And, of course,
when R&gt;0 is made sufficiently small then this volume generically  becomes
exactly zero.)
</blockquote>
In the physical case D=3 this can be summarized as
"A raindrop-triangle feels the same no matter where it goes, but
<b>tetrahedra</b> know where they are"!
Based on that, one might naively expect that rain of bricks physics might start to
differ, or anyhow differ more, from old-style QED for processes 
described by Feynman diagrams with 
"&#8805;6 loops" (in the poor nomenclature
physicists in this area employ; the complete graph K<sub>5</sub> is
not allowed as a Feynman diagram but if it were it would have "6 loops" since it has
10 edges which is 6 more than the number 4 of edges needed to connect the 5 vertices
in a tree).
These, and only these, processes could hope to exhibit statistical
discrepancies from perfect Poincare-Lorentz symmetry.
As far as I know, so far zero theory-experiment 
comparisons have ever been carried out for any QFT for any 
process involving &#8805;6 Feynman loops.
The closest we have yet come is 
Aoyama, Hayakawa, Kinoshita, Nio 2010's 5-loop calculation
of the electron magnetic moment in QED<sub>5</sub>, but the experiment is
not precise enough to see 6-loop effects even if they were calculated.
<p></p>

<a name="brickshape"></a>
<h3>13. What are D, E, and the parallelipiped shape &amp; size parameters? </h3>

<p>
Let D be the number of soace dimensions and E the number of time dimensions in a brick.
</p><p>
If both D&gt;0 and E&gt;0 (spacetime parallelipiped bricks) then we might, irritatingly,
predict the possible existence of an unbounded number of kinds of ultrahigh-energy
(above Planck scale) particles each "trapped within a single brick" 
e.g. as a Fourier-like mode
of some Dirac or Maxwell equation.  
If we make E=0 and D&gt;0 (spatial-only bricks) then that bogeyman is avoided,
there being no such thing as energy in the absence of time 
(since, according to quantum mechanics,
an "energy eigenstate" is the same thing as a "time-invariant state").
[I'm unsure whether it would also
be possible to avoid this bogeyman by making E&gt;0 and D=0 (time-only bricks).]
<!--
in which case the only possible multitime-invariant state within a single brick, would
be a constant-valued wavefunction (with zero energy)???   HUH? ACTUALLY FOURIER MODE WILL DO,
FOR PARALPD BRICKS, SO WE HAVE TO DEMAND PURE-SPATIAL BRICKS?
-->
</p><p>
So our first recommendation about the integers D and E is "D&#8805;0 and E&#8805;0 and DE=0."
</p><p>
We also shall require 0&lt;D+E&lt;4.
</p><p>
The demand D+E&gt;0 is because if D+E=0, i.e. if a brick were just a <b>single point,</b>
then a photon emitted by one brick-point would (since the photon propagator contains a
"delta function"
of exactly zero width) <i>never be able to hit</i> any other brick-point
because those points would always appear at not quite the correct t-coordinate
given their x,y,z coordinates. 
More precisely: with probability 1 the delta function component of the photon propagator
would never exert any effect.
This would obviously make all
nontrivial Feynman diagrams
return wrong answers.
</p><p>
A weaker effect of the same sort – a delta function component prevented from
having an effect – also happens if D+E=1, for more subtle reasons. 
(See <a href="#betterbehaveddelta">D=1 analysis</a> 
in <a href="#propformulas"> §15</a>.)  This would mean D+E=1 also should be forbidden,
except that because the delta function in this case is of a different and weaker kind,
it tentatively appears to be ok – if it is so weak that it really <i>shouldn't</i> have
any effect, there is no contradiction.   
</p><p>
<small>
Whether D+E=1 is permitted is actually 
a subtle question, and I am not 100% sure of my answer (which is "yes").
Essentially, "brick smearing" over a 1-dimensional brick weakens the delta function; but
because it can occur squared or integrated times power-law-infinite test functions,
it turns out the stars are aligned just right to allow
this weakened delta function to still possibly make nonzero finite contributions
(which would be missed by Monte Carlo integration).
However, then one realizes that the only way that could happen is in Feynman diagrams containing
a 2-arc loop ("double arc") with both endpoints zero distance apart; and in that case we can
argue from postulates <a href="#post3">IIIb</a> and 
<a href="#postU">IV</a> 
that by symmetry the coefficient of such a contribution ought to be 
exactly zero.
<a name="oventimer"></a>
<br>&nbsp;&nbsp;&nbsp;
Another possible UV-cutoff scheme which at first seemed promising
was the <b>oven timer</b>.  That is: we postulate that any electron, 
after a photon emission/absorption event, must <i>wait</i> for at
least some  fixed amount of <i>proper</i> |time|, 
(presumably of order 1 in Planck units) before another such event, but aside 
from that acts normally.    
<!--
Because QED is fine with electrons going both forward and backward in time, or at
lightspeed, or superluminal, so this kind of would present a problem... but could still
do it...  
-->
This is an attractively simple idea which plausibly would cure QED's 
ultraviolet infinities
at any fixed order in perturbation theory.
However, 
</small></p><ol type="A"><small>
<li>
I doubt it would work to cure <i>non</i>renormalizable QFTs such as gravitons,
which would still generate enormities (high powers of Planck) at
high fixed orders in perturbation theory.
</li><li>
The other problem QED has – that the perturbative series diverges –
still apparently would persist.
Consider a modified form of Dyson's negative-&#945; collapse argument 
(<a href="#dyson">§5</a>):
Clouds of electrons with <i>unboundedly great density would
in no way be prevented</i> by the oven timer, and in the Dyson negative-&#945;
scenario in which electrons attract, they still would attract.
This attraction would be lessened by the "oven timer" at subPlanck distances,
but by making <i>two</i> ultra-high-density clouds of (mutually attracting) 
electrons separated by several Planck lengths, the attraction energy
could be made to hugely outweigh
everything else (pair creation energies, Fermi repulsion energies, etc).
Hence we would expect that oven-timer QED would still feature QED &#945;-power-series
with zero radius of convergence.
</li></small></ol><small>
For these reasons I dismissed the oven-timer idea as not a good way to save physical QFTs.
</small>
<p></p><p>
The purpose of demanding D+E&lt;4 is 
to abolish QED infinities.
Essentially, the idea is that all the logarithmic 
infinities of QED
arise in 4 spacetime dimensions and would have been finite in any dimension below 4.
</p><small><p>
<b>About lower dimension:</b>
One of t'Hooft and Veltman's (1972)
famous contributions was to develop "dimensional regularization" 
for QFTs, in which the integrals were performed, not in 4 dimensions, 
but rather in "4-&#949; dimensions."  One then later would consider the limit 
&#949;&#8594;0+.  (Of course, it isn't clear a priori what 
"4-&#949; dimensions" actually should <i>mean</i>, but
t'Hooft and Veltman were able to define a meaning sufficient at least for their purposes.
Actually it is better to regard this as a set of regularization rules whose design was
motivated by thinking about other dimensions, but actually we stay in 4 dimensions
and no fractional dimensions are really involved.
This is discussed by both Peskin &amp; Schroeder and Mandl &amp; Shaw; see also
Leibbrandt 1975, Velo &amp; Wightman 1976.)
We don't need any of that fancy machinery, all we need is the comparatively trivial fact that
in dimensions 1, 2 and 3, the logarithmically infinite QED
(in 4D) integrals all become finite.
We can also enjoy the fact that our bricks each automatically generate a positive time-delay 
(or positive spatial distance, in the spatial version), usually of order 1 in Planck units,
whose net effect is to slow things down or spread things out,
tending to  prevent infinitely fast or spatially-compact
sequences of N interactions from happening when N&#8594;&#8734;.
</p>
</small>
<p>
The rain of bricks arena appears, to an observer with vision insufficiently
acute to perceive tiny Planck-scale details, to be
ordinary (1+3=4)-dimensional Minkowski space.  However, 
viewed at microscopic scale,
the rain of bricks arena actually is (&lt;4)-dimensional.
</p><a name="borderdim" <="" a=""></a><p><a name="borderdim" <="" a="">
It is useful to introduce the concept of <b>borderline dimension</b>
(also often called "critical" dimension).
That is, QED and Yang-Mills theories have borderline dimension <b>4</b>, 
meaning only in this dimension they exhibit scale-free behavior and logarithmic infinities;
in lower dimensions the infinities vanish.  
The way we can determine the value of the borderline dimension is
to demand that the lagrangian density must yield a
dimensionless, i.e. scaling-invariant, action integral.
This forces such scale-free behavior in exactly 4 dimensions.
For Einstein gravity, the lagrangian density is the Ricci scalar curvature, which
has dimension -2.  Hence the Einstein-Hilbert action integral's
action is dimensionless in 2 dimensions, implying that for Einstein gravity the 
borderline dimension is <b>2.</b>
This indicates that bricks with dimension&lt;4 will cure the UV infinities in QED and
QCD; to hope to cure graviton infinities (</a><a href="#maxstrength">§23</a>) we should use
brick dimension D+E&lt;2.  
</p>
<a name="hierdiscuss"></a>
<p><small>
Also D+E=2 might be acceptable if we were willing
to handle quantum gravity infinities with the aid of "renormalization" techniques; indeed
if we insisted on such techniques presumably D+E=2 would be 
<a href="#carlipdisc">forced</a>.  Further, the 
<a href="#hierbricks">hierarchical bricks</a> idea presumably would
allow quantum gravity without infinities and without renormalization
even using D=2 at the "outer level"
of the hierarchy to get what without hierarchy 
would merely have been a renormalizable theory.
</small>
</p><p>
Assuming integer (D,E), our restrictions have reduced us to only six possibilities:
</p><center>
(0,1), (0,2), (0,3), 
(1,0), (2,0), and (3,0).
</center><p>
D+E=3 gives the nicest (continuous) propagators,
while D+E=1 seems to grant the most power to regularize QFT infinities in the sense
the dimension is smallest.
</p><p>
The alternate form IIIb of (the upcoming) postulate
<a href="#post3">III</a> seems to enjoy
certain advantages
<i>provided</i> we have time-only bricks.  
Specifically with IIIb and time-only bricks
postulates III and IV can pretty much be discarded
since they arise automatically as theorems, i.e. as 
consequences of the other postulates (see
discussion after
postulate <a href="#postU">IV</a>);
and in a slightly different setup we can seek Hamiltonian and Lagrangian formulations
of rain of bricks physics (see 
<a href="#opprobaction">open problem 7</a> at the 
<a href="#opensprobs">end</a>).
This would
reduce us to only three possibilities:
</p><center>
(D,E) = (0,1), (0,2), or (0,3).
</center>
<a name="tiltlemma"></a>
<p>
On the other hand, there also could be legitimate reasons to favor space-only bricks, 
and the same discussion also provides a tentative argument that space-only
1-dimensional-circular bricks also could allow discarding IIIb.
My favorite <b>reason for preferring space-only bricks</b> is this:
with them, Minkowski distances can become more-spacelike but cannot become more timelike.
I.e. the "light cone" of timelike rays (i.e. acceptable subluminal velocities)
in the view of any Lorentz observer, with
space-only bricks will always be a <i>subset</i> of the truly-acceptable direction set,
hence any observer's notion of "the future" (i.e. a disk in a spacelike hyperplane in
Minkowski space) will genuinely be in the future, even after applying raindrop-caused
distance distortions.  We shall later 
(<a href="#sheafview">sheaf view</a> and <a href="#whataboutsym">§25</a>)
use this lemma 
to argue that rain-of-bricks physics is <i>unitary,</i> i.e. 
still enjoys "probability conservation."
<!--
Could we then drop postulate
<a href="#post3">IIIb</a> by arguing that 
for a 1D space-only brick, 2 or more interactions of same particle within that brick
would since collinear somehow be imposible or cancel???
-->
With space-only bricks we would
instead be reduced to only these three possibilities:
</p><center>
(D,E) = (1,0), (2,0), and (3,0).
</center><p>
Finally, since it seems most desirable that quantum gravity also be treatable in the same way,
the demand that D+E&lt;2 reduces us to only two possibilities
</p><center>
(D,E) = (1,0), and (0,1).
</center><p>
As I've said I prefer space-only, which would reduce us to a <b>unique possibility D=1, E=0.</b>
These 1-dimensional bricks also have the further advantage of <i>uniqueness</i>:
"Round" and "square" bricks are the same thing, if they are one-dimensional; there is only
a single 1-dimensional boundaryless compact manifold.
</p><p>
<b>Conclusion from Theoretical Considerations:</b> 
The above restrictions (at least, if you buy them all) have reduced down to
a unique possibility: 1-dimensional spatial bricks.   There would then still be a little 
nonuniqueness, namely it could 
either "flip orientation" at random or preserve orientation.   Preservation has the
advantage that it fits in with Kaluza-Kleiny rain of bricks interpretations, but
under <a href="https://dl.dropboxusercontent.com/u/3507527/postU">postulate IV</a> this choice makes no difference.
</p>
<!--shape=hypercube wlog???No.  You can rotate, that is all.-->
<a name="runningcouplingevidence"></a>
<p>
<b>Experimental evidence</b> based on high-energy electron scattering
(Eichten, Lane, Peskin 1983, but extended to take into account 
electron-positron scattering
data from the Large Electron Positron collider LEP at CERN, 
whose energy topped out at 209 GeV in 2000)
indicates behavior consistent with an electron pointlike 
down to length scales safely below 10<sup>-6</sup> times its Compton wavelength,
say 10<sup>-18</sup> meters.
[If it were possible to redo those experiments using the 
<a href="http://en.wikipedia.org/wiki/Ultra-high-energy_cosmic_ray">highest energy cosmic rays</a>
yet seen instead of LEP,
then if those yielded the same results we could get down to
10<sup>-27</sup> meters.]
Less clearly,
the tinyness of the discrepancy between
the theoretical and experimental values of the magnetic moment &#956;<sub>e</sub>
suggests the electron radius is below
<nobr><b>&#8804;10<sup>-24</sup>meters=6×10<sup>10</sup></b></nobr><b> Planck lengths.</b>

This sets a (probably very weak) upper bound on brick size.
</p><p>
In the other direction, 
the small-distance-measurement corollary in <a href="#quantgrav">§11</a>
suggests that bricks must be large enough to contain two points
at least 
2(ln2)<sup>1/2</sup>&#960;<sup>-1/2</sup>&#8776;<b>0.9394</b> Planck length units apart.
</p><p>
<b>Is near-equality of "running coupling coefficients" at certain 
small length scales, evidence for "bricks" at those length scales?</b>
There is one other experimental-extrapolation fact
that suggests the possibility that
the brick length scale might not be the Planck length
1.6×10<sup>-35</sup> meter, but instead 
perhaps between 5 and 10<sup>5</sup> times longer.
Consider the dimensionless so-called "running coupling constants."
(Since they "run," they 
actually are not "constants" and would better be called "coefficients."
In rain of bricks physics, however, there will be one particular length scale,
the brick scale, and hence one fixed value for all these, so
they then would again become "constants.")
There are three, for the electromagnetic (1),
weak (2), and strong (3) forces.
The below picture apparently was first plotted by
Amaldi, de Boer, Fürstenau 1991 and 
also is found in fig. 22.1 of Peskin &amp; Schroeder,
fig. 6.8 in 
Martin's <i>Supersymmetry Primer</i>, etc.
We have added the green curve depicting the "gravitational coupling."
As the picture indicates, the three running coupling coefficients under the MSSM 
(minimal supersymmetric extension of standard model; red curves) 
all approximately <i>meet</i> at 
<nobr>
25.35&#8804;&#945;<sup>-1</sup>&#8804;25.65
</nobr>
at an energy scale of approximately 10<sup>16</sup> GeV
(corresponding to a length scale of about 1220 Planck lengths).
It currently appears that this meet is not quite exact and that 
exactness can be ruled out with about 3&#963; worth of confidence.
Meanwhile, in the plain standard model SM without supersymmetry, there definitely
is <i>not</i> a 3-way meet for the weak, strong, and electromagnetic &#945;<sup>-1</sup>s
(ruled out with &#8805;12&#963; of confidence). MSSM and SM have different
particle sets, causing different running formulas.
Despite their non-meet, the three SM curves (blue) nevertheless come
fairly close to meeting.
They form a triangle whose vertices have energies between 10<sup>13</sup> and 
2×10<sup>16</sup> GeV.
At the particular energy 10<sup>14.2</sup> GeV (corresponding
to a length scale &#8776;77000 Planck lengths)
all three SM &#945;<sup>-1</sup>s lie between 40 and 44.
If we also consider gravity, then we get approximate <i>4</i>-way meets of
all coupling constants with the greatest being &#8804;1.45 times the least,
for both MSSM or plain SM at the right energy which is within the interval
<nobr>
(2-4)×10<sup>18</sup> GeV.
</nobr>
This contrasts with the experimentally 
measured situation at energy M<sub>Z</sub>&#8776;91.2 GeV,
where the greatest of the three &#945;<sup>-1</sup>s is over 7 times the least,
and where gravity is about 33 orders of magnitude weaker than these.
</p><a name="runcoupconstplot"></a><p>
<b>The point</b> we want to make is that, regardless of whether you prefer SM or MSSM,
either way the theoretically extrapolated coupling coefficients
get much closer to equality at energy
scales somewhere between 10<sup>14.2</sup> and 10<sup>18.4</sup> GeV
(corresponding to length scales between 5 and 77000 Planck lengths), than they are
at low (i.e. experimentally accessible)
energies.  This near-equality would make sense if there 
were an underlying microscopic <i>reason</i> 
– such as the existence of "bricks" or perhaps "superstrings" – for why
it would be natural for all the forces to have about the same strength at that length scale.
</p><p>
The remainder of this section will simply be
our plot of the running of the couplings and our explanation of the meaning
of that plot and how we created it.
</p>
<img src="WarrenSmithQED131123_files/MSSM3way.png" width="100%">
<p>
<b>Running coupling constants in SM and MSSM.</b>
The horizontal axis is the log<sub>10</sub> of the energy scale in GeV,
ranging from 10<sup>0</sup> to 10<sup>20</sup> GeV.
The vertical axis is the <i>reciprocated</i>
dimensionless coupling "constant," 1/&#945;,
ranging from 0 to 110.
Colors of curves: blue=SM; red=MSSM; green=Gravitational coupling.
At the left bottom of the plot, the three Xs in downward-moving order are 
for the U(1), SU(2), and SU(3) couplings.
We also have made a vertical yellow line at the Planck energy, and painted an orange
"danger zone" to indicate a region of too-strong couplings in which QFTs are known to
break down.  Also, near-meets of all 3 kinds of coupling are indicated by grey blobs –
or in the case of the rightmost blob this is a 4-way meet.
</p><p>
<b>Definitions/warning.</b>
In our plot, &#945;<sub>1</sub> is defined differently from what you might naively think.
Specifically, we plot:
</p><ul><li>
1/&#945;<sub>1</sub>=(3/5)cos(&#952;<sub>W</sub>)<sup>2</sup>/&#945;<sub>EM</sub>
where
&#945;<sub>EM</sub> is the running electromagnetic fine-structure "constant"
<nobr>(1/&#945;<sub>EM</sub>&#8776;137.036</nobr> at low energy 
and <nobr>1/&#945;<sub>EM</sub>&#8776;128.94</nobr> at energy M<sub>Z</sub>)
– often &#945;<sub>EM</sub>
is just called "&#945;" –
and
&#952;<sub>W</sub>
is the weak mixing angle (which also runs).
At M<sub>Z</sub> we then have 1/&#945;<sub>1</sub>=59.5.
</li><li>
1/&#945;<sub>2</sub> = sin(&#952;<sub>W</sub>)<sup>2</sup>/&#945;<sub>EM</sub>.
At M<sub>Z</sub> we have 1/&#945;<sub>2</sub>=29.8.
</li><li>
1/&#945;<sub>3</sub> = 1/&#945;<sub>S</sub>.
At M<sub>Z</sub> we have 1/&#945;<sub>3</sub>=8.4.
</li><li>
1/&#945;<sub>grav</sub> = (M<sub>pl</sub>/m)<sup>2</sup> is our
definition of gravitational coupling (plotted in green) at energy scale
(as a mass) m.
</li></ul>
The subscripts 1,2,3 on our &#945;s are for the U(1), SU(2) and SU(3) gauge groups
of the forces (electromagnetic, weak, strong) they respectively pertain to.
The reason for the factor 3/5 alteration of the naive &#945;<sub>1</sub>
is to 
"agree with the canonical covariant derivative for grand unification of the gauge group 
<nobr>
SU(3)×SU(2)×U(1)
</nobr> 
into SU(5) or SO(10)" as Aitchison 2005 puts it.  That
is because this plot's most famous use has been to consider the hypothetical
embedding of SM or MSSM inside an SU(5) grand unified theory (GUT).
The 3-way meet of the red curves would not have occurred without
inserting the 3/5 factor and this meet has been regarded by many
as evidence that perhaps SU(5) or SO(10) SUSY-GUT is a truer theory of physics 
than the plain standard model SM, or that MSSM is more compatible
with grand unification than plain SM.   
But as of year 2013, essentially zero convincing or direct evidence 
for the validity of any GUT, or of the SUSY concept, has been found
(despite over 10,000 papers being published on these topics)
and the "more compatible" claim is undercut by recent computations showing
that the standard model
"augmented with adjoint fermionic multiplet 24<sub>F</sub>"
in a <i>non</i>supersymmetric manner, <i>also</i> exhibits a high-accuracy 3-way meet
at 10<sup>16</sup> GeV but with &#945;<sup>-1</sup>&#8776;35
(Di Luzio &amp; Mihaila 2013; Mihaila 2013).
Their 24<sub>F</sub>-extension of the standard model was motivated by their desire
to embed SM <i>including massive neutrinos</i> inside an SU(5) GUT.  (They
also claim another SM extension they call 15<sub>H</sub> is similarly promising and
say they'll examine it in a future paper.)
<p></p><p>
The SM &#945;<sub>1</sub><sup>-1</sup> curve <i>without</i> inserting the
artificial [if GUTs are not being considered] 
3/5 factor is also shown, as the uppermost blue curve, labeled "EM."
It actually yields a <i>four</i>-curve meet about <b>twice as tight</b> as
either SU(5)-GUTified SM or MSSM (rightmost gray blob).
Specifically, this meet involves &#945;<sup>-1</sup>=52±5
at an energy of 2.5×10<sup>18</sup> GeV, which is 1/5 of
the Planck energy.
Thus the plain (no-GUT, no-SUSY) standard model
seems the one, among these three, most compatible with rain of bricks, and presumably 
indicating a brick length scale of <b>&#8776;5 Planck lengths.</b>
</p><p><small>
In contrast, the best MSSM 4-way meet 
involves &#945;<sup>-1</sup>=24±4
at an energy of 4×10<sup>18</sup> GeV, which is 1/3 of
the Planck energy.
</small></p><p>
If indeed all 4 coupling constants become equal at the brick scale,
then rain of bricks would yield a <b>simplification</b> of physics versus the old
standard model in the sense that 3 fewer magic constant parameters would be needed.
(This simplification could indeed be more than enough to compensate for the extra parameters
needed by rain of bricks physics to describe brick sizes and raindrop density, in
which case rain of bricks actually would be regardable as <i>simpler</i> than old-style
physics.)
It would seem very "elegant" that "interactions" only happen on bricks,
and only a single number would governs
all 4 kinds of interaction.
</p><p>
Note we in the present paper, and "rain of bricks" in general,
do not directly attempt to resolve controversies such as "SUSY or not?".
(Albeit the "superspace" concept underlying SUSY could offer interesting possibilities
for generalizing rain of bricks, which we have not explored at all.)
Rain of bricks simply regards SM, or MSSM, or whatever other particle
QFT, as <i>input</i>, and provides a way to convert those QFTs to have algorithmic 
and mathematical <i>meaning</i>.
Nevertheless, rain of bricks, if correct physics, can in this way "favor" one QFT 
versus another.  Also we shall see 
<a href="#susyforbid">later</a> that rain of bricks seems best set in de Sitter,
not Minkowski, space, and  we find there that if so, then SUSY (at least 
versions of SUSY in which the supersymmetry-breaking is 
accomplished by Higgs boson couplings)
are <a href="#susyforbid">forbidden</a>.
</p><p>
<b>What <i>is</i> a "coupling constant?"</b>
For each 3-valent 
QED Feynman diagram <i>vertex</i> (where two electron lines and one photon line meet)
one needs to introduce a factor of 
<nobr>
-igs<sub>w</sub>&#947;<sup>&#956;</sup>.
</nobr>
See Veltman 1994's appendix E for all standard model Feynman rules, 
this one is copied from his page 271.
Here <nobr>s<sub>w</sub>=sin(&#952;<sub>W</sub>)</nobr>
is the sine of the weak mixing angle
<nobr>[(s<sub>w</sub>)<sup>2</sup>&#8776;0.231</nobr> at M<sub>Z</sub>], and
g obeys 
<nobr>
&#945;/(s<sub>w</sub>)<sup>2</sup>=&#945;<sub>w</sub>=g<sup>2</sup>/(4&#960;).
</nobr>
For the purpose of our picture, <i>we regard the "coupling constant" as 
(4&#960;)<sup>-1</sup> times the absolute square of 
the Feynman diagram vertex-factor</i>, i.e. in this case just
the usual electromagnetic "fine-structure constant" <b>&#945;</b>
(since |i|<sup>2</sup>=1 and the |square| of any Dirac &#947;-matrix is 1).
Other people, such as Feynman, have the different
view that we should neither take the square, nor
divide by 4&#960;;
i.e. Feynman would regard the "electron-photon coupling constant" as just
the electron charge e in units where &#945;=e<sup>2</sup>/(4&#960;).
This squaring-or-not definition-disagreement does not matter, provided you know about it.
(With Feynman's no-squaring convention our picture's ±10% meet of all the 
coupling constants would improve to ±5%.)
The reason the coupling constants "run" is the unobservable bare charge of the electron varies
(under the usual old-style interpretation of QED renormalization) at different energy scales.
</p><p>
Now, consulting other Feynman rules given by Veltman:
</p><ul><li>
On page 270 we find that the 
vertex factor for a W-electron-neutrino vertex is
ig8<sup>-1/2</sup>&#947;<sup>&#956;</sup>(1+&#947;<sup>5</sup>)
hence the coupling constant for that sort of vertex should be
<b>&#945;<sub>w</sub>/2</b>=g<sup>2</sup>/(8&#960;)
provided we regard the matrix 
1+&#947;<sup>5</sup> as having "|square|=4" since its eigenvalues are 2,2,0,0.
</li><li>
Also on page 270 we find that the 
vertex factor for a Z-neutrino-neutrino vertex is
ig&#947;<sup>&#956;</sup>(1+&#947;<sup>5</sup>)/(4c<sub>w</sub>)
hence the coupling constant for that sort of vertex should be
<nobr>
<b>&#945;<sub>w</sub>/(4c<sub>w</sub><sup>2</sup>)</b>
=
g<sup>2</sup>/(16&#960;c<sub>w</sub><sup>2</sup>)
</nobr>
where <nobr>c<sub>w</sub>=cos(&#952;<sub>W</sub>).</nobr>
</li><li>
For a quark-quark-gluon vertex (Veltman page 272)
<nobr>
coupling=<b>&#945;<sub>s</sub></b>=(g<sub>s</sub>)<sup>2</sup>/(4&#960;).
</nobr>
</li></ul>
<p>
Of course, there are many other kinds of Feynman vertices (I've here only listed the ones
with the simplest formulae) whose <i>net</i> effect is why the weak constant 
<nobr>
&#945;<sub>w</sub>=g<sup>2</sup>/(4&#960;)
</nobr>
is conventionally defined to be what it is, rather than, say, half as much.
</p><p>
<b>How I created this picture:</b>
The reason 1/&#945;, not &#945;, are plotted is because in 1-loop 
approximate MSSM and SM, the six 1/&#945;<sub>j</sub> 
curves would be exact straight lines,
see Aitchison 2005 pages 100-108.
That makes it easier to draw their graphs.
So I placed X's on the graph representing the
three blue-blue curve-intersection points with coordinates
(13.044, 42.2), (14.20, 40.5), and (16.45, 46.5)
according to the 3-loop full-SM computation in
Mihaila, Salomon, Steinhauser 2012 (employing about 300,000 Feynman diagrams
processed on 100 computers).
I also placed X's representing the
three red-red curve-intersection points with coordinates
(15.89, 25.48), (16.06, 25.65), and (16.17, 25.35)
according to the 3-loop MSSM computation in
Martens, Mihaila, Salomon, Steinhauser 2010.
Finally I placed X's at the three measured 1/&#945; values at energy M<sub>Z</sub>
then drew appropriate lines through the X's, <i>assuming</i> the transitions from the blue
SM to the red MSSM curves occur at exactly 1000 GeV energy (since that was the 
assumption made by Martens et al).
Actually the true curves calculated at 3-loop accuracy are <i>not</i>
exactly straight lines, hence the curves I drew will be slightly inaccurate,
albeit all the X's arise from allegedly-accurate 3-loop computations 
(or directly from experiments) by others.
</p>
<!--2008 PhD thesis on this:
http://dare.uva.nl/document/113242
gives plots on page 74
finds the 3-way meet is 1/alpha=24.3 and log(Egut)=37.30 whatever that means.
p.75 zooms in. Cites
S. Raby: SUSY GUT Model Building. ArXiv e-prints, 807, July 2008.
Wikipedia:
http://en.wikipedia.org/wiki/Minimal_Supersymmetric_Standard_Model#Gauge-Coupling_Unification
claims 3-way meet occurs albeit 3 stddev off expectations.
Gordon Kane, The Dawn of Physics Beyond the Standard Model, Scientific American, June 2003, 
page 60 and The frontiers of physics, special edition, Vol 15, #3, page 8 
"Indirect evidence for supersymmetry comes from the extrapolation of interactions 
to high energies."
Discussion by Jeff L Jones: http://arxiv.org/abs/0812.2106
The particle data group global average 
[C. Amsler et al. (Particle Data Group), Physics Letters B667 (2008) 1]
finds
alphaS(MZ) = 0.1176 +- 0.002.
Non-SUSY GUTs predict 0.073+-0.001, far too low.
MSSM embedded inside SU(5) GUT predicts 0.130+-0.01 at 2 loops which is acceptable.
1-loop MSSM finds 0.1178, which is only 0.2% larger than the experimental value. 
However, since the 2-loop RG equations and the 1-loop thresholds make the prediction 
worse, the seemingly impressive closeness of the 1-loop prediction to the experimental 
value can only be regarded as a coincidence.
3-loop MSSM calculation:
http://arxiv.org/abs/1008.3070
fig5 indicates 3way point at 16.0 GeV and 1/alpha=25.6 if there is one.
Using Mplanck=1.2209*10^19 GeV we find 1/alpha[grav]=1490600 at the middle cross-pt energy.
1/alpha[grav]=25 happens at energy Mplanck/5=2.44*10^18 GeV.

3-loop SM calculation:
http://arxiv.org/abs/1208.3357 which is abbreviated as
http://arxiv.org/abs/1201.5868 = Phys.Rev.Lett. 108 (2012) 151602
zoom-in to fig3 suggests the SM cross point energies are:
10^13.044, 10^14.20, 10^16.45 GeV respectively at 1/alpha=42.2, 40.5, 46.5
  At 10^14.20 GeV the alpha window is a factor 46.5/40.5 = 1.25 wide.
Using Mplanck=1.2209*10^19 GeV we find 1/alpha[grav]=5.934*10^9 at the middle cross-pt energy.
1/alpha[grav]=49 happens at energy Mplanck/7=1.74*10^18 GeV.
Using nG=3 in the PRL's eqn for beta1 page 2-3 we find that if alpha2=alpha3=etc=0 then
beta1=(al1/pi)^2 * [1/40 + (al1/pi)*199/800 + (al1/pi)^2 * (-388613/1536000)]
at 3 loops, note the change to a - sign at the last term.  This quadratic(al1/pi) has
only positive root at al1/pi=1.075099458 i.e. at al1=3.377524560.
al[EM] at MZ is 128.93+-0.15+-0.15 (thy and expt errors) according to
http://cds.cern.ch/record/375002/files/9812465.pdf
and cites within.

In http://arxiv.org/pdf/hep-ph/9709356v6.pdf pages 60-61
it is claimed at 1-loop order the SM and MSSM curves all are straight lines with 
2*pi*slopes exactly -41/10, 19/6, 7 for SM  
[if use natural log of energy as horiz axis; if log10 then must multiply slopes by 
ln(10)=2.302585]
and 2*pi*slopes exactly -33/5, -1, 3 for MSSM.
It also is claimed alpha1 has been rescaled: alpha1=(5/3)*alpha[EM]
by a factor 5/3 to 
"agree with the canonical covariant derivative for grand unification of the gauge group 
SU(3)xSU(2)xU(1) into SU(5) or SO(10)."  Unfortunately 1/Alpha[EM] at MZ is 128.93
so (3/5) times that is 77.35, whereas his picture shows about 59.1.
1/Alpha[EM] at low energy is 137.035 which times 3/5 would be 82.22.

3-way meet in a non-SUSY SM model "augmented with adjoint fermionic multiplet 24_F":
http://arxiv.org/abs/1305.3111
http://arxiv.org/abs/1305.2850

Wikipedia 
http://en.wikipedia.org/wiki/Minimal_Supersymmetric_Standard_Model#Gauge-Coupling_Unification
says 1/alpha at MZ is 8.5, 29.6, 59.2 for SU3, SU2, U1 and the b0 slope-governing
coeffs are -3, +1, +33/5 [when alpha1 is measured in SU5 normalization, a factor 3/5
different from SM]. These numbers yield an exact 3-way meet using 1-loop linear extrapolation.
"There are two loop corrections and both TeV-scale and GUT-scale threshold corrections that 
alter this condition on gauge coupling unification, and the results of more extensive 
calculations reveal that gauge coupling unification occurs to an accuracy of 1%, 
though this is about 3 standard deviations from the theoretical expectations."

Ian J R Aitchison
Supersymmetry and the MSSM: An Elementary Introduction
http://arxiv.org/abs/hep-ph/0505105
page 101 says MSSM-GUT predicts sin(thetaW)^2 = 3/8 = 0.375 at unification scale.
EQ 492 finds the slopes
  1/alphaJ = 1/alphaJ(at Q0) + bJ*ln(10)/(2*pi) * log10(Q/Q0)
where on page 104
   b1 = -41/10, b2 = 19/6, b3 = 7,  in SM.   Agrees http://arxiv.org/pdf/hep-ph/9709356v6.pdf.
   b1 = -33/5,  b2 = -1,   b3 = 3   in MSSM.  Agrees Wikipedia.
They then state that at MZ we have
sin(thetaW)^2 = 0.231  small uncert  [more acc: 0.23116(12)]
alpha3 = 0.119 with 1/alpha3=8.40 with 2% uncertainty   [more acc: 0.1184(7)=1/8.446(50)]
1/alphaEM=128  [more accurate is 128.93(22)?]
1/alpha2 = sin(thetaW)^2 / alphaEM = 29.6   [more acc: 29.803(50)] 
1/alpha1 = (3/5) * (1/alpha2) * cot(thetaW)^2 = (3/5) * cos(thetaW)^2 / alphaEM = 59.12.
  [more acc: 59.48(12)]

Zooming in to fig 5b in
http://arxiv.org/pdf/1008.3070v2.pdf 
we find the MSSM crossing points are located at
2+3: 10^15.89 GeV     1/alpha=25.48
3+1: 10^16.06 GeV     1/alpha=25.65   1.2% alpha-factor apart!
1+2: 10^16.17 GeV     1/alpha=25.35

The intersection with the gravity curve is an alpha window a factor 100:140 wide MSSM
or 100:144 wide SM.  At MZ, the alpha window in contrast is a factor>7 wide.

http://arxiv.org/abs/1307.1907
measures the strong coupling constant at MZ and mentions "current
world average" is
alpha[strong,MZ] = 0.1184 +- 0.0007
here MZ=91.1876(21) GeV.
Also from http://pdg.lbl.gov/2012/reviews/rpp2012-rev-phys-constants.pdf . 
1/alpha[EM] = 1/137.035 999 074(44) and 1/alpha[EM] at MZ=127 approx.
GF = sqrt(2)*gW^2 / (8*MW^2)
GF = (hbar*c)^3 * 1.1663787(6)*10^(-5) GeV^(-2)
MW = 80.385(15) GeV/c^2
gW = sqrt(GF * 8) * MW / sqrt(sqrt(2))
  = sqrt(8*(hbar*c)^3 * 1.1663787*10^(-5) * GeV^(-2))*80.385*GeV/c^2 / sqrt(sqrt(2)) 
  = 0.65295357 * hbar^(3/2) * c^(-1/2) 
http://www2.warwick.ac.uk/fac/sci/physics/current/teach/module_home/px435/weak.pdf
says alpha[weak] = gW^2/(4*pi) = 1/30 approx.
Wikipedia http://en.wikipedia.org/wiki/Gravitational_coupling_constant
disagrees with my formula alpha[grav]=m^2 in Planck units, instead saying
alpha[grav]=m^2/(4*pi).

Defintions in the plot
alpha[1] = (5/3) * alpha[EM] / cos(thetaW)^2
alpha[2] = alpha[EM] / sin(thetaW)^2
alpha[3] = alpha[strong]
Wikipedia: The 2004 best estimate of sin(thetaW)^2, at Q=91.2 GeV/c=MZ, 
in the MSoverbar scheme is 0.23120+-0.00015.
So alpha[2] = 0.031563(20) = 1/31.683(21)
---
<p>
The <b>size</b> of a brick is not variable independently of &rho;<sub>rain</sub>.
Once we have chosen the dimensionality and shape of a brick, its size (i.e. measure)
is <i>forced</i> upon us by the following
(which seems required to
prevent rain of bricks and old-style physics from exhibiting large experimental 
disagreements caused by on-light-cone Dirac delta functions):
</p><p>
<b>Brick size demand:</b> 
Consider a large ball &nbsp;
<nobr>
t<sup>2</sup>+x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>&le;R
</nobr>
&nbsp; in the limit R&rarr;&infin;.
The volume V<sub>brick</sub>
of a brick must be chosen (by scaling a fixed pre-specified brick shape)
in such a way that a brick whose raindrop point is chosen uniformly randomly within that ball,
has the <i>same</i> expected value of 
<nobr>
&delta;(dist(0, Q)<sup>2</sup>) 
</nobr>
&nbsp; &ndash;
where Q is a randomly chosen point in the brick,
0 is a fixed point in a different brick whose raindrop point is 
located at (0;0,0,0),
and &delta; is the
Dirac delta psuedofunction &ndash; as
the expected value of
<nobr>
&delta;(t<sup>2</sup>-x<sup>2</sup>-y<sup>2</sup>-z<sup>2</sup>)
</nobr>
where (t;x,y,z) is uniformly distributed within the ball.
</p>
???Actually this demand is automatically satisfied.  Any brick size.
-->

<p><b>What about the Higgs?</b>
In addition to the electromagnetic, strong, and weak forces (and gravity) 
with the recent discovery of the Higgs boson there now is another
kind of force, the "Higgs force."
So perhaps our picture also should have had more curves plotted 
to describe the coupling of this new force.
(Albeit in MSSM there would be not merely one, but actually
five kinds of Higgs!)
</p><ul>
<li>
For a Higgs-W-W vertex (Veltman's page 260),
<nobr>
coupling
=
<b>M<sup>2</sup>&#945;<sub>w</sub></b>
=
g<sup>2</sup>M<sup>2</sup>/(4&#960;).
</nobr>
where M is the rest mass of the W-boson.
</li><li>
For a Higgs-Z-Z vertex (see 3rd diagram from bottom on Veltman's page 260),
<nobr>
coupling
=
(M/c<sub>w</sub><sup>2</sup>)<sup>2</sup>&#945;<sub>w</sub>
=
g<sup>2</sup>M<sup>2</sup>/(4&#960;c<sub>w</sub><sup>4</sup>)
</nobr>
where M is the rest mass of the W-boson, or where M<sub>Z</sub>=M/c<sub>w</sub>,
hence this also may be written
<nobr>
coupling=<b>(M<sub>Z</sub>/c<sub>w</sub>)<sup>2</sup>&#945;<sub>w</sub></b>.
</nobr>
I am somewhat suspicious that Veltman had a typo and did not mean to square c<sub>w</sub>.
If so, then we would instead more simply conclude 
<nobr>
coupling=(M<sub>Z</sub>)<sup>2</sup>&#945;<sub>w</sub>.
</nobr>
</li><li>
For a Higgs-quark-quark vertex (Veltman's page 270)
<nobr>
coupling
=
<b>4<sup>-1</sup>(m<sub>quark</sub>/M)<sup>2</sup>&#945;<sub>w</sub></b>
=
(m<sub>quark</sub>/M)<sup>2</sup>g<sup>2</sup>/(16&#960;)
</nobr>
where
m<sub>quark</sub> is the rest mass of that particular kind of quark.
In the case of the top quark this is
<nobr>
coupling&#8776;<b>0.47&#945;<sub>w</sub></b>.
</nobr>
</li><li>
The Higgs boson self coupling (for a Higgs-Higgs-Higgs vertex, bottom of Veltman page 261)
<nobr>
coupling=9(m<sub>h</sub>/M)<sup>2</sup>g<sup>2</sup>/(16&#960;)
</nobr>
where m<sub>h</sub> is the rest mass of the Higgs boson, note
m<sub>h</sub>/M&#8776;1.56 so that this is
<nobr>
coupling&#8776;<b>5.5&#945;<sub>w</sub></b>.
</nobr>
</li></ul>
<p>
The dimensionless constant <nobr>&#945;<sub>Higgs</sub></nobr>
coupling the Higgs boson to the 
top-quark,
charged W<sup>±</sup>-boson,
and
neutral Z-boson 
(i.e. the list of standard-model particles whose rest masses necessarily
are largely or entirely due
to the Higgs mechanism), would seem best described by
</p><center>
1/&#945;<sub>Higgs</sub> = Km<sup>-2</sup>
</center><p>
where K is a suitable (particle-dependent) constant and m is the 
<i>rest</i> mass of the top, W, or Z, respectively.
<!--M[HiggsBoson] = 125.7(6) GeV.-->
(This dependence 
on squared mass m<sup>2</sup> is just like gravity, except 
that the Higgs coupling
depends only on the part of the <i>rest</i> mass attributable to
the Higgs mechanism, while gravity couples to <i>total</i> mass-energy.)
For the purpose of coupling top quarks, which seems the most relevant thing to compare
versus the other forces, and also for the Higgs self-coupling, the 
Higgs force coupling constant is then seen to be the <i>same</i> as the 
usual weak force coupling constant
to within a factor of 6.
The Higgs is different from the other three nongravitational forces:
</p><ol><li>
With the Higgs, there is not "one" coupling constant that is easily naturally defined since
electric charge is quantized and conserved (ditto for "color charge").  
Instead, we get a different coupling
constant for each particle (with a different Higgs-attributable rest mass) the Higgs couples to.
Rest mass, as least as far as is currently known, is <i>not</i> quantized and 
certainly is not conserved.  E.g. in an electron-positron annihilation, 
their rest masses vanish.
So if some "ultratop" quark were discovered in future,
having rest mass 1.45 times that of the top quark,
then its Higgs coupling constant would be about 1.45<sup>2</sup> times stronger than
for the top quark.  This ultratop quark then would have a Higgs coupling constant of
1.0&#945;<sub>w</sub>.
</li><li>
The "Higgs effect" depends inherently on the self-coupling among Higgs bosons and
the consequent fact that the ground state of the Higgs field is not zero,
but rather a uniform value v&#8776;246 GeV.  This is unprecedented and
means that the "Higgs force" is <i>not</i> just a direct consequence of coupling between
Higgs bosons and whatever is feeling the force.  
That in turn makes it a bit less clear how one should compare the Higgs force's
coupling versus coupling coefficients for other forces.
</li></ol>


<a name="dophys"></a>
<h3>14. How to do physics in the rain of bricks </h3>

<p>
One does not actually need the wave-equations of Dirac and Maxwell to do QED.
It suffices merely to know the "propagators" 
<!-- NOT the same as what we will call "Green's functions" --> 
of these equations.  These propagators can be regarded as fundamental and the wave-equations
as secondary, reversing the usual view.
Do that.
</p><p>
Although QED is usually done (via Feynman diagrams) in <i>momentum space</i> 
there is nothing stopping us, in principle, from doing everything in position space.
(The integrands arising from Feynman diagrams 
are rational functions in momentum space, but involve Dirac-delta pseudofunctions and "modified
Bessel" and trigonometric functions in position space.   That is why everybody 
almost always has preferred to work in
momentum space.  While it makes the analyst's life more pleasant to
have rational functions, this is in no way essential.)  Do that too.
</p><a name="post3"></a><p>
<b>Postulate IIIa (No two vertices in same brick):</b>
The vertices of any particular Feynman diagram must lie in <i>distinct</i> bricks.
In other words, no brick is allowed to contain more than one vertex of
any given Feynman diagram.
</p>
<blockquote>
Incidentally, one of Feynman's first ideas when he was trying to invent QED,
was to solve the electron self-energy and self-force problems by declaring the 
<i>nonexistence</i> of
electric and magnetic fields, and replacing them 
with equivalent <b>"action at a distance"</b> formulas
for the effects of point charges on each other, where 
only <i>distinct</i> charges were allowed to interact.
Wheeler &amp; Feynman 1945
actually succeeded in using this idea to resolve the power-law-infinite
self-energy and self-force problems
of <i>classical</i> electrodynamics, albeit only by going to a rather weird 
"half advanced, half retarded" action-at-a-distance formalism, in which the 
indefinitely-far-future could and did affect the past.   
Feynman and Wheeler also needed to assume (dubiously – indeed current cosmological
views about the far future indicate this is false) the eventual absorption
of all emitted radiation in the far future and at far distance.
Feynman initially hoped he would
be able to employ this same approach, but somehow made quantum instead of classical,
to construct QED.  However, neither he nor
Wheeler were able to do so
and both ultimately <i>abandoned</i> that whole idea.  
(Furthermore, electron self-interaction via the photon field
definitely <i>occurs</i> in QED and has been experimentally
verified e.g. by comparing renormalized QED calculations with Lamb shift measurements.)
Feynman's eventual construction of QED
instead was based on other ideas.  
It is possible to regard rain of bricks, via its "no two vertices in one brick" restriction,
as simply and successfully <b><i>implementing</i> 
Feynman's original "charges cannot act on themselves, 
only on others" vision;</b>
the role of the "action at a distance" retarded/advanced Green's function
is now played by propagators and the role of "charges" now
is played by "Feynman vertices"; and finally an analogue of their
"100% absorption" assumption arises automatically
as a trivial consequence of the rules of Feynman diagrams, hence no longer is
a "dubious assumption."
</blockquote>
<p><small>
<b>Remark.</b>
Postulate IIIa's "no two vertices in same brick" restriction has similarity to 
W.Pauli's 1925 
<a href="http://en.wikipedia.org/wiki/Pauli_exclusion_principle">exclusion principle</a>
that no two identical fermions may occupy the same quantum state simultaneously.
(Which is experimentally well-confirmed.)
As of May 2012 Wikipedia prefers the stronger alternative 
formulation "the total wave function for two 
identical fermions is anti-symmetric with respect to exchange of the particles."
If one wanted to take the latter sort of attitude here, one could 
proclaim that, e.g. multiphoton emission events <i>were</i> permitted in a single brick,
<i>but</i> if so they always acted to multiply
the amplitudes for the <i>n</i> histories arising from the 
<i>n</i> emission events by the <i>n</i>th 
roots of unity.  This would cause the effects of n-photon emission events always to cancel
out to zero probability if n&gt;1.   My point is that it should be very hard for
anybody who supports the Pauli exclusion principle to object to postulate IIIa.
</small></p><p>
It is also thinkable (much of this paper still seems to work, although one must argue harder)
to replace IIIa by the inequivalent 
</p><p>
<b>Alternative weaker Postulate IIIb (No Feynman edge from brick to itself):</b>
The two endpoints of any arc of any particular Feynman diagram must lie in <i>distinct</i> bricks.
(This now permits a brick to contain more than one Feynman vertex.)
</p><p>
My reasons for perhaps
prefering IIIb (even though it makes certain arguments more difficult) 
were both for reasons we'll discuss 
immediately after stating
<a href="#postU">postulate IV</a>, 
and also since it might be crucial to allow us to
formulate rain of bricks QFTs in Hamiltonian and Lagrangian manners (see
<a href="#opprobaction">open problem 7</a> at the 
<a href="#opensprobs">end</a>; one could also speculate oppositely
that IIIa suffices)
and just since it is weaker.
Unfortunately our series-convergence arguments in <a href="#convser">§38</a>
seem to rely on our original form <a href="#post3">IIIa</a> of the postulate,
and unless somehow strengthened would <i>disallow</i> IIIb and IIIc.
(Also, postulate IIIa would follow from the just-stated "<i>n</i>th root of unity" argument;
not so for IIIb and IIIc.)
So until such a strengthening occurs, I'll <b>prefer IIIa.</b>  
Of course IIIa implies IIIb and IIIc
as corollaries, but the reverse implications do not hold.
</p><p>
<b>Alternative even-weaker Postulate IIIc (No Feynman edge cycle within brick):</b>
No graph-theoretic <i>cycle</i> of arcs can have all vertices lying within the same brick.
(This now permits a brick to contain an arbitrarily large number of Feynman vertices
linked via a tree.)
</p><a name="rainbasedinteg"></a><p>
<b>Raindrop-based Monte Carlo integration:</b>
Now express QED<sub>N</sub>'s output-coefficients in terms of
integrals arising from 
N-loop Feynman diagrams as usual, <i>except</i> that "integrals over (1+3)-space"
[i.e. where each Feynman vertex is allowed to be located at any point of (1+3)-space]
now are <i>replaced</i> by 
<nobr>"(&#961;<sub>rain</sub>)<sup>-1</sup>×sums</nobr>
over all the raindrop points 
and 
<nobr>(V<sub>brick</sub>)<sup>-1</sup>×integrations</nobr>
over each brick"
[where the constant V<sub>brick</sub> is the (D+E)-volume of a brick]
–
i.e. each Feynman vertex is allowed to lie in any as-yet-unoccupied brick, and
be located at any point inside it.
The same formulas (based on propagators, which in turn are based on Lorentz-invariant
"squared distances") are used in the integrands as in old-style QED<sub>N</sub>,
except that we always employ the new rain-of-bricks distance formula in place of the old 
(Minkowski) one.
</p><a name="sheafview"></a><p>
<b>Worry:</b> this will slightly alter the form of those propagators, thus destroying
exact "probability conservation," i.e. <b>destroying unitarity.</b>
</p><a name="sheafview"></a><p>
I believe this worry can be overcome in the following manner.
The <b>"sheaf view"</b>
of the rain of bricks universe is as follows.
The sum over all Feynman diagrams can be viewed as a "sum over all possible histories"
of a physical process. 
With rain of bricks, each arc in a Feynman diagram 
(particle motion from A to B) can be regarded as taking place in a
<i>tilted</i> version of flat
Minkowski (3+1)-space (the tilt is into D+E extra dimensions beyond the 3+1 physical ones, 
if the bricks are D+E dimensional) 
using the true propagator formula within that tilted space.  
Now with space-only bricks (E=0), by
a preceding <a href="#tiltlemma">lemma</a>, any spacelike hyperplane within any such tilted
space orthogonally-projects
down into a hyperplane within the "true" Minkowski space which <i>also is spacelike.</i> 
Hence every Lorentz observer agrees that this propagation in the tilted Minkowski
space, preserves probability (i.e. the probability integrated over any spacelike
hyperplane subset of that Minkowski space, 
is the same as the integral over a parallel later spacelike hyperplane).
Hence each Feynman arc represents a propagation which perfectly preserves probability.  
Now one might object here that the "tilt" could distort the amount of probability by
multiplying it by some constant (<i>that</i> rescaled probability 
is preserved), and could also object that
each Feynman arc is happening in a different tilted Minkowski space.  I do not believe
those objections matter.  The fact the probabilities are rescaled by different constants
arising from different tilts
is merely equivalent to supposing that each Feynman-vertex emission/absorption
event had different values of &#945;.  That is ok: it is well known to be
<i>permissible</i> in QED to have 
any artificially imposed space-time-<i>varying</i> &#945; 
(<i>arbitrary</i> within some bounds, e.g. see Scharf 1995) without 
affecting probability conservation in the slightest; that is an entirely separate issue.
The fact each arc-propagation can be regarded as happening in a differently-tilted Minkowski
space from "the" Minkowski space, and many different tilts can be involved in a
single Feynman diagram, are not the rules of the universe that you previously
were used to using, but nevertheless seem to be acceptable rules of the universe.
The emission event at A can be regarded as emitting into all eligible different-tilted
Minkowski spaces simultaneously with different probabilistic weights (&#945; values)
for each (all these
spaces include the source point A) and with each such space including exactly two points at
which &#945; is nonzero (its source &amp; destination points).
The full set of all tilted Minkowski spaces arising from
all possible pairs of source and destination
points from two bricks is the "sheaf."  See picture.
It is rather like a sheaf of paper
(each individual sheet is one Minkowski space) except that sheets are allowed to depart
from being parallel ("tilts") and are allowed to "pass though one another without
noticing that they did."  (Particles can only change sheets when emitted or
absorbed; they propagate wholy within one sheet.)
</p><p><small>
The histories involving particles which miss their destination points and just continue propagating
forever within their Minkowski-sheet, are disregarded/disallowed, in the same way that
everybody already is used to: 
i.e. when considering sums over all possible histories, we only allow the
histories which satisfy the
right boundary conditions at the start – initial conditions – and 
at the end – measuring the output – of the game, so that we can 
calculate the the probabiities of different allowed outcomes of an experiment.
The "Feynman rules" for which diagrams are allowed and which are not, automatically do 
most of that for us: they already forbid particles emitted into space and 
never absorbed which are not part
of the "output particle" set.
</small></p>
<a name="FIG4sheafview">
<img alt="fig4" src="WarrenSmithQED131123_files/BrickSheaf.png" align="left" width="40%"></a>
<p><small>
<b>Alternative option of "wraparound propagators":</b>
We also remark that with D=1, E=0 "round bricks" the sheaf view suggests possible alternate
and inequivalent laws of physics, namely that each particle in propagating from A to B, 
could "wind around"
the circular brick any integer number n of times, as opposed to demanding n=0.  That is,
the <a href="#post2">fundamental distance formula (postulate II)</a> 
would now become infinitely <i>multivalued</i>.  
(And there would need to be additional summations to handle that.)
Actually, one also could consider such a modification for brick dimensionalities greater than 1,
by considering the full set of geodesics joining two points within the same brick.
</small></p><p>
</p><p><small>
<b>Alternative option of setup in de Sitter space:</b>
In <a href="#desit">§35</a> we shall consider setting up rain of bricks in 
de Sitter space rather than Minkowski space.  The "sheaf view" can still be formulated
using sheaf members that are de Sitter spaces...
</small></p><p>
</p><p>
I see nothing stopping
us from running QED in such a sheaf-universe.
The advantage of the sheaf-view is that it makes it manifest that the sheaf
is clearly built entirely out of old-style
(3+1)-dimensional QED components.
Therefore (unless some mistake was made in this construction) it clearly <b>loses zero
self-consistency and mathematical usability versus old-style QED.</b>
</p><p><small>
This is reminiscent of the way that the nonEuclidean geometries were
constructed entirely within Euclidean
geometry, thus demonstrating they both are at least as mathematically self-consistent.
</small></p><p>
In particular in the sheaf view we can regard QED as being "unitary," i.e. 
"conserving probability" to the same extent old-style QED can be regarded that way.
</p><p>
The disadvantage of the sheaf view is that it is rather annoying to use and to talk about,
compared to the normal "raindrops and bricks" view with simply an altered distance function
doing most of the work for us.
<!--
However we restore that by means of
</p><a name="post4"></a><p>
<b>&alpha;-Rescaling Postulate IV:</b>
The value of the coupling constant (&alpha;, in QED also called the "fine structure constant")
<i>depends</i> on the raindrop
in such a way that the for a photon hypothetically emitted
from that raindrop, its |propagator|<sup>2</sup> times that raindrop's &alpha;, summed over
all other raindrops, is <i>constant.</i>
NICE TRY BUT NOT NORMALIZABLE???
</p><p>
The goal of postulate IV is to rescale the raindrop "weights," i.e. &alpha; values, to 
compensate for raindrop-distribution nonuniformities in such a way that, in some oversimplified
sense, a photon emitted from a raindrop is "100% likely to be absorbed at another," 
but not 101% or 99%.
I.e. crudely speaking,
a raindrop which luckily has more than usual closer-than-usual neighbor raindrops, will 
require a smaller than usual rescaling constant.  
<a href="photprop">Note</a> that the 
photon |propagator|<sup>2</sup> is time-reversal symmetric, hence there
is no distinction, for the purpose of choosing the scaling factors in
postulate IV, between "emit" and "absorb."
--
Note that the concept of"emit" and "absorb" are debatable and depend on the observer,
e.g. one  Lorentz observer might claim that a particle is emitted from A and later absorbed at B,
while another with a different notion of "time"
could claim it was emitted from B and absorbed later at A (and it was the antiparticle).
This is a mere semantic issue which
does not matter because of the symmetry of the propagator formulas and
since we can regard the "destination" as possibly being
<i>any</i> raindrop, whether in
the past or future???
-->

</p><p>
<a name="limitequivthm"></a>
</p><p>
<b>Limit-equivalence of new and old (unrenormalized) QED:</b>
There is a limiting process in which 
<nobr>&#961;<sub>rain</sub>&#8594;&#8734;</nobr>
and in which the brick-size parameters go to 0+
(consequently the Planck length&#8594;0+)
which causes our new summation &amp; integration process
to yield the same results as the old QED integration process
(with probability=1) whenever the latter yields either a finite result
or a result which is any fixed complex number times +&#8734;.
(This is for any finite-order-QED result and without any QED renormalization.)
And those are the only two possibilities.
</p><p>
<b>Derivation:</b>
This is merely a statement of the limit-validity of "Monte Carlo integration" which
in turn is a trivial consequence of the "strong law of large numbers" in probability theory.
The reason these are the only two possibilities is because the QED integrals done
in position space are known to be equivalent to the momentum space versions, and those,
since they have <i>rational-function</i> 
integrands whose infinities (if any) are known to arise from
the large-|momentum| region only (i.e. <i>never</i>
are due to poles in momentum-space), cannot "diverge by oscillation"
– and must yield a limiting value (whether finite or infinite) of the sort stated
(and see Lowenstein &amp; Zimmermann 1975, Zimmermann 1968, etc
re that).
</p><p>
The proof would now be over, except for one worry.   Monte Carlo integration works for 
integrating <i>functions.</i>  QED propagators in position space can involve Dirac delta
<i>pseudo</i>functions and their (pseudo)derivatives.   
Monte Carlo integration  can deliver wrong values for
such integrals with probability 1.  
(To re-say the same thing in the language of probability theory: the strong law of large
numbers works for determining expectation values of probability distributions over the
<i>real numbers</i>. But when we have an integrand involving a Dirac delta function, the 
"probability distribution" corresponding to the integrand at a random point is
<i>not</i> necessarily over the real numbers, but over a larger set.)
Fortunately, 
</p><ol type="a"><li>
The delta functions that arise
are always 
<nobr>
&#948;(t<sup>2</sup>-x<sup>2</sup>-y<sup>2</sup>-z<sup>2</sup>)
</nobr>
or its derivative
<nobr>
&#948;'(t<sup>2</sup>-x<sup>2</sup>-y<sup>2</sup>-z<sup>2</sup>),
</nobr>
and the effect of a brick (thanks to our fundamental distance formula)
is merely to add offsets to the argument of those
delta functions, hence 
</li><li>
Expected values of integrals are unchanged.
</li><li>
The effect of integration <i>within</i> a brick ("brick smearing") is to
smooth out the delta pseudofunctions
to yield <i>genuine</i> functions (of raindrop position).
</li></ol>
And for <i>that</i>, Monte Carlo integration works.
<b>Q.E.D.</b>
<p></p><p>
In old-style Minkowski-space QED, 
we would now smash into the problem that many of these integrals
would yield the limiting value &#8734;.   However, since with the rain of bricks
we are <i>not</i> taking the limit as 
L<sub>pl</sub>&#8594;0+,
but rather keeping 
L<sub>pl</sub> <i>fixed</i>, we can hope to escape that.
</p><p>
<b>Finiteness Claim:</b>
In the rain of bricks,
these summation/integration processes
always (with probability=1 arising from the underlying Poisson randomness)
return <i>finite</i> answers for any finite-order (unrenormalized) QED result.
Hence we now can do QED at any finite order without needing to renormalize.
</p><p>
The main reasons
are the fixed nonzero brick size and consequent
"brick smearing" (which weakens or eliminates all short-distance propagator singularities)
and the "no two Feynman vertices on a single brick" demand.
<!--
More ingredients helpful for this claim:
A "UV cutoff" at the "Planck wavelength scale" exists somewhat automatically 
within the rain of bricks both due to fixed nonzero brick size and consequent
"brick smearing" and the "no two Feynman vertices on a single brick" demand;
and "Nyquist" band-limits; and the "tetrahedron lemma."
-->
Hence the UV infinities of QED will not occur, and no "renormalization" will be needed
to dodge them.  Full discussion of the reasoning underlying the 
finiteness claim will be deferred until
<a href="#byeinfin">near the end</a>
of this work, e.g. 
the brick-smearing <a href="#momspaceremark">momentum-space remark in §15</a>,
and in <a href="#byeinfin">§37</a>.
For the moment it is just an unsupported, but plausible, assertion.
</p><a name="infrarednote"></a><p>
<b>Note on infrared (IR) infinities:</b>
Some QED calculations also involve
"IR infinities" which occur at <i>large</i> wavelength (i.e. small momentum) scales.
However, it is generally agreed that these are not genuine.  
To make an oversimplified analogy, they are like worrying 
that the behavior of <nobr>(sinX)/X</nobr> or 
<nobr>X<sup>-2</sup>+(1-X<sup>-2</sup>)</nobr>
when X&#8594;0 might involve an infinity because of the 
"division by 0."  Really, this "singularity" is an illusion.  
(Albeit, despite being an illusion, it can cause many difficulties in 
numerical computational work!)
Similarly, all the so-called IR "infinities"
in QED also are illusions which really cancel out and 
vanish when appropriate formalisms 
are employed. (Several suitable calculational systems are known.) 
Only the UV infinities in QED are genuine problems that genuinely exist.
</p><p>
In any case we are (for the most part)
going to <i>assume</i> in this paper that QED or whatever other
QFT we are rain-of-bricks modifying, is infrared-finite in some suitable sense.
Rain of bricks mostly seems to have no effect, good or ill, on infrared infinities,
so such a precondition on whatever QFTs rain of bricks is to be applied to, is needed.
The questions of whether this precondition is true in any given QFT, and to what extent 
that can be rigorously demonstrated, are then separate ones that we offload to other authors
of other papers.
</p><p><small>
<b>More careful look at IR infinities:</b>
The "Bloch-Nordsieck theorem" asserts that physically-measurable cross sections
in QED do not exhibit IR infinities.  
The infinities arise because processes such as Bremstrahlung can emit unboundedly large numbers
of unmeasurably "soft" photons,
e.g. consider an infinite set of photons with energy K<sup>-2</sup> for the Kth
photon.  The probability your detector will activate (which can be expressed as a 
certain sum over all those photons), though, remains finite.  The theorem roughly
claims that if such sums over soft photons in output states are performed,
then QED yields finite answers for cross sections.
This (as usual in physics) actually is <i>not</i> a "theorem" as yet, in the sense that the 
paper examining it the most deeply – Yennie, Frautschi, Suura 1961 –
admits their treatment is not fully rigorous, although they claim it should suffice 
to remove "reasonable doubts."
This is discussed in ch.16 and supplement S4 of Jauch &amp; Rohrlich and pp.148-161 of Nash, and
see also the papers of Lowenstein.
For worked examples illustrating the mechanics of doing some summations, see 
§6.5 of Peskin &amp; Schroeder, where it also is claimed that the IR infinities
in QED<sub>N</sub> all are like |log&#956;|<sup>N</sup> or less severe, 
where &#956; is an artificial photon mass.
This topic is also discussed in §13.3 and 13.4 of Weinberg 
<!--sec13.3 "Real soft photons;
cancellation of divergences" and sec13.4 "General infrared
divergences" addresses.
Says the IR divergence problem is solved by agreeing to consider only
the right kinds of states, corresponding to stuff that actually is
measurable.
-->
and in Weinberg 1965 where he claims IR infinities also don't really occur for gravitons.
It has also been claimed that doing QED in the "Fried-Yennie gauge" 
(Tomozawa 1980; this has also been claimed for "Coulomb gauge")
or using the calculational system advocated by Grammer &amp; Yennie 1973,
or doing QED with a positive "photon mass" &#956; (then at the end of the calculation 
taking the limit as &#956;&#8594;0+) prevents any artificial IR infinities from
arising.   
Bach, Fröhlich, Pizzo 2007
and
Chen, Fröhlich, Pizzo 2010
claim some rigorous results about avoiding infrared infinities in
"<i>non-relativistic</i> QED."
If QED is done within a finite size "cavity" then IR infinities 
obviously are impossible since
photons with wavelength greater than the size of the cavity cannot exist.
Again if we after the end of the calculation take the limit as cavity size&#8594;&#8734;
no artificial IR infinities will appear.
</small></p><p><small>
The Bloch-Nordsieck theorem is false in QCD, according to a counterexample by
Doria, Frenkel and Taylor 1980.   However, it is generally believed that QCD also is free of
IR infinities (order by order in perturbation theory)
provided summing over soft gluons is done in <i>both</i> the
incoming and outgoing states.  This is discussed in ch.6 of Muta 1998 and 
ch.13 of Sterman 1993.  
The Doria counterexample is repaired (as several authors cited by Muta
found 1 year later)
if the incoming quarks are coupled to soft gluons.
(And there are presumed to be no free quarks in which case the whole
"counterexample" was unphysical anyhow.  See
Nishijima 1996 for an argument "color confinement" in QCD is forced by "asymptotic freedom" and 
"BRS symmetry" – a free quark or gluon would imply a violation of BRS symmetry.)
But QCD's infrared finiteness has only been partially proved;
both Muta and Sterman discuss the "Kinoshita Poggio Quinn theorem"
and "Kinoshita Lee Nauenberg theorem" which apply to <i>massless</i> QCD
and QED (and a wide class of massless renormalizable QFTs) to prove IR finiteness for
Green's functions and transition rates, respectively.  It is generally believed that
QFTs without massless particles cannot exhibit IR infinities, and the fewer
massless particles we have the better things become; this belief in combination
with these theorems would totally settle the matter.
</small></p><p><small>
We will consider in 
<a href="#desit">§35</a>
the probable possibility that QFTs set in <i>de Sitter</i> rather than Minkowski
spacetime, can avoid IR infinities because de Sitter space contains a large observer-independent
length scale, the "distance to the horizon."   That, if it worked, would provide a general
purpose way to get rid of these infinities, just like rain of bricks' 
observer-independent short length scale gets rid of the UV infinities.
</small></p>

<p></p><a name="postU"></a><p>
<b>Postulate IV (Uniformity*):</b>
Quantum fields are uniform within each brick.
</p><p>
The reason for the asterisk is that I believe this postulate is <b>not really needed</b>
because it arises automatically
as a <i>consequence</i> of the other postulates.  To understand why that might be, 
consider a nonuniform quantum field within a brick. One can use Fourier 
modes within each brick as a basis and our postulate is saying the only mode 
that is needed is the trivial
zero-frequency constant-valued mode.  Now this nonuniform quantum field, 
after propagation into some other brick, will tend to get "smeared out" by convolution of
the propagator and the original field.  
<!--This by the Fourier-series convolution theorem
will have the effect of multiplying the Fourier series of the original field and the 
propagator-->
Because our bricks are parallelipipeds with periodic boundary conditions –
or spherical surfaces (in the "round bricks" alternative) – 
every point of a brick is equivalent to every other.
This smearing will tend inexorably to make the fields more uniform and average-out all the
other Fourier modes to zero exponentially.   Once for this or some 
other reason all quantum fields in all bricks became uniform to 100 decimal places, 
<i>it would be impossible</i>
for nonuniformity ever to arise.  In other words, uniformity, once achieved, obviously
is permanent.
</p><p>
The uniformity postulate
also is not really needed in the sense that one could still do physics without it (or try).
</p><p>
As long as we are in the business of <b>abolishing redundant postulates</b>, 
I also point out that
it arguably is possible to set up closely-related rules
in which <a href="https://dl.dropboxusercontent.com/u/3507527/post3">postulate IIIb</a> also is not needed since <i>it</i> is
implied by the others (including 
<a href="#postU">postulate IV</a> which as we saw also is abolishable)! 
Namely, suppose we postulated (contradictorily)
that propagation (Feynman diagram arcs)
occurs <i>both</i> through Minkowski (1+3)-space <i>and</i>
within bricks, but emission and absorption (Feynman vertex)
events as usual occur only within bricks.  
Further assume that bricks are all-timelike, 
i.e. E&gt;0 and D=0.
Using the fact that all known propagator formulas are oscillatory
and with amplitude nonincreasing with time 
(Zhang et al 2010), if we choose the brick size and shape correctly 
relative to the mass of the particle being propagated
we can make the propagator within a brick average to <i>zero</i> 
–
and then, using 
<a href="#postU">postulate IV</a>, 
conclude that this averaging actually happens.
The zero then implies postulate IIIb.
If we accept this then actually <b><i>neither</i> 
<a href="#postU">postulate IV</a> nor 
<a href="https://dl.dropboxusercontent.com/u/3507527/post3">IIIb</a> are needed</b>
since they are implied automatically (in that order).
</p><p><small>
Note that this abolition plan might only allow (or instead might <i>exclude</i>!)
a certain discrete spectrum of allowed particle masses
for any given brick size – forcing some sort of diophantine relationship
between the electron and quark masses etc – it depends on the brick dimension and
precisely what notions of "averaging" and 
"amplitude summation" we are willing to accept.
However, the spectrum would be sufficiently fine-grained
that at least 16-figure precision in mass measurements would seem needed to
reveal it (for currently-known particles), and with a discrete set of
<i>excluded</i> masses
I see no way to detect it even with enormous precision.
</small>
</p><p>
It also is conceivable – albeit it is unclear to me that this works –
that a space-only brick might also be usable (perhaps this only would work
for a 1-dimensional space-only brick)
to imply (and hence abolish the need for) 
<a href="https://dl.dropboxusercontent.com/u/3507527/post3">postulate IIIb</a> or 
<a href="https://dl.dropboxusercontent.com/u/3507527/post3">IIIc</a>, due to another sort of cancelation
reminiscent of <a href="#furrythm">Furry's theorem</a> 
that a QED Feynman diagram featuring an electron loop
containing an odd number of vertices, exactly cancels the same diagram
but with that loop oppositely oriented.  For a 1-dimensional (circle) brick containing a 
Feynman diagram cycle going around the circle clockwise we would either
get such cancellation with the same diagram with opposite-oriented cycle, or not.
And in the latter case, the uniformity postulate
<a href="#postU">IV</a> would prevent the loop or cause all such loops to 
average out to have zero effect.   
(End of speculations about abolishing redundant postulates; 
now return to main exposition.)
<!--
</p><p>
Another idea???:
Instead regard the arena with (say) E-dimensional (time-only) bricks
as (1+[3+E])-dimensional Minkowski spacetime with the E extra dimensions being closed
microscopically.
The bricks are then embedded inside this arena.   Now postulate that propagation is
via (1+[3+E])-dimensional propagators, which look (1+3)-dimensional to observers with
imprecise vision.  As usual all the interaction takes place on bricks and
the rest of the space is merely used for free-particle propagation.  
Now any attempt to propagate from a brick to itself would automatically <i>cancel out</i>
because all known propagator formulas
are oscillatory and with amplitude decreasing with time 
so that integration over a (time-periodic) brick 
to create a "brick smeared" propagator would
effectively integrate an infinite number of such oscillations when used within a single brick
corresponding to an infinite sum of oscillating-sign and decreasing-amplitude terms
(hence presumably convergent).
It might be necessary to choose the brick size &amp; shape parameters
exactly correctly to make that convergence be to
<i>zero</i> to make this cancellation work &ndash; and if there are enough
particle types this should
force certain relationships among the particle masses; they would no longer
be arbitrarily independently variable real numbers.
-->
</p><p>
<b>The tremendous advantage</b> of the uniformity postulate (or theorem, if it is one)
it that with it, it becomes tremendously <b>easier</b> to do physics because integrating 
over bricks becomes <i>trivial</i>.
Indeed, we can now essentially  pretend the <i>bricks</i> do not exist – the only thing
that we need to worry about are the raindrop <i>points</i> provided we replace the
"propagator" formulas by new 
<b>"brick-smeared" propagator</b> formulas,
got by integrating the old formulas over the 
source and destination bricks.
The brick-smeared propagators are functions of the source and destination 
raindrop coordinates t,x,y,z.
Actually, one can simplify further: 
the propagators can be regarded as a function of the <i>difference</i>
between the source and destination raindrop point coordinates, and due to symmetry (all points
of a brick are equivalent) the smearing integration over
both bricks, can be replaced by an integration over only one of those bricks, times the volume
of the other.
</p><p>
We'll discuss propagator formulas next section.  
</p>

<!--
<a name="kkview"></a>
<h3>X. Kaluza-Kleiny (almost)Equivalent View/Version of Rain of Bricks </h3>

<p>
There is another way to view rain of bricks.   Consider the Cartesian product of
(3+1)-dimensional Minkowski space, with a (D+E)-dimensional brick.
This is a (3+1+D+E)-dimensional manifold,
D+E of whose dimensions are microscopic.
Call this the <b>&Uuml;berspace.</b>
</p><p>
In the "conventional" view  of rain of bricks physics, 
"propagation" occurs in Minkowski (1+3)-space while "interaction" occurs on the bricks,
which do not (except for  single point each) lie in Minkowski space.
But we can also view propagation as through
&uuml;berspace, and the
bricks as lying inside that &uuml;berspace.
The &uuml;berspace appears to an observer with imperfectly-acute vision, to <i>be</i>
Minkowski (1+3)-space.
</p><p>
<b>Advantages</b> of the &uuml;berspace view:
</p>
<ol><li>
The &uuml;berspace is nice and symmetric.
</li><li>
The distance formula arises naturally.
</li><li>
We can make the propagators be the natural Dirac (or Maxwell, or whatever) propagators
in (1+E<b>+</b>3+D)-dimensional space 
(with appropriate "wraparound summation")
which exactly solve the Dirac (or whatever) equations in that space.
By "wraparound" I mean that there are for a parallelipiped brick many different geodesics
from A to B, i.e. distance is really a multivalued function,
hence one needs to sum the propagators based on all of those distances.
Another advantage of round bricks over parallelipipeds is that the geodesic arc from A to B is 
<i>unique</i> but there still is multivaluedness arising from traveling N times around that
unique circle.
<br> &nbsp;&nbsp;&nbsp;
This leads easily to relativistic invariance and 
energy and probability <b>conservation</b>
claims.  But in the conventional rain of bricks, the propagator with "brick smeared" source
is <i>not</i> an exact solution of the (1+3)-dimensional Dirac (or whatever) equation, and
so such claims become more difficult, weakened, and/or actually wrong.
The &uuml;berspace propagators, although higher-dimensional, thanks to the wraparound 
and effects of the squares in the distance formulas
become in the limit of macroscopic distances the same as the ordinary
(1+3)-dimensional propagators.
That is because the wraparound "smears them out" over the brick and hence
makes them behave uniformly, i.e. independently of
the values of the E+D microscopic coordinates,
in the limit.  Once this uniformity is attained, they <i>must</i> solve the
ordinary (1+3)-dimensional Dirac, Maxwell (or whatever) equations.
<br> &nbsp;&nbsp;&nbsp;
Note also that this would seem to impose further
limits on brick dimensionality, since e.g. we need for a 4-component
Dirac equation to <i>exist</i> on a 
(3+1+D+E)-dimensional manifold, and since it is simply 
<a href="http://en.wikipedia.org/wiki/Gamma_matrices">impossible???</a> for
more than ??? 4&times;4 Dirac matrices to exist
CITE???, bricks cannot be more than ??? dimensional.
Maxwell???
</li></ol><p>
Note that the propagator formulas slightly differ in the 
&uuml;berspace and conventional rain of bricks (although they become the same at
macroscopic distances) so the two versions of the theory are slightly <i>inequivalent</i>.
The <b>disadvantages</b> of the &uuml;berspace view ???
</p>
<ol><li>
It is higher dimensional.  It is simpler to regard things as lower dimensional.
</li><li>
But thanks to <a href="#postU">postulate IV</a>
and integration over bricks in Feynman diagrams
(i.e. we have propagation from a source point within a brick, but only use this <i>integrated</i>
over all possible locations of that source point with uniform source-amplitudes),
we effectively get a propagator
uniform <i>everywhere</i> with respect to the
(E+D) microscopic coordinates even at microscopic distances.  Due to this uniformity we can 
<i>regard</i> everything as really being only (1+3)-dimensional. Hence 
&uuml;berspace rain of bricks physics with 
<a href="#postU">postulate IV</a>
is <i>exactly</i> describable
in a conventional (1+3)-dimensional setting just using 
slightly different (albeit same in the macroscopic limit)
propagator formulas which arise via (D+E)-dimensional integration of the (1+E+3+D)-dimensional
wrapped propagators.
</li><li>
Hence even with the 
&uuml;berspace view, we can still get all the simplicity advantages of the conventional 
lower-dimensional view (just with more complicated formulas defining the propagators).
</li><li>
A more bothering feature is that our bricks are regarded as having
fixed (x,y,z;t) coordinates.  In other words, regarding the extra microscopic
dimensions as perpendicular
to the usual ("horizontal") ones, 
we are demanding our bricks always be "exactly vertically oriented."
This seems a clear violation of higher-dimensional Lorentz invariance.
This simply must be accepted.
</li></ol><p>
What about the <b>bogeyman</b> that, with Kaluza-Klein, we might get &asymp;Planck-mass
particles got by N wavelengths "orbiting around inside a brick" and trapped
forever?    In other words, why is uniformity ala 
<a href="#postU">postulate IV</a>, going to happen,
while other nonconstant Fourier modes within bricks will be forbidden?
It seems to me that the uniform constant Fourier mode within a brick is the uniquely favored 
<a href="http://en.wikipedia.org/wiki/Haar_measure">Haar measure</a>.
Only it has nonzero integral.  One could imagine, then, that 
any other Fourier modes would be irrelevant if we take the view that a macroscopic
observer can only see <i>integrals</i> over the microscopic dimensions.
This gives us some basis for simply insisting as 
<a href="#postU">postulate IV</a>
(if it is felt necessary):
those other
modes are not allowed to exist.
</p><p>
And what about the other bogeyman that if we go to <i>higher</i> dimensions, QED infinities
become <i>worse</i> behaved?  The fact that, as we just indicated,
our "higher" dimensional &uuml;berspace view
is equivalent to a (1+3)-dimensional view would seem to eliminate that bogeyman too.
</p><p>
<b>An example of integration:</b>
To give an example in an oversimplified scenario of what is going on when converting the
&uuml;berspace to the conventional lower-dimensional view,
consider 2- and 3-dimensional classical electrostatics.  The potential from a point charge
in 3D behaves like 
r<sup>-1</sup> 
whereas in 2D it behaves like 
C-log(r)
for some constant C.
Regard 2D space as "really" having a microscopically-wrapped third dimension with
circumference=L.  Then the 3D potential would after wrapping and integration become
</p><center>
lim<sub>N&rarr;&infin;</sub>
&sum;<sub>-N&lt;n&lt;+N</sub>
&int;<sub>|u|&lt;L/2</sub> 
(r<sup>2</sup>+[u+nL]<sup>2</sup>)<sup>-1/2</sup>du
</center>
<center>
= 
lim<sub>N&rarr;&infin;</sub>
&sum;<sub>-N&lt;n&lt;+N</sub>
<big>{</big>
ln( ([nL+L/2]<sup>2</sup>+r<sup>2</sup>)<sup>1/2</sup> + nL+L/2 )
-
ln( ([nL-L/2]<sup>2</sup>+r<sup>2</sup>)<sup>1/2</sup> + nL-L/2 )
<big>}</big>
</center>
<p>
The sum telescopes so that we get
</p><center>
= 
lim<sub>N&rarr;&infin;</sub>
<big>{</big>
ln( ([NL+L/2]<sup>2</sup>+r<sup>2</sup>)<sup>1/2</sup>+NL+L/2 )
-
ln( ([NL+L/2]<sup>2</sup>+r<sup>2</sup>)<sup>1/2</sup>-NL-L/2 )
<big>}</big>
</center>
<center>
= 
-ln(r) +
2 lim<sub>N&rarr;&infin;</sub>
ln( ([NL+L/2]<sup>2</sup>+r<sup>2</sup>)<sup>1/2</sup>+NL+L/2 )
</center>
<p>
We have thus derived the exact 2D potential, provided we don't worry about the
constant C.   As N&rarr;&infin; the limit loses its r-dependency, thus indeed
becoming a constant within additive error of order &plusmn;L/N;
the only issue is that the value of this constant tends logarithmically to infinity when
N&rarr;&infin;.  However, if we regard electrostatic potentials as
only <i>defined</i> up to additive constants, this does not matter.
</p>
<p>
Note that in this example we actually got 2D electrostatics <i>exactly</i> &ndash;
we did not merely get an approximate result exact only in the macroscopic 
(r&gt;&gt;L) limit.  Such "bonus exactness"
can happen in rain of bricks too
(under the right circumstances with the right rain of bricks versions), but 
won't always.
</p><p>
<b>Is this "string theory"?</b>
No.   String theory replaces particles with nonrigid extended objects.
Rain of bricks particles, even in the &uuml;berspace picture, still are points.
They only <i>interact</i> on certain extended (but zero-measure) objects, i.e. the "bricks."
Bricks are rigid.
This may not immediately 
sound like a huge difference, but 10 seconds later you realize it is.
Particles that vibrate in an infinite number of ways are far more complicated than points.
Rain of bricks is <i>vastly</i> simpler than string theory.
</p>
-->

<a name="propformulas"></a>
<h3>15. Propagator Formulas (and their "brick smeared" forms)</h3>
<blockquote>
Thirty-one years ago [1949], Dick Feynman told me about his "sum over histories" 
version of quantum mechanics. "The electron does anything it likes," he said. "It 
just goes in any direction at any speed, forward or backward in time, however it likes, 
and then you add up the amplitudes and it gives you the wave-function." 
I said to him, "You're crazy." 
But he wasn't.
<br>&nbsp;&nbsp;<b>–</b> 
Freeman J. Dyson, 1983.
</blockquote>
<p>
We will write
</p><center>
s<sup>2</sup> = t<sup>2</sup> - x<sup>2</sup> - y<sup>2</sup> - z<sup>2</sup>.
</center><p>
It is best to regard propagators as functions of s<sup>2</sup> rather than (say) s,
because s<sup>2</sup> takes <i>real</i>
values everywhere between -&#8734; and +&#8734;.
We shall use units with c=1 and &#8463;=1.
</p><p>
The Feynman propagator in momentum space for the <b>Klein-Gordon</b> 
<a href="http://en.wikipedia.org/wiki/Klein%E2%80%93Gordon_equation">equation</a>
</p><center>
&#934;<sub>tt</sub>
- &#934;<sub>xx</sub>
- &#934;<sub>yy</sub>
- &#934;<sub>zz</sub>
+ m<sup>2</sup>&#934; 
= 0
</center><p>
(where the subscripts denote partial derivatives) is
</p><center>
<u>G</u><sub>Feyn</sub> =
lim<sub>&#949;&#8594;0+</sub>
[E<sup>2</sup> -
(p<sub>x</sub>)<sup>2</sup> - (p<sub>y</sub>)<sup>2</sup> - (p<sub>z</sub>)<sup>2</sup>
- m<sup>2</sup> + i&#949;]<sup>-1</sup>
</center><p>
where m is the particle mass and the energy-momentum special relativistic 4-vector is
(E;p<sub>x</sub>,p<sub>y</sub>,p<sub>z</sub>).
This arises immediately by simply rewriting the Klein-Gordon equation 
[but with right hand side not 0 but rather &#948;<sup>1,3</sup>(t;x,y,z)]
in momentum space, where it
no longer is a partial differential equation but now only is a polynomial equation,
then solving it. (Peskin &amp; Schroeder EQ 2.59.)
We use the underline to denote "in momentum space."
This may be converted to position space by integrating it times
<nobr>
(2&#960;)<sup>-4</sup>exp(-itE+ixp<sub>x</sub>+iyp<sub>y</sub>+izp<sub>z</sub>)d<sup>4</sup>p.
</nobr>
<!--Greiner EQ2 sec2.5 p67 -->
The three 
p<sub>x</sub>, p<sub>y</sub>, p<sub>z</sub>
integrals may be done with the aid of the 
</p><p>
<b>Hankel transform theorem</b>
(Stein &amp; Weiss theorem 3.3 p.155)
that the Fourier transform of a 
function of radius, also is a function of radius:
</p><center>
<u>f</u>(p) = &#8747;exp(-ip·x)f(x)d<sup>n</sup>x, &nbsp;
f(x) = (2&#960;)<sup>-n</sup>&#8747;exp(ix·p)<u>f</u>(p)d<sup>n</sup>p;
</center>
<center>
If &nbsp; f(x)=F(r), &nbsp; <u>f</u>(p)=<u>F</u>(q), &nbsp; where q=|p| and r=|x|, &nbsp; then
</center>
<center>
<u>F</u>(q) = 
q<sup>1 - n/2</sup>
(2&#960;)<sup>n/2</sup> 
&#8747;<sub>0&lt;r</sub> J<sub>n/2-1</sub>(qr)r<sup>n/2</sup>F(r) dr,
</center>
<center>
F(r) = 
r<sup>1 - n/2</sup>
(2&#960;)<sup>-n/2</sup> 
&#8747;<sub>0&lt;q</sub> J<sub>n/2-1</sub>(qr)q<sup>n/2</sup><u>F</u>(q) dq.
</center>
<p>
Here J<sub>k</sub>(z) denotes a Bessel function of order k.
<!-- thm3 from http://math.arizona.edu/~faris/methodsweb/hankel.pdf -->
<!-- MAPLE:
KGp := (E^2 - p^2 - m^2 + I*eps)^(-1);
BesselJ(1/2, p*r) * p^(3/2) * KGp / (2*Pi)^(3/2) * r^(-1/2);
int(%, p=0..infinity );
int(%/(2*Pi) * exp(I*t*E), E = -infinity .. infinity );
-->
</p><p>
The remaining E integration can be done after expanding in a series in powers of 
m<sup>2</sup>;
a slicker approach involves a hyperbolic change of integration variable.
Finally we take the limit as &#949;&#8594;0+.
The result 
(Greiner  &amp; Reinhardt EQ26 in §2.5, modified using Abramowitz &amp; Stegun 9.6.4b)
<!--Abramowitz+Stegun 9.6.4b is used to have K1 not H1(2).
Huang's expression on p30 of
Kerson Huang: Quantum Field Theory: From Operators to Path Integrals (J. Wiley 1998),
is recapitulated by
http://en.wikipedia.org/wiki/Propagator_%28Quantum_Theory%29#Feynman_propagator
and it appears to differ from ours and Greiner/Reinhardt's.
Timelike: When s=sqrt(5.5):
8*Pi*Huang:     HankelH1(1,sqrt(5.5))/sqrt(5.5) = 0.2265926528 + 0.03172610325*I
8*Pi*Greiner:   HankelH2(1,sqrt(5.5))/sqrt(5.5) = 0.2265926528 - 0.03172610325*I   note -
8*Pi*Me:        -(2/Pi)*BesselK(1, I*sqrt(5.5))/sqrt(5.5) = 0.2265926527 - 0.03172610325*I
8*Pi*ZhangEQ24: (8*Pi)*(-I)^3 / 2^(6/2) / Pi^(2/2) / sqrt(5.5) * HankelH2(1, sqrt(5.5)) =  
                                                       0.03172610324 + 0.2265926528*I
so Huang=ComplexConjgt(me) and Greiner=Me and Zhang=I*Greiner=I*me if s>0 is real,
Note Huang's expression is an analytic function of s since he conjugates.

Spacelike: s=sqrt(-5.5) we have
4*Pi^2*Huang:   -I/sqrt(5.5) * BesselK(1, sqrt(5.5))          = -0.03824942357*I
4*Pi^2*Me:       1/sqrt(-5.5) * BesselK(1, -I*sqrt(-5.5))     = -0.03824942357*I
4*Pi^2*Greiner:  (Pi/2)/sqrt(5.5) * HankelH2(1, -I*sqrt(5.5)) = -0.03824942357*I
4*Pi^2*ZhangEQ25:
(4*Pi^2) / (2*Pi)^(4/2) / sqrt(5.5) * BesselK(1, sqrt(5.5)) =  0.03824942357
and we all agree except for Zhang=I*us.

Zhang combined (D+1)-diml expression on p4 EQ27.  Zhang says he agrees with Greiner+R when D=3. 
Zhang says he agrees with other authors in D=2 and D=1.
Zhang's D=3 combined is EQ28, and when I compare with Greiner EQ26 p72, I find agree
for timelike, but for spacelike I find Zhang=I*me=I*Greiner but disagrees with me:
GreinerEQ26: I/(8*Pi*sqrt(5.5)) * HankelH2(1, -I*sqrt(5.5))  = -0.0009688692172*I
Me:          1/sqrt(-5.5) / (2*Pi)^2 * BesselK(1, -I*sqrt(-5.5)) =  -0.0009688692170*I
ZhangEQ28:   1/(4*Pi*Pi*sqrt(5.5)) * BesselK(1, sqrt(5.5))   = 0.0009688692170

For lightlike   
ZhangEQ26:    -1/(4*Pi^2 * s^2)
GreinerEQ27:  I/(4*Pi^2 * s^2) = me.
disagreeing:  Zhang=Greiner*I.

On analytic formulas of Feynman propagators in position space
Zhang Hong-Hao et al 2010 Chinese Phys. C 34 (2010) 1576
claims to correct an error.
-->
is
</p><a name="FeynMinkKGprop"></a><center>
G<sub>Feyn</sub>(t;x,y,z)
= 
-(4&#960;)<sup>-1</sup> &#948;(s<sup>2</sup>) 
- sign(s<sup>2</sup>)(m/s)(2&#960;)<sup>-2</sup> K<sub>1</sub>(sign(s<sup>2</sup>)ims)  
<!--checked!-->
</center><p>
where K<sub>1</sub>(z) denotes a modified Bessel function of order 1
(defined in the complex z-plane slit along the negative real axis,
see Abramowitz &amp; Stegun ch.9; we shall use 
positive real s for timelike and positive imaginary s for spacelike separations) 
and &#948; is the 
Dirac delta pseudofunction.
<!-- Hankel,Bessel, modified Bessel:
Hv^(1) = Jv + i Yv.   AS9.1.3.
K0(z) = i pi/2 H0^(1) (z)    AS9.6.4a.
K1(z)   = -pi/2 * H1^(2) (-i*z)       AS9.6.4b.  Valid if z=real+ or imag+.
K1(i*z) = -pi/2 * H1^(2) (z)     also AS9.6.4b.  Valid if z=imag+ or real+.
--MAPLE:
I verified AS9.6.4b:
FF := (z) ->  BesselK(1,z) + Pi/2 * HankelH2(1, -I*z);
FF := (z) ->  BesselK(1,I*z) + Pi/2 * HankelH2(1, z);
evalf(FF(5.5));  #0
evalf(FF(5.5*I));  #0
It is important that BesselK(1, x)  if x real be for POSITIVE x.  
Get expl decrease that way not growth.
Greiner EQ26 of sect2.5 always has the argument of the HankelH2 either +real or -imag.
Their argument is sign(s^2)*m*s.
Hence the argument of BesselK is +imag or +real.
FKG := (s,m) -> -sign(s*s) * (m/s) * (2*Pi)^(-2) * BesselK(1, sign(s*s)*I*m*s);   #checked.
#verify behaves ok when s large and +real:
evalf(FKG(999999.9, 1));  #get complex of size 10^(-10)
#verify behaves ok when s large and +imag:
evalf(FKG(59.9*I, 1));  #get 6.6*10^(-31) * I
-->
Note to use this as a "propagator" it should be regarded as a function
of the <i>difference</i> (t;x,y,z) between the destination and source points.
<!--
K_1 the order 1 becomes D/2-1
(m/s)^1 the 1 becomes (D-2)/8
(D-4)/2 more powers of pi on the bottom
factors of i float around
-->
</p><a name="euclideanpot"></a><p>
<small>
<b>The Euclideanized problem:</b>
Had all 4 dimensions been <i>spacelike</i>, the Klein-Gordon equation would
instead have been the simpler
<a href="http://en.wikipedia.org/wiki/Helmholtz_equation">Helmholtz equation</a>,
and then the role of the propagator would have been played by the 
Helmholtz "Green's function"
which is
<br>
G(x)=(2m)<sup>-1</sup>exp(-m|x|)
in 1 space dimension,
<br>
G(x)=(2&#960;)<sup>-1</sup>K<sub>0</sub>(m|x|)
<!-- =(i/4)H<sub>0</sub><sup>(1)</sup>(im|x|) -->
in 2 space dimensions,
<br>
G(x)=exp(-m|x|)/(4&#960;|x|)
in 3 space dimensions ("Yukawa potential"),
and finally
<br>
G(x)=i (2&#960;)<sup>-2</sup> |x|<sup>-1</sup>K<sub>1</sub>(m|x|) 
<!--
G(x)=-i/(8&pi;) ?? |x|<sup>-1</sup>H<sub>1</sub><sup>(1)</sup>(im|x|) -->
in the case where x is a 4-dimensional vector.
<br>
All of these fall off with large |x| 
asymptotically proportionally to
<nobr>|x|<sup>(1-d)/2</sup>exp(-m|x|)</nobr>
in d dimensions; 
for small |x| they behave asymptotically proportionally to |x|<sup>(1-d)/2</sup>.
</small>
</p><p>
G<sub>Feyn</sub> is a rather <b>badly-behaved</b> object.  
It isn't a function, it is a pseudofunction,
aka "distribution," thanks to the Dirac delta term on the light cone s<sup>2</sup>=0.
Furthermore, even the remaining term is still nasty:
</p><ul><li>
It is a <b>nonanalytic</b>
function of s because of the use of
sign(s<sup>2</sup>). 
[No matter how many QFT textbooks erroneously give formulas
analytic in s! –
such as when Birrell &amp; Davies EQ2.77 claim to generalize this result
to n dimensions.]  It could, however, correctly
be regarded as (a) an analytic function of the
<i>complex conjugate</i> of s, or (b)
<i>two</i> analytic functions, 
with one used when s is imaginary and the other when s is real.
This nonanalyticity does not particularly matter since we only use
positive real or positive imaginary s, but it would matter if one tried to employ reasoning
involving excursions off these axes into the complex s-plane.
For the reader who is wondering how such nonanalyticity can arise, 
I suggest considering simpler (since only 1-dimensional) integrals such as
<center>
&#8747;<sub>-&#8734;&lt;p&lt;+&#8734;</sub>[p<sup>2</sup>+a<sup>2</sup>]<sup>-1</sup>exp(ipx)dp
=
(&#960;/a)exp(-a|x|).
</center>
Note this is not an analytic function of x, but can be regarded as 
two analytic functions,
with one used when x&gt;0 and the other when x&lt;0.
</li><li>
As we approach
the light cone (s<sup>2</sup>&#8594;0)
it approaches infinity
both because of the s<sup>-1</sup> multiplying 
the modified Bessel function, and
also because
<p></p><center>
K<sub>1</sub>(z) = z<sup>-1</sup> + (1/2)zlnz + (2&#947;-1-ln4)z/4 + O(z<sup>3</sup>|lnz|)
</center><p>
has a pole (and also a logarithmic nonanalyticity)
when z&#8594;0.
The net effect is 
G<sub>Feyn</sub>&#8776;i/(2&#960;s)<sup>2</sup>
when s<sup>2</sup>&#8594;0. 
<!--This agrees with Greiner EQ27 of sect2.5.-->
</p></li></ul>
<p></p><p>
The Klein-Gordon propagator
formula is fundamental in QED because all the other propagators can be written
in terms of it (see Greiner &amp; Reinhardt §2.5).
The 
<b>Dirac propagator</b>
<!-- Wikipedia's 
<a href="http://en.wikipedia.org/wiki/Propagator#Other_theories">Dirac propagator</a> 
seems wrong since has no delta fnction.
-->
can be expressed as a linear combination of it and its first
derivatives with respect to t,x,y,z.
<a name="photprop">The</a>
<b>photon propagator</b> is -ig<sup>uv</sup> times
its massless limit:
<!-- Birrell and Davies EQ2.78 give this too, I agree with them,
and with Greiner+Reinhardt EQ27, good: -->
</p><center>
-ig<sup>uv</sup> [
-(4&#960;)<sup>-1</sup> &#948;(s<sup>2</sup>) + i/(2&#960;s)<sup>2</sup> ].
</center>
<small><p>
<b>Comparison with previous works:</b>
Zhang et al 2010, motivated by errors in various published versions of our formula,
rederived the <b>Klein-Gordon</b>
Feynman propagators in (D+1)-dimensional
Minkowski spacetime for <b>arbitrary D&#8805;1</b>, as their EQ27.
They then correctly give its
D=3 special case as their EQ28, and somewhat
misleadingly claimed this agrees with the result
found by Greiner &amp; Reinhardt EQ26 of §2.5.
<!--This false claim is refuted by (x,m)=(-5.5, 1).-->
The two formulas actually <i>do</i> agree but only if Greiner's is multiplied by <i>i</i>:
ZhangEQ28=<i>i</i>GreinerEQ26  
is true for both the timelike and spacelike cases x&gt;0, x&lt;0, and
also is true for the lightlike case x=0 as is seen by
ZhangEQ26=<i>i</i>GreinerEQ27
when D=3.
Zhang et al remark that this multiplication by <i>i</i> is ok 
"owing to the convention that D<sub>F</sub>(x) here equals i&#916;<sub>F</sub>(x) there,"
and note that their method differed from Greiner's.
(My formula is completely equivalent to Greiner's.)
Zhang et al also claim that their formula when D=1 and D=2 agrees with formulas in previous
papers by Di Sessa 1974 and Gutzwiller 2003 respectively.   In view of all this it
seems likely Zhang got it right, and therefore that my formula also is correct
(but keep in mind the <i>i</i>-factor definition-difference).
</p><p>
However
Huang (p.30) also gives a formula for the D=3 propagator, and his formula was
reprinted by
<a href="http://en.wikipedia.org/wiki/Propagator_%28Quantum_Theory%29#Feynman_propagator">
wikipedia</a> along with some enjoyable grayscale graphical pictures of
|G<sub>Feyn</sub>(x,0,0;t)|.
Unfortunately Huang=ComplexConjugate(Greiner) in the timelike case x&gt;0,
but Huang=Greiner in the spacelike case x&lt;0.
Huang's formula, 
<!--thus necessarily has two different contradictory massless limits? No, the sign gets
squared so Huang has same massless limit.-->
therefore, is wrong – but since |Huang|=|Greiner|
the pictures in Wikipedia remain correct.  As we remarked, 
Birrell &amp; Davies EQ2.77 gave an incorrect D-dimensional formula,
although their EQ2.78 for the <i>photon</i> propagator agrees with me and Peskin &amp; Schroeder
(and Huang's massless limit, which exists, agrees with ours).
Zee 2003 gives, with only a sketchy derivation since he cites other papers, 
a graviton propagator in momentum space, which he notes
is <i>not</i> quite the same as the massless limit of the propagator of a massive spin-2 particle.
</p><p>
Zhang et al then continued on to obtain the propagator for the <i>Dirac</i>
equation by applying the operator
<nobr>
(i&#947;<sup>&#956;</sup>&#8706;<sub>&#956;</sub>+m) 
</nobr>
to their Klein-Gordon formula.  The result is their EQ30 (and EQ33 is the D=3 special case).
This, Zhang claims,  was done incorrectly by §2.5 of
Greiner &amp; Reinhardt.
</p></small>
<a name="asymptprop"></a>
<p>
<b>Asymptotic behavior of Klein-Gordon propagator at large spacelike or timelike distances:</b>
We can use the asymptotic
<nobr>K<sub>v</sub>(z)&#8764;[&#960;/(2z)]<sup>1/2</sup>exp(-z)</nobr>
<!--NIST DLMF 10.25.3-->
when 
<nobr>|z|&#8594;&#8734;</nobr>
with arg(z)&#8804;3&#960;/2-&#949; for any &#949;&gt;0,
to deduce that
for large spacelike (i.e. imaginary) separation s 
</p><center>
G<sub>Feyn</sub>(t;x,y,z)
&#8776; 
-|s|<sup>-3/2</sup> 
m<sup>1/2</sup> 
2<sup>-5/2</sup> 
&#960;<sup>-3/2</sup> 
exp(-m|s|),
&nbsp;
when s&#8594;i&#8734;.
</center><p>
This drops off exponentially with 1/e falloff length equal to the 
<a href="http://en.wikipedia.org/wiki/Compton_wavelength">Compton wavelength</a>
of the particle.  For large timelike (i.e. real) separation s we instead get
</p><center>
G<sub>Feyn</sub>(t;x,y,z)
&#8776; 
-|s|<sup>-3/2</sup> 
m<sup>1/2</sup> 
2<sup>-5/2</sup> 
&#960;<sup>-3/2</sup> 
exp(-ims),
&nbsp;
when s&#8594;±&#8734;.
</center><p>
This behaves in an oscillatory manner with amplitude declining like
the 3/2 power of the time-separation and with period 2&#960;w/c
where w is the particle's
<a href="http://en.wikipedia.org/wiki/Compton_wavelength">Compton wavelength</a>
and c is the speed of light (where c=1 in the Planck units we are using in this section).
</p><a name="betterbehaveddelta"></a><p>
<b>Effect of brick-smearing on delta function terms within propagators:</b>
These propagators become much <b>better behaved after brick-smearing.</b>
This is important, because as, e.g, Smirnov 2006's §2.2 makes clear,
the underlying source of infinities in QED viewed in position space, is
exactly the singularities in propagators that occur at small |distances|;
so brick-smearing's improvement of these singularities can remove the divergence problems.
Regard the propagator as a function of s<sup>2</sup>.
Then the effect of brick-smearing, assuming
hypercube D-dimensional bricks of sidelength 2L,
is to integrate the propagator of 
s<sup>2</sup>+|a|<sup>2</sup>
over the D-dimensional hypercube 
|a<sub>1</sub>|&lt;L,
|a<sub>2</sub>|&lt;L,...,
|a<sub>D</sub>|&lt;L,
then (if desired) normalize by dividing by (2L)<sup>D</sup>.
</p><p>
The effect of this on the Dirac delta function is <b>healthful</b>,
as we now demonstrate.
With <b>D=1</b> we have that
&#8747;&#948;(s<sup>2</sup>+a<sup>2</sup>)da
is 0 if s<sup>2</sup>&gt;0,
is <nobr>|s|<sup>-1</sup></nobr> if <nobr>s<sup>2</sup>&lt;0,</nobr>
and is <nobr>&#960;&#948;(s)</nobr> if s=0.
That reduced the nastiness of the delta pseudofunction, but not enough to make it become
an ordinary function.
However, one could argue that since 
<nobr>&#8747;&#948;(|x|<sup>P</sup>)f(x)dx=0</nobr>
if 0&lt;P&lt;1 and |f(x)| is bounded
(for integration from x=-1 to x=+1)
the delta function's nastiness has been reduced so much that it <i>effectively
no longer exists</i> (at least for the purpose of integrating d(s<sup>2</sup>)
times well-behaved functions
of s<sup>2</sup>).
A more precise statement is that
<nobr>&#8747;&#948;(|x|<sup>P</sup>)|x|<sup>Q</sup>dx</nobr> 
(for <nobr>0&lt;P&lt;1)</nobr>
is 0 if <nobr>Q+1&gt;P</nobr>
and is 
&#8734; if <nobr>Q+1&lt;P.</nobr>
<!-- and is (1-P)<sup>-1</sup> if Q+1=P? -->
In the borderline case
Q+1=P
we get a finite nonzero quantity,
as is
<nobr>&#8747;&#948;(|x|<sup>1/2</sup>)<sup>2</sup>dx.</nobr>
Amazingly enough (see 
<a href="#twoarc">below</a> re the 
<nobr>s<sup>-1</sup></nobr> singularity arising from
Bessel function term) these borderline cases actually
seem to be activated when we consider a Feynman diagram with a double arc
(two-arc loop) when the two endpoint vertices lie in bricks whose raindrops are
distance=0 apart.   However, in such cases this loop is effectively equivalent to
having its two vertices in the <i>same</i> brick and then via postulates
<a href="https://dl.dropboxusercontent.com/u/3507527/postU">IV</a>
and 
<a href="https://dl.dropboxusercontent.com/u/3507527/post3">IIIb</a>
(see discussion soon after <a href="#postU">postulate IV</a>)
we can argue the contribution should cancel out to exactly zero averaged over that brick.
</p><p>
With <b>D=2</b> 
we have that
<nobr>
&#8747;&#8747;&#948;(s<sup>2</sup>+a<sup>2</sup>+b<sup>2</sup>)dadb
</nobr>
is 0 if s<sup>2</sup>&gt;0,
is &#960;/2 if s<sup>2</sup>=0,
and is &#960; if s<sup>2</sup>&lt;0.
This is just a step function, which note is <i>bounded.</i>
Finally if <b>D=3</b>
we have that
<nobr>
&#8747;&#8747;&#8747;&#948;(s<sup>2</sup>+a<sup>2</sup>+b<sup>2</sup>+c<sup>2</sup>)dadbdc
</nobr>
is 0 if s<sup>2</sup>&#8805;0,
and is 2&#960;|s| if s<sup>2</sup>&#8804;0.
The lattermost is just
an ordinary continuous function of s<sup>2</sup>, which is bounded 
in a neighborhood of 
s<sup>2</sup>=0.
<!-- Its |derivatives| with respect to x,y,z,t bounded by O(1/|s|). -->
<small>
<b>Note:</b>
These formulas have been based on integration from -&#8734; to +&#8734; which is enough
to show that the smearing regularizes the delta functions. 
The formulas based on the (correct) integration from -L to +L are more complicated
and are zero whenever |s<sup>2</sup>|&gt;DL<sup>2</sup>. 
Thus the true formulas are bounded <i>everywhere.</i>
</small>
</p>
<a name="twoarc"></a>
<p>
Brick smearing also diminishes (or if D=3 removes) the infinity on the light cone arising
from the <b>Bessel term.</b>
If D=1 we have, considering only the most dominant term in K<sub>1</sub>(z)=1/z+...'s
series expansion, 
</p><center>
&#8747;<sub>|a|&lt;L</sub>(s<sup>2</sup>+a<sup>2</sup>)<sup>-1</sup>da
=
2·s<sup>-1</sup>arctan(L/s)
</center><p>
which now only has s<sup>-1</sup> type singularities when s<sup>2</sup>&#8594;0, 
rather than (as before)
s<sup>-2</sup>.
If D=2 then things are even nicer:
</p><center>
&#8747;&#8747;<sub>a<sup>2</sup>+b<sup>2</sup>&lt;L<sup>2</sup></sub>
(s<sup>2</sup>+a<sup>2</sup>+b<sup>2</sup>)<sup>-1</sup>dadb
=
&#8747;<sub>0&lt;u&lt;L&#8730;2</sub>
(s<sup>2</sup>+u<sup>2</sup>)<sup>-1</sup>
2&#960;u du
=
&#960; ln(1+2L<sup>2</sup>s<sup>-2</sup>)
</center><p>
which has only a logarithmic infinity when s<sup>2</sup>&#8594;0.
<small>
Note here we integrated over a disk rather than square region (the latter would
have made the formula messier) which is good enough for our purpose of 
demonstrating the regularization.  
</small>
Finally, if D=3 we have
(integrating over a ball)
</p><center>
&#8747;&#8747;&#8747;<sub>a<sup>2</sup>+b<sup>2</sup>+c<sup>2</sup>&lt;L<sup>2</sup></sub>
(s<sup>2</sup>+a<sup>2</sup>+b<sup>2</sup>+c<sup>2</sup>)<sup>-1</sup>dadbdc
=
&#8747;<sub>0&lt;u&lt;L&#8730;3</sub>
(s<sup>2</sup>+u<sup>2</sup>)<sup>-1</sup>
4&#960;u<sup>2</sup> du
=
4&#960; [L&#8730;3 - s arctan(s<sup>-1</sup>L&#8730;3)]
</center><p>
which is even nicer.  This is continuous and bounded for all real
s<sup>2</sup>.
<!--
</p><center>
&int;<sub>|b|&lt;L</sub>(s<sup>2</sup>+b<sup>2</sup>)<sup>-1/2</sup>arctan(L/(s<sup>2</sup>+b<sup>2</sup>)<sup>1/2</sup>)db
</center><p>
behaves (ignoring constant factors) like
</p><center>
&int;<sub>0&lt;b&lt;L</sub>|s<sup>2</sup>+b<sup>2</sup>|<sup>-1/2</sup>db
=
-ln|s| + ln(L + [s<sup>2</sup>+L<sup>2</sup>]<sup>1/2</sup>)
</center><p>
which now only has a logarithmic singularity when s&rarr;0.
If D=3 then things are nicer still:
</p><center>
&int;<sub>|a|&lt;L</sub>ln|s<sup>2</sup>+a<sup>2</sup>|da
</center><p>
is a continuous as a function of s at s=0.
-->
</p><p>
For E-time-dimensional bricks we would instead integrate
the propagator of 
s<sup>2</sup>-|a|<sup>2</sup> 
using E-dimensional integration and would normalize by dividing by (2L)<sup>E</sup>.
</p><p>
For "round bricks" of radius L, we instead would integrate
the propagator of 
s<sup>2</sup>+&#952;<sup>2</sup>L<sup>2</sup> 
times 
&#937;<sub>D-1</sub>(sin&#952;)<sup>D-1</sup>d&#952; 
for 0&lt;&#952;&lt;&#960;,
where
&#937;<sub>D</sub> is the surface area of a 
unit-radius sphere in Euclidean D-space.
One could then, if desired, normalize by dividing by
&#937;<sub>D</sub>L<sup>D</sup>.
I am actually able to evaluate the brick-smeared (bricks with D=1) 
<b>photon propagator
as a closed formula</b> (we omit the constant multiplicative 
factor -ig<sup>uv</sup> for simplicity):
</p><center>
L<sup>-1</sup>&#8747;<sub>0&lt;x&lt;L</sub>
[i(2&#960;)<sup>-2</sup>(s<sup>2</sup>+x<sup>2</sup>)<sup>-1</sup>
- (4&#960;)<sup>-1</sup> &#948;(s<sup>2</sup>+x<sup>2</sup>)]
dx
&nbsp; = &nbsp;
(4&#960;<sup>2</sup>Ls)<sup>-1</sup> i arctan(L/s)
- 
(4L)<sup>-1</sup>&#948;(s)
-
(4&#960;L|s|)<sup>-1</sup> <b>1</b><sub>-L<sup>2</sup>&lt;s<sup>2</sup>&lt;0</sub>
</center>
<p>
where <b>1</b><sub>q</sub> means "1 when q is <small>TRUE</small>, otherwise 0."
Note that this has C(s)|s|<sup>-1</sup>-style divergency when |s|&#8594;0, where the multiplier
C(s) stays <i>bounded</i>.  That ignored the delta-function term, but as we just
discussed for many purposes &#948;(s) can be regarded as effectively not present.
</p><p>
We conclude:
</p><p>
<b>Brick-Smeared Propagator Regularization Theorem:</b>
The brick-smeared Klein-Gordon and Photon propagators
are ordinary functions of s<sup>2</sup>
which are
<i>bounded</i> 
for all real s<sup>2</sup>,
and
<i>analytic</i> for all s<sup>2</sup>&#8800;0,
provided the brick has dimension <b>D=3</b> (or E=3 for E-time-dimensional bricks;
theorem also valid for "round bricks" of surface-dimension D=3 or E=3; always ED=0).
The brick-smeared Dirac propagator
also is
analytic for all s<sup>2</sup>&#8800;0
but tends to infinite values 
like s<sup>-1</sup>
when s<sup>2</sup>&#8594;0±.
<!-- ACTION FINITE???  I think not for Dirac, sorry. -->
<br>
If <b>D=2</b> the brick-smeared Klein-Gordon, Photon, and Dirac propagators all are
analytic for all real s<sup>2</sup>&#8800;0, but the former two tend logarithmically
to infinite values when s<sup>2</sup>&#8594;0±, while the latter
tends to infinite values when s<sup>2</sup>&#8594;0±
like s<sup>-2</sup>.
<br>
If <b>D=1</b> brick-smearing reduces the degree of the singularity when |s|&#8594;0 by
1.  More generally (we now observe the unified pattern, which once you know it
is easy to prove) each additional
brick dimension reduces the degree of singularities like |s|<sup>-K</sup>
by 1 to |s|<sup>1-K</sup>, with the asterisk that when K=1 the
replacement has a logarithmic singularity so that it requires one further 
brick-dimension-increment
to fully eliminate it.
<br>
These behaviors when s<sup>2</sup>&#8594;0±
are less-singular than the unsmeared propagators.
But when s<sup>2</sup>&#8594;±&#8734; the same asymptotic behaviors happen
for the smeared and unsmeared propagators.
</p><p>
<b>Action remark:</b>
The <i>action</i> (QED-lagrangian density, integrated over all Minkowski spacetime) for
these propagators remains infinite even after brick-smearing, although it is only logarithmically
infinite with 3-dimensional bricks using the Klein-Gordon action and propagator.
</p>
<a name="momspaceremark"></a>
<p>
<b>Momentum-space remark:</b>
Although we want to work in position space, you can gain a lot of intuition about
regularization via brick smearing, by thinking in momentum (Fourier) space.
The effect of brick smearing is, roughly,
to integrate over a region near s whose width
W is order
s<sup>-1</sup> wide.   
As is a well known consequence of the Fourier convolution theorem,
this kind of "smoothing" tends in Fourier space to 
provide a multiplicative factor tending to <i>cut off frequencies p
above order W<sup>-1</sup>.</i>  
Such factors have obvious beneficial effects on all
QED Feynman-diagram integrals.   Because all those integrals were already
known to be at worst logarithmically divergent, i.e. "right on the verge
of becoming convergent," this 
<i>already seems to make it "obvious" that rain of bricks abolishes
all the divergencies of IRFrQFTs.</i>
</p><p>
<b>But wait! Those brick-smeared propagators no longer are exact 
solutions of the Dirac (or Maxwell) partial differential equation!</b>
There are two answers to that:
</p><ol><li>
So? PDEs in (1+3)-dimensional space only
make sense as the "laws of physics" if the arena
of physics is "the Minkowski spacetime continuum."
But with rain of bricks, it isn't anymore.  
Indeed, the arena is no longer a continuum at all.
Therefore, it is not "bad" and "strange" that PDEs should be abandoned as the foundation
of physical law, it is "expected," "natural," and "good."   Since the rain of bricks arena
<i>approximates</i> Minkowski space the laws will be approximately described by such PDEs
(and both these approximations will seem better to observers to less-acute vision), but
the underlying truth can only be based on the true metrical structure.
</li><li>
If we use the Kaluza-Kleiny view of rain of bricks, then we'd be using
the Dirac (or whatever) equation in a higher space (and Zhang's higher-dimensional propagator
formulae) and then brick-smearing would
simply be effecting a linear combination of solutions to a linear equation, which is
still a solution.  So the Dirac (or whatever) PDEs in the higher dimensional
space would be satisfied exactly.
</li></ol>
<p></p>
<!-- <p>I prefer the second answer.</p> -->
<a name="asymnosmear"></a>
<p>
<b>Asymptotic no-smearing theorem:</b>
Brick-smeared propagator formulas are asymptotically equal to the unsmeared propagators
when |s|&#8594;&#8734;.
</p><p>
<b>Proof:</b>
The crux is that the brick-smearing alters distances L by additive
amounts ±O(L<sup>-1</sup>) <i>which go to zero</i> when |L|&#8594;&#8734;.
<b>Q.E.D.</b>
</p>
<a name="asymspeed"></a>
<p>
<b>Corollary: The speed of light does not change:</b>
A photon will travel long distances in vacuum 
(we have in mind the <i>limit</i> 
of travel-distance&#8594;&#8734;)
at a speed c which does <i>not</i> depend on
the frequency of the photon.  Indeed, to the extent c depends on frequency, the total
time-lag for a photon of one frequency versus another, will be at most O(1) Planck units
no matter how far they travel.
</p>

<a name="classicalelstat"></a>
<h3>16. Brick-smeared classical electrostatics (an illustrative parallel) </h3>

<p>
For the most part, I have been unable to compute closed formulas for the brick-smeared Dirac and
Maxwell propagators (although expressions as infinite series or integrals can be produced).
To develop our intuition, it perhaps is worthwhile to take a detour into
a simpler area of physics – classical electrodynamics, or,
even more simply, classical electro<i>statics</i>, in which the analogous formulas
<i>can</i> be found.
</p><p>
The classical
Coulomb potential between two positive point charges separated by distance r is (up to a 
constant multiplicative factor, which we shall ignore) 
<nobr>&#934;<sub>Coul</sub>(r)=r<sup>-1</sup>.</nobr>
With smearing over a 1-dimensional brick, this would become
</p><center>
&#934;(r)
&nbsp; = &nbsp;
L<sup>-1</sup>&#8747;<sub>0&lt;a&lt;L</sub> (r<sup>2</sup>+a<sup>2</sup>)<sup>-1/2</sup>da
&nbsp; = &nbsp;
[ln(L+[r<sup>2</sup>+L<sup>2</sup>]<sup>1/2</sup>) - ln(r)] / L
&nbsp; = &nbsp;
L<sup>-1</sup>arcsinh(L/r).
</center><p>
This is better behaved near r=0 since
</p><center>
&#934;(r)
&nbsp; = &nbsp;
L<sup>-1</sup>ln(2L/r) + L<sup>-3</sup>r<sup>2</sup>/4 - 3L<sup>-5</sup>r<sup>4</sup>/32 + ...
&nbsp;&nbsp;
(converges if |r|&lt;L)
</center><p>
only has a logarithmic, instead of power-law, singularity when r&#8594;0+.
But the behavior when r&#8594;&#8734; is unchanged (the electrostatic equivalent of the
asymptotic 
<a href="#asymnosmear">no-smearing</a> theorem):
</p><center>
&#934;(r)
&nbsp; = &nbsp;
r<sup>-1</sup> - L<sup>2</sup>r<sup>-3</sup>/6 + 3L<sup>4</sup>r<sup>-5</sup>/40 - ...
&nbsp;&nbsp;
(converges if |r|&gt;L).
</center><p>
The "self energy" of a point charge classically was power-law infinite due to 
short-distance behavior.
However, with smearing over a 1-dimensional brick, it becomes finite:
</p><center>
(1/2)&#8747;<sub>0&lt;r&lt;&#8734;</sub> &#934;'(r)<sup>2</sup> 4&#960;r<sup>2</sup>dr
&nbsp; = &nbsp;
&#960;<sup>2</sup>/L.
</center><blockquote>
This compares with the 
self energy the old r<sup>-1</sup> potential would have delivered (for a radius=a 
hollow-sphere electron)
<p></p><center>
(1/2)&#8747;<sub>a&lt;r&lt;&#8734;</sub> r<sup>-4</sup> 4&#960;r<sup>2</sup>dr
&nbsp; = &nbsp;
2&#960;/a
</center><p>
which would suggest, based on comparing these and now restoring physical units, 
that the correct size of a 1D brick
ought to be 2L=&#960;r<sub>e</sub>
where <nobr>r<sub>e</sub>&#8776;2.8179×10<sup>-15</sup>meter</nobr>
is the "classical electron radius."
Of course this estimate should not be taken seriously because the <i>classical</i> self energy
is very wrong.   The classical (i.e, non-quantum)
picture of the electron begins to fail seriously at length
scales comparable to the electron's Compton wavelength, which is about 1000× larger.
<!-- 2.4&times;10<sup>-12</sup> -->
</p></blockquote>
<!--
this is the wrong intergal, need a sqrt on the denominator...:
int( sin(x)/(r^2 + x^2), x ) =
(Ci(-I*r + x)*sinh(r) + Ci(I*r + x)*sinh(r) + I*cosh(r)*(Si(I*r - x) + Si(I*r + x)))/(2*r);

int( sin(x)/(r^2 + x^2), x=0..Pi ) =
1/2*(Ci(-I*r+Pi)*sinh(r)+Ci(I*r+Pi)*sinh(r)-I*cosh(r)*Si(-I*r+Pi)+I*cosh(r)*Si(I*r+Pi)-2*Ci(-I*r)*
sinh(r)-I*sinh(r)*Pi+2*cosh(r)*Shi(r))/r

When r=10:    0.01943858    (bad numerics, need hi precision)
When r=100:   0.000199941
When r=1000:  0.0000019999941304

series(%, r) = 
-ln(r) + 0.4964522470 - ln(r)*r^2/6 + 0.2047440661*r^2 - ln(r)*r^4/120 + 0.01450050242*r^4 
+ O(r^6);
where 1-gamma+Ci(Pi)=0.4964522470

Note, the right integral apparently cannot be done in closed form.

Another wrong integral:
int( sin(x)^2/(r^2 + x^2), x ) =
(2*arctan(x/r) + I*cosh(2*r)*Ci(-2*I*r + 2*x) 
- I*cosh(2*r)*Ci(2*I*r + 2*x) - sinh(2*r)*Si(2*I*r - 2*x) 
+ sinh(2*r)*Si(2*I*r + 2*x))/(4*r);

int( sin(x)^2/(r^2 + x^2), x=0..Pi ) =
-1/4*(-2*arctan(Pi/r)-I*cosh(2*r)*Ci(-2*I*r+2*Pi)+I*cosh(2*r)*Ci(2*I*r+2*Pi)+sinh(2*r)*Si(2*I*r-2*
Pi)-sinh(2*r)*Si(2*I*r+2*Pi)+cosh(2*r)*Pi)/r;
=
1.418151576 - 1.570796327*r + 1.051537679*r^2 - O(r^3)
where
Si(2*Pi)=1.418151576
Pi/2=1.570796327
-->

<a name="nyquist"></a>
<h3>17.
<a href="http://en.wikipedia.org/wiki/Nyquist-Shannon_sampling_theorem">Nyquist's theorem</a>
&#8658;
The rain of bricks naturally induces a Planck scale "UV cutoff" </h3>

<p>
<b>What is "Nyquist's Sampling Theorem"?</b>
Traditionally this theorem has been linked to Harry Nyquist (1888-1976), an electrical 
communication engineer, via his 1928 paper.  But better versions of the theorem were
proven by numerous other authors both before and after Nyquist.  All theorems in this class
aim to prove that any "band limited" function, meaning one containing only Fourier components
below some |frequency| <i>bound</i> <b>B</b>, 
can be reconstructed from its values at a discrete set of points,
provided the set of sampling points is "dense enough" – 
while (in the other direction) any assignment of values to points that are 
sufficiently <i>sparsely</i> spaced, is
achievable by some such function.  
The best theorem versions assert that
there is a unique "Nyquist density" that is neither too dense nor too sparse.
</p><p>
The initial version of the theorem was only valid in <i>one dimension</i>
using <i>equally-spaced</i>
sampling points with <b>number density&#8805;2B</b> (the "Nyquist rate").  
If the rate is exactly equal to Nyquist, this can be 
<a href="http://en.wikipedia.org/wiki/Nyquist-Shannon_sampling_theorem">proved</a>
by using sinc-function interpolation to reconstruct the function, 
using the lemma that sinc(t)=sin(&#960;t)/(&#960;t)
contains only Fourier components of |frequency|&#8804;1/2.
Whittaker proved in 1915 that this sinc-interpolation would
yield the <i>unique</i> band-limited function which happened to be
"entire," i.e. analytic everywhere in the complex t-plane.
Also Landau 1967
showed it to be the unique band-limited continuous function with 
finite L<sup>2</sup> norm.
</p><p>
The obvious questions then became:
</p><ol>
<li>
What if we have <i>irregular</i> sampling points?
What criteria must these points meet in order to assure reconstructibility?
</li><li>
Under what circumstances (and in what senses)
can we assure the function will be <i>unique</i>?
</li><li>
What about <i>higher dimensions</i>?
</li><li>
Can we go beyond proving reconstructibility <i>existentially</i>, to indeed present 
efficient and numerically-stable <i>algorithms</i> to construct the function?
</li></ol>
<p>
Some answers are as follows:
</p><p>
<b>1.</b>
Kadec 1964 showed that provided the <i>n</i>th sampling point, instead of being forced to be
located exactly at 
<nobr>(2B)<sup>-1</sup>n</nobr>,
was located anywhere distance&lt;(8B)<sup>-1</sup> away from there (and this was
true for every integer <i>n</i>), i.e. strictly between 
<nobr>(2B)<sup>-1</sup>(n-1/4)</nobr>
and 
<nobr>(2B)<sup>-1</sup>(n+1/4)</nobr>,
then a certain explicit reconstruction formula would work.
[Warning: This formula can exhibit poor numerical stability.]
Kadec's constant "1/4" is best possible.
</p><p>
Beutler 1966 gave a far weaker condition on the sampling points – that a certain integral
should have value &#8734; (see his theorem 4 on his page 332).  Beutler's
reconstructibility
criterion is satisfied with probability=1 by Poisson-random Nyquist-density points.
</p><p>
<b>2.</b>
Landau 1967
gave arguments that 
</p><ol type="a">
<li>
It is impossible, via a band-limited L<sup>2</sup>-normable 
signal, to transmit data at a rate exceeding
the Nyquist rate, but every L<sup>2</sup>-normable sequence of numbers <i>can</i> be 
transmitted at the Nyquist rate by such a signal;
</li><li>
It is impossible to <i>stably</i> reconstruct the signal 
(i.e. in a way such that small variations in the data will produce small variations 
in the function being
reconstructed) from samples
if those samples are less-dense than the Nyquist rate; but sinc-interpolation performs
stable reconstruction at the Nyquist rate.
</li></ol>
<p>
One can cause the interpolating function to be 
<i>unique</i> if it exists by demanding, among all
interpolating functions with a given bandwidth bound, the one with the least L<sup>2</sup> norm.
Or among all interpolating functions, choosing the one with the smallest bandwidth bound.
These uniqueness claims both are trivial consequences
of viewing them "geometrically" as, e.g, projection operations,
in Hilbert space.
</p><p>
<b>3.</b>
In 1967 Landau and Beurling &amp; Malliavin
showed – and this was redone by
Gröchenig &amp; Razafinjatovo 1996 –
that for a function of D-dimensional vectors "band-limited" 
in Fourier frequency-component D-space
in such a way that only vector "frequencies" lying inside
some region of D-dimensional volume V were permitted, 
it was impossible to reconstruct the function from samples,
unless those samples had density at least 2V<sup>-1</sup>.
However, obviously such reconstruction is possible when the sample
points form a cubic lattice with exactly this density, and if the
bandlimiting region is of hypercube shape
(by simply using sinc-interpolation one dimension at a time).
Thus the bound is tight.
We shall call this density the "D-dimensional Nyquist rate."
</p><p>
<!--
A.Beurling &amp; P.Malliavin: On the closure of characters and gthe zeros of entire functions,
Acta Math 118 (1967) 79-95.
Henry J. Landau: 
Necessary Density Conditions for Sampling and Interpolation of Certain Entire Functions,
Acta Math. 117,1 (1967) 37-52.  (a)LandauNyquistND67.pdf
Henry J. Landau: 
Sampling, Data Transmission, and the Nyquist Rate,
Proc.IEEE 55,10 (1967) 1701-1706.
-->
</p><p>
<b>4.</b>
Efficient reconstruction algorithms have been devised by using ideas of the following 
rough kind.  Interpolate your data by some scheme (such as piecewise linear). This interpolating
function may fail to obey the bandwidth limit.  Now remove all Fourier components disobeying it.
The resulting function may fail to interpolate the data.  But here is the crux:  if the
<i>difference</i> between the data and this function is <i>smaller</i> in some norm than
the original data, we can re-iterate the process on those differences to correct our
function.  Then the errors in that can again be corrected, etc.  The process will
converge provided we can prove a norm-decrease lemma.  (Such a lemma will depend on the 
irregularity properties of our sample points and on the precise details of our
interpolation and smoothing schemes.)
Such lemmas have been proven and the resulting algorithms tested, 
but the last word on this subject has not yet been spoken.
(I will not attempt to survey the literature that tries to do this.)
<!--??-->
</p><p> 
<b>Relations to other parts of physics &amp; mathematics:</b>
Physicists have long used their own (substantially less well-done, albeit perhaps more convenient)
version of the Nyquist theorem, in the form of the notion of <b>phase-space volume.</b>
That is, they count each volume h of position-momentum "phase space" as one "state."
(This is for 1-dimensional position and
1-dimensional momentum.)
Hence if positions were restricted to a set of N points at some density &#961;<sub>3D</sub>
and a physical system had N
states, we would expect the available momenta to be constrained within a region in momentum
space of volume h<sup>3</sup>&#961;<sub>3D</sub>, which in Planck units is
(2&#960;)<sup>3</sup>&#961;<sub>3D</sub>.
</p><p> 
In 1D, points at density 2B (e.g. the integers divided by 2B) would then correspond to 
a region |p|&#8804;2&#960;B=hB in momentum space, i.e. |frequency|&#8804;B
(defining spatial frequency as 1/period and using the de Broglie wavelength=h/p).
I wrote that merely to verify that this physicist phase-space volume
notion yields the same answer as Nyquist.
</p><p> 
The physicist's phase-space volume notion was not without foundation.  
One sort of mathematical foundation
was a theorem, whose earliest versions date to Hermann <b>Weyl</b> (1885-1955), 
which asymptotically
counts the number of
eigenmodes of the Laplacian operator 
within
a region in space 
up to some |eigenvalue| cutoff.
Fancier versions of these theorems allow other linear partial-differential
operators instead of merely "the laplacian" (e.g. Dirac, Schrodinger, Maxwell...),
allow the region to be on a general D-dimensional Riemannian manifold rather than merely
in flat space, allow various kinds of boundary conditions to be imposed,
and provide series of correction terms which go beyond merely the
first-order term arising from the <i>D-volume</i> of the region; the
lower order terms also involve
the (D-1)-area of its boundary and certain integrals of
curvature-related quantities in the region and on its boundary.
There is a large literature on these kinds of theorems of which we shall mention only 
McKean &amp; Singer 1967, Sakai 1971, Lance Smith 1981, Vassilevich 2003, and
Waechter 1972
–
but there appear to be zero cites in the Nyquist literature to the Weyl literature, and
vice versa.
</p><p>
<b>Application of Nyquist to rain of bricks physics:</b>
Consider any Lorentz observer.  Within some volume V comoving with (and measured by)
that observer, during
a timespan T measured on his clock, let the number of raindrop points be N.
The expectation value of N of course is &#961;<sub>rain</sub>VT.
Let us temporarily suppose (for simplicity) that N actually is exactly equal
to this expectation.
As usual employ units with c=1 so that time and length are on the same footing.
Suppose the value of some function F(t;x,y,z) is
known on each of those points <i>but not known anywhere else</i>.
Then by the 4-dimensional 
Shannon-Nyquist sampling theorem, F is <i>indistinguishable</i> from
a function which is <b>band-limited</b> in such a way that it only contains 
Fourier components of period and 
<nobr>wavelength&#8805;(&#961;<sub>rain</sub>)<sup>-1/4</sup>K</nobr>,
where 1&#8804;K&#8804;2.  Further, if K is chosen to be minimum possible within the allowed range, then
this band-limited "reconstructed" function will be (with probability=1) <i>unique</i>.
</p><p>
If now we allow N to differ from its expectation value, this will not 
change anything by much because large deviations (due to Poisson's law) are very improbable
– especially increases, whose probability 
<a href="#thepoissonlaw">falls</a>
off ultimately factorially.
For example, if N were twice its expectation that would reduce the cutoff wavelength by 
multiplication by 2<sup>-1/4</sup>.  
Now in the <a href="#rainofbricks">rain of bricks</a> model, 
the arena of physics simply <i>does not contain</i>
any Minkowski-spacetime points besides the raindrop points.
We conclude from the usual quantum relation between frequency and energy
(or between wavelength and energy for ultrarelativistic particles)
that in the rain of bricks universe there will be effective <b>upper bounds</b>,
of order 1 in Planck units, on the expectation value for any measurement of any
particle energy – 
and indeed on any polynomial or simple-exponential function of energy.
(However, sufficiently quickly factorially-growing functions of energy, will not
enjoy finite bounds on expectation value.)
</p><p>
Specifically, 
<nobr>|momentum|&#8804;(&#961;<sub>rain</sub>)<sup>1/4</sup></nobr>
and
<nobr>|energy|&#8804;(&#961;<sub>rain</sub>)<sup>1/4</sup></nobr>
in Planck units.
</p><a name="accelsuperplanck"></a><p>
<b>What if (in rain of bricks physics) you push a Planck-momentum particle to go faster?</b>
Suppose (1) there are two Lorentz observers A and B of a particle.  In the view of A,
the particle is moving very fast: near the Planck-scale momentum limit.
But in the view of the (moving at intermediate speed) B, 
the particle's momentum is safely below Planck scale.
B now (2) exerts force on the particle to push it to greater momentum but still to (in
B's view) keep it safely below the limit.
What will A and B observe?   In B's view, everything will behave normally; the particle will
speed up approximately as expected by special relativity. But in A's view
special relativity would have predicted the particle would now exceed the Planck scale
and go outside the allowed interval.  So what happens?  The answer is: if A tries now to measure
the particle's momentum, she will observe a result equivalent to if a particle
in a quantum probability-amplitude <i>mixture</i> of many different momentum values, all within
the permitted interval.   To good approximation, A should observe a "random momentum" in
the same (or exactly opposite) direction and with magnitude close
to uniform-random in the permitted range.
</p><p>
Why?
This is because the particle at step (1) had a wave function that essentially was a Fourier mode
which when sampled at raindrops and then reconstructed from the sample values
by A or B, still yielded that same Fourier mode.
After step (2), in B's view, the particle still had a Fourier-mode (with higher
momentum, i.e. shorter wavelenth) as wavefunction, and it still was reconstructible.
But in A's view, this Fourier mode had too-short wavelength, corresponding
to a frequency above the allowed Nyquist range.  It thus in A's was <i>not</i> reconstructible
from its raindrop sampling data as that same Fourier mode, but since 
still L<sub>2</sub>-bounded it still in A's view
is reconstructible as <i>some</i> Nyquist-band-limited signal.
This signal should due to the raindrop location-randomness act
very much
like <i>random</i> data sampled from a distribution uniform in some real interval
– say [-1, 1] for concreteness.
And that, when Nyquist-reconstructed, should yield a result statistically independent
of frequency throughout the Nyquist range.
</p><p>
Note if A <i>waits longer</i>
so that she can sample raindrops within a <i>larger</i> spacetime box,
then her spatial-periodicity sensitivity (spatial Nyquist bandwidth limit)
grows, i.e. her wavelength sensitivity grows.  By adopting that strategy she perhaps could 
measure arbitrarily high super-Planckian momenta with arbitrarily short
sub-Planckian wavelengths. 
(I say "perhaps" because although the necessary data now is there,
it is not entirely
clear A will be able to access and use it... in particular due to uncertainty principles in 
<a href="#quantgrav">§11</a> the raindrop locations are not precisely knowable by A,
and performing the reconstruction seems to require knowing them – and 
arbitrarily precisely in the long-waiting limit.)
So this whole randomizing effect
will only happen for measurement attempts by A taking place over a 
short enough duration.
</p><p>
<b>Is this self-contradictory?</b>
No.  One might naively imagine that it is contradictory, because in B's view a particle with
definite momentum has in A's view a random momentum, and after A measures it
(so that it now has a definite value) this value will be incompatible 
(under special relativity) with the value claimed by B!
However, this contradiction only arises once A <i>measures</i> the particle momentum.
That requires A to <i>interact</i> with the particle, and this interaction should
in B's view <i>change</i> the particle momentum to a value compatible with A's.
There then is no contradiction.   Further as we said by waiting long enough A perhaps could
still measure superPlanck momenta without randomizing and hence with no disagreement.
</p><p>
The net effect could be roughly summarized as "measurements of particle momenta for 
huge (Planck-scale) momenta, are difficult." They can cause dramatic changes.  I.e, the effect of
raindrop physics is to cause <b>an extra kind of "uncertainty principle"</b>
for very-high-momentum or high-energy particles if measured quickly.
</p><p>
But we already <i>knew</i> such uncertainty had to happen!!
For example consider a particle "fly by."   
Suppose the particle if at rest would have width 
W<sub>0</sub>.
When it flies by you at velocity v, the "fly by time" is t=W/v where
W is the <i>Lorentz contracted</i> width.
Equivalently t=W<sub>0</sub>m<sub>0</sub>/p
where m<sub>0</sub> is the particle's rest-mass and
p is its momentum.
Hence for a particle of known rest-width and rest-mass, measuring fly-by time is the same thing
as measuring momentum.  But assuming the "rest-width" is essentially the same thing
as the particle's 
<a href="http://en.wikipedia.org/wiki/Compton_wavelength">Compton wavelength</a>,
we find
t=constant/p
independent of particle-type.
Thus having super-Planck momentum is <i>equivalent</i> to having a sub-Planck-time flyby.
But as we already <a href="#quantgrav">saw in §11</a>, quantum gravity tells us that
it is <i>impossible</i> to distinguish a sub-Planck-time interval from a zero-time interval.
<!--
W = W0 * sqrt(1-v^2/c^2)
p = m*v/sqrt(1-v^2/c^2)
-->
</p><p>
<b>But doesn't this prediction contradict conservation or energy and momentum?</b>
It does contradict a naive view of those things. 
Rain of bricks physics still enjoys energy and momentum conservation theorems,
but these theorems are different from they used to be.
See <a href="#whataboutsym">§25</a>.
</p><p>
<b>Connection to experiment?</b>
This dramatic 
prediction of what happens when one tries to accelerate a particle to super-Planck momenta
may differ from what superstring theory or loop quantum gravity (LQG) predict –
and certainly differs from what naive special relativity predicts.
But after perusing the string and LQG literature I have been unable to find anything
that actually makes any such prediction.  (LQG and string theory both are deterministic,
while rain of bricks is here making an explicit prediction of random behavior,
so in that sense they certainly differ.)
</p><p>
Unfortunately it is almost certainly beyond the ability of humanity to build a 
<b>Planck-energy accelerator</b> –
so this experiment seems not doable.  
E.g. a naive analysis 
indicates that 
scaling up SLAC (Stanford Linear Accelerator Center,
currently one of the world's largest linear accelerators)
to 68000 light years long, to mass comparable to the planet Earth, and to power
supply comparable to about 2% of the power output of the Sun,
would do the job.
</p><p>
One might try to argue that such an accelerator, although difficult to build,
nevertheless in principle is possible, and
even buildable by a hypothetical
advanced alien civilization.  It is unclear to me whether that is really true
because at ultrahigh energies various new loss mechanisms may become important, which 
the designers of SLAC justifiably neglected.
</p><p>
<b><i>Can</i> you "push a Planck momentum particle to go faster"?</b>
This whole puzzle is at least somewhat obviated just due to the great engineering difficulty of
accelerating particles to Planck or greater energies.
But aside from the usual engineering difficulties, 
there may also be additional more fundamental difficulties.   
To accelerate
electrons, you use electric fields, i.e. photon-electron interactions. 
These interactions occur
on bricks.  At least in the 
usual accelerator designs, the "pushing"
photons have low energy (radio frequency), i.e. long wavelength,
while our electrons are postulated to have huge energy and tiny wavelength.  Eventually
we would reach momenta where, no matter what reference frame you examine it from, 
all observers always agree that
at least one of the {photons, electrons} must have tiny (way sub-Planck) wavelengths.
One would then think that brick-smearing would tend to cause probability amplitudes 
for interactions to become tiny.  As a result, your accelerator would lose its ability
to "push" electrons at these high energies.
One could imagine in principle overcoming that by, e.g,
accelerating "second stage" accelerators,
which accelerate "third stage" accelerators,...
which accelerate the particles themselves,
but it at least naively 
seems as though it is exponential(K) hard to accelerate particles to K times
the Planck energy in that fashion.
</p>

<a name="casimirforce"></a>
<h3>18. Casimir force </h3>

<!-- 
M.Kardar &amp; Ramin Golestanian: 
The 'friction' of vacuum, and other fluctuation-induced forces
Rev Mod Phys 71,4 (1999) 1233-1245
  We employ a path integral formalism to study these phenomena for boundaries of arbitrary shape. 
This allows us to examine the many unexpected phenomena of the dynamic Casimir effect due to 
moving boundaries. With the inclusion of quantum fluctuations, the EM vacuum behaves essentially 
as a complex fluid, and modifies the motion of objects through it. In particular, from the 
mechanical response function of the EM vacuum, we extract a plethora of interesting results, 
the most notable being: (i) The effective mass of a plate depends on its shape, and becomes 
anisotropic. (ii) There is dissipation and damping of the motion, again dependent upon shape 
and direction of motion, due to emission of photons. (iii) There is a continuous spectrum of 
resonant cavity modes that can be excited by the motion of the (neutral) boundaries.
 J. Feinberg, A. Mann, M. Revzen: Casimir Effect: The Classical Limit, 
Ann. Phys. (N.Y.) 288 (2001) 103-136
 R. Balian and B. Duplantier, Ann. Phys. 112 (1978) 165;
 R. Balian and B. Duplantier Ann. Phys. 104 (1977) 300. 
 L. H. Ford and N. F. Svaiter, 
  Vacuum energy density near fluctuating boundaries, Phys. Rev. D 58,6 (1998) 065007
Feinberg: in the high-temp limit he wants the energy density of the radiation field to
be a function of temperature only.  This implies Casimir forces all vanish (exponentially) in
the high-T limit.  This desire can only be satisfied if we agree a "zero-point energy" exists
and depends upon the positions of the metal objects.
   hf/[exp(hf/[kT])-1] = kT - hf/2 + O(1/T)
hence we need a positive zero point energy hf/2 for each mode f.
  http://arxiv.org/pdf/quant-ph/0003021 says (2000)
it ought to be experimentally possible to examine Casimir(T) but no experiemnt so far has.

The experimental status of the effect was tenuous until 1997 when vindication of
the theoretical prediction to an accuracy of a few percent was reported 
  S. K. Lamoreaux, Phys. Rev. Lett. 78 (1997) 5;
  U. Mohideen and A. Roy, Phys. Rev. Lett. 81 (1998) 4549.

-->
<p>
As we <a href="#casimir1">remarked</a> in <a href="#whatswrong">§4</a>,
the Casimir force for smooth "magic mirror" perfect-reflector surfaces,
is generically infinite in old-style QED.
Afficionados could perhaps defend QED by arguing/hoping that magic mirrors are an 
unachievable idealization.  I agree they are not achievable as precisely-localized rigid
surfaces, since precise localization is forbidden by quantum uncertainty principles,
while rigidity goes against special relativity.  
But I see no obvious reason very high reflectivity must be forbidden.  
</p><p>
With the rain of bricks, no "defense" is necessary.
Energy density is bounded via the natural Nyquist UV cutoff and we therefore would
always get finite forces even with perfect rigid mirrors
– albeit these forces could be very large, e.g. of
order 1 in Planck-pressure units in the maximum scenarios, 
if mirrors capable of reflecting Planck-scale
photons were somehow possible in combination with Planck-scale mirror-spacings.
</p>

<a name="entropybds"></a>
<h3>19. Entropy and Bounds on it </h3>

<p>
The uniformity postulate <a href="#postU">IV</a>, in combination with the Nyquist theorem,
yields the consequence that in rain of bricks physics
there is an 
</p><p>
<b>Entropy bound</b>: It is impossible to store more than a constant average
density of bits of information in a volume 
(assuming there are only a bounded number of kinds 
of quantum field).
Indeed the maximum density is of order 1 using Planck 3-volume units.
</p><p><small>
Without the uniformity postulate
<a href="#postU">IV</a>, by exploiting the infinity of Fourier modes 
within each brick it
would have been possible to store any of an
unboundedly great number of mutually-orthogonal quantum field states
within only a single brick.
</small></p><p>
This volume-density bound also leads to an area-based entropy bound.
We now explain these bounds.
</p><p>
First of all, naively, in any
chunk (4-volume=Q) of 4-dimensional spacetime, there are only about 
N=&#961;<sub>rain</sub>Q
raindrops, hence only O(N) different Fourier modes are accessible for any quantum field
(given the effective bandlimiting 
arising from the Nyquist bound).  
However, only some of these modes are "self consistent."
That is, viewing the modes as indexed by 
(E;p<sub>x</sub>,p<sub>y</sub>,p<sub>z</sub>),
we know from special relativity (as well as the wave equations themselves) that
</p><center>
E<sup>2</sup>=m<sup>2</sup>+(p<sub>x</sub>)<sup>2</sup>+(p<sub>y</sub>)<sup>2</sup>+(p<sub>z</sub>)<sup>2</sup>
</center><p>
for any Fourier mode <i>allowed</i> by, e.g, the Dirac wave equation.
Thus, really, there are only a <i>three</i>-parameter family of allowed modes.
The reader might complain here that thanks to rain of bricks, we have propagation from brick to
brick using the Dirac propagator, which might not be exactly the same thing
as obeying the Dirac wave equation and then sampling at the raindrop points.
But that distinction shouldn't matter at our level of precision.  
The important thing is that the subspace of
solutions to these linear equations has appropriately lower dimension than the full space's N.
Hence instead of the naive result that the entropy in a 4-dimensional volume Q was bounded
by O(&#961;<sub>rain</sub>Q), we get the more physically sensible result that the entropy
in a <i>three</i>-dimensional volume V is bounded by
O([&#961;<sub>rain</sub>]<sup>3/4</sup>V).
</p><p>
We have freedom to choose the shape (but not the 4-volume)
"Nyquist region" of Fourier 4-space.  For convenience, let us make it a 
"pillbox," that is, the Cartesian product of a momentum-ball with an energy-interval of
equal diameter.  This choice causes our new notions of "energy" to most-resemble our old ones,
and also causes the region in position space to be an identically-shaped pillbox.
Then for a massless spin=1/2 fermion field, the information-theoretic
entropy S would be exactly 
2×2×(&#960;/6)<sup>1/4</sup>N<sup>3/4</sup> bits
where the first factor of 2 is from spin up or spin down, 
the second is for positive or negative energy,
and the remaining factor arises from the 3-volume of the momentum-ball.
<!--
N = 4pi/3 p^3 * 2p = 8pi/3 p^4, 
#modes = 4pi/3 p^3 = N/(2p) = (pi/6)^(1/4) N^(3/4)
-->
By converting this back to position-space we conclude: 
</p><p>
<b>Volume-based entropy bound Theorem:</b>
The maximum possible entropy (in bits) of a spin=1/2 massless fermion field 
in a "pillbox"-shaped region
of (1+3)-space is
</p><center>
S = 2<sup>5/4</sup> (&#961;<sub>rain</sub>)<sup>3/4</sup> V.
</center><p>
where V is the 3-volume of the spatial ball defining the pillbox-cylinder's round "face."
(For fermion fields with <i>non</i>zero rest-mass tiny compared to the Planck mass, the formula might
change exceedingly slightly.  For boson fields, in view of the postulate that at most 
a constant number of particles are allowed to reside in any one brick, an entropy bound
of the same order should apply.)
</p><p>
This is 1 bit per raindrop per spin per fermion field, provided we agree to take all the raindrops lying
in the Cartesian product of our spatial volume V, with
a time-interval
</p><center>
 &#916;t = 2<sup>1/4</sup>(&#961;<sub>rain</sub>)<sup>-1/4</sup>
</center><p>
wide.  (Note this interval-width is of order 1 in Planck time units.)
</p><p>
<b>What does entropy <i>mean</i>?</b>
With rain of brick physics, we can get a more interesting and I think more realistic answer to
this question, than was available in old-style physics.  
</p><blockquote>
<b>An old-style physics answer:</b>
If a physical system has W possible configurations, each equiprobable, its entropy is log<sub>2</sub>W bits.
</blockquote><p>
This old answer has some bad properties. 
If you measure, or are told, the state of the system, its entropy becomes zero(?!).  
But the entropy remains large
according to me (since I was not told).
Then, in view of the deterministic nature of old-style physics, it would stay zero forever,
assuming you were smart enough to know and use the laws of physics.  On the other hand
if you were less smart, it would increase above zero.
</p><p>
In rain of bricks physics, in contrast, the microscopic state of any system becomes
re-randomized on a Planck time scale (due to the Poisson-random nature of the raindrops).
So even if you were told everything about a system of "entropy S," then, its entropy
in rain of bricks physics O(1) Planck times later could <i>still</i> be &#8776;S, not zero.
This seems superior because entropy then is more about the physical system itself, not about 
"who knows what" and who is "intelligent enough" to "qualify" to be an "observer."
</p><blockquote>
<b>Rain of bricks physics' apparent answer:</b>
If the physical system <i>during a timespan
<nobr>&#916;t=2<sup>1/4</sup>(&#961;<sub>rain</sub>)<sup>-1/4</sup></nobr></i>
wide starting now,
has W possible configurations, each equiprobable, its entropy is log<sub>2</sub>W bits.
</blockquote><p>
More generally, of course, the entropy is the information content 
<nobr>&#8721;<sub>k</sub>p<sub>k</sub>log<sub>2</sub>(1/p<sub>k</sub>)</nobr>
of the probability distribution of possible states k.
</p><p>
<b>What if the raindrop locations count as part of the "state" of a physical system?</b>
The locations of all the raindrops could (and perhaps should)
be regarded as part of the "state" of the
physical system.  However, that contention cannot affect our bounds above because
it would be possible to encode 
the locations of all the raindrops by making a mental 4-dimensional Planck-unit 
hypercube grid, then for each cell say how many raindrops lie inside it, then finally 
data-compress that sequence of integers using, e.g, "arithmetic coding"
(Witten-Neal-Cleary 1987;
Bell-Cleary-Witten 1990;
use the formula for the entropy of a Poisson distribution, and use the fact arithmetic coding
will asymptotically achieve it).
This would not fully specify the locations of all the raindrops, of course, since
within each Planck-size hypercube, a raindrop still could be anywhere.  But due to the fundamental
uncertainty principles stated in 
<a href="#quantgrav">§11</a>, 
its location is not knowable much more precisely.
The total
number of additional bits of information about each raindrop location that are obtainable 
by any measurement process, is O(1).  [Also we could not always know which grid cell a
raindrop was in, but that only makes our upper bound of O(1) bits per raindrop, more true.]
</p><p>
We conclude:
all that is possibly 
knowable about the raindrops could be encoded using O(1) bits per raindrop.
This can be regarded as "background" entropy.  It just adds a big constant to the entropy of
any fixed-volume physical system.
This constant makes no real difference to anything, since in thermodynamics we always only
are concerned with entropy differences or derivatives.
</p><p>
We now explain how we can derive from our 3-volume-based bound, 
a tighter entropy bound (but not counting raindrop location info as "entropy")
instead based on
the 2-dimensional <b>area</b> enclosing it.
Consider a 3-dimensional cubical box of space with surface area&#8776;A,
which contains quantum fields with some entropy S bits.
These fields are described (to an accuracy good enough for 
our purposes in the present argument)
as linear combinations of constant-momentum modes.
The maximum allowed |momentum| is of order 1 in Planck units.
We shall work in the ultrarelativistic limit, or deal only with massless particles, so
that particle energy and |momentum| are the same.  We also shall work in Planck units.
</p><p>
To get an entropy upper bound,
the key question is:
"In how many ways can one choose a subset of the modes in the box, such that 
their total mass-energy sum is O(&#8730;A)?" 
The information-theoretic entropy is the logarithm of this count.
The reason the mass must be O(&#8730;A) is that otherwise the box would, according
to general relativity, be a black hole.  Indeed, in the below we shall assume spacetime has
approximately the flat Minkowski metric in which case
the total mass in the box would need to be
o(&#8730;A).
</p><p>
There are order A<sup>3/2</sup> modes to choose from, the vast majority of which have energy 
of order 1 in Planck units.   If we chose &#8730;A of them uniformly at random, we would
meet the energy upper bound and get entropy of order A<sup>1/2</sup>logA.
However, "uniformly at random" does not yield the maximum possible entropy.
What does is instead to choose modes at random with probabilities that <i>depend</i>
on mode energy according to a thermal distribution with temperature&#8776;A<sup>-1/4</sup>.
This yields:
</p><p>
<b>Area-based entropy bound:</b>
In rain of bricks physics,
any box in 3-space, of surface area A Planck area units, whose mass is small enough 
that the box is not a black hole and
has a spacetime metric (under general relativity)
close to that of ordinary flat Minkowski space,
will contain O(A<sup>3/4</sup>) bits of entropy.
</p><p>
Observe that this upper bound is much smaller than the "holographic bound"
discussed in <a href="#quantgrav">§11</a>.  
In other words, in rain of bricks physics there is a large <b>gap</b> between the maximum entropy
O(A<sup>3/4</sup>) of any system for which gravity is unimportant, versus a black hole
(enclosed by its event horizon),
which has Bekenstein-Hawking entropy of order A.
</p>
<a name="horizon"></a>
<h3>20. Low-tech explanation of Black Hole (and other) Horizon Entropy; and some numbers </h3>
<blockquote>
Black holes... are the most perfect macroscopic objects in
the universe;
the only elements in their construction are our concepts of space and time.
<br>&nbsp;&nbsp;<b>–</b> 
Subramanyan Chandrasekhar (prologue of his 1983 book).
</blockquote>
<p></p>
Following up on ideas by J.Bekenstein, S.W.Hawking in 1974 via quantum considerations 
in curved spacetime found that black holes
must have a nonzero <i>temperature</i> and actually radiate like a black body.  
Therefore, black holes have <i>entropy</i>.
Since then the calculations have been redone in many ways by many authors for many 
kinds of black holes.    The conclusion is
that 
<i>the entropy of a black hole is A/(4ln2) bits per Planck area unit (of its event horizon).</i>
<p></p><p>
This entropy represents an enormous amount of information, and stands in nearly total opposition
to "no hair" theorems in nonquantum general relativity claiming that black holes are 
completely characterized by only three parameters: their mass, charge, and angular momentum.
</p><p>
Two <b>major mysteries</b> immediately arose and still have not been satisfactorily answered:
</p>
<ol><li>
Where and in what form is this entropy stored? What is its "microscopic explanation"? 
</li><li>
What happens to information in, e.g,
an "encyclopedia that we toss into a black hole"?
Is the encyclopedia in principle capable of being deduced/reconstructed from measurements of 
the (later) Hawking radiation, or not?
</li>
</ol>
<p>
We shall now argue
that rain of bricks physics provides a simple microscopic explanation of
black hole entropy and resolves both mysteries.  At the end we shall try to
estimate numerical values more precisely.
</p><p>
The answer to the encyclopedia problem is <b>"no"</b> in the sense that the rain of bricks physics,
unlike old-style physics, is <i>nondeterministic</i>.   Thus we expect Hawking radiation truly
will be random, not determined by the encyclopedia. <small>This prediction 
also stands in opposition to recent string theory
claims based on the so-called "AdS-CFT correspondence."</small>
</p><p><small>
Of course, there is a larger question of 
whether quantum gravity still is "unitary."   
I do not claim to know what quantum gravity is (nor do I even contend I know
the correct QFT for nongravitational physics – our attitude in this paper is
that rain of bricks is a tool for getting rid of the foundational problems 
in a wide class of QFTs, but
the question of what those QFTs are, is left to other authors), so
we do not claim to know the answer to that.  If, however, some version of rain of bricks is
all that is needed to turn graviton-QFT into a rigorous quantum gravity theory 
(see <a href="#maxstrength">§23</a>), then 
that theory presumably indeed will be <b>unitary</b> in the sense of 
<a href="#whataboutsym">§25</a>
–
<i>but</i> the unitary transformations
that physics time-evolution applies, will involve some true randomness
because raindrops are randomly located.
</small></p><p>
Now let us turn our attention to the "microscopic explanation" problem.
As we saw last section, "entropy" in rain of bricks physics appears to 
consist of "the possibly-extractible information content in all the bricks whose time-coordinate 
is within a 
<nobr>&#916;t=2<sup>1/4</sup>(&#961;<sub>rain</sub>)<sup>-1/4</sup>-wide</nobr>
span including 'now'."
The brick locations themselves could be regarded (optionally) as part of that "information,"
which we saw (via an encoding scheme based on a hypercube grid)
would have the uninteresting effect of adding 
a large constant to entropy of any fixed-volume physical system.
This large constant could only have an interesting effect if somehow it were altered.
</p><p>
<b>Simple idea: "Inside-or-Outside Entropy":</b>
Now <i>in addition</i> to that, if there is a black hole horizon, for
all the hypercubic cells that intersect the horizon-surface, we need to know whether their raindrops
lie <i>inside</i> or <i>outside</i> the horizon, i.e. outside or inside
the observable universe – since that makes a big difference
as far as their effect on observable physics is concerned!
That is an <i>additional</i>
1 bit per applicable raindrop worth of
entropy. 
(To avoid issues arising from the precise orientation and location of the hypercubic grid, let us
agree to define this kind of "entropy" in terms of an <i>expectation value</i>
for randomly 3D-rotated and 3D-translated grids. Also, of course, no extra bits
are needed for grid hypercubes lying entirely inside or outside the surface.) Then:
</p><p>
<i>The total amount of this entropy
is a constant of order 1, times the horizon-area
(measured in Planck area units).</i> 
</p><p>
Note this kind of entropy-increase is associated
with every kind of horizon (for every kind of black hole, cosmological effect, or
accelerative effect that produces a horizon – albeit acceleration-caused "horizons"
vanish once you stop accelerating, hence their entropy seems of no real importance)
and the constant is universal.
Except:
This conclusion could be <i>criticized</i> because
horizon area is measured in "curved space" whereas rain of bricks physics has been set in 
flat (Minkowski 1+3) space.
</p><p>
The rest of this section is devoted to trying to answer
that criticism.  Essentially, we will argue that the conclusion should be valid
<i>provided</i> we restrict attention to
"nice" curved space metrics, meaning those obeying the following <b>niceness postulates:</b>
</p><ol><li>
The entries of the curved space metric tensor g<sub>&#945;&#946;</sub>
are smooth functions of coordinates <nobr>(t;x,y,z).</nobr>
The curved-space and flat-space metric tensors agree at at least one point (t;x,y,z) –
counting asymptotic limits such as <nobr>"x&#8594;+&#8734;"</nobr> as a "point" –
and these agreement-points should include the location we expect the "observer" to be.
</li><li>
The curved-space length element
ds<sup>2</sup> is independent of t, i.e. invariant under t&#8594;t+&#948;.
(In other words dt is a "Killing vector" and we shall call t a "Killing time" after 
Wilhelm Killing 1847-1923.)
</li><li>
The infinitesimal element of 4-volume is dtdxdydz.
</li><li>
Throughout any Killing time interval [t, t+&#948;] it is possible to place an 
arbitrarily-fine 3D-cubic grid in
the (t;x,y,z) Minkowski (1+3)-space, such that the curved-space distances between all 
the grid points are time-invariant, and the grid-points all move at sublight (curved space) speeds.
(It may be necessary to cover the curved-space metric with several overlapping "patches", each containing 
a grid; we demand that the entirety of the "observable universe," i.e. the part on our side of the horizon,
must be covered.)
</li><li>
If there are any "horizons" we are interested in, then the curved-space
metric is nonsingular everywhere both on them and everywhere on the observer's side of them,
and the distance to the horizon is finite.
<br>
<a name="schwdist"></a>
<b>Side-note about finite distances:</b>
Although a particle dropped into a black hole will take an infinitely long time
before entering the horizon as perceived by an external observer, 
it takes only finite time measured by its
own clock (and then only at most &#960;M more time before hitting the central singularity, in
the case of radial infall into a Schwarzschild hole).  <!--exercise 31.4 MTW derives piM.-->
For the mass=M Schwarzschild metric
<center>
ds<sup>2</sup> =
(1-2M/r)dt<sup>2</sup>
- (1-2M/r)<sup>-1</sup>dr<sup>2</sup>
- r<sup>2</sup>[d&#952;<sup>2</sup>+(sin&#952;)<sup>2</sup>d&#966;<sup>2</sup>] 
</center>
in which r is defined "circumferentially" (i.e. the circumference of the sphere r=R is 2&#960;R)
the distance from r=A to r=B is <nobr>F(B)-F(A)</nobr> where 
<nobr>F'(x)=(1-2M/x)<sup>-1/2</sup></nobr> so that
<center>
F(x) 
&nbsp; = &nbsp;
[(x-2M)x]<sup>1/2</sup> + M ln([(x-2M)x]<sup>1/2</sup> + x - M).
</center>
Hence the distance from the horizon at r=2M to r=B&gt;2M 
(or between r=A and r=B with 2M&#8804;A&lt;B)
always is <i>finite.</i>
Indeed 
<center>
F(L+2M)-F(2M)
&nbsp; = &nbsp;
(8LM)<sup>1/2</sup> [1 + (L/M)/12 - (L/M)<sup>2</sup>/160 + (L/M)<sup>3</sup>/896 - ...]
if L&lt;&lt;M
<br>
&nbsp; = &nbsp;
L + M ln(2eL) + M<sup>2</sup>/(2L) - M<sup>3</sup>/(4L<sup>2</sup>) + ...
if L&gt;&gt;M
</center>
</li></ol>
<p>
Since any plain Minkowski metric satisfies all the niceness
postulates, these can be regarded as a demand
that the curved-space metric "resemble" flat spacetime.
(General curved spacetimes do <i>not</i> satisfy these postulates, e.g.
do not possess a Killing time.)
</p><p>
It is not obvious what rain of bricks should do in general curved spacetimes.   But
for all <i>nice</i> curved spacetimes, it seems fairly obvious that 
there is a uniquely favored "horizon entropy" formula which is a universal constant of order 1
(in Planck units) times the area of the horizon, valid for large-area horizons.  
Note that "horizon area" is agreed on by all
Lorentz observers within any nice curved space.
</p><p>
<b>Purposes of niceness postulates:</b>
Postulate 3 means that 
it makes no difference 
which metric (curved or flat) we use when sprinkling our random-Poisson raindrops.
Postulate 2 means that there is a uniquely favored notion of "time"  so that the notion of "entropy
within a given time interval" can make sense.
The Killing time might not actually <i>be</i> a timelike
vector in some portions of curved space, but it corresponds by postulates 1 and 2 to 
observer-time.  
Postulate 4 means that there is a 
sufficiently-reasonable "encoding scheme"
to allow our information-theoretic entropy concepts to enjoy a clear meaning.
Postulate 5 means that a fine cubic grid in flat space, corresponds locally approximately to
a parallelipiped grid in any small chunk of curved space, and everything is well-enough behaved.
</p><p>
Please do not become fixated on any alleged need for a <i>cubic</i> grid.  
I do not believe any such need
exists.  The "encoding scheme" above was based on a cubic grid for simplicity.   
However, a different 
(and probably better for our present purposes) encoding scheme could be based on a 
Poisson-<i>pseudo</i>random
distribution of points selected by the encoder (where the "pseudo" means these random points 
actually are
deterministically chosen) in 3-space.   The 
<a href="http://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi diagram</a>
of the worldlines of such points produces a cell-decomposition of
constant number density within
both metrics, which can be used in curved-space
for encoding just as we earlier explained how to use hypercube cells.
The Voronoi-based scheme has the advantage of being isotropic.  
The important thing is that for any curved-space obeying our niceness postulates, it 
seems clear that (i) the metric is nice enough that no absurd pathology can happen, (ii)
reasonable encoding schemes exist, (iii) on top of the entropy arising from encoding all the 
raindrop
locations, there will be additional "inside or outside" entropy amounting to
a constant of order 1 times the horizon area.   Here we do not claim that "seems clear" rises to
the level of a theorem of mathematics.
</p><p>
<b>The rest of this section</b> will exhibit important specific <b>example metrics</b>
satisfying our
niceness postulates (plus some additional "even nicer" properties).  
These will demonstrate that "nice" curved spaces, while exceedingly rare,
nevertheless seem common enough to include all black holes, the simplest expanding-universe model,
and the simplest way in which "acceleration horizons" can arise.  In all of these cases,
rain of bricks provides the simplest available (by far) microscopic explanation of horizon entropy.
This provides hope that a future more careful analysis of the same type might allow deducing the
exact value of &#961;<sub>rain</sub>.  We then shall 
<a href="#attemptnums">end</a>
by trying to get some actual numbers.
</p><p>
<b>Black holes:</b>
The (it is generally believed) 
most general family of black holes, the
<a href="http://en.wikipedia.org/wiki/Kerr-Newman_metric">Kerr-Newman</a>
(charged, rotating) family of metrics which exactly solve the classical Einstein-Maxwell
(combined gravity &amp; electromagnetism) equations, satisfy all the niceness postulates if we
employ the "Kerr-Schild coordinate system" (Debney, Kerr, Schild 1969)
</p><center>
ds<sup>2</sup> =
dt<sup>2</sup>  - (dx<sup>2</sup>+dy<sup>2</sup>+dz<sup>2</sup>) 
-  <!--Wikipedia gave wrong sign-->
[dt + zdz/r + (rxdx+rydy+Jydx-Jxdy) / (r<sup>2</sup>+J<sup>2</sup>)]<sup>2</sup> 
r<sup>2</sup> 
(2Mr-Q<sup>2</sup>) / (r<sup>4</sup>+J<sup>2</sup>z<sup>2</sup>)
</center><p>
where M is the mass, Q is the charge, and J is the angular momentum per unit mass
of the hole (in Planck units), and where r&gt;0 is defined as a function of (x,y,z) by
</p><center>
r<sup>4</sup>
 - (x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>-J<sup>2</sup>)r<sup>2</sup> 
 - J<sup>2</sup>z<sup>2</sup> 
= 0
</center><p>
This hole is axisymmetric with
the z-axis as its spin axis; for large r it is asymptotically flat, i.e.
the metric becomes identical to the usual Minkowski metric.
<!-- with r=z and x=y=0 (on axis) as r->0+ we get asymptotic flatness also if Q=0.
If Q is nonzero but is small compared to J we get a small deviation from flatspace metric.
Numbers:
electron is horizonless "naked singularity" with large spin compared to charge:
electron mass=4.2*10^(-23)  electron spin=1/2   electron charge=0.085
Quarks and W boson and other leptons similar remarks (only more so).
These particles have small gravitational self energy, and the "bare" gravitational
energy associated with the point particle is finite.
   Z boson is neutral but still a naked singularity (neutrinos more so).
These particles have small gravitational self energy, and the "bare" gravitational
energy associated with the point particle is zero.
The Higgs boson is a Schwarzschild spinless neutral black hole.
It has small gravitational self energy, and the "bare" gravitational
energy associated with the horizon is infinite.
-->
There is a horizon located at
</p><center>
r = r<sub>+</sub> = M + [M<sup>2</sup>-Q<sup>2</sup>-J<sup>2</sup>]<sup>1/2</sup>
</center><p>
(We shall only consider the "astrophysical"
holes, for which M&gt;0 and r<sub>+</sub> is real; otherwise
there is no horizon and the metric describes a naked singularity.)
There is no singularity, not even a coordinate-singularity, at the horizon.
The only singularity is located at x<sup>2</sup>+y<sup>2</sup>=J<sup>2</sup>, z=r=0 
and is enclosed by the horizon.
The curved-metric 2-dimensional area A of the horizon is
</p><center>
A = 4&#960; [(r<sub>+</sub>)<sup>2</sup> + J<sup>2</sup>]
</center><p>
agreed by all observers in curved space.  This "agreement" also 
includes as a bonus the further claim that this is
invariant under Lorentz
transformations of flat (t;x,y,z)-space; the surface area is measured within a 3-dimensional subspace t=0,
where t is the time-coordinate either before or after the Lorentz transformation.
The horizon has a fixed angular velocity
</p><center>
&#937; = J / ([r<sub>+</sub>]<sup>2</sup> + J<sup>2</sup>).
</center><p>
In the spinning case J&#8800;0 there is another interesting surface, which wholy encloses the horizon, called
the "static limit."  It is defined by the condition that dt be a null (i.e. lightlike) vector,
i.e. by
</p><center>
2Mr<sup>3</sup> - J<sup>2</sup>r<sup>2</sup> = r<sup>4</sup> + J<sup>2</sup>z<sup>2</sup>
</center><p>
The region outside the horizon but inside the static limit is called the "ergosphere."
To make this coordinate system satisfy our niceness postulates, one
can use a hypercube grid in flat space rigidly <i>rotating</i> at angular speed &#937; 
within the ergosphere, and a second nonrotating grid outside the static limit.
</p><p>
The only thing that (slightly) bothers me about all this is the fact that in 
the spinning case J&#8800;0
it is necessary to make any cubic grid within the ergosphere move at superluminal speed
within at least one of the two spaces (flat or curved).  I chose to make it move at sublight
speeds in the curved space since that presumably is the physical space where the 
measurement uncertainty
principles of 
<a href="#quantgrav">§11</a>
act to constrain entropy and validate any encoding scheme –
and since only this delivers the right 
entropy=area formula!
This whole issue does not arise in
the spinless J=0 case, since then there is no ergosphere.
A related worry is the fact that we in the rotating case needed to employ <i>two</i> grids;
does this mean there should be some sort of additional surface-entropy on the surface
separating the two grids?  Both these <b>worries can be assuaged</b>, as we'll now explain.
</p><p>
First:
Instead of using two grids, one could use three (or more) layers, each with its own
grid, and each rigidly
rotating at a different angular speed, so long as the innermost layer (touching the horizon) 
rotates at the same angular speed as the horizon, while the outermost layer does not rotate.
That would allow all the grids to move at sublight speeds with respect to the neighboring
grids.  Then using the pseudorandom-based encoding scheme, there would seem to be
nothing special about the intergrid separation surfaces at any given value of the Killing time,
hence no reason to ascribe to them any surface entropy.
</p><p>
Second:
Another idea is to use the pseudo-random-based encoding scheme but now with
an angular velocity for each pseudorandom point that <i>depends</i> continuously on
x<sup>2</sup>+y<sup>2</sup> in a suitable monotone-decreasing-to-0
manner.   (This is <i>not</i> rigid rotation.  This idea is something
like the multiple-layers-of-grids idea, but with an infinite number
of infinitesimally-thin layers.)    
The point of this is that dt is not the only Killing vector.  
Since the Kerr metric is axisymmetric, xdy-ydx also is a Killing vector.
Linear combinations (with coefficients in the linear combination depending arbitrarily on 
x<sup>2</sup>+y<sup>2</sup>)
of these two vectors also are Killing vectors in the weakened sense
that the resulting map yields the same metric back again <i>but</i> with
its points relabeled.  This idea enables us to construct a (weakened definition)
Killing time which actually globally <i>is</i> timelike in the curved metric.
</p><p>
For me, those two arguments wholy assuage both my remaining worries and convince me that:
</p><p>
For a large Kerr-Newman black hole, every Lorentz observer agrees on the surface area
and also agrees that the entropy of this "raindrop inside or outside" sort,
is a constant of order 1 times that area.
</p>
<a name="newdesitcoord"></a>
<p>
<b>de Sitter "expanding universe":</b>
An exact solution of the Einstein vacuum equations 
in the presense of a "cosmical constant" &#923;,
is the "de Sitter metric":
</p><center>
ds<sup>2</sup> =
(1-H<sup>2</sup>r<sup>2</sup>)dt<sup>2</sup>
- r<sup>2</sup>(d&#952;<sup>2</sup>+sin<sup>2</sup>&#952;d&#966;<sup>2</sup>) 
- dr<sup>2</sup>/(1-H<sup>2</sup>r<sup>2</sup>) 
</center><p>
where 3H<sup>2</sup>=&#923; and there is a "cosmological horizon" at r=H<sup>-1</sup>
(whose distance from the origin is &#960;H<sup>-1</sup>/2) 
<!-- MAPLE:
int( (1-K*r^2)^(-1/2), r = 0 .. K^(-1/2) );
-->
with area=4&#960;H<sup>-2</sup>.
(The "observable universe" lies inside it.  Photons that start outside the horizon cannot reach
the origin because the universe expands too fast.)
Again all Lorentz observers agree about the area of the horizon, although not necessarily 
about its location.
This metric satisfies all the niceness postulates <i>except</i> the last,
because the horizon is a metrical (though not genuine) singularity.  
An (apparently new) recoordinatization of de Sitter space that avoids that criticism is
</p><center>
ds<sup>2</sup> =
dt<sup>2</sup>  
- (dx<sup>2</sup>+dy<sup>2</sup>+dz<sup>2</sup>)
- [rdt + xdx + ydy + zdz]<sup>2</sup> H<sup>2</sup>
&nbsp;
where
&nbsp;
r<sup>2</sup>=x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>
</center><p>
As before, 3H<sup>2</sup>=&#923; and the horizon is at r=H<sup>-1</sup> 
with area=4&#960;H<sup>-2</sup>.
To verify that these new coordinates indeed represent de Sitter space, I 
computed that
det(g)=-1
with Ricci scalar curvature
R=12H<sup>2</sup>
and 
R<sub>&#956;&#957;</sub>=3H<sup>2</sup>g<sub>&#956;&#957;</sub>
everywhere.
<!--
grtw();
makeg(dsit);
2;
[t,x,y,z];
- d[t]^2 + (d[x]^2+d[y]^2+d[z]^2) + H^2 * (sqrt(x^2+y^2+z^2)*d[t]+x*d[x]+y*d[y]+z*d[z])^2;
{};
0;
grcalc(Ricciscalar); 
grdisplay(Ricciscalar);  #result Ricciscalar=12*H^2.
grdisplay(detg);  #result -1.
grdef ( `FF{a b} := R{a b} - K*g{a b}` ):
grcalc(FF(dn,dn));
grdisplay(FF(dn,dn));
grdef ( `MM{a b} := R{a b} - 3*H*H*g{a b}` ):
grcalc(MM(dn,dn));
grdisplay(MM(dn,dn));  #all components zero
--
grtw();
makeg(dsit);
2;
[t,th,ph,r];
- d[t]^2*(1-K*r^2) + d[r]^2/(1-K*r^2) + r^2*(d[th]^2+sin(th)^2*d[ph]^2);
{};
0;
grcalc(Ricciscalar); 
grdisplay(Ricciscalar);  #result Ricciscalar=-12K if +--- and +12K if -+++. 
#WARNING???!!! GRTW evidently fucks up the sign of R if switch metric signature.
grdisplay(detg);  #result -sin(th)^2*r^4.
-->
</p><p>
<b>"Horizons" perceived by accelerated observer:</b>
A recoordinatization of the usual Minkowski (1+3)-spacetime by W.Rindler is
</p><center>
ds<sup>2</sup> =
(3xg)<sup>2/3</sup> dt<sup>2</sup> 
- (3xg)<sup>-2/3</sup> dx<sup>2</sup> - dy<sup>2</sup> - dz<sup>2</sup> 
</center><p>
and represents flat space as seen by an accelerating observer
(constant acceleration=g as perceived by that observer).  
Ordinary Minkowski space (T;X,y,z)
can be recovered via the coordinate transformation
<nobr>X=(g<sup>-2</sup>3x)<sup>1/3</sup>cosh(gt),</nobr>
<nobr>T=(g<sup>-2</sup>3x)<sup>1/3</sup>sinh(gt).</nobr>
<!--
The x used in http://en.wikipedia.org/wiki/Rindler_coordinates we shall call q.
x = x(q)
x'(q) = g^2 q^2
x(q) = g^2 q^3 / 3
q = (3x/g^2)^(1/3)
g^2q^2 = (3xg)^(2/3) 
-->
There is a "horizon" at x=0 caused by the fact that if an accelerated observer has a 
sufficiently large head start on a photon, then the photon will never be able to catch up.
(For 1 Earth-gravity acceleration, the necessary head start is roughly 1 light year.)
Hence only the part x&gt;0 of the Minkowski-space "universe" will ever be visible to such an 
observer.
This again suffers from the defect (for our purposes) that the horizon is a metrical singularity.
But again a new nowhere-singular form of the Minkowski flat space metric, 
obeying <i>all</i> niceness-postulates
</p><center>
ds<sup>2</sup> =
dt<sup>2</sup> 
- (dx<sup>2</sup>+dy<sup>2</sup>+dz<sup>2</sup> )
- Kx (dx+dt)<sup>2</sup>  
</center><p>
with det(g)=-1 everywhere, comes to the rescue.   This has a horizon at x=1/K.
<!--
grtw():
ds^2 = dx^2 - dt^2 + A(x) * (dx+dt)^2  has scalar curvature R=A''(x) and detg=-1.
So any A(x)=linear(x) function yields Minkowski space.  Simplest is
ds^2 = dx^2 - dt^2 - K*x*(dx+dt)^2  has scalar curvature R=0.
A surface with dt=null is x=-1/K.   |x|<1/K is the region with x spacelike and t timelike.
Photon geodesics:
 dsolve( 1 - diff(t(x),x)^2 = K*x*(diff(t(x),x)+1)^2, t(x) ); 
 t(x) = c1-x 
 t(x) = c1-x + (2/K)*ln(1+Kx)   horizon at x=-1/K.
-->
</p><p>
<b>Unified super-family of nice metrics</b>
There are known multiparameter families of stationary
Einstein-Maxwell(&#923;) exact solutions allowing varying 
mass M, charge Q, spin J, acceleration g, and cosmical constant &#923; (and
it also is possible to put in other parameters, e.g. magnetic-monopole "charge") and which include
all three of the above metrics as special cases.  
(See Stephani et al. 2003.)
I have not explicitly checked, but suspect, that these (or a goodly chunk of them)
may be rewritten in a form
obeying our niceness postulates.
</p>
<!--
http://arxiv.org/pdf/gr-qc/0511091v1
This does not do Kerr-Schild.
-->
<a name="attemptnums"></a>
<p>
<b>Attempt to get more precise numbers:</b>
The reader is probably annoyed by our continual use of phrases such as "order 1 in Planck units"
and wants an actual number.   It feels as though a more-precise
examination of the entropy of a large Schwarzschild black hole ought to enable us
to deduce the <i>exact</i> value of &#961;<sub>rain</sub>.   Unfortunately
I presently am unsure
how to do that.  But I will now deduce 
an upper bound on &#961;<sub>rain</sub> which probably is decently tight.
</p>
<p>
<b>Lemma: The "which side" entropy of standard normal deviate:</b>
Let x be a random normal real number with variance=1 and mean=&#956;,
where &#956; is uniformly distributed on the real interval [-N, N].
Suppose there are 2N such &#956;'s 
(all independently sampled)
with their
2N corresponding x's.
Then the average amount of extra information about all the x's 
(beyond, for each x, the amount we already knew from its &#956; value) 
that we get by being told whether each x&gt;0, is:
</p><center>
2.60607648966845742760244189229 &nbsp; bits
</center><p>
in the limit N&#8594;&#8734;
(and this number of bits counts all the information about all the x's <i>together</i>).
This ought to be accurate to all decimals stated although I have only proven 8 decimals.
</p><p>
<b>Proof:</b>
The probability density for 
<nobr>z=x-&#956;</nobr> 
is 
<nobr>(2&#960;)<sup>-1/2</sup>exp(-z<sup>2</sup>/2)</nobr>
and the corresponding cumulative probability
function is
<nobr>F(x)=[1+erf(2<sup>-1/2</sup>z)]/2.</nobr>
Defining Shannon's entropy function
<nobr>
H(P)=(-1/ln2)[Pln(P)+(1-P)ln(1-P)]
</nobr>
for 0&#8804;P&#8804;1 
we have that the expected amount of extra entropy (in bits) is
</p><center>
&#8747;<sub>-&#8734;&lt;x&lt;&#8734;</sub> H(F(x)) dx &#8776; 2.606 bits.
</center>
<p>
whose value is the number stated.  (The accuracy claim may be shown using 
numerical integration with |derivative| and tail bounds.  David J. Broadhurst tells
me he has independently confirmed my numerical value to all decimals,
and indeed obtained 1000-decimal accuracy.)
<b>Q.E.D.</b>
<!--
P := (z) -> exp(-z*z/2) / sqrt(2*Pi);
F := (x) -> (1/2)*erf(sqrt(1/2)*x)+1/2;
H := (x) -> -(x*ln(x) + (1-x)*ln(1-x))/ln(2);
simplify(H(F(z)));
evalf(int( %, z = -infinity .. infinity ), 30);
2.606076490
2.60607648966845742760244189229
-->
</p><p>
<b>Corollary: The "which side" entropy of normal(&#963;) deviates in 3-space:</b>
Let points be sprinkled Poisson randomly in 3-space at some density &#961;<sub>3D</sub>.
Suppose for each point we are told an estimate of its location and that 
the point is normally-distributed with standard deviation &#963; (in each coordinate) 
centered at the estimate.  Then the amount S of additional entropy we would get by
(further) being told for each point whether or not it is inside some smooth surface
of area A  (in the limit of scaling the surface so A&#8594;&#8734;) is
</p><center>
S &nbsp; = &nbsp; 2.60607648966845742760244189229 A &#961;<sub>3D</sub> &#963; + o(1) &nbsp; bits.
</center><p>
<b>Putting it all together:</b>
We know from the min-variance Gaussian 
<a href="#minvargauss">result</a> in 
<a href="#quantgrav">§11</a>
that &#963;&gt;0.141968,
and from the formula for Bekenstein-Hawking entropy that S=A/(4ln2) bits,
and finally we believe or suspect from 
<a href="#entropybds">§19</a>
that the relevant 3D density &#961;<sub>3D</sub> 
arises from the 4D
density &#961;<sub>rain</sub> via
</p><center>
&#961;<sub>3D</sub> 
&nbsp; = &nbsp;
(&#916;t)&#961;<sub>rain</sub> 
&nbsp; = &nbsp;
2<sup>1/4</sup> (&#961;<sub>rain</sub>)<sup>3/4</sup>
</center><p>
in Planck units. 
From this all we deduce
<!--
solve({r3=2^(1/4)*rr^(3/4), S=A/(4*ln(2)), sg=0.141968, S=2.6060764896684574*A*r3*sg}, rr ); 
rr=2.0^(-1/3)*(4.0*ln(2.0)*2.6060764896684574*0.141968)^(-4/3);
-->
</p><center>
<b>
&#961;<sub>rain</sub> 
&nbsp; &#8804;</b> &nbsp;
2<sup>-1/3</sup>[4×ln(2)×2.60607649×0.141968]<sup>-4/3</sup>
&nbsp; &#8776; &nbsp;
<b>0.76720.</b> 
</center><p>
in Planck units, and 
<nobr>&#961;<sub>3D</sub>&#8804;0.97485.</nobr>
From that in turn we deduce, by considering a ball-shaped Nyquist region,
that the maximum possible momentum is (see <a href="#nyquist">§17</a>)
</p><center>
<b>
|p|<sub>max</sub>
&nbsp; &#8804;</b> &nbsp;
(6&#960;<sup>2</sup>&#961;<sub>3D</sub>)<sup>1/3</sup>
&nbsp; &#8776; &nbsp;
<b>3.8649</b> &nbsp; Planck momentum units.
</center>
<!--MAPLE:
4*Pi/3 * pmx^3 = (2*Pi)^3 * rho3d;
solve(%, pmx);
subs(rho3d = 0.97485, %);
-->
<p>
One could (I do not, but one could)
further fantasize that bricks are 3-dimensional and that the 
"most natural" choice of their size 
then is such that the 3-volume of a brick is
1/&#961;<sub>3D</sub>&#8805;1.0258 Planck 3-volume units
(causing the "3-volumes of the rain of bricks and old-style universes to be the same").
</p>

<a name="bulkentropy"></a>
<h3>21. If surface entropy (of horizons) is so important, why isn't <i>bulk</i> entropy even more important? </h3>

<p>
At first, it seems obvious that the "bulk entropy" (of the information involved in encoding all
the raindrop locations in a given 4-volume) 
should vastly outweigh all other kinds of entropy.  However
a deeper look leads me to conclude that actually this sort of entropy 
has essentially no physical importance!
</p><p>
Not all entropy necessarily matters physically.   Some form of 
information decoupled from the rest of physics would not matter.
In the case of black hole "inside or outside" entropy it is obvious 
that the raindrop locations near the horizon <i>matter</i>, i.e. tremendously affect 
(far more than anything else) the
nature of the Hawking radiation, and thus associating that entropy with the Hawking
temperature was an entirely reasonable thing to do.
</p><p><small>
The reader who 
doesn't find it obvious is advised to revisit 
the original derivations of Hawking radiation; Wald 1984 reviews the topic
in his ch.14.
They begin by writing 
wave equations for spherical waves in the Schwarzschild geometry
as a 1-dimensional ordinary differential equation
(after factoring out an arbitrary spherical harmonic angular dependency)
employing the Regge-Wheeler "tortoise coordinate"
<nobr>
x=r+2Mln([2M]<sup>-1</sup>r-1)
</nobr>
where r is the Schwarzschild (circumferential) radial coordinate.
Then x ranges from -&#8734; at the horizon r=2M, to +&#8734; when r&#8594;+&#8734;.
The important point for us is that
using this coordinate immensely magnifies
the region very near to and just outside the horizon (any such region, no matter how small,
corresponds to an infinitely-wide interval of the negative x-axis) –
just where our raindrops are.
Then it is found (precisely as a consequence of their using x and not some other coordinate)
that the equation is
mathematically identical to that describing
relativistic quantum particles tunneling through a certain
1-dimensional potential barrier, where all
outgoing-wave solutions (flowing from x=-&#8734; to x=+&#8734;) correspond to massless
and all ingoing-wave solutions to massive particles (unless the wave equation was for a 
massless field, in which case both are massless)
and... ultimately they find that the outgoing wave spectrum must be determined by the 
microscopic details of the quantum state near the black hole horizon
and is exactly thermal with temperature=(8&#960;M)<sup>-1</sup>
in Planck units.
</small>
</p><p>
But for "bulk entropy," what sort of "radiation" or "temperature" do we get?  And 
how can we "change the volume of space" (if we cannot, then this kind of entropy
is just a constant and hence does not matter)?
</p><p>
The answer to all these questions is this.   The volume of the universe can change 
(expanding universe).  In rain of bricks physics,
there is an entropy density of order 1 bit per Planck volume
unit, i.e. about
<nobr>2×10<sup>134</sup> bits/meter<sup>3</sup>,</nobr>
associated with that.
There also is an <i>energy</i> density 
<nobr>&#961;<sub>&#923;</sub>&#8776;+7.035×10<sup>-27</sup>kg/meter<sup>3</sup>,</nobr>
known as the "Einstein cosmical constant,"
associated with it.
In view of the definition
<nobr>k<sub>B</sub>T=&#8706;E/&#8706;S</nobr>
of temperature T in terms of energy E and information-theoretic (in nats) entropy S
of a physical system, this means that there 
is an
<b>intrinsic temperature floor</b> of order
</p><center>
T<sub>RainBricks</sub> &#8776; +4×10<sup>-138</sup> Kelvin
</center><p>
in the rain of bricks universe.  
<!--REDO NUMBERS USING LATEST EXPERIMENT VALUES?  Pointless since do not know ent density or 
raindrop density very precisely-->
 It is not possible to refrigerate colder than that.
This temperature is another (in principle experimentally detectable)
way in which rain of bricks physics differs from previous 
physical theories.
</p><p>
But the trouble is that this temperature is so <i>small</i>.   This is far smaller than 
black hole Hawking temperatures (even for a galactic-mass hole)
or the Gibbons-Hawking cosmological-horizon temperature, and <i>far</i> smaller than
the  <a href="http://arxiv.org/abs/hep-ph/9602417">current temperature</a>
(&#8776;2.725 Kelvin) of the universe.
</p><p>
<small>
It actually is so small that I believe this temperature is too low for
humans to reach.   Carnot's efficiency law for refrigerators and quantum uncertainty principles
(such as energy×time) both limit the performance of refrigerators.
I believe that their combination will prevent any human from ever cooling a Planck-mass
(or greater) object below about 10<sup>-40</sup> Kelvin.  
<br>
In 1999 a group at the University of Helsinki cooled
an 0.6-gram chunk of rhodium metal to 10<sup>-10</sup> Kelvin.
That is colder than the Hawking temperature of a 1-solar-mass black hole,
T=M<sub>pl</sub>c<sup>2</sup>/(8&#960;k<sub>B</sub>M<sub>sun</sub>)&#8776;6.2×10<sup>-8</sup> Kelvin.
<!--Maldecena calculated this wrong.  I have recalculated based on wikipedia and based on
Bekenstein, all agree.
Thawking = (planck mass)^2 * c^2 / (8*pi*kB*M)
where M is black hole mass.
planck mass = 2.2*10^(-8) * kg 
mpl^2 / (8*pi) = 1.9*10^(-17) kg^2
kB = 1.38 * 10^(-23) m^2 kg / (sec^2 Kelvin)
kB/c^2 = 1.54 * 10^(-40) kg/kelvin
Thawking = 1.227*10^23 kelvin / (M in kg)
confirming wikipedia number-->
The Gibbons-Hawking temperature of a cosmological horizon based on the current Hubble constant
H&#8776;71 km/second/megaparsec=2.3×10<sup>-18</sup>second<sup>-1</sup> is
<!-- H=75 km/sec/megaparsec=???
T=3.86*10^(-53) * joules / (Boltzmann constant)
-->
T=&#8463;H/(2&#960;k<sub>B</sub>)=2.8×10<sup>-30</sup> Kelvin.
</small>
</p><p>
Further, the fact that some spacetime volume may contain some much hotter stuff seems
<i>irrelevant</i> in the sense that there is no way for that heat to cause the volume
of space to expand (the raindrops are generated by a Poisson process which <i>ignores</i>
everything else that is going on).   The raindrop locations do affect observable physics
but there is no way for that physics to "back react" to affect the raindrop locations.
So in view of both the complete lack of back reaction and the smallness of 
T<sub>RainBricks</sub> and consequent smallness of any "forward" heat transmission,
the effect of this kind of entropy on thermal physics seems very small indeed.
</p><p>
I therefore make the 
</p><p>
<b>Vague conjecture of the physical meaninglessness of "bulk" vacuum entropy:</b>
The only physically-important way for this sort of bulk entropy to matter, is
indirectly by means of surface entropy.
<!-- (For what arguably is a counterexample, see &sect;???.)-->
</p><p>
Because it is not clear to me
what quantum gravity is, nor how best to incorporate gravitons
into the rain of bricks framework (if that is what it is)
this <i>is</i> a conjecture.
</p>

<a name="crudenumer"></a>
<h3>22. Some crude numerical calculations of (now finite) values of the QED infinities </h3>

<p>
<!-- 
We consult EQ 7.29 in &sect;7.1 of Peskin &amp; Schroeder???
One may also employ the slightly different 
EQ 5.87 in &sect;5.3 of Greiner &amp; Reinhardt &ndash;
the difference only affects the 40th decimal place.
This calculation was first done
by V.Weisskopf 1939, see his EQ 26, and also redone by Feynman 1949, see EQ 20 in his
appendix A <i>except</i> the "1" in our formula is "3" 
in Feynman's formula, presumably because his
&Lambda; is exp(2)&asymp;7.39 times ours???; 
Suraj N.Gupta in EQ 47.18 his 1977 book
<i>Quantum electrodynamics</i> gives yet another derivation of another
equation which the same as ours <i>except</i> Gupta adds 1/4 to our logarithm.
Andrey G. Grozin:
Lectures on QED and QCD: Practical Calculation and Renormalization of One- 
and multi-Loop Feynman diagrams, Wrld Scientific 2007
gives the Zm remormalization at 1-loop as EQ 2.91 agreeing us.
-->
<b>1.</b>
The QED<sub>2</sub> prediction for the electron
mass is
</p><center>
m<sub>e</sub> &#8776;
 m<sub>bare</sub> · 
[1 
+ (3/2)(&#945;/&#960;) ln(&#923;/m) 
+ (3/8)(&#945;/&#960;)
- (19/64)(&#945;/&#960;)<sup>2</sup> ln(&#923;/m)<sup>2</sup>
+ (7/24)(&#945;/&#960;)<sup>2</sup> ln(&#923;/m)]

<!--The following is from  EQ 7.29 in &sect;7.1 Peskin+Schroeder:
&asymp;
 m<sub>bare</sub> &middot; (1 + 3&alpha;/(2&pi;) &middot; [ln(&Lambda;/m) + O(1)])
so it appears that Feynman's e^2 is P+S's alpha.
-->
</center><p>
where m<sub>e</sub> is the observable "dressed" <b>mass of the electron</b>, consisting of its
"bare mass" plus the mass-energy of all the photon- and electron-field 
"baggage" and "clothes" it carries around with it (meaning
all the Feynman diagrams in the electron mass class).
Here "&#923;" is a UV energy cutoff which they assumed to be magically imposed.
The (&#945;/&#960;)<sup>1</sup> 
terms are from EQ 9 of Feynman 1948, and also were derived in a different way in
EQ 21 (and appendix A) of Feynman 1949; 
and also found somewhat later by Schwinger (1949, QED II paper).
The 
(&#945;/&#960;)<sup>2</sup> terms
are from EQ 19 of Frank 1951, who used the same cutoff convention as  Feynman.
Actually in unrenormalized QED, since it is scale-invariant, no finite UV cutoff is possible,
so &#923;=&#8734; and a logarithmically-infinite m<sub>e</sub>/m<sub>bare</sub>
is predicted.
However, with the rain of bricks, there is a natural UV cutoff.  
If we take &#923;=M<sub>pl</sub>/100 and 
&#923;=100M<sub>pl</sub> as safe-looking guesses for the lower and upper bounds
on the energy cutoff imposed by the rain of bricks,
I compute
</p><center>
m<sub>e</sub>/m<sub>bare</sub>  &nbsp; &#8776; &nbsp; 
1+0.16349+0.00087-0.00008+0.00007 = 1.16436
<br>
&nbsp; and &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
1+0.19558+0.00087-0.00009+0.00009 = 1.19645
</center><p>
<!--
al = 1/137.035999679
mr = 2.38952* 10^22
 F := (L) -> [1, 3*(A/Pi)/2 * L, 3*(A/Pi)/8, -19 * (A/Pi)^2 / 64 * L, 7*(A/Pi)^2/24 * L];
F(ln(mr*0.01));
1.+0.1634897695+0.0008710572955-0.00007516028254+0.00007384168109=1.164359509
F(ln(mr*100.0));
1.+0.1955807061+0.0008710572955-0.00008991327827+0.00008833585234=1.196450186
-->
respectively.  Observe that we get a reasonable-looking
prediction accurate to few percent despite 
making essentially no attempt to overcome our huge ignorance about the precise nature of
the bricks; and it is reassuring that all the higher order corrections are tiny.
The reason this was so successful is that
<i>logarithmic</i> infinities are quite insensitive to
the precise value of any enormous cutoff.
</p><p>
<b>The reason I stopped</b> at order &#945;<sup>2</sup>
was that literature searches for "self energy" and the like surprisingly
revealed no calculations to higher order after 1951.  However, I've recently realized 
that such calculations <i>have</i> been done under other names.
Shirkov &amp;  Kovalev 2001
explain the connection in the "Bogoliubov-Shirkov renormalization picture"
between "photon self energy," and
"charge renomalization" and the "beta function" of QED.  As they also explain, there is another
function &#947;, the "gamma function" of QED, which is similarly related to
the electron propagator and hence the electron self-energy.  And then if you by 
an unbelievable mental leap get the notion that &#947; and &#946; should be
called "anomalous dimensions," then you can search the literature under <i>that</i> term 
to find out that, e.g. the QED and QCD &#947; functions have been computed up to 
&#945;<sup>4</sup> order
by Chetyrkin 1997, e.g.
</p><center>
&#947;<sub>QED</sub> =
- (3/4)(&#945;/&#960;) 
+ (11/96)(&#945;/&#960;)<sup>2</sup> 
- [719/3456 + 3&#950;(3)/4](&#945;/&#960;)<sup>3</sup>
+ [119165/165888 + 461&#950;(3)/288 + 15&#950;(5)/8 - 3&#950;(4)/8](&#945;/&#960;)<sup>4</sup>
- ...
<!-- 3*Zeta(4)/8-119165/165888-461*Zeta(3)/288-15*Zeta(5)/8  =  -4.180840247 
so the signs alternate in the gamma series.
gamma =
- (3/4)*x 
+ (11/96)*x^2
- (719/3456 + 3*Zeta(3)/4)*x^3
+ (119165/165888 + 461*Zeta(3)/288 + 15*Zeta(5)/8 - 3*Zeta(4)/8)*x^4
- ...
and hence
1/gamma =
-1.333333333*x^(-1) - 0.2037037037 + 1.941477105*x - 6.834621110*x^2 +
-->
</center>
<p>
<b>2.</b>
According to EQ 5.45 in §5.2 of
Greiner &amp; Reinhardt
<!-- one may also employ the slightly different 
equation between EQs 7.90 and 7.91 in &sect;7.5 of
Peskin &amp; Schroeder; 
Peskin & Schroeder also derive running alpha
from beta function as EQ12.88 and EQ12.89 p424 and EQ7.96p255;
 Gupta EQ47.18 gives 
deltae/e = -alpha/(12pi) * (-ln(2)/2 + ln(cutoff/k))
-->
(one of the original calculations of this was by Feynman 1949,
see the bottom left of his page 780 after EQ 33; 
and this was known even earlier by Dirac 1933)
the observed <b>electron charge e</b> is predicted by low-order QED
to be
</p><center>
e<sup>2</sup> &#8776;
 (e<sub>bare</sub>)<sup>2</sup> · [1 - (2/3)(&#945;/&#960;) ln(&#923;/m)].
</center><p>
This  calculation has been taken to higher order, see
de Rafael &amp; Rosner 1974,
Kinoshita, Kawai, Okamoto 1991,
and
Nigam 1999:
</p><center>
Z<sub>3</sub> 
&nbsp; &#8801; &nbsp;
(e / e<sub>bare</sub>)<sup>2</sup> 
&nbsp; &#8776; &nbsp;
1 
+ [5/9 - 2L/3] (&#945;/&#960;)
+ [5/24-&#950;(3) - L/2] (&#945;/&#960;)<sup>2</sup> 
+ [a<sub>3</sub> + (47/48 - 2&#950;(3)/3)L - L<sup>2</sup>/6] (&#945;/&#960;)<sup>3</sup> 
+ ... 
</center><p>
where L=ln(&#923;/m) and 
<nobr>
a<sub>3</sub>
=
5&#950;(5)/2+(24ln2-23)&#950;(2)/12-1703/1728-173&#950;(3)/288
&#8776;
0.012290603310227
</nobr>
(completely contradicting, without comment, the earlier claim that
a<sub>3</sub>&#8776;-4.1458 by Nigam &amp; Acharya 1993, but this a<sub>3</sub>
discrepancy will only affect the last decimal of our result). 
Again, this with infinite &#923; yields insane results, but
if we take &#923;=M<sub>pl</sub>/100 and 
&#923;=100M<sub>pl</sub> 
I compute  
<!-- first: L=46.922704658, second: L=56.133045030
zeta(3) = 1.20205690315959428539973816151144999076498629234049 
alpha/pi = 0.00232281946593603037
-->
</p><center>
Z<sub>3</sub> 
 &nbsp; &#8776; &nbsp; &nbsp; 
1
+ 0.00129046
- 0.07266198
- 0.00000536
- 0.00012659
+ 0.00000000
+ 0.00000010
- 0.00000460
=
0.92849203
<br>
&nbsp; and &nbsp; &nbsp; 
1 
+ 0.00129046
- 0.08692462
- 0.00000536
- 0.00015143
+ 0.00000000
+ 0.00000013
- 0.00000658
=
0.91420258
</center><p>
respectively. 
<!--
 Perhaps profoundly (but more likely this is a delusion!)
q<sub>pl</sub>=&alpha;<sup>-1/2</sup>e&asymp;e<sub>bare</sub>7&radic;3.
al = 1/137.035999679
mr = 2.38952* 10^22
pi = 3.14159265358979323844
1 - (2*al)/(3*pi) * l( 0.01*mr )
1 - (2*al)/(3*pi) * l( 100*mr )
-->
</p><p>
Renormalized QED includes three different "infinite constants" – 
which after renormalization
become finite – arising from the three different fundamental types of 
logarithmically-infinite Feynman diagrams.
They are traditionally called 
Z<sub>1</sub>, Z<sub>2</sub>, and Z<sub>3</sub>, where we just estimated the lattermost.
Källen 1952 claimed (and I believe this Källen result
is undisputed, unlike his <a href="#kalleninf">other</a>) his nonperturbative QED formulation shows
0&#8804;Z<sub>3</sub>&lt;1
[see also Källen 1972 page 215, Banks 2008 pages 178-180,
and discussion of "Källen's theorem"
by Ambjørn &amp; Hughes Nuclear Physics B 197,1 (1982) 113-131
about that], in agreement with us.
<!--
The next-order QED correction to Z<sub>3</sub> beyond the approximation we used,
was computed by Gross 1999 pages 560-568, see especially his EQ 16.87 and 16.114.  
It does <i>not</i> involve a squared logarithm, only
the usual unsquared log, and with a coefficient 
3(&alpha;/&pi;)/8&asymp;0.000871 times ours.   
Thus it is safe for us to neglect it.
Franz Gross:
Relativistic quantum mechanics and field theory,
p559-568
does 4th order Z3.
2nd order, EQ16.87 p560: 
2*alpha/(3*pi) * [1/eps - ln(LAMBDA/m)]
4th order, EQ16.114 p568:
2*alpha/(3*pi) * [1/eps - ln(LAMBDA/m) + alpha*3/(8*pi)/eps - alpha*3/(8*pi)*ln(LAMBDA/m)]
note that we do not get a squared log.
The correction is alpha*3/(8*pi)=0.000871 of the ln term from the first correction.
Gross EQ11.137p362 relates the electron self energy to the
vertex-correction.
-->
However, there are <b>really only two infinities</b> because of the identity 
Z<sub>1</sub>=Z<sub>2</sub> 
(which is EQ 7.70 in §7.4 of Peskin &amp; Schroeder 
and
EQ 5.103 in §5.4 of Greiner &amp; Reinhardt).
So we have effectively computed them both.
</p>

<a name="maxstrength"></a>
<h3>23. How do we make it strong enough to handle graviton QFTs? </h3>

<p>
What if we put gravitons in our QFTs?
Can we, by simply putting gravitons and rain of bricks in a pot and stirring,
get a QFT theory that includes gravity?
This section will provide some initial discussion of this question.
</p><p>
The fact that the infinities are only 
<i>logarithmic</i> in renormalizable QFTs is for rain of bricks
both good and bad.  We saw the good aspects last section.
The bad aspect is that it enormously <i>weakens</i> our ability to deduce 
rain of bricks parameters from observable physics.
An obvious idea to overcome that is to consider 
gravitons, which yield
<i>non</i>renormalizable QFTs (Goroff &amp; Sagnotti 1986).
</p><p>
One might naively expect, therefore, that
with gravitons QFTs would suffer
power law infinities (not mere logarithmic ones) 
at each level of perturbation theory, with the severity increasing with level.
This might naively have been expected to yield (with a Planck scale cutoff 
of those integrals caused by rain of bricks) 
self-energies of order 1 in Planck units at each level of
perturbation theory (if indeed the infinities can be finitized at all).  
Either one of these naive expectations would yield a
huge disagreement with experiment.
On the bright side, the graviton power-law infinities,
when and if they are finitized by rain of bricks, ought (as a further naive expectation)
to powerfully determine
rain of bricks parameters, <i>without</i> enormous weakening.
</p><p>
However, the deeper examination below will indicate
that all those naive expectations were wrong:
</p><ol type="a">
<li>
It looks plausible that at least some rain of bricks 
flavors should be able to finitize graviton QFTs at every level of perturbation theory.
</li><li>
Gravity then will yield extremely small mass corrections, fully in agreement with experiment.
</li><li>
The smallness of all the gravitational effects I can think of
(e.g. see our running coupling constants <a href="#runcoupconstplot">plot</a>
to see that the gravitational force is enormously weaker than all the other forces until 
energies approach the Planck energy scale) then
makes them almost useless for determining rain of bricks
parameters.
</li></ol>
<p>
<b>Getting rid of infinities: Four successive ideas of increasing power</b>
</p><p>
<b>1.</b>
It is possible to consider using <i>both</i> rain of bricks <i>and</i> noisy distances.   
That is, make
the distances within any single brick be "noisy."
As was already discussed in 
<a href="#noisydists">§9</a>,
if this is done we can, by making the noise be high-enough-dimensional,
get arbitrarily enormous strength for getting rid of UV infinities in QFTs.
That is, we can get rid of "power law" infinities for as large a 
fixed power as we desire.   
</p><p>
However, that by itself would not be good enough for the purposes
of regularizing unrenormalizable QFTs like gravitons, because these QFTs contain
(assuming naive power counting works) power-law infinities
of unboundedly great degree.
</p><p>
<b>2.</b>
We now make the more interesting claim
that with finite bounded D,
i.e. <i>finite-dimensional noise</i>, we can get rid
of all the infinities, even in graviton QFTs!
The reason is that in graviton QFTs the degree of the power law infinity, is bounded
by a linear function of the number of vertices in the Feynman diagram.
In other words, <i>per vertex</i> the degree is bounded.   The regularization
occurs on a per-vertex basis with both noisy-distances and
with rain of bricks too (noting the <a href="#post3">rule</a> that Feynman diagram
vertices must reside in <i>different</i> bricks).
</p><p>
<b>3.</b>
Most interestingly of all,
it appears 
that <i>we do not even need noise</i>; we still gain enough strength to
eliminate the divergencies of graviton QFT 
merely by agreeing to employ 1- or perhaps 2-dimensional bricks.
Why?
We note that D-dimensional bricks, 0&lt;D&#8804;4, get rid of infinities of
power-law type as bad as &#8747;<sub>1&lt;z&lt;&#8734;</sub>z<sup>4-D-&#949;</sup>dz,
for any &#949;&gt;0 no matter how small.
<!--
[With (4-&epsilon;)-dimensional Minkowski space QED's logarithmic infinities are removed.  Each
dimension we remove, we allow one more power.]
-->
Because of the fact that the infinities occur vertex by vertex,
and based on the naive divergence degrees mentioned by Weinberg in his 
<a href="https://dl.dropboxusercontent.com/u/3507527/weinbergdivquote">quote</a>
in our <a href="#whatswrong">§4</a>, 
plus the fact that each new level of perturbation theory requires
at least <i>two</i> more diagram vertices,
this would seem sufficient to abolish all graviton-QFT infinities.  
</p><p>
(But we remind the reader that this is not a rigorous conclusion, but
merely a heuristic "power counting" argument.)
</p><p>
<b>4.</b>
Finally, the "<a href="https://dl.dropboxusercontent.com/u/3507527/borderdim" <="" a="">borderline dimension</a>"
is 2 for gravitons, i.e. it is known that gravity becomes
renormalizable (or else, which is better, "super-renormalizable") in dimensions&#8804;2.  
(For QED and Yang-Mills, in contrast,
the critical dimension is 4, i.e. the dimension we actually live in.)
This suggests that rain of bricks using bricks with D+E&lt;2 should simply <i>work</i>
to produce a quantum gravity theory!
(Also see <a href="#wasysafe">below</a>
where we shall discuss Weinberg's "asymptotic safety" ideas 
vis-a-vis lower-dimensional gravity.)
Unfortunately, it probably isn't quite that easy and this was probably over-simplistic.
The problem is we are trying to use ordinary 4D gravity inside a &lt;2D brick.
It isn't obvious what happens to the "critical dimension" calculation if you act that way.
Still, the possibility we 
<a href="#hierdiscuss">discussed</a>
before of 
<a href="#hierbricks">hierarchical bricks</a>
plus the fact 
(see <a href="#epitaph">§41</a>)
that "constructive field theory" tends to succeed in rigorizing renormalizable
QFTs in 2 spacetime dimensions, all seems to reassure that we have enough
tools in our arsenal to handle such problems, if necessary.
</p>
<a name="rosquistparadox"></a>
<p>
<b>Rosquist paradox and standard model particles:</b>
A previous draft of this work had here included 
a long discussion of the set of standard model particles and a look at
how classical general relativity (GR) would have modeled them.
The present draft omits it but we shall now summarize the omitted material.
Most standard model particles have charge and/or spin far too great compared to their
mass, to be allowable in GR as a 
<a href="http://en.wikipedia.org/wiki/Kerr-Newman_metric">Kerr-Newman</a>
black hole, and hence instead would have
to be modeled as Kerr-Newman "naked singularities."  It is noteworthy
(and noted by some previous authors)
that in classical GR the "self-energy" of a point 
charge or point spinning particle is <i>finite</i>
as reckoned by the asymptotic behavior of the spacetime metric (which "weighs" the particle).
Also omitted was a long discussion of a paper by Rosquist 2006 which brought
up what would seem naively to be a devastating paradox.
That is this. One might naively imagine that classical GR is totally inapplicable to modelling
elementary particles because classical physics becomes invalid below the Compton wavelength
length scale, which for known elementary particles 
far exceeds the gravitational Kerr-Newman-metric length scales.
But Rosquist pointed out that the "spin radius" of an electron (his name for one of
the several characteristic length scales within the Kerr-Newman exact GR solution)
in fact is predicted to
be of the <i>same</i> order as the Compton wavelength, hence the classical model <i>should</i> 
have some validity!
</p><blockquote>
Specifically, the Kerr-Newman metric involves three length scales,
which are, respectively, M, Q, and J, the mass, charge, and spin angular momentum
of the particle (each in Planck units, and the length scales then
are the same numbers of Planck length units).   
These length scales are listed in increasing order for the electron.
The largest of them, which Rosquist calls the "electron spin radius,"
is J/c&#8776;1.9×10<sup>-13</sup> meter,
which equals the electron's Compton wavelength divided by 4&#960;.
The Kerr-Newman electron's naked singularity is a <i>ring</i> whose circumference
is h/(2m<sub>e</sub>c)=1.2×10<sup>-12</sup> meter, equal to half the Compton wavelength.
This (i) contradicts QED's treatment of the electron as a point, and (ii)
since the Kerr spin-related length scales are <i>not</i> much smaller than the 
electron's Compton wavelength, one would naively a priori expect the 
classical Kerr model still to retain some validity.
</blockquote><p>
This in turn should cause a metrical distortion that <i>would</i> have observable effects,
in particular causing the electron to have 
an <b>electric <i>quadrupole moment</i> of –124 barns</b> (times the elementary charge)
contradicting QED's prediction of 0
for every electric and magnetic moment besides the electric monopole (charge) and magnetic dipole.
Indeed, Rosquist hence would expect essentially every high precision QED calculation 
to be experimentally wrong – but it isn't – paradox!
I then had a long discussion analysing atomic spectral experimental 
evidence pertaining to this,
with the net conclusion that we <i>already know</i> the electron's quadrupole |moment| is 
below 6 barns.  
(This contradicted Rosquist's belief this moment had never been measured –
which was technically true in the sense those measurements were not 
<i>intended</i> for that purpose, 
but I claim nevertheless unintentionally show the moment is small.)
Finally, I then had a long discussion explaining why Rosquist's "paradox" really
does not happen.  The reason, essentially, is that there are senses in which
the GR metric differs only
an extremely <i>tiny</i> amount 
from the flat metric (<i>despite</i> Rosquist's correct calculation of
large "spin radius"!)
except within a region of very <i>tiny</i> (well below the spin radius) extent.
Specifically, I demonstrated 
</p><ol type="a">
<li>
 Kerr-Newman metrics with parameters appropriate for the
elementary particles known in 2000
distort all distances K times longer than Q and M, by relative amounts of
order 1/K or less (even if J is immense).  
</li><li>
 These metrical distortions
are sufficiently small that even the highest-precision QFT experiments 
(atomic clock comparisons accurate to about 1 part in 10<sup>18</sup>)
should be unable to detect any effect from them.
</li><li>
 The quadrupole moments arising from Kerr-Newman general-relativistic
particle models <i>depend inherently</i> on phenomena taking place in extremely small
(sub-Planck length) regions.   Therefore, classical models cannot be expected to be valid.
</li></ol>
<p>
The net effect is
that classical gravity ought to leave QED
unaffected to perturbation bounds
far below anything humanity could hope to measure
in the forseeable future.
So Rosquist's devastating paradox is not a paradox and not devastating, but
is interesting and initially scary.
</p>
<!--
<b>Classical general relativistic look at the known (mass&gt;0) point particles OMIT???:</b>
</P>
<table>
<caption><b>Table OMIT ???.</b>  The "standard model" particles (in Planck units)
</caption>
<tr><th>Particle</th><th>Mass M</th><th>Spin S</th><th>|Charge| Q</th><th>Specific Angular Momentum J=S/M</th><th>Classically would be</th></tr>

<tr><td>Electron, Muon, Tauon, Quarks</td><td>&lt;1.4&times;10<sup>-17</sup></td><td>1/2</td><td>&le;0.085</td><td>&gt;3.6&times;10<sup>16</sup>; 1.2&times;10<sup>22</sup> for electron
</td><td>Kerr-Newman naked singularity</td></tr>
-- top quark mass 173 GeV or less --

<tr><td>W boson</td><td>6.7&times;10<sup>-18</sup></td><td>1</td><td>0.085</td><td>7.4&times;10<sup>16</sup></td><td>Kerr-Newman naked singularity</td></tr>
<tr><td>Z boson</td><td>7.5&times;10<sup>-18</sup></td><td>1</td><td>0</td><td>6.7&times;10<sup>16</sup></td><td>Kerr naked singularity</td></tr>

<tr><td>neutrinos</td><td>&lt;1.3&times;10<sup>-21</sup></td><td>1/2</td><td>0</td><td>&gt;3.8&times;10<sup>20</sup></td><td>Kerr naked singularity</td></tr>
<tr><td>Higgs boson</td><td>(8-90)&times;10<sup>-18</sup>?</td><td>0</td><td>0</td><td>0</td><td>Schwarzschild black hole</td></tr>

-- Higgs mass 100 to 1000 GeV (probably) -- Planck mass = 1.22 * 10^19 GeV --
</table>
<p>
If we ignore quantum considerations and just ask what classical general relativity would
say about each nonzero-mass 
point particle in the standard model based on its known mass, charge,
and spin, we see
that every known fundamental particle has (in Planck units) extremely tiny mass M, 
moderately small charge Q, and enormous J. This causes all their Kerr-Newman metrics to 
be "spin-dominated" naked singularities with no horizon and no ergosphere. The sole exception is
the (so far only hypothetical) "Higgs boson," which has Q=J=0 and hence classically 
would be a Schwarzschild black hole, albeit one whose horizon would be far smaller 
than the Planck length. 
</p><p>
The "self energy" of every particle is finite, in the sense that, in general relativity, the 
large-radius asymptotics of the metric itself "weigh" the particle including the energies
of all its associated (including gravitional) fields, and of course
yield the actual observed mass, charge, and spin.  (Thus general relativity by allowing
spacetime to curve, avoids the infinite self energies of a classical point charge 
and point magnetic moment in flat space.  Without GR, a point with nonzero angular momentum
also would have infinite energy.)
The magnetic moment of
each charged particle is predicted by GR to exactly equal its lowest-order QED prediction
&ndash;
i.e. with Dirac "g-factor" g=2,
but without Schwinger et al's "anomalous" corrections to 2+&alpha;/&pi;+...
&ndash;
as was noted by Brandon Carter in 1968.
</p><p>
This total energy may be regarded as being due to the two components: 
<ol type="A">
<li> the fields "dressing" our particle, and 
</li><li> the "bare" particle itself.
</li></ol>
One can determine (B) by using the Kerr-Newman metric
to compute the classical energy cost of gradually
bringing in
infinitesimal-mass portions of the particle until the whole particle is assembled.
This cost is found in every naked case to be <i>finite</i> &ndash;
if we agree to bring in the mass all the way to the point itself, along the z (spin) axis,
and assess the gravitational potential energy via the redshift factor in
a stationary form of the Kerr-Newman metric.
</p><p>
But for the Higgs, the same assessment technique shows
we gain an <i>infinite</i> amount of energy by bringing
a tiny amount of mass from far away to the (infinite redshift) black hole horizon.
Thus the classical bare mass of the Higgs (or, for that matter, any black hole)
could be regarded as &ndash;&infin;,
with the dressing contributing M<sub>Higgs</sub>+&infin;.
</p><p>
One could argue that it is insane to treat an electron with a <i>classical</i> gravitational
model because the electron presumably behaves quantumly at length scales below
its Compton wavelength (2.4&times;10<sup>-12</sup> meter).
However, Rosquist 2006 responds that the Kerr-Newman metric involves three length scales,
which are, respectively, M, Q, and J (regarded as being in Planck length units).   
These length scales are listed in increasing order for the electron,
and the largest of them, which Rosquist calls the "electron spin radius,"
is J/c&asymp;1.9&times;10<sup>-13</sup> meter,
equal to the electron's Compton wavelength divided by 4&pi;.
The Kerr-Newman electron's naked singularity is a <i>ring</i> whose circumference
is h/(2mc)=1.2&times;10<sup>-12</sup> meter, equal to half the Compton wavelength,
contradicting QED's 
treatment of the electron as a point.
<b>Since these Kerr spin-related lengths are <i>not</i> much smaller than the 
electron's Compton wavelength, one would naively a priori expect the 
classical Kerr model still to retain some validity.</b>
</p><p>
Therefore, Rosquist justifiably expects the 
spin-dominated Kerr-Newman metric curvature ought to have a noticeable effect on QED,
which one a priori would expect to <b>invalidate essentially every 
high-precision QED calculation!</b>
In particular, Rosquist notes that the Kerr-Newman electron model 
predicts the electron should have 
an <b>electric <i>quadrupole moment</i> of -124 barns</b> (times the elementary charge)
contradicting QED's prediction of 0
for every electric and magnetic moment besides the electric monopole (charge) and magnetic dipole.
</p><p>
These reasonable general relativistic expectations, however, are falsified by experiment.
I will demonstrate elsewhere (Smith 2011???) why known experimental data suffices to show the
electron's quadrupole |moment| is far smaller than 124 barns.  (Unfortunately there
has never been an experiment
<i>directly</i> aiming to measure its quadrupole moment, and one would help.)
And the magnetic moment of the electron agrees with the QED prediction (with
small non-QED QFT corrections) to
better than 2 parts in 10<sup>12</sup>, a precision nearly impossible to reconcile with
the naively expected gravitational corrections.
</p><p>
The <b>resolution of these paradoxes</b>
(Smith 2011???) is that the <i>theorem</i> that 
Kerr-Newman metrics with the particle parameters in table ???
distort all distances K times greater than Q and M, by relative amounts of
order 1/K or less (even if J is immense).   These metrical distortions
are sufficiently small that even the highest-precision QFT experiments 
(atomic clock comparisons accurate to about 1 part in 10<sup>18</sup>)
should be unable to detect any effect from them.
Smith 2011 also argues that the 
quadrupole moments arising from Kerr-Newman and related general-relativistic
particle models <i>depend inherently</i> on phenomena taking place in extremely small
(sub-Planck length) regions.   Therefore, they cannot be expected to be valid 
&ndash; and with rain of bricks these length scales behave 
vastly differently than general relativity (or old-style QED, for that matter) 
assumes they do.
</p><p>
Note that the Higgs horizon is 10<sup>-19</sup>
Planck lengths in size, or smaller, i.e. well below the smallest 
measurable length.   Therefore, it could not exist 
in rain of bricks physics, and the energy of bringing in infinitesimal mass chunks
should not be assessed based on motion to that (nonexistent) horizon, but
rather based on getting within O(1) Planck lengths away, e.g. reaching the same brick.
With <i>that</i> assessment method, the "bare" energy of a Higgs boson would
be its rest mass itself, plus a very small relative adjustment due to
gravitational potential energy (of order 1 part in 10<sup>38</sup>)
and also a comparably small adjustment (of order 1 part in 10<sup>38</sup>) due to 
gravitational dressing.
</p><p>
More generally we might expect the gravitational self |energy| of every particle
to be smaller than its classical estimate (since quantum self energies always
seem to  be less-singular than classical self energies) &ndash; which already was very
small in every non-Higgs case because every particle mass is tiny compared to the Planck mass.
A particle with mass of order m in Planck units would be expected to have 
gravitational self energy and other adjustments of order m<sup>2</sup> 
at each level of rain of bricks perturbation theory.  (I.e, tiny.)
And if the perturbation series in rain of bricks converges we would expect (barring 
unusual slower-than-geometric 
series convergence behavior)
the total series sum to have the same order as its first few terms.
</p>
-->
<p>
<b>Gravity without gravity:</b>
It also might be possible to introduce gravity into rain of bricks physics <i>without</i>
any gravitons.  Andrei Sakharov (look up "Sakharov" in
Misner, Thorne, Wheeler 1973 for a better explanation than
Sakharov gave) in 1968
explained how Einstein-gravity could arise as a side effect of the "vacuum zero point energies" 
of other fields.  (I have done a tremendous amount of unpublished research on Sakharov gravity, 
which I rediscovered in 1998... but have not been able to treat it in any rigorous+nonperturbative
manner.  It would take us far too afield to discuss it here.)
</p>
<a name="wasysafe"></a>
<h3>24. S.Weinberg's "asymptotic safety" program and "reduced-dimensional gravity" </h3>
<p>
In a nonrenormalizable QFT (such as gravitons) presumably an <i>infinite</i> 
number of renormalization constants would be required, which would rather undermine the 
usefulness of the theory as a predictor of nature.
Weinberg 1979 invented an idea he called "asymptotic safety" to try to escape that trap.
It is this.  Recall in QCD there is "asymptotic freedom" which means the renormalization flow,
as we go to higher and higher energy scales, approaches a point where the effective
coupling constant &#945;<sub>s</sub>
goes to zero.  This observation converted QCD from being essentially
useless due to essentially all series diverging immediately, to having small to moderate usefulness
due to now being able to do lattice gauge theory computations (with at least some hope
they have some validity!) and also to make connection (now without even needing a computer!)
to certain high energy experimental phenomena such as "Bjorken scaling." 
For graviton-QFT, Weinberg 
considered the renormalization flow on an <i>infinite-dimensional</i>
space of renormalization constants.  
QED's "beta function" of one real argument
becomes in this context one with an infinite set of reals as its input and its output,
i.e. is best regarded as a "functional."
Weinberg then hoped this flow would get attracted into a <i>fixpoint</i>
at high energies.   He then hoped this fixpoint would be computable, in which case we would
<i>not</i> have an infinite set of renormalization constants we'd need to
deduce from only a finite amount of experimental data – because they'd all have
known values!  
Further, even if this fixpoint was not just one point, but actually some finite-dimensional
attractor surface, that still would be ok, since again all values would be
deducible from just a few.
Hence, at high energies the theory hopefully would become usable, similarly
to the way asymptotic freedom makes QCD (somewhat) usable and to the way renormalization
makes QED usable.
</p><p>
Weinberg 1979 then investigated 
"gravity in 2+&#949; dimensions" (0&lt;&#949;&lt;&lt;1).
He concluded that this <i>was</i> an asymptotically safe theory!
But he also noted:
"Matter fields may or may not change this conclusion,
depending on type and number.
[Our] analysis is extremely 'soft', much is conjectured but little is proved."
Weinberg also noted, albeit without apology 
</p><blockquote>
Of course, general relativity is not much of a theory in 2 dimensions.
The lagrangian R&#8730;|g| is a total derivative for D=2 [Gauss-Bonnet theorem]
and hence the
Einstein tensor vanishes identically [Weinberg 1972 §6.7].  
</blockquote><p>
I would also remark that the 4-indexed Weyl
tensor (often considered to be "the gravitational field") is identically 0 when 
<nobr>1&#8804;D&#8804;3.</nobr>
The following authors (and this is an incomplete list) then followed up on Weinberg 1979:
</p><ul><li>
Kawai, Kitazawa, Ninomiya 1993 and 1996;
Aida, Kitazawa, Nishimura, Tsuchiya 1995:
"We formulate a renormalizable quantum gravity in 2+&#949; dimensions by
generalizing the nonlinear sigma model approach to string theory. We
find the theory possesses the ultraviolet stable fixed point if
the central charge of the matter sector is in the range 0&lt;c&lt;25. This
may imply the existence of consistent quantum gravity theory in 3 and
4D... We prove to all orders that the counter terms can be
supplied by the coupling and the wave function renormalization of the
tree action which is invariant under the full diffeomorphism...
We thereby put the (2+&#949;)-dimensional expansion of quantum gravity
on a solid foundation.
We have constructed a proof of the renormalizability of the theory to
all orders in the perturbative expansion of G...
The renormalization of the cosmological constant operator is analogous. We have
shown that the cosmological constant operator is multiplicatively
renormalizable. The
anomalous dimension of the cosmological constant operator is
generically O(1) near two
dimensions and we need to be more careful to calculate it. However, it
can be calculated
for small &#949; by the saddle point method..."
</li><li>
Aida &amp; Kitazawa 1997:
"We have developed the (2+&#949;)-dimensional expansion of quantum gravity
extensively...
We explicitly show that the theory is
renormalizable to the two-loop level in our formalism..."
</li><li>
Niedermaier 2003:
"4D Einstein gravity coupled to scalars and abelian gauge fields in its
2-Killing-vector reduction is shown to be quasi-renormalizable 
[one can achieve strict cut-off independence] to all
loop orders at the expense of introducing infinitely many essential
couplings. 
Fortunately all these couplings can be arranged into a single scalar
function h(·) of one
real variable, whose flow is governed by the beta functional (3.14)
expressible in closed form in terms of the (<i>one</i>-coupling)
beta function of a symmetric space sigma-model. Generically the matter
coupled systems are asymptotically safe, that is the flow possesses a
non-trivial UV stable fixed point at which the trace anomaly vanishes.
We show that
dimensionally reduced gravity theories are asymptotically safe in this sense.
The main exception is a minimal coupling of 4D Einstein gravity to
massless free scalars, in which case the scalars decouple from gravity
at the fixed point."
</li><li>
Percacci &amp; Perini 2003 and 2003.
(Add matter.)
</li><li>
Niedermaier 2009.
(Considers higher-derivative gravity.)
</li></ul>
<p>
Recent reviews of this area include:
</p><ul><li>
Niedermaier 2007:
"It is argued that as a consequence of the [asymptotic safety in quantum gravity]
scenario the self-interactions [<i>must</i>]
appear two-dimensional in the extreme ultraviolet."
</li><li>
Litim 2009:
"For quantum gravity, this asymptotic safety scenario has been introduced by Weinberg 1979.
In the vicinity of two dimensions, a non-trivial fixed point has been identified within
perturbation theory, to leading 
(Weinberg 1979,
Kawai-Kitazawa-Ninomiya)
and subleading 
(Ambjorn, Jurkiewicz, Loll 2001)
order in &#949;=d-2&lt;&lt;1...
From a renormalisation group point of view, 
the 'critical' dimension of quantum gravity is 
d<sub>crit</sub>=2...
In this contribution, we discuss the asymptotic safety scenario in the context of quantum 
gravity. Based on a Wilsonian renormalisation group, we provide unique analytical
fixed point solution in the Einstein-Hilbert truncation for 
<i>any</i> dimensions d&gt;d<sub>crit</sub>=2.
The approach is related to the integrating-out of momentum modes from a path integral
representation of the theory, amended by an appropriate optimisation."
</li>
<a name="carlipdisc"></a>
<li>
Carlip 2012:
"One attractive line of research is
to look for places in which different candidates for quantum gravity agree. Even if
none of our current models is ultimately correct, such areas of agreement suggest deeper
underlying structures that might persist in the correct theory. A classic example is black
hole thermodynamics...
Over the past few years, another area of agreement has started to emerge. Hints
from several different models of quantum gravity suggest that at very short distances –
perhaps an order of magnitude above the Planck scale – spacetime becomes effectively
two-dimensional. Let me stress that this evidence is far more tentative than the evidence
for black hole thermodynamics, and may well turn out to be a mirage. But the idea of
'<i>spontaneous dimensional reduction</i>' is intriguing enough to deserve further study...
As Ambjorn, Jurkiewicz, Loll first found and [Carlip] has confirmed,
[in a dynamical triangulation computerizable model of quantum gravity at small length scales,
the]
spectral [spacetime] dimension is 4 for
'long' random walks, as required for the emergence of a good classical limit, but falls
to 2 for 'short' random walks...
This phenomenon, which occurs at scales of about 15 Planck lengths, 
is perhaps the strongest piece of evidence for spontaneous dimensional reduction...
there are fairly general arguments that if an ultraviolet fixed point
[of quantum gravity in Weinberg asymptotic safety picture]
exists, it must look effectively two-dimensional 
(Percacci &amp; Perini 2004)... [many other pieces of evidence cited]...
We are left with an intriguing picture of the small scale structure of spacetime:
<ul><li>
At distances of a few times the Planck scale, quantum fluctuations cause light cones
to collapse, resulting in a causal disconnection of nearby points in spacetime.
</li><li>
...short distance spacetime [from consideration of the Wheeler-deWitt equation]
looks like a nearly random, weakly coupled,
chaotically changing collection of Kasner-space domains.
</li><li>
The effective two-dimensional behavior of Kasner space, in which the dynamics is
concentrated along a preferred direction, can be interpreted as a sort of spontaneous
dimensional reduction of spacetime.
</li><li>
Lorentz violations occur near the Planck scale, but these are nonsystematic and
average out at larger scales."
</li></ul>
</li></ul>
<p>
I'm not sure what to make of these papers.
Is the result of combining them with rain of bricks, a successful quantum gravity theory?
I am optimistic since, e.g, Niedermaier 2003 seems exactly what would be wanted. 
</p><p>
<b>Conclusions:</b>
</p><ol>
<li>
It seems plausible that in at least some flavors of rain of bricks physics, graviton
QFTs become finite.  If rain of bricks does work with gravitons, then the resulting
theory will presumably be totally finite, e.g. no gravitational singularities  (black holes,
big bang, etc) will exist anymore.
</li><li>
Further, if so, then gravitational self energy effects
are expected (based on our crude and nonrigorous analysis)
to lead only to adjustments so tiny 
(both at each level of perturbation theory, and in total)
that we cannot hope to detect them
in the foreseeable future!
</li><li>
The tentative hypothesis
"rain of bricks will allow unifying gravity with the other forces in a quantum field theory"
must be considered confirmed by all (crude) investigations so far.
</li>
</ol>
<p>
<b>About "spontaneous dimensional reduction":</b>
The above asymptotic-safety gravity investigators found
"spontaneous dimensional reduction" at microscopic length scales.  This is
qualitatively similar to our "rain of bricks arena" which (by explicit design) when viewed
microscopically is lower dimensional.
Also, in <b>"superstring theory,"</b>
another attempt to generate quantum gravity and save QFTs
from their internal demons, 0D particles become 1D "strings."
This is in some sense roughly equivalent, as far as interparticle
collision-interactions are concerned, to still having 0D particles
but which get to interact on "bricks" which are 2D, instead of just at 0D collision points
(the old-style QED view).  Rain of bricks is in this sense more  general than superstrings
because, e.g, it also permits 1D or 3D bricks, which is something no "string" could ever duplicate.
So: all three approaches – asymptotic safety, strings, and bricks –
here involve the same kind of phenomenon.
However, rain of bricks seems superior to the other two approaches in the sense that since
this microscopic dimension reduction is "designed in" the simplest and
most explicit way I could, rain of bricks is <i>far simpler to work with</i> than these
rival approaches.
</p>


<a name="whataboutsym"></a>
<h3>25. Lorentz invariance? Probability-conservation? Energy &amp; Momentum conservation? Unitarity? </h3>

<blockquote>
Perhaps the idea that two points can be infinitely close together is wrong. 
If we make the minimum possible distance between two points be 10<sup>-100</sup>cm...
then [QFT] infinities disappear... but [then] other inconsistencies arise such as the total 
probability of events adding up to slightly less or more than 100%, or
[slight energy nonconservation].
<br>&nbsp;&nbsp;<b>–</b> 
Richard P. Feynman (p.129 of <i>QED: strange theory</i>).
We now shall contend Feynman's worries were unjustified; 
rain of bricks does what he wanted without
suffering those "other inconsistencies."  Feynman's problem really was that 
his metrical notions were too crude.
</blockquote>
<p>
All these symmetries
can still be regarded as exactly correct in rain of bricks QED, albeit
they have to be interpreted somewhat differently and with care, and formulas describing
the exact meanings of these statements also are not exactly the same.
</p><p>
To examine them, we shall regard the purpose of Minkowski (1+3)-space
in rain of bricks QFTs as solely to propagate non-interacting quantum fields.
(Interactions take place only on the bricks, not in Minkowski space.)
We shall consider Minkowski space as containing these fields.
We shall also in some places make heavy use of the 
<a href="https://dl.dropboxusercontent.com/u/3507527/sheafview">sheaf view</a> of the rain of bricks universe.
</p><p>
<b>Lorentz-Invariance fact I:</b>
All Poincare/Lorentz observers agree the raindrop points are Poisson-random and 
all agree on the value of &#961;<sub>rain</sub>, so all agree on the <i>statistical 
nature</i> of the metric of rain-of-bricks spacetime.  However, observers disagree on
the <i>precise microscopic details</i> of that metric, causing Lorentz non-invariance, 
including rotation, spatial-mirroring ("P"), and time-reversal ("T") non-invariance.
</p>
<p>
<b>Lorentz-Invariance II:</b>
The photon and electron "propagators" (as well as any of the usual propagators for other kinds
of particles) all are exactly Lorentz-invariant in the sense that their formulas are functions only
of the Lorentz-invariant Minkowski "distance" between two spacetime points.
(See our <a href="#propformulas">§15</a> for propagator formulas; also discussed 
in e.g, §3.5 of Greiner &amp; Reinhardt for the electron propagator; also derived
by Zhang et al 2010; Weinberg vol.1 §5.7 gives more propagator formulas.)
</p><p>
<b>Lorentz-Invariance III:</b>
The rain of bricks spacetime metric actually is <i>exactly</i> Lorentz invariant in
the sense that all Poincare/Lorentz observers agree on the 
values of the integrals of certain classes of
"test functions" (e.g. those band-limited within the intersections of the Nyquist regions
of the different observers).  In particular consider situations in which all
the quantum fields happen to be thus band-limited.
<!-- (See also &sect;??? about exactly sym) -->
</p><p>
<b>Warnings about conservation laws generally:</b>
In old-style physics in Minkowski spacetime, the concept of a "conserved quantity" was easy 
to understand.  You could integrate its density over all space at some time t=A, and 
then at some other time t=B, and the two had to be the same.
But things become less easy and comprehensible in more advanced physical theories:
</p><ol><li>
In (old-style) finite-order-QED, energy conservation does <i>not</i> hold, for example
it is violated during every particle-emission/absorption (Feynman vertex) event.
(More precisely: you can regard energy E and momentum p as
being preserved if we violate E<sup>2</sup>=m<sup>2</sup>+p<sup>2</sup>, or
regard the latter as valid but E is non-conserved. You cannot have both.)
However, it is believed to hold "macroscopically" in the sense that violations large compared to
the energy-time uncertainty principle are very unlikely.
</li><li>
In general relativity in curved spacetimes, there usually 
is no such thing as a globally conserved "total energy"
but there are local differential notions of mass-energy conservation.
</li><li>
In the rain of bricks arena, there in general will be <i>no</i> points with t=A.
Quantities simply do not exist except instantaneously (raindrops being
<i>points</i> in spacetime).   So the meaning of "conserved quantity" is not so clear, and
indeed the whole notion of continuously variable "time" and "spatial location"
must be largely abandoned, or at least used with caution.
The whole idea of the "lagrangian density" which underlies all mainstream QFTs so far,
might seem no longer to have a meaning.  (Actually, it still has a meaning:
it encapsulates in a small package, a compressed description of the entire theory.
Also see <a href="#opprobaction">open problem 7</a> at the 
<a href="#opensprobs">end</a>.)
</li></ol>
<p></p><p>
Nevertheless, as we will see, there are still plenty of useful "conserved quantity" notions.
</p><p>
<b>Probability, energy, and momentum conservation:</b>
We know from the 
<b><a href="https://dl.dropboxusercontent.com/u/3507527/sheafview">sheaf view</a></b> in 
<a href="#dophys">§14</a>  how to regard probability as
being exactly preserved.
</p><p>
Similarly, within the Minkowski spaces that are members 
of the "sheaf" at any one particular "tilt,"
we have exact preservation of momentum and energy since
the exact Dirac (or Maxwell, or whatever)
equation is obeyed. We then have overall 
energy and momentum conservation by summing over all tilts.
These are still valid notions of probability, energy, and momentum conservation, <i>but</i>
the reader is <b>warned</b> that they is not quite
the same as the old-style-QED notions!
</p><p>
In Minkowski space, a further remark is this.
Via, essentially, a <b>Fourier transform</b>, 
one may decompose the Dirac field, or Maxwell field, 
into
a linear combination of eigenstates of the energy and momentum operators 
– i.e, essentially, Fourier modes. 
[The precise formulas for these eigenfunctions may
be found in, e.g. <!--Greiner &amp; Reinhardt &sect;???-->
§15.2 pp.236-238 
and p.480 in Appendix A of Landau 1996 (Dirac), and 
Jackson §7.1 and 7.2 (Maxwell).]
Divide Minkowski spacetime into the "past" (t&lt;0) and the "future"
(t&gt;0).   Consider the propagation of one of those energy-momentum eigenstates 
from the past, into the future.   
In old-style QED, we could accomplish this by integrating over the domain 
-T&lt;t&lt;0, the wave function times the propagator to the new point (t;x,y,z), 
scaled by T<sup>-1</sup>.
In rain of bricks QED, we do not use an integral, we instead use a 
<i>sum</i> over raindrop points.
However, because the eigenfunction is <i>periodic</i> this sum really is a Monte Carlo integration
over each 3D period-parallelopiped, with an infinite number of copies of the 
period-parallelopiped and hence an infinite number of Monte Carlo integration points.
It thus, with probability=1 (by the "strong law of large numbers") will return the 
<i>exactly</i> correct
answer at the new point (t;x,y,z) if averaged over the new point and all its
periodic images.  In other words, there ought to be a sense in which
<i>the Fourier components of
the wavefunction propagated into the future, will be exactly the same</i>
as they were in the past –
there is no way to convert
a Fourier mode at one frequency or wavelength into one at any other
without violating macroscopic Lorentz invariance (which we cannot), and probability conservation
then forces mode-amplitudes to stay the same.
<!-- provided we never leave the "Nyquist bandwidth" region of Fourier space? 
I have no need of this assumption.
-->
So additionally in some Fourier-sense,
energy-momentum eigenstates should be exactly preserved by propagation into the future
in <i>both</i> old-style and rain of bricks QED
provided the wavefunctions are well-enough behaved to Fourier transform.
By the same argument, <b>probability</b> also should be conserved in a Fourier-sense.
</p>
<!--
<p>
<b>There is a gotcha:</b>
The brick-smeared propagators are not exactly the same as the unsmeared propagators.
--However, they become the same in the large-time limit, so that, e.g, Dirac 
energy-momentum eigenstates ought to be preserved more-exactly the longer the time interval.--
The smeared propagators no longer exactly obey the Dirac partial differential equation.
Therefore, we should not expect eigenmodes of the Dirac equation to be exactly preserved.
Instead, the eigenmodes of "the brick-smeared-propagator-based integral operator
that accomplishes time translation" will be 
exactly preserved!
But 
<ol><li>
This operator, being translation-invariant, should
still have Fourier modes as eigenfunctions.
</li><li>
Our <a href="asymnosmear">theorem</a>
about the asymptotic equality of brick-smeared and unsmeared propagators is strong enough
to assure that at long times, these eigenmodes ought to be 
preserved up to a constant (perhaps frequency-dependent, but asymptotically time-<i>independent</i>)
factor.
</li><li>
If any |eigenvalue| were 
unequal to 1, we would get exponential growth or decay at long times,
contradicting that.  Therefore, all these |eigenvalues|=1.
</li></ol>
<p>
So the "gotcha" is defeated; the argument we gave should still 
(keeping this in mind) be valid.
</p>
-->
<p>
Note that different Poincare/Lorentz observers will in general have quite different hyperplanes
separating their "past" from their "future."   Consequently each will experience these 
Fourier-style conservation laws through different microscopic mechanisms.
</p><p>
One might claim that energy conservation is no longer <b>possible,</b>
nor even definable, <i>locally</i>
in rain of bricks QED.  To have energy conservation, you need time-translation symmetry,
and need to have a <i>notion</i> of what time-translation <i>is</i>.
With rain of bricks, there no
longer is such a symmetry or notion,
microscopically speaking, although they do exist in a
macroscopic statistical sense.
One could make similar remarks about momentum conservation and space-translation symmetry.
None of these symmetries hold
microscopically anymore but do hold in macroscopic statistical senses.
(<i>However,</i> these impossibility
claims are wrong in the sense that the sheaf view <i>does</i> provide some
local conservation notions.)
Probability conservation corresponds to the symmetry of multiplying all wavefunctions
by an overall complex phase factor, and <i>that</i> still should hold in 
rain of bricks QED,
even locally microscopically, to the same extent it holds in old-style QED.
</p><p>
The conservation of "probability" (which is a quadratic normlike functional)
demonstrates <b>"unitarity."</b>
</p><p>
The above demonstrations only concerned "free propagation" and not "interaction"
(which is the other thing that happens in QED).  However, we claim that the truth-value of
"interaction also conserves these things" is the same in both old-style and rain of bricks
QED.   That is because old-style QED claimed to conserve them <i>even if the
parameter &#945; governing the interaction-strength varied.</i>  
In rain of bricks QED we
argue that we effectively have &#945;=0 within Minkowski spacetime and &#945;&#8800;0 within
the bricks.   The fact that &#945; thus-varies has no effect on the validity (or not)
of probability-conservation, just as old-style QED 
would not stop preserving probability, charge, etc if the value of &#945; were
strangely assumed to depend upon spacetime location.  (Indeed, Maxwell's 
equations <i>in linear materials</i>
do exhibit spatial variation of coupling constants.  See also our discussion
of gauge invariance in 
<a href="#mandlgauge">§5</a>
and <a href="#gaugeinvar">25</a>, and Scharf 1989.)
</p><p>
<b>Limitations on the conservation and symmetry Fourier laws:</b>
Our argument for Fourier-style (and sheaf-style) momentum and energy conservation 
work for <i>all</i> Fourier modes including those with super-Planckian ultrahigh frequencies.
However, the <i>interpretations</i> by any observer of such modes, especially one
whose mind is filled with old-style (pre-rain-of-bricks) physics notions,
are not going to be happy unless the frequencies are below the 
Nyquist bandlimit (<a href="#nyquist">§17</a>).
<i>Provided</i> all wave functions stay
bandlimited within the intersected Nyquist bandwidths of
all observers, everything should seem fine to them.
For what presumably would be observed when you attempt to
accelerate a particle to superPlanckian
kinetic energies, see the 
<a href="#accelsuperplanck">discussion</a>
in <a href="#nyquist">§17</a>.
</p><p>
In the view of an observer (if they agree to adopt a certain interpretation of what they observe) 
there simply is <i>no such thing</i> as a super-Planckian-momentum particle.  Any such 
particle would appear to, and be interpreted by, 
our observer as (and have identical physical effects to)
some unique complex linear combination of lower-|momentum|
particles and hence in some sense 
there would then <i>appear</i> to be a violation of old-style
momentum and/or energy (and/or particle-number?)
conservation despite the validity of our conservation laws;
<i>but</i> this illusion perhaps could be made to vanish by longer-duration
examination of data.
</p>

<!--
these conservation laws seem exact to observers;
if we get a very high (super-Planck) momentum particle, well outside
the Nyquist region in Fourier space?   
The answer is, in rain of bricks QED, information about things can only be obtained, and in
some sense only exists in the form of, its values on the bricks, e.g.
at the raindrop (sampling) points.
The Nyquist theorem tells us that any such collection of sample values can be interpreted
as a <i>unique</i> band-limited continuous function.   However, without bandlimiting,
the interpretation becomes non-unique.   Assuming every observer agrees (as a matter
of mental standardization allowing us to share a common descriptive language) to mentally
stay within the Nyquist bandwidth so that their interpretations are unique, then

</p><p>
Furthermore, each Lorentz 
observer has a different private notion of what the Nyquist bandwidth <i>is</i>,
so your notion of an ultrahigh momentum "superPlanckian" particle
will not necessarily coincide with mine
about any particular particle.   Any observer sees self-consistent physics and has
self-consistent notions,
though.
</p>
-->

<a name="furrythm"></a>
<h3>26. Charge Conjugation Invariance and Furry's theorem </h3>
<p>
"Furry's theorem" states that any Feynman diagram containing a 
a closed loop (polygon) of fermion lines, with an <i>odd</i> number of vertices,
can safely be ignored.
That is because its amplitude (and effect on the electromagnetic field...) 
exactly cancels that from the same diagram but with the direction
of the arrows round that loop reversed (i.e. positron, not electron).
[For an even number of vertices we get a doubling instead of cancelling effect.]
</p><p>
That in turn is a consequence of "charge conjugation invariance."  
(See §10.1 of Weinberg vol.2.)
</p><p>
Both charge conjugation invariance and hence 
Furry's theorem are still valid in rain of bricks QED.
</p>

<a name="gaugeinvar"></a>
<h3>27. Exact gauge invariance, and charge (or lepton number) conservation</h3>
<p>
"Gauge invariance" (see p.78 of Mandl &amp; Shaw 1993)
is the idea that adding any total-derivative to the 
Maxwell A<sub>&#956;</sub> field, makes
no difference provided the Dirac wavefunctions (and their Dirac adjoints)
are appropriately adjusted by multiplying by
certain spacetime-dependent complex phase factors.
We can argue that rain of bricks QED obeys gauge invariance in
a similar sense (and via a similar argument) to the one we just
used to argue for energy, momentum, and probability conservation.
Namely, if the additive alteration to A<sub>&#956;</sub> 
were by a total derivative <i>decomposed into Fourier modes</i>
then we could again argue by Monte Carlo integration over an infinite number of period
parallelograms, that the Fourier components in old-style and rain of bricks QED
must come out the same (with probability=1)
at any fixed order in perturbation theory –
and will still seem ok to observers <i>provided</i> we stay
within their Nyquist bandwidth region in Fourier-space.
</p><p>
Further, it seems possible to argue that gauge invariance is valid not only (as above)
macroscropically, but also <b><i>micro</i>scopically</b>, in the sense that the old-style QED
lagrangian (see EQs 4.3-4.6 of Peskin &amp; Schroeder) <i>even if the value of the
elementary charge e is 
regarded as a smoothly-varying real-valued 
function of spacetime location</i> would still be invariant under their 
gauge-transformation provided the e is repositioned inside the derivative in their EQ 4.6.
(This requires e never to pass through zero.)
Now actually in rain of bricks QED it is as though e is zero outside of the bricks and 
discontinuously jumps to a nonzero value upon entering a brick.  But presumably this
can be handled by writing appropriate limits of smooth positive-valued functions.
</p><p>
Gauge symmetry immediately implies <b>charge conservation</b> via Noether's theorem
(Peskin &amp; Schroeder §2.2) – or in the more general scenario with
spatially-varying e, instead it implies conservation of the expected <i>lepton number</i>.
</p><p>
Lepton number conservation also would seem implied order by order in QED 
(whether in rain of bricks or not) because of
the Feynman diagram rules which only create or
destroy electrons as electron-positron pairs.
</p>

<a name="wardident"></a>
<h3>28. Ward-Takahashi identity </h3>
<blockquote>
Often in the literature the terms <i>Ward identity</i>,
<i>current (charge) conservation</i>, and
<i>gauge invariance</i>
are used interchangeably.
This is quite natural, since the Ward identity is the diagrammatic
expression of conservation of electric current, which in turn is a
consequence of
gauge invariance.
...However, we will distinguish the three concepts.
<br>&nbsp;&nbsp;<b>–</b> 
Peskin &amp; Schroeder (in their §7.4).
</blockquote>
<p>
The "Ward-Takahashi identity" 
<!-- Shows Z1=Z2.
CITES??? relates momentum derivatives of Feynman diagrams, to
the values of certain modified diagrams. -->
is still valid in rain of bricks QED
in the senses above in which 
charge conservation and
gauge invariance
remained valid.
Also, the proof of the Ward identity as a diagrammatric identity
in QED<sub>N</sub>
(§7.4 of Peskin &amp; Schroeder)
can be duplicated step by step in rain of bricks QED
provided we stay within the Nyquist region of Fourier space and use
the same "exact Monte-Carlo integration" trick as above to handle 
the fact Peskin &amp; Schroeder are working in momentum-space.
At one point in this proof Peskin &amp; Schroeder suddenly resort to the "LSZ reduction formula"
– which might not be valid in rain of bricks QED –
but that difficulty could be dodged by simply <i>assuming</i>
the validity of that formula in their particular application as part of the definition of
the Feynman rules for computing S-matrices in
QED<sub>N</sub>.
<!-- Folland page 189:
Itykson+Zuber 5-1-3, Pesjkin+Schroeder 7.2, and Weinberg 10.3 give nonrigorous nonperturbative
derivations of LSZ.. and rigorous proofs within the Wightman axiom framework with Haag-Rielle 
scattering thoery: see Araki and also cites Bogolubov et al.
-->
</p>

<a name="causality"></a>
<h3>29. Exact Causality </h3>
 
<p>
<!--P&S discuss in sec2.4.-->
Feynman's Dirac propagator, as well as other solutions of the Dirac equation,
can exhibit
motion (of something) "faster than the speed of light."
In particular, electrons can reach
places outside the light cone.
Early in the history of Dirac's equation, this "acausality" caused great worry.
(See Thaller 1992 for discussion of this and other Dirac equation "paradoxes" which 
historically caused great angst, including to Dirac himself.)
The propagator decays exponentially in the forbidden region
outside the light cone, with e-folding distance of the same order as the
electron Compton wavelength, so it "isn't <i>hugely</i> acausal" – 
but this defense is not adequate.
The real defense, indicating how quantum field theory <i>perfectly</i>
obeys causality, is described in §2.4  of Peskin &amp; Schroeder
and is quite brilliant.
A particle moving from X to Y acausally, automatically will be accompanied by
its <i>anti</i>particle moving from Y to X (the two processes can be viewed as
the same thing, and indeed in the view of appropriate Lorentz observers Y is
temporally <i>before</i> X so that the latter interpretation is more natural)
in such a way as to exactly cancel amplitudes, causing
any experimental measurement at X necessarily
to be <i>unrelated</i> to any experimental
measurement at Y – that is, the operators for the two
observables <i>commute</i>.  This is proven by evaluating
the commutator and proving it cancels to 0.
Hence information cannot be transmitted faster than light and we live
in a causal world.
</p><p>
For spacelike separations, the propagator formula based on Minkowski "distance"
yields the same amplitude for X-to-Y as for Y-to-X, which is why this
cancellation can and does happen; there is a continuous path of
time-direction-preserving Lorentz transformations
connecting the identity to a transformation that exchanges X with Y so we know
the two amplitudes <i>must</i> be equal using <i>any</i>
propagator invariant under time-direction-preserving Lorentz transformations.
</p><p>
If X and Y instead were <i>time</i>like separated, then
there would be no such continuous path, and indeed the two could <i>not</i> be
exchanged by a time-direction-preserving Lorentz.
An electron traveling forward in time is equivalent in QED
to a positron traveling backwards in time, but – crucial point –
all observers can <i>agree</i> to regard
the latter as impossible.
Hence the amplitude cancellation then would not happen in this case,
and hence information <i>is</i> permitted to flow slower than light.
</p><p>
This all works if and <i>only</i> if antiparticles have exactly
opposite charge
(and exactly opposite every other quantum number too),
and equal masses. 
Thus causality and the existence and nature of antimatter are
inextricably related.
</p><p>
Since rain of bricks QED uses the same propagator formulas as ordinary QED, with
the same Lorentz and time-direction properties,
its causality properties are the same.
</p>

<a name="noangmom"></a>
<h3>30. <i>Non</i>conservation (but approximate conservation) of angular momentum </h3>
<p>
Angular momentum conservation is tied via Noether's theorem to spatial-rotation symmetry. 
However, with rain of bricks we no longer have  
spatial-rotation symmetry, microscopically speaking.
If we examine some m<i>a</i>croscopic region of rain of bricks space, it statistically will 
closely resemble the same region of space but rotated about some axis.  That is because
a cubic meter of space, examined for 3 nanoseconds, contains about 1.5×10<sup>139</sup>
Planck 4-volumes.  Statistical averages of that many independent things tend to have
very small (reduced by a factor of 3.9×10<sup>69</sup>) deviations.
But unlike for ordinary (i.e. non-angular) 
momentum and space-translation symmetry, this, while large, is <i>not</i>
infinite.  So we only get <i>approximate</i> rotation symmetry with rain of bricks and
expect only approximate angular momentum conservation albeit
getting arbitrarily better for larger objects.
Further, it is no longer clear exactly what angular momentum <i>is</i>.
We would still expect angular momentum
to be "quantized" in some sense but not via exactly-equispaced steps
&#8463; anymore; instead the allowed values could depend on, and vary slightly with,
space and time.
</p><p>
This is one way in which rain of bricks QED may make
an experimentally-refutable prediction
differing from most other theories of physics.
However in practice it would be very difficult to detect this kind of discrepancy.  
Even in a volume
the size of a proton (for a time comparable to the speed of light transit time)
there are over 10<sup>80</sup> Planck 4-volume units, so measuring angular momenta accurate to at
least 40 decimal places presumably would be necessary.
</p><p>
Furthermore, any fourier-component of a wavefunction analysed into
rotational Fourier components about some axis perceived by some observer,
would if that observer <b>waited long enough</b> correspond more and more accurately
to the approximate Fourier coefficients got using the raindrop-sum 
"Monte Carlo numerical integeration."
This indicates that angular momentum with rain of bricks still is preserved to 
<i>arbitrarily good approximation</i> if observer waits long enough.
</p>

<a name="hamsand"></a>
<h3>31. Time-invariant "bound states"; Trotter product formula.
Hamiltonian? "Emergent" spacetime? </h3>

<p>
The hydrogen atom ground state is a prototypical "bound state."
One solves the Dirac equation to find the wave function,
and this solution (according to old-style relativistic quantum
mechanics, anyhow)
persists eternally without ever changing.
In QED, the Dirac solution is regarded only as the
leading order approximation to the truth,
and one can compute a series of smaller and smaller perturbative
corrections, e.g. the "Lamb shift," to it.
And that series of corrections, although initially getting "smaller and smaller"
actually presumably ultimately diverges.
</p><p>
But for the purpose of the present discussion, ignore that.   What I
want to focus on
now, is the fact that this state was supposed to be <i>time invariant.</i>
What happens to that with rain of bricks?  With rain of bricks,
there is no old-style continuum "time" and spacetime itself (due to
the random raindrops)
is not time-translation invariant.  So in rain of bricks QED,
hydrogen and other molecules are <i>not</i> time-invariant states.
Nevertheless, a good approximation of the usual time-invariant states
is presumably
output by rain of bricks QED!  The electron propagates from brick to
brick, on bricks
interacting with virtual photons communicating the electromagnetic
field generated by the
proton, and in this way,
averaged over timescales much larger than the Planck time, the electron wavefunction
presumably tends to behave essentially the same
way the old-style treatment said it should behave.
</p><p>
Can this intuition be formalized?  
A key ingredient is surely the "Trotter product formula"
</p><center>
exp(A+B) = lim<sub>N&#8594;&#8734;</sub> [exp((A/N)exp(B/N)]<sup>N</sup>.
</center><p>
This limit is valid if A and B are finite-dimensional real or complex matrices,
and also holds, more generally, if A and B are nonnegative self-adjoint <i>operators</i>,
or operators satisfying certain other criteria.  
(Trotter 1959; Reed &amp; Simon 1980; Engel &amp; Nagel 2000; and much work by
Takashi Ichinose.)
More generally
</p><center>
exp(A+B+C) = lim<sub>N&#8594;&#8734;</sub> [exp((A/N)exp(B/N)exp(C/N)]<sup>N</sup>,
</center><p>
and the "A+B+C" can similarly be replaced by a sum of any finite number of operators,
and indeed infinite sums (integrals, averages) can be handled in the right circumstances.
(I have no intention here of delving into exactly what criteria must be satisfied by
the operators and their generalized summation to assure 
the validity of Trotter, and indeed best possible results probably are not known.)
</p><p>
In the "Hamiltonian formulation" of quantum mechanics,  the (unitary)
time-evolution operator is
</p><center>
exp(-iHt/&#8463;)
</center><p>
where H is the (self-adjoint) "Hamiltonian operator."  This, multiplied by the initial
state (at time 0, viewed as a vector in a Hilbert space) yields the state at time t.
</p><p>
The point I am driving toward, is this.  Time-slice Minkowski spacetime into
N time intervals of duration t/N each.  Within each time interval, there are raindrops.
The interactions on their bricks, and the propagation of wavefunctions between bricks
acting during that timeslice, can be described by an operator. The same operator
would happen for any other timeslice, except that its raindrop locations are different.
Considering the raindrop locations are independent random, 
in some sense the "average" such operator
is approximately "what would have happened in the old-style QED
picture without any raindrops or bricks."  (Except for the fact that is not actually defined
and has no meaning, due to the foundational problems of old-style QED.)
Hence 
upon multiplying all such operators in time-order and taking the limit as N&#8594;&#8734;, then
we ought by the Trotter product formula to get the same time evolution
as old-style QED.
</p><p>
Thus, in some <i>nonrigorous</i> sense, rain of bricks QED in the limit 
L<sub>pl</sub>&#8594;0+ 
(and hence &#961;<sub>rain</sub>&#8594;&#8734;)
approaches old-style QED and in this limit, true time-invariant
energy-eigenstates, such as the hydrogen ground state, happen.
</p><p>
There are a couple of reasons that was neither rigorous nor correct.  
In addition to old-style QED
not actually existing, rain of bricks does <i>not</i> take the limit
&#961;<sub>rain</sub>&#8594;&#8734;, but rather employs a
fixed and large – but finite – raindrop
4-density, and fixed and small brick size.  (Which is a good thing, because old-style QED 
still remains fairly sane at Planck energies, as opposed to at much higher
energies which would have been encountered if you really tried to go to the limit,
where QED completely loses any claim to validity due to 
"<a href="#landaupole">Landau pole</a>" phenomena.)
However, it still might be the case that in the limit t&#8594;&#8734;,
using an approaching-infinite number of  timeslices of small and fixed duration, we
would have, for (say) the hydrogen system, a time-evolution operator which
in the limit could be written as the <i>t</i>th  power of some fixed operator.
Or (more likely) such 
that its <i>t</i>th root approached a fixed operator in the limit t&#8594;&#8734;.
</p><p>
<b>If so,</b> then there would be a rigorous sense in which a <i>"Hamiltonian" was defined</i>
in rain of bricks QED, and in which "time invariant" (or essentially time-periodic)
energy eigenstates <i>could</i>
be regarded as existing and defined in an averaged sense in the t&#8594;&#8734; limit.
(A state with energy E has period h/E.)
Further, such <i>t</i>th-root limit-defined operators would presumably be just what have
been long-sought (unsuccessfully so far) by the purveyors of axiomatic "constructive field theory."
</p><p>
<b>If not,</b> then we instead would conclude that the whole Hamiltonian formalism 
irretrievably breaks down and really cannot be used in rain of bricks
physics, at the most fundamental level – and that presumably "constructive field theory" 
just cannot be used for QED.  Either result would be an excellent step forward.
</p><p>
I suspect that
the latter option happens, or anyhow that 
if there is a deterministic hamiltonian formulation of rain of bricks QED, it is limited
in applicability and only approximate.
The reason I suspect that is that I think "raindrop randomness
cannot be averaged out."  That is, I suspect that raindrop randomness can irretrievably 
alter history resulting in a macroscopically very different future.    A nucleus decays,
or not.  This causes somebody to get cancer, or not.  That person, after not dying of
cancer, alters world history.
</p><p>
To switch topics, consider space and time.  Continuum spacetime in some sense is 
not present, microscopically, in
rain of bricks QFT.  There are only bricks, propagation, and interaction.
I do not think it is really right to say that macroscopic space and time
magically "emerge," since of course, the rain of bricks distance formula was defined based on a 
concept of an overlying Minkowski spacetime, even if one now dismisses that as "not real."
Thus this "emergence" is hardly "magical."  (In LQG, in contrast, such emergence seems
considerably more magic. If it happens at all.)
In any case, the physical predictions
rain of bricks outputs  should act very much as though there is a "locality" notion 
defined by the Minkowski space, regardless of whether we consider that space to 
"really exist"; and this section's Hamiltonians/bound states discussion
has outlined one particular way in which, perhaps,
that might in future be made precise.
</p>

<a name="badsyms"></a>
<h3>32. Other non-symmetries (now not even approximate): Scaling, and Cunningham "inversions"</h3>

<blockquote>
I cannot quite imagine it possible that any physical meaning be afforded to substitutions of 
reciprocal radii...
It seems to me that you are very much over-estimating the value of purely formal approaches...
<br>&nbsp;&nbsp;<b>–</b> 
Albert Einstein in letter to Felix Klein in 1916.
</blockquote>
<!-- MAPLE9:
with(linalg):
RS := (x,y,z) -> x^2+y^2+z^2;
FF := (x,y,z) -> 1/sqrt( (x-1)^2 + (y-2)^2 + z^2 );
L1 := laplacian( FF(x,y,z), [x,y,z] );
simplify(L1);  #get 0
Q1 := laplacian( (1/sqrt(RS(x,y,z)))*FF(x/RS(x,y,z),y/RS(x,y,z),z/RS(x,y,z)), [x,y,z] );
Q2 := laplacian( (1/RS(x,y,z))      *FF(x/RS(x,y,z),y/RS(x,y,z),z/RS(x,y,z)), [x,y,z] );
simplify(Q1);  #get 0
simplify(Q2);  #do NOT get 0.
-->
<p>
We already saw in 
<a href="#scalinvar">§11</a> that
old-style QED and QCD obey <i>scaling</i> symmetries.
However, this is no longer true with rain of bricks, at all.
Scaling would alter the value of &#961;<sub>rain</sub> and hence is totally forbidden.
</p>
<a name="tabclassicalgroup"></a>
<table>
<caption>
<b>Table 10:</b>
Some "classical groups."
</caption>
<tbody><tr><th>Group</th><th># degrees of freedom</th></tr>
<tr bgcolor="pink"><td>O(N): the (multiplicative) group of
N×N orthogonal matrices; describe reflections and rotations 
preserving the origin and all distances
in N-dimensional space</td><td>(N-1)N/2. If adjoin scalings, then add
one more degree of freedom. SO(N) is the subgroup of O(N) defined th same way except we
also demand determinant=+1.</td></tr>
<tr bgcolor="aqua"><td>SO(N,1): the "Lorentz transformations" of
N+1 dimensional Minkowski spacetime
preserving the origin and all pseudodistances
</td><td>(N+1)N/2.  We again induce a natural subgroup SO<sub>+</sub>(N,1) when we 
demand preservation of the direction (sign) of time.</td></tr>
<tr bgcolor="pink"><td>EucIsom(N):
Group of distance-preserving maps of Euclidean N-space
</td><td>Since rotation in O(N) followed by translation, (N+1)N/2.
</td></tr>
<tr bgcolor="aqua"><td>Poincare(N):
Group of pseudodistance-preserving maps of Minkowski N+1 dimensional spacetime
</td><td>Since Lorentz followed by spacetime translation, (N+1)(N+2)/2.
</td></tr>
<tr bgcolor="pink"><td>Conformal(N):
Group of conformal transformations of <b>R</b><sup>N</sup>
</td><td>If N&#8804;2 then infinite, but if N&#8805;3 then by Liouville's theorem 
(Blair 2000) this is a finite-dimensional group, abstractly 
isomorphic to SO(N+1,1), with therefore (N+2)(N+1)/2 degrees of freedom.
</td></tr>
<tr bgcolor="pink"><td>Subgroup of conformal transformations of <b>R</b><sup>N</sup> 
preserving the standard unit sphere centered at 0
</td><td>(N+1)N/2 if N&#8805;3</td></tr>
<tr bgcolor="aqua"><td>Cunningham(N):
Group of conformal transformations of N+1 dimensional Minkowski spacetime
</td><td>(N+3)(N+2)/2</td></tr>
<tr bgcolor="aqua"><td>
Subgroup of conformal transformations of N+1 dimensional Minkowski spacetime
preserving the standard unit pseudosphere 
</td><td>(N+2)(N+1)/2</td></tr>
<tr bgcolor="yellow"><td>GL(N):
The (multiplicative) group of all N×N real invertible matrices.
</td><td>N<sup>2</sup></td></tr>
</tbody></table>
<p>
It is not well known (and those who do know it often write it incorrectly), 
but Maxwell's <i>vacuum</i> equations (that is, 
for photons only, without matter and charge) enjoy a substantially larger symmetry
group than merely the Lorentz/Poincare group. This was discovered by 
two Cambridge students, Ebenezer Cunningham
and Harry Bateman, and published by them in 1909 and 1910 in the 
Proceedings of the London Mathematical Society.   
Dirac 1936 then re-studied this from a more 
abstract and sophisticated perspective.
Cunningham's symmetries are conformal maps described by
15 continuously variable parameters. 
Meanwhile the Poincare(3) group is described by only 10
parameters and is a subgroup (or limiting case of a subgroup) of Cunningham(3).
Rescaling symmetry (a 1-parameter group) also is always present as a subgroup if Cunningham
is available.
Each Cunningham symmetry enables converting (via certain known rules)
any solution of Maxwell's vacuum equations into another.
</p><p>
<b>Notes:</b> Cunningham maps convert Maxwell vacuum solutions to other Maxwell vacuum solutions
but do <i>not</i> necessarily preserve energy.  They can interconvert spacelike and 
timelike vectors, although they always preserve local light cones. Also, Cunningham maps affect
fields in a somewhat nontrivial manner.
Specifically, the fields are altered by whatever Poincare/Lorentz transformation 
the local conformal map is proportional to (in the usual manner) and then also by a "conformal
factor" by multiplying by a constant power of the local conformal length-change factor –
which power, depends on what field we are speaking of, see
<a href="#scalinvar">table 8</a>:
for 
<a href="#masscunn">mass</a>
it would be -1, for Maxwell fields it is -2,
for the Maxwell 4-potential the power is -1, 
and for Dirac fermion fields it is -3/2. 
</p><p>
The simplest example to make it clearer what we mean by such "conformal factors"
is the plain "wave equation" 
<nobr>
&#8706;<sup>2</sup>F/&#8706;t<sup>2</sup>=&#8711;<sup>2</sup>F.
</nobr>
As Bateman showed (in 1908 work preliminary to considering Maxwell's equations)
this equation in Euclidean spaces is invariant under the "Cunningham inversion" conformal
map of replacing each coordinate-vector x by x<sub>new</sub>=x/|x|<sup>2</sup>,
where |x|<sup>2</sup> is computed using Lorentz pseudonorm where x is a spacetime 4-vector,
<i>provided</i>
<nobr>
F<sub>new</sub>(x) = F(x<sub>new</sub>)/|x|.
</nobr>
This division by |x|
is the "conformal factor," in this case, for F.
</p><p><small>
<!--From Kastrup's introduction, but he has typos!-->
This discovery by Bateman can be accomplished by simply selecting one coordinate to
now regard as "timelike," i.e. <i>imaginary</i>,
inside Lord Kelvin's 1845 discovery
that if &#934;(x) solves the Laplace equation 
&#8711;<sup>2</sup>&#934;=0 
governing vacuum electro<i>static</i> potential,
then
A|x|<sup>-1</sup>&#934;(A<sup>2</sup>x/|x|<sup>2</sup>)
is another solution, where now |x|
is the ordinary 3D Euclidean length and A&gt;0 is any constant
(see Kellogg 1929's page&#8776;232).  <!--confirmed by MAPLE9 as above.-->
Kelvin sent his discovery to Joseph Liouville prompting the latter's
theorem (sometime before 1850)
that the inversive transformations
x&#8594;Ax/|x|<sup>2</sup> together with Euclidean isometries generated 
<i>all</i> smooth conformal maps of Euclidean N-space for each N&#8805;3.
</small></p><p>
It later was realized that the 
non-interacting vacuum field equations for 
<i>every</i> presently-known kind of <b>massless</b>
physical particle (as well as some not yet known to exist), 
also enjoy conformal symmetries, including:
</p><ul>
<li>
massless scalar fields (the "wave equation"),
</li><li>
2- and 4-component "massless neutrinos" (spin=1/2),
</li><li>
Yang-Mills SU(N) [or indeed with any gauge group] theory
<i>in 4 spacetime dimensions only</i>;
this includes "gluons" when N=3, naked W- and Z-bosons when N=2, and 
Maxwell's equations when N=1 (all spin=1),
</li><li>
gravitons (spin=2; Einstein vacuum gravity <i>linearized</i> about the flatspace metric)
</li><li>
Bargmann-Wigner (1946) equations for massless fields of arbitrary helicity
[includes neutrino and Maxwell equations as special cases]
</li></ul>
<p>
I recommend the review article by
Fulton, Rohrlich, Witten 1962 as a good starting point to learn about this area.
They point out that Cunningham's 15-parameter group is isomorphic to O(4,2).
They discuss three successively more general
kinds of "conformal symmetry." 
The first kind is Cunningham's
kind in flat (1+3)-dimensional space.  
The second is conformal transformations of
general Riemannian metrics (meaning, transformations of the coordinate system which multiply the
metric tensor by a positive-real-valued function of the coordinates).  
The third is the same but applied to H.Weyl's generalization of 
Riemannian geometry.
They prove in their §C5 (also redone variationally in C8) that Maxwell's equations
enjoy all three kinds of symmetry.   In C9 they show that Dirac and Maxwell's 
mutually-interacting QED equations (also the combination of Maxwell equations and 
the Lorentz-force law for 
movable nonquantum point-charges)  obey Cunningham symmetry in the limit of massless electrons.
<a name="masscunn">They</a>
also enjoy Cunningham symmetry even with <b>massive</b> electrons <i>provided</i>
we strangely regard the electron mass, 
not as a constant, but rather as a position-and-time-dependent
quantity, which can vary along a particle's world-line,
that transforms with "conformal weight -1/2"; this is normally regarded as 
clearly unphysical so that only in the absence of mass could Cuningham transformations
be considered of posible physical interest.
</p><p>
Birrell &amp; Davies 1984 claimed without any proof or citation that
a form they gave for Dirac's equation on general curved manifolds enjoyed conformal invariance;
however, the extensive literature
survey by Kastrup 2008 failed to find any proof of any such claim.
<!--
They also note 
W.E.Pauli: 
Uber die Invarianz der Dirac'schen... Ruhemasse, Helv. Physica 13 (1940) 204-208
considered general curvedspace conformal invariance of Dirac eq
and they imply he succeeded, but give no details???
Obituary by R. E. Peierls: Wolfgang Ernst Pauli. 1900-1958
Biographical Memoirs of Fellows of the Royal Society
Vol. 5, (Feb., 1960), pp. 175-192 
-->
</p><p>
Other important papers on this (all in flat Minkowski space) include
<!-- Dirac 1936 (spin=1/2 and Maxwell), he used hexaspherical coords, but his Dirac eqn
is NOT equivalent to his original one and its mass is not mass... situation later clarified 
by W.A.Heppner: The inhomogeneous Lorentz group and the conformal group, 
Il Nuovo Cimento (10) 26,2 (1962) 351-368.
-->
Lomont 1961 (any-spin particle provided governed by wave
equations of "Dirac-like" form based on certain families of matrices),
Schwarz 1982 (proof that the SU<sub>2</sub> gauge group and conformal group 
constitute the full Lie symmetry group
of SU<sub>2</sub> Yang-Mills equation; the conformality proof of Yang-Mills does not depend on
the gauge group and Schwarz's result was extended to arbitrary semisimple gauge group
by Pohjanpelto 2004),
Bracken &amp; Jessup 1982 (re-examination of all the cases listed above except Yang-Mills,
with correction of widespread errors in literature).
For Yang-Mills, see below.
</p><p><small>
In 1979 Fushchych &amp; Nikitin
went further and found 23-parameter continuous groups of symmetries for both the
Maxwell and massless Dirac equations (!) but their
new "non-Lie" symmetries are not about coordinate-transformations but rather involve transforming
the fields and wavefunctions, possibly involving their spacetime derivatives.
The simplest example (which is well known) of such a symmetry is the 1-parameter group
of "duality rotations" which interchange the electric and magnetic fields and 
constitute a symmetry of
Maxwell's sourceless equations.
Duality rotation
symmetry still holds for the usual version of Maxwell's equations in a general "curved space"
metric, but there is no analogous symmetry for Yang-Mills fields
(see Deser &amp; Teitelboim 1976, Deser 1982, Schwarz 1982, Pohjanpelto 2004).
Just in the last 10 years(!), new computer-aided investigations have found two new classes of
conservation laws for Maxwell's equations, see the papers by Anco &amp;
Pohjanpelto (and the same authors in reverse order) about this.
Although these new symmetries and conservation laws probably will be important,
they seem of little interest for the present paper's purposes,
hence we shall not discuss them.
<!--Bateman showed that for the Maxwell equations
the Cunningham group was the full Lie 
group of coordinate transformations.-->
</small></p><p>
The reason the Cunningham transformations remained little-known for decades
is probably because no great use
for them was ever found in electromagnetism (except in electro<i>statics</i>) 
and also there seemed to be no
profound "reason for their existence."  
However, I shall argue later (<a href="#desit">§35</a>)
that the conformal symmetries serve a very useful role
in helping us to find <i>unique</i> forms of equations of mathematical physics reset in 
certain curved spacetimes such as de Sitter space.
These equations would otherwise be non-unique.
It is an interesting question whether the recent new kinds of symmetry can be used
to obtain a unique candidate for the equations of physics in general curved spacetimes.
</p><p>
<b>Some field equations which are <i>not</i> conformally invariant (and why):</b>
</p><ul><li>
The quantum field equations for particle types with nonzero constant rest-mass M
do <i>not</i>  enjoy conformal invariance because M (in combination with
&#8463; and c) sets a length scale
forcing a violation of scale invariance.
</li><li>
Einstein vacuums do <i>not</i> enjoy conformal invariance if there is a nonzero
Einstein cosmical constant &#923; (since this sets a length scale).
</li><li>
The Maxwell equations, although defined in any dimension when written in the form used
by special relativists (Misner, Thorne, Wheeler EQs 22.19a,b and 22.17a,b), do <i>not</i>
enjoy conformal symmetry in any dimension greater than 3+1.
(In lower dimensions such as 2+1, the Maxwell equations degenerate to comparatively trivial forms
since there no longer is any magnetic field and everything is described by a scalar potential.)
</li></ul><p>
The latter two 
failures both can be ascribed to the fact that the conserved stress-energy tensor
acquires a nonzero trace T<sup>&#956;</sup><sub>&#956;</sub>, in contrast to its tracelessness 
when &#923;=0 or in (3+1)D respectively.
In Einstein gravity that is because 
the stress tensor is proportional to
&#923;g<sub>&#956;&#957;</sub>.
For Maxwell this can be verified from the formula for the stress-energy tensor
(see Misner Thorne Wheeler EQ 5.22 and note the "1/4"), but its root cause is that
only in n=4 spacetime dimensions 
are the electric and magnetic fields "equivalent" in various senses, most obviously
the sense that they have the same dimensionality 3.   [The electric field
dimension is n-1 and the magnetic 1+(n-3)n/2.]
It seems to be impossible for conformal symmetry to hold for a <!--quantum--> field whose
stress-energy tensor has nonzero trace.
When trace=0, that corresponds to a relativistic
"gas" with pressure equal to its density, i.e. made of particles with momentum
equal to their energy (in units with c=1); but for nonzero trace this equality fails,
corresponding to a "gas" with nonzero rest-mass particles, thus setting a length scale.  
<!-- Less fuzzily, one may show the
action integral is changed, under an infinitesimal conformal transformation, by an amount
proportional to the integral of this trace, which causes the lagrangian to be
conformally invariant if traceless T.
Book: Philippe Di Francesco, Pierre Mathieu, David Senechal:
Conformal field theory, Springer 1997
-->
</p><ul><li>
More generally, the Yang-Mills equations are not scaling-invariant in any spacetime dimension
other than 4.  (And even in 4, they are not if fixed-strength
interactions with the Higgs boson are added.)
</li><li>
<i>Interacting</i> fields, even if they all are massless and 
conformally invariant in isolation, 
still conceivably might disobey
conformal symmetry, although I think in the known physical cases they still enjoy it
(provided issues associated with renormalization are ignored).
For example, I believe that 
(a) QED with massless electrons interacting with a Maxwell field,
and
(b) QCD with massless quarks and gluons,
<!--
and 
(c) the linearized classical theory of
Einstein-Maxwell <i>small</i> gravity+electromagnetic fields--except 
Maxwell would source Einstein only if Einstein sources it too, which
means not small and now nonlinear; or no sourcing, in which case noninteracting
-->
both ought to be Cunningham-symmetric.
<!--
W.R&uuml;ml: On conformal invariance of interacting fields
Communications in Mathematical Physics, Volume 34, Issue 2 (1973) 149-166
seems not very comprehensible.
Maybe good:
http://arxiv.org/abs/hep-th/9607110 = Nucl.Phys. B495 (1997) 433-450
-->
Note that (b)
would seem to prove, as a corollary, the impossibility of a discrete spectrum of
stationary "glueball" bound states 
made solely of interacting gluons (without quarks).  But in the real world, since
it is impossible to exclude virtual quarks, glueballs might still be possible.
Also in the real world there is renormalization,
or with rain of bricks violation of scale invariance,
which either way would effectively violate conformal symmetry, thus permitting glueballs.
</li></ul>
<p>
<!--
We will now show (perhaps for the first time???) that gluon fields (i.e. QCD in the absence
of quarks) also are symmetric under the full Cunningham group, and that indeed full QED 
and full QCD 
both would be  Cunningham-invariant <i>if</i> the electron and quark mass parameters
m were all changed to <i>zero</i>.    NO, THINK THIS PROOF TOO FACILE???
If fieldeqs are 1st order linear then think ok, but QCD has nonlinear so scaling can
kill you.  KleinGordon has 2nd order field eqs and conformal invariant if massless only
in 2D or if put scalefac depending on local conformal factor in other D.
Dirac if massless is nonlinear if put in EM fields hence cannot; if only electrons then 
I think you are ok.  J.Baez says 4D YangMills eqs are invariant under conformal group too
despite being nonlinear.  C.Lanczos says Maxwell eqs are Cauchy-Riemann eqs for quaternion 
analyticity.
[R.Jackiw: Gauge-covariant conformal transformations, PRL 41 (978) 1635-1638.]
[The Maxwell conserved mom-en tensor is traceless only in (3+1)D.
There appears to be no way to imprve it to become traceless in higher
dimensions.]
[John C. Baez: Conserved quantities for the Yang-Mills equations, Adv Math 82 (1990) 126-131.
Incomprehensible.]
[the principle (first proposed by Weyl and lucidly discussed by Hoyle and Narlikar) that all 
the fundamental equations of physics should be invariant under local (spacetime-dependent) 
transformations of units (principle of conformal invariance)]
[G.Mack &amp; A.Salam: Finite-component field representations of the conformal grpup, 
Ann Phys 53,1 (1969) 174-202]
[Thomas P. Branson:
Quasi-invariance of the Yang-Mills equations under conformal transformations and conformal 
vector fields, J. Differential Geom. 16,2 (1981), 195-203.  Says in the 3+1D flatspace case
this invariance is well known.  Incomprehensible as usual.]
[R. Jackiw and C. Rebbi: Conformal properties of a Yang-Mills pseudoparticle,
Phys. Rev. D 14,2 (1976) 517-523  only concerns a particular Yang-Mills solution, not all]
[J.S.Lomont: Conformal invariance of massless Dirac-like wave equations
Il Nuovo Cimento 22,4 (1955-1965) 673-679.
It is shown that all Dirac-like wave equations with positive integral or half-integral spin, 
zero rest mass, and no interaction are conformally invariant.
The associated conservation laws are given in a very simple form.]
[V.I. Fushchich &amp; A.G. Nikitin: 
Conformal invariance of relativistic equations for arbitrary spin particles,
Letters in Mathematical Physics 2,6 (1978) 471-475
http://www.imath.kiev.ua/~fushchych/papers/1978_6.pdf
first established for Maxwell, then spin=1/2, then any spin, now generalize+unify all that]
[V.I. Fushchich &amp; A.G. Nikitin: On the new invariance group of Maxwell equations,
Lettere al Nuovo Cimento 24,7 (1979) 220-224]
--
J.A.McLennan: Nuovo Cimento (10) 3 (1956) 1360-1379 is claimed to be invalid.  
It supposedly showed all poincare-invariant type 1A eqns are conformally invarient when massless.
Any manifestly-Lorentz-invariant helicity.
And
Conformally invariant wave equations for non-linear and interacting fields,
5,3 (1957) 640-647 is about weird probably unphysical eqns
-->
</p><p>
<b>"Conformal" maps</b> in Euclidean D-dimensional space are continuous and differentiable maps
of that space into itself which preserve angles between crossing infinitesimal line segments.
It is well known that when D=2 every analytic function mapping the complex plane to itself
is a conformal map; but for each D&#8805;3 the full set of conformal maps is known 
by a theorem of Joseph Liouville (1809-1882)
to be the isometries (e.g. rigid rotations and translations)
of D-space, together with the <i>spherical inversions</i> (Blair 2000).
An "inversion" of D-space caused by a sphere of radius R, maps the sphere center to infinity
(and vice versa) and maps a point at distance s to the sphere center, to the point on the 
same ray from the center but now at distance R<sup>2</sup>/s.  The name "inversion" is motivated by
the fact that this map interchanges the sphere's interior and exterior (and has
no effect on its surface). 
</p><p><small>
<b>Historical note:</b>
Kastrup 2008 traced the invention of 
inversions, and the recognition of their most basic properties,
to the little-known Southern-French self-educated mathematician/physicist
J.B.Durrande (1798-1825) who
<a href="http://archive.numdam.org/article/AMPA_1820-1821__11__1_0.pdf">published</a> in 1820-1825
in Annales de Mathematiques Pures et Appliquees (edited by J.B.Gergonne).
Durrande might have gone on to greater fame had he not died so young.
Inversions then were popularized by the Swiss geometer Jakob Steiner (1796-1863).
For the reader who knows complex analysis, it is a trivial matter to see that inversions in 2D
are the only <i>1-to-1</i> conformal maps (aside from Euclidean isometries); if one does not
know complex analysis then because of scale invariance
it suffices to prove they preserve angles of
smooth curves crossing the unit circle [and because of rotation invariance it
suffices merely to prove it for curves crossing it at (x,y)=(1,0); and this in turn
follows from 
<nobr>1/(1-z)=1+z+z²...],</nobr> to show conformality everywhere.
</small>
</p><p>
That all can be generalized to Minkowski (D+1)-dimensional space<i>time</i> by using all the exact 
same formulas but regarding the time-coordinate as
<i>imaginary</i> so the Euclidean distance changes to the
Minkowski distance. Instead of inversion in spheres
like 
x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>=R<sup>2</sup>
we now get inversion
in "pseudospheres" (which are hyperboloid surfaces) such as
x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>-t<sup>2</sup>=R<sup>2</sup>
where R can now be either real or imaginary.
These are the <b>Cunningham coordinate-transformations.</b>
</p><p>
Inversions map spheres to spheres (although the <i>center</i> of the first sphere is mapped to
a noncentral point of the second); 
Cunninghams map pseudospheres to pseudospheres and in particular
light cones to other light cones.
</p><p>
Bateman indeed showed that these were the <i>full</i>
set of continuous maps preserving Maxwell's equations,
by showing that
any Maxwell-preserving transformation had
to be locally (to first order) a scaling &amp; Lorentz, i.e. the map had to be conformal.
</p><p>
To <b>prove the <a href="http://en.wikipedia.org/wiki/Yang%E2%80%93Mills_theory">Yang-Mills</a> 
equations are conformally invariant</b>, combine
the facts that (1) the Maxwell equations are, and (2) the Yang-Mills non-Maxwell terms are,
with the second part being obvious from combining (i) Lorentz invariance, (ii) scaling invariance,
(iii) these quantities each involve only first derivatives.
</p><p>
"Conformal maps" are locally just rotations and scalings, i.e. provided we get to ignore
2nd derivative and higher ("bending") effects.  The Yang Mills
lagrangian densities are rotation-invariant (meaning Lorentz invariant, since
time is "imaginary" in the Cunningham math,
which converts "rotations" to "Lorentz transformations"). And in 4 spacetime dimensions
they are scale
invariant. (These are easy to see.)
And finally part (1) is by Cunningham and part
(2) is since the field equations can be regarded as involving only
<i>first-order</i> differential operators – and hence
cannot perceive bending effects.
<b>Q.E.D.</b>
</p><p>
This is a stunningly short proof, far shorter than Cunningham and Bateman's demonstrations,
and the same argument would seem employable for the most commonly seen kinds of interaction term
(re the discussion above on interacting fields which are conformally invariant in isolation).
<!--It actually shows that <i>any</i> massless quantum field whose
Lagrangian density obeys a scale-invariance property, and Lorentz-invariance, and such that it
and the field equations can be
written as a first-order
differential operator (while still obeying the scale- and Lorentz- symmetries), 
obeys the Cunningham symmetries.  -->
</p><p>
<b>With rain of bricks</b> 
physics, the Cunningham transformations no longer are exact symmetries.  
Furthermore, they are not even good approximate almost-symmetries since
they would distort the raindrop-density far away from being constant (it would be arbitrarily
near zero and near infinity in different locations).    
</p><p>
So rain of bricks largely vindicates those who believed Cunningham symmetries 
were a useless coincidence with no real physical importance;
those who thought otherwise (if any) presumably are unhappy.
</p>

<a name="moresyms"></a>
<h3>33. Overall <i>gain</i> of symmetry? </h3>
<p>
Physicists love symmetry. So presumably they are bemoaning the loss, with rain of bricks, of
various kinds of symmetry and conservation laws as noted above.
But do not be too quick to grieve.
</p><p>
First, although it is true that, e.g, microscopic Lorentz symmetry is lost,
if we regard the true universe as Minkowski 1+3 space and then <i>declare</i> 
that a Poisson-random process drops in the rain of bricks...  then <i>that description</i> is
perfectly Lorentz invariant.  (Note: if a point lattice had been used instead of Poisson-random
raindrops, then we would <i>not</i> have enjoyed this perfect Lorentz invariance.)
So <i>in that sense</i>, all of the approximate symmetries mentioned above, actually are
exact and were not lost.
</p><p>
But the Cunningham and scaling symmetries genuinely <i>are</i> lost.
</p><p>
Second, the rain of bricks arena is (please recall) the Cartesian product of the
raindrop points
in Minkowski space, with a brick.  And each brick is a symmetrical object.
For (D+E)-dimensional parallelipiped bricks, there is a (D+E)-parameter
continuous group of symmetries, while for
D-surface-dimensional "round bricks," there is a (D+1)D/2-parameter
continuous group of symmetries.  This group automatically is a symmetry group
of the entire arena and constitutes <i>extra</i> symmetry beyond that enjoyed
by old-style QED set in plain Minkowski space.  This extra symmetry constitutes as many
(for 1-dimensional bricks), 
or more (for higher-dimensional bricks), 
parameters than for the lost scaling and Cunningham symmetries.
(Further, it is good that those symmetries were lost, since they are not and cannot be 
symmetries of the true theory of physics.)
</p><p>
So the claim that rain of bricks lost symmetry is debatable.  
Arguably, it <i>gained</i> symmetry.
</p>

<a name="decoherence"></a>
<h3>34. A benefit of broken symmetry: quantum "decoherence" abolishes "weirdness" </h3>

<!--
<blockquote>
Any good quote?
<br>&nbsp;&nbsp;<b>&ndash;</b> 
</blockquote>
-->
<p>
As we've said, rain of bricks breaks Lorentz-Poincare invariance.  That means that a chunk
of vacuum <i>here</i> is a <i>different environment</i> than a chunk of vacuum <i>there</i>.
This automatically causes position-based dephasing (cf. 
<a href="#densdecoh">this</a> in our
<a href="#whatswrong">§4</a>, the
<a href="http://en.wikipedia.org/wiki/Quantum_decoherence">"decoherence"
article in Wikipedia</a>, Tegmark 1993,
and Zurek 1991), 
<b>solving the measurement problem</b> in quantum mechanics.
For example, a proton located in one place, will have 
slightly different mass-energy than if it were located in another, due to the interactions 
among the quarks, gluons, etc inside the proton (which are responsible for &#8776;99%
<!-- http://en.wikipedia.org/wiki/Quark#Mass -->
of its mass-energy) occurring in bricks whose locations
are differently random.  This will change its mass-energy by &#916;E for some
timespan &#916;t, causing a phase-angle change to its wavefunction of
&#916;E·&#916;t/&#8463; 
radians.  (We shall regard a phase-angle change of order 1 radian, as "a dephasing.")
</p><p>
This dephasing effect automatically happens all the time to all atoms, causing their positions to
be continually "measured" by the rain of bricks universe, thus preventing 
quantum weirdness.    (I.e, "Off diagonal" terms in the Von Neumann density matrix
in position basis, get multiplied by random complex phase-angle factors; on-diagonal terms
are unaffected.  This is known to have essentially the same mathematical effect
as classical measurement, cf. Zurek 1991.)
This is an ongoing extremely small <i>but not zero</i> effect.
</p><p>
The rain of bricks universe similarly
also "measures" <i>orientations</i> of objects (due to broken rotation invariance)
and their <i>velocities</i> (broken boost invariance).
We'll now try to make some crude numerical estimates
of these effects, using several different techniques so the 
reader can judge how unreliable they are.  We'll see that different
dephasing effects in rain of bricks QED can yield
hugely different dephasing rates.
</p><p>
<b>Lepton dephasing.</b>
Leptons, being (unlike the proton)  true <i>point</i> particles, whose
self-energy (i.e. mass) are substantially affected by very <i>small</i>
length scales, might be suspected to be especially affected by dephasing.
Leptons in QED are "dressed" in a "cloud" of
photons and electron/positron virtual pairs.  As we saw in <a href="#crudenumer">§22</a>,
this dressing is responsible for 14-16 percent of the lepton's mass-energy
in rain of bricks QED,
and <i>is</i> sensitive to the precise details of the raindrop locations.
Rain of bricks, therefore, <i>should</i> cause position-based dephasing for electrons.
<!-- although the effect ought to be smaller than it is for nucleons? -->
</p><p>
As we saw in <a href="#crudenumer">§22</a>,
the self-energy of a lepton grows proportionally
to the |logarithm| of the energy (or 1/length) cutoff,
and ought to ultimately constitute about 15%
of the observed dressed mass of the lepton.
That logarithmic growth means as we go from the
electron mass scale 511 KeV up to the Planck scale at 1.2×10<sup>25</sup>KeV –
an increase factor of 3×10<sup>22</sup> –
the final decade ought to exert about the same
effect as each of the other 21.5 decades.  In other words, the region within 10 Planck lengths
away from the electron ought to be responsible for about  0.7% of the electron's observed mass.
This tiny region for a timespan comparable to its lightspeed transit time, should
contain on the order of 10<sup>5</sup> raindrops, and hence we should expect fluctuations
of the order of 0.3% of that 0.7%.  We conclude: <i>over a 10 Planck timespan, the dressed
electron mass should fluctuate (fractionally) by about 2 parts in 10<sup>5</sup></i>,
and essentially the same also should be true for the muon and tauon.
</p><p>
Now summing these fluctuations over larger timespans and assuming the fluctuations should
sum <i>in quadrature</i>, we see that our lepton should <i>dephase</i>,
meaning its
mass fluctuation averaged over a timespan T
(this average value diminishes proportionally to T<sup>-1/2</sup>), 
<i>multiplied</i> by T, should first exceed
&#8463;/2,
when
</p><center>
T·(T/10)<sup>-1/2</sup>·2×10<sup>-5</sup>·M &#8776; 1/2
</center><p>
(where M is the lepton's mass and the above equation employs Planck units)
in other words
</p><center>
T<sub>dephase</sub> 
&#8776; 
6×10<sup>7</sup> (M<sub>pl</sub>/M)<sup>2</sup> Planck time units
&#8776; 
3×10<sup>-36</sup> (M<sub>pl</sub>/M)<sup>2</sup> seconds.
</center><p>
which is
</p><center>
T<sub>dephase</sub>=60 years, &nbsp;&nbsp;&nbsp; 12 hours,  &nbsp;&nbsp;&nbsp; 3 minutes
<br> 
for an 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
electron, &nbsp;&nbsp;&nbsp; muon,  &nbsp;&nbsp;&nbsp; tauon
</center><p>
respectively. 
I warn the reader that these estimates are presumably only accurate
to within a factor of 10<sup>±6</sup> given that the raindrop density is only
known to me very approximately.
The muon and tauon are unstable with respective rest-frame 
mean lifetimes  
<nobr>2.2×10<sup>-6</sup></nobr>
 and
<nobr>2.9×10<sup>-13</sup></nobr> seconds 
(versus electrons which are stable) indicates that experimentalists probably would be unable
to see raindrop-caused muon and tauon dephasing.  However,
remarkably, electron dephasing seems nearly(?) <b>within reach of experiment.</b>
<!--
The measurement-like effects should tend to be greater for heavier objects.
We can make a very rough lower estimate of the magnitude of this effect.
An object with 3-dimensional volume V and mass M will be expected to dephase
90 degrees after a time t of order at most 
</p><center>
M<sup>-2</sup>V
</center><p>
(everything measured in Planck units).   This is because the number of
raindrops in the object during time t will be of order Vt in expectation and
typically will deviate from that expectation by
&plusmn;(Vt)<sup>1/2</sup>. 
This presumably will alter the strength (expressed as a time-averaged mass-energy) of
the interactions inside the object, by an amount of order
&Delta;E&asymp;(Vt)<sup>-1/2</sup>M. 
We then choose t to make t&Delta;E be of order &#8463; to get 90-degree dephasing.
</p><p>
For a proton, this dephasing time is t&asymp;2&times;10<sup>143</sup> seconds.
</p><p>
But dephasing might be much faster than this.
Our estimate was based only on fluctuations in the total <i>number</i> of
raindrops; there might be larger dephasing effects arising from fluctuations in their locations and
times. 
-->
</p><p>
<b>Other electron quantities</b> besides position
also are thus "measured."  An electron that has
spin <i>up</i> versus spin down will, due to interactions with virtual particles
modulated by its local raindrop environment, have slightly
different energies and hence these two components of its wavefunction will gradually dephase.
A ferromagnet containing 10<sup>20</sup> correlated spins, all of them up (or all down)
would, if in a "Schrödinger cat"
quantum superposition of the all-up &amp; all-down states, decohere at least 
10<sup>20</sup> times faster.
In a recent experiment claimed to break a world record, the spins of phosphorus-31 nuclei
inside a silicon-28 crystal
were placed in a superposition of "up" and "down" and then found to decohere with
remarkably long characteristic
time of 3 hours at 4.2 Kelvin and 39 minutes at room temperature
(Saeedi, Simmons et al 2013).
</p>

<p><b>Proton dephasing, estimate I:</b>
We attempt to take advantage of
the experience of those
undertaking enormous "lattice QCD" computations to determine QCD theoretical predictions
of the mass of the proton, neutron, and other particles.   These computations replace
spacetime with a 4-dimensional grid, usually 
100×100×100×100 or less,
replace the partial differential equations
of physics with certain discrete approximations; and perform huge-dimensional
(dimension&#8776;10<sup>8</sup>) integrations by 
Monte Carlo methods, usually with 10<sup>5</sup> sampling points or fewer.  
There are then three sources of error (assuming this whole procedure has
any validity at all, which certainly has never been proven mathematically, but there is
computational/experimental evidence it does)
</p><ol>
<li>
Systematic error due to the nonzero grid-spacing
</li><li>
Statistical errors arising in the Monte Carlo integrations.
</li><li>
Are we simulating the correct laws
of physics, or something else?  (The simulators often intentionally use wrong laws
to save computation time.  Also, they use "renormalized" laws adapted to their particular lattice
spacing.)
</li></ol>
<p>
After a long period of development, lattice QCD codes are
now enjoying success: as of 2010 they usually predict particle masses which
agree with experiment to ±3% and also to within estimated computational error
(Dürr et al 2008).
Lattice QCDers normally design their programs to roughly equalize these three kinds of error.
(Since making any one kind of error smaller is "wasted computational effort.")
The first kind of error is removable by extrapolation of the grid-spacing A to zero.
While this again has never been rigorously justified, experiments and heuristics both
indicate that simply 
assuming quadratic dependencies on A, works well.
Dürr et al 2009
found that their error, below some critical lattice spacing&#8776;0.1fm, 
became almost flat as a function of A.
Monte Carlo error is reducible with more compute-time and can be estimated 
by "bootstrap" techniques (Efron &amp; Tibshirani 1994).
</p><a name="protondephase"></a><p>
By linearly extrapolating reducing their lattice spacing A&#8776;0.1fm down
to Poisson-random <i>non</i>lattice points at the Planck scale 19 orders of
magnitude smaller (a crude assumption/guess) I, extremely crudely, estimate
that the typical discrepancies between 
proton-masses M<sub>p</sub>
arising from different raindrop configurations
will be comparable to ±3×10<sup>-21</sup>M<sub>p</sub>.
This suggests 10<sup>43</sup> proton-light-transit times will suffice
to dephase a <b>proton</b>, i.e. 
10<sup>20</sup> seconds (approximately 200 times the age 
of the universe).
<!--10^43 * 10^(-15) * 10^(-8)-->
</p><p>
<b>Proton dephasing, estimate II:</b>
A quite different estimate of proton dephasing (which now ignores lattice QCD experience)
is this.
As is well known,
the proton's mass is 99% the result of self-energy, i.e. virtual gluons and photons inside,
with only about 1% of its mass arising directly
from the masses of its three constituent quarks.
In other words, <i>interactions</i> (which of course happens only on bricks) generate the
lion's share of the mass.
These interactions take place within a volume the size of a proton (radius&#8776;0.8 femtometer,
compton wavelength&#8776;1.3 femtometer)
and over a timespan comparable to the speed of light transit time across that volume
(5×10<sup>-24</sup> seconds).
Since this region should contain on the order of 10<sup>80</sup> raindrops,  we expect a fractional
fluctuation of order 1 part in 10<sup>40</sup> in the proton mass during this timespan.
(Note this is considerably smaller than my crude extrapolation of  lattice QCD down
to the Planck scale, which was 3 parts in 10<sup>21</sup>.)
Hence we'd expect a proton to dephase after about 10<sup>80</sup> of these time units,
i.e. about 5×10<sup>56</sup> seconds, i.e. 10<sup>49</sup> years.
</p><p>
<b>Planet earth, etc.</b>
Assume optimistically that the larger of these two proton 
dephasing-rate estimates is correct.
Then the Earth (3.6×10<sup>51</sup> proton masses) would then be estimated 
(by dividing this timespan by the square root of 
3.6×10<sup>51</sup>)
to dephase 
due to this effect in about 2 microseconds.
That computation was assuming all the  
Earth's component protons 
and neutrons dephase independently and make additive contributions to the 
Earth's overall phase-angle.
However, one could (and I would)
argue that they are not independent: measuring the position of just one
nucleon suffices to measure the position of the whole planet.
The fastest few dephasors among the Earth's component protons or neutrons 
presumably would dephase in only about 
<nobr>10<sup>-31</sup></nobr> seconds (assuming individual "dephasing
events" are distributed uniformly in time), which then
would suffice to localize and orient
the whole planet accurate to the size scale (1fm) of a proton.
More time would then presumably localize it more precisely than that.
</p><p>
A human being (4×10<sup>28</sup> proton masses) would by the first estimate dephase in
&lt;1 week but by the second estimate it would take only 2.5 nanoseconds.
</p><p>
But all those estimated dephasing rates for humans, planets, etc
become at least 
10<sup>13</sup> times faster if based on electrons instead of protons –
or if we consider the fact protons contain quarks, charged point particles, which
we could treat the same way we treated leptons.
</p><p>
That seems entirely sufficient to explain the appearance of classical physics.
</p><p>
<b>Important note: Photons</b> do not dephase.
The proton arguments we just gave rested on the fact that protons and neutrons
are composite particles,
&#8776;99% of whose mass arises from energies of interaction between their
internal components.
<i>Photons</i> are not composite particles, and so long as they simply propagate 
<i>without</i> interacting,
the rain of bricks vacuum will <i>not</i> induce any position-based dephasing.
(They can dephase if they interact with normal matter, e.g. gamma ray tracks
are observed in cloud chambers, not spherically symmetric waves; but we would argue this is
due to the cloud atoms being positionally localized due to <i>their</i> dephasing.)
Also note, QED indicates photons have no mass, despite their interactions with the
"QED vacuum," due to exact cancellations
at each order of perturbation theory.
The resulting "photons in vacuum don't dephase" prediction also agrees with experiment: 
</p>
<ol type="a">
<li>
Photons exhibit interference phenomena 
even after traveling 10<sup>9</sup> light years
</li><li>
Correlated photon-pairs that travel on two quite different paths many km long,
interfere excellently (reason LIGO and <a href="http://en.wikipedia.org/wiki/EPR_paradox">EPR</a>
experiments work).
</li></ol>
<p><b>More details about "QED says photons have zero mass."</b>
<small>
One might object that photons can interact with virtual electron-positron pairs in the
QED vacuum.
However, the net effect of such interactions on the photon mass must cancel to
<i>exactly zero</i>
at each order of QED (including rain of bricks QED).  That is because U(1) gauge invariance
makes it impossible for there to be a photon mass term (proportional to
A<sub>&#956;</sub>A<sup>&#956;</sup>) in the lagrangian (see §15.1
of Peskin &amp; Schroeder between EQ 15.13 and 15.14).
An illuminating detailed look at the first order correction to
the  photon propagator is in §5.2 of Greiner &amp; Reinhardt,
and the same sort of calculation
also is done in 
Peskin &amp; Schroeder §16.5 for general massless gauge bosons, e.g. gluons,
with the same conclusion: they remain massless even when first-order diagrams are 
considered.  (Admittedly, there are certain infinities involved in these calculations
if done unrenormalized.  But those affect an overall scalar factor in the wave function, not
the mass which stays 0.)
</small></p><p><small>
Admittedly,
it would be possible in 
principle for a photon to acquire mass via the Higgs mechanism without losing gauge 
invariance.
But that would take us outside of pure QED (as a theory of electrons and photons <i>only</i>, 
without any Higgs spinless boson).   In classical general relativity, 
Smith 2003 pointed out that any nonzero photon
mass, no matter how small, is incompatible
with the existence of charged black holes and charge conservation.  
(Massive gravitons also are impossible.)
Therefore I presume (and the "standard model" agrees) that
the true laws of physics do not involve any interaction between 
photons and any Higgslike 
boson so that the photon mass truly is exactly zero.   
</small></p><p></p><p>
The 
<a href="http://pdg.lbl.gov/2009/tables/rpp2009-sum-gauge-higgs-bosons.pdf">particle data group</a>
after compiling known experimental evidence not involving black holes as of year 2009,
concludes the photon mass is &lt;2×10<sup>-54</sup>kg, and 
Dmitri D. Ryutov, the physicist whose analysis of solar system plasma and magnetic field data
give that bound, now (2010) claims to have further improvements 
from Voyager mission data in the zone between Pluto 
and the termination shock,
reducing this by a further
factor of 5.
The gluon is believed massless also, but for it only much weaker
experimental upper bounds –
in the range 2×10<sup>-44</sup> to 2×10<sup>-31</sup>kg,
depending on which dubious reasoning you are willing to believe –
are currently available (Avila 2001, Yndurain 1995).
The strongest gluon mass bounds so far alleged arise from the nonexistence of free quarks.
If gluons had mass, then a quark very far away from all others presumably could exist, 
if enough energy were applied to push it there,
since its attraction to other quarks, mediated by gluons, would then become 
exponentially tiny.
</p><p>
A particle with zero mass necessarily travels at speed exactly c in the "classical"
large-distance limit,
experiences no proper time, and 
cannot dephase.
</p>


<a name="desit"></a> 
<h3>35. De Sitter spacetime 
and its distance formula, propagators, and Poisson raindrop process
(and the nonexistence of massless fermions, and
the improved infrared behavior of the propagators)
</h3>

<p>
In <a href="#doublesr">§10</a>
we saw that rain of bricks could be set in de Sitter rather than Minkowski
spacetime, and the result would be "triply-special relativity."
There are several reasons to prefer de Sitter as a background:
</p><ol>
<li>
De Sitter is a better match to astronomical observations of the universe, than Minkowski space.
</li><li>
De Sitter is equally symmetric.
</li><li>
De Sitter allows us to <i>tune</i> the Einstein cosmical constant &#923; "by hand."
As Weinberg 1989 discussed, 
it is a horrible problem for physics to explain why &#923;=0;
but explaining why it is not zero but actually very slightly positive (repulsive)
is an even <i>more</i> horrible problem.   <i>If</i> rain of bricks combined with
future new QFTs – new ones are needed because
the "standard model" is known to be wrong
(neutrino mass) and incomplete (dark matter particles) –
somehow can make &#923; very small
or zero, then
that still might fail to obtain exact agreement with &#923;'s experimental value.
If so, that final obstacle could presumably be overcome
by setting everything in de Sitter space.
</li><li>
It seems naively obvious that setting QFTs in de Sitter space
should effortlessly yield better behavior about
"infrared infinities" because  
triply-special relativity has <i>both</i> UV-cutoff and
IR-cutoff length scales for taming QFTs.
It will take some time, but by the end of this section we shall provide some backing for this
intuition.
</li></ol>
<p>
De Sitter space (Coxeter 1943) is really a family of metrics parameterized by &#923;&gt;0
which includes Minkowski space as the limit case &#923;=0.  So any successful 
attempt to generalize things this way would indeed be a "generalization," not a "replacement," 
of Minkowski space physics.
</p><p>
One might (naively) continue that the only change we need to make to 
physics in order to reset it in
de Sitter spacetime
is to <b>replace</b> the usual Minkowski-space propagator and distance
formulas, and the Poisson raindrop process,  by their de Sitter analogues.
We shall now derive what those analogues are.
</p><p>
<b>The "mother" coordinate system for de Sitter spacetime:</b>
De Sitter spacetime dS<sup>1,3</sup> is the 4-dimensional hyperboloid surface
</p><center>
a<sup>2</sup>+b<sup>2</sup>+c<sup>2</sup>+d<sup>2</sup>
= e<sup>2</sup> + H<sup>-2</sup>
</center><p>
in (4+1)-dimensional  (a,b,c,d;e)
Minkowski spacetime.
Use the metric induced on the surface. 
<!-- We shall call this the <b>mother metric.</b> -->
It is analogous in Minkowski spacetime to a <b>sphere</b> in Euclidean space.
Indeed, it is formally the same thing if one of the coordinates is made imaginary to make it
timelike; this allows us to get many de Sitter formulas from spherical geometry formulas
(with appropriate alterations and analytic continuation).
</p><p>
Notice de Sitter spacetime features exactly one
characteristic length scale 
H<sup>-1</sup>
(which is somewhat analogous to the "radius" of a sphere).
Here H is the <b>Hubble constant</b>; one can either use it or
the "Einstein cosmical constant" &#923;=3H<sup>2</sup> 
(since each determines the other) as the basis for everything, but it is more
convenient to use H.
The Ricci curvature scalar is 12H<sup>2</sup>, indicating that de Sitter spacetime, 
like a sphere, has positive curvature.
Using H, &#8463;, and the speed of light,
we could construct a complete system of "cosmical units" (just like the Planck system
was instead based on the microscopic length L<sub>pl</sub>) including characteristic time,
energy, momentum, and mass scales.
</p><p>
Our universe locally resembles a de Sitter spacetime and the "best fit,"
maximizing this local resemblance, is obtained by using
the latest astronomical estimate 
H&#8776;71/km/sec/megaparsec&#8776;2.3×10<sup>-18</sup>Hz.
After converting this to an inverse length by dividing by the speed of light this is
<nobr>H<sup>-1</sup>&#8776;1.3×10<sup>26</sup>meters=1.4×10<sup>10</sup>lightyears.
</nobr>
</p><p>
In de Sitter space this length H<sup>-1</sup>
is the "|distance| to the horizon."  
That is, every unaccelerated observer in
de Sitter spacetime perceives a "horizon" beyond which she cannot see (any photon on the
other side of the horizon will never be able to reach her). 
</p><p>
This metric is an exact solution of the vacuum Einstein gravity equations
with Einstein cosmical constant &#923;&gt;0.
There are numerous ways to coordinatize it – we shall call them <b>"daughters."</b>
For example, if we define 
<nobr>T=Hln([a+e]H)</nobr>
and 
<nobr>(x,y,z)=exp(HT)(b,c,d)</nobr>
then in (x,y,z;T) coordinates
the metric of the <i>half</i> of de Sitter space with a+e&gt;0 becomes
</p><center>
ds<sup>2</sup> = 
dT<sup>2</sup> - exp(2Ht) [dx<sup>2</sup>+dy<sup>2</sup>+dz<sup>2</sup>].
</center><p>
(<a href="http://en.wikipedia.org/wiki/De_Sitter_space">Wikipedia</a> calls this the 
"flat slicing.")
This allows us to interpret this half of de Sitter spacetime as an 
<b>exponentially-expanding <i>Euclidean 3-space</i> universe</b>, with scale factor exp(HT),
and makes it obvious that we get Minkowski when H=&#923;=0.
But this interpretation is not sacred.
By changing our timelike coordinate T to 
t=-H<sup>-2</sup>/(a+e)=-H<sup>-1</sup>exp(-H<sup>-1</sup>T)
we instead get the important <b>conformal halfspace model of de Sitter space</b>.
This has coordinate system 
</p><center>
(t;x,y,z) = -(H<sup>-1</sup>,b,c,d)H<sup>-1</sup>/(a+e)
&nbsp; &nbsp; &nbsp; 
for &nbsp; &nbsp; &nbsp; 
-&#8734;&lt;t&lt;0, &nbsp; 
-&#8734;&lt;x,y,z,&lt;+&#8734;
</center><p>
and metric
</p><center>
ds<sup>2</sup> = (Ht)<sup>-2</sup>
[dt<sup>2</sup> - dx<sup>2</sup> - dy<sup>2</sup> - dz<sup>2</sup>].
</center><p>
Note the minus signs: t ranges from -&#8734; in the past to t&#8594;0<sup>–</sup> 
in the far future.
(The surface t=0 itself is not regarded as part of de Sitter space, it instead is 
its "limit boundary.")
In these (x,y,z;t) coordinates, our half of de Sitter space is an expanding
(but now like a power law, not exponentially)
<i>Minkowski spacetime</i>. 
<i>However</i>, this metric provides 
the <i>whole</i>, not merely half, of de Sitter space by allowing
t to range from -&#8734; to +&#8734;...   but to make this interpretation we need to
require time to move forward in a nonobvious manner:  It starts at t=0+ in the far past, 
advances to t=+&#8734; (during which, spacetime contracts), <i>then</i>
immediately 
advances further to t=-&#8734;, then continues to finally reach t=0- 
(during which, spacetime expands) in 
the far future.
The way the (t;x,y,z) coordinate system was obtained from the mother system
in a 1-dimension-higher Minkowski spacetime,
is somewhat analogous to the way the "stereographic projection"
conformally and rationally
maps a sphere in a 1-dimension-higher Euclidean space, to flat
Euclidean space.   Our map also is conformal and rational in the Minkowki 5D
metric, but as far as the 4D surface metric (i.e. of de Sitter space itself) is concerned,
it of course is merely the identity map.  The conformal halfspace model system
is particularly useful for our purposes because it has an obvious conformal relation to
ordinary Minkowski space ("conformally flat" is the technical term).
That makes it trivial to construct vierbein fields:
</p><center>
(1<sub>t</sub>; 1<sub>x</sub>, 1<sub>y</sub>, 1<sub>z</sub>) Ht
</center><p>
are four vectors in the tangent space to conformal-model de Sitter
space, which are mutually orthonormal and thus play the role that
</p><center>
(1<sub>t</sub>; 1<sub>x</sub>, 1<sub>y</sub>, 1<sub>z</sub>)
</center><p>
play in ordinary (t;x,y,z) Minkowski space.
</p><p>
De Sitter spacetime also has <b>other coordinatizations</b> in which the spatial sections instead
are spherical or hyperbolic <i>non</i>Euclidean
3-space (which also both ultimately, but not initially,
expand exponentially):
</p><a name="hyperbolicslice"></a><ol><li>
<b>Slicing of de Sitter into hyperbolic nonEuclidean geometries:</b>
The metric
<p></p><center>
ds<sup>2</sup> = dt<sup>2</sup> - H<sup>-2</sup> sinh(Ht)<sup>2</sup> dG<sup>2</sup>
</center><p>
is de Sitter (1+D)-spacetime if
dG<sup>2</sup>
is the infinitesimal element of squared length in D-dimensional hyperbolic geometry.
(<a href="http://en.wikipedia.org/wiki/De_Sitter_space">Wikipedia</a> calls this the 
"open slicing.") 
</p></li><li>
<b>Slicing of de Sitter into spherical nonEuclidean geometries:</b>
The metrics
<p></p><center>
ds<sup>2</sup> = dt<sup>2</sup> - H<sup>-2</sup> cosh(Ht)<sup>2</sup> d&#937;<sup>2</sup>,
&nbsp;&nbsp; and &nbsp;&nbsp;
ds<sup>2</sup> = H<sup>-2</sup> sec(&#951;)<sup>2</sup> [d&#951;<sup>2</sup> - d&#937;<sup>2</sup>],
<br>
(-&#8734; &lt; t &lt; +&#8734;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
(-&#960;/2 &lt; &#951; &lt; +&#960;/2)
</center><p>
both represent de Sitter (1+D)-spacetime if
d&#937;<sup>2</sup>
is the infinitesimal element of squared length in D-dimensional spherical geometry.
(<a href="http://en.wikipedia.org/wiki/De_Sitter_space">Wikipedia</a> calls this the 
"closed slicing.")
In the left coordinates, the spherical geometry is contracting to a minimum, then re-expanding.
</p></li></ol><p>
and in 
<a href="#horizon">§20</a> 
we 
<a href="#newdesitcoord">saw</a>
some in which the spacetime is <i>stationary</i>, i.e. invariant with respect to
translation of its time-coordinate!  De Sitter spacetime
evidently is a many-splendored thing.
</p><p>
<b>Geodesics:</b>
The geodesics (in the mother system)
are the intersection of 2-dimensional planes through the origin of (a,b,c,d;e)
spacetime, with the de Sitter hyperboloid.  (The timelike geodesics are hyperbolas;
the spacelike ones are ellipses.)
</p>
<a name="desitdistformula"></a>
<p>
<b>The fundamental <i>distance formula</i> in de Sitter spacetime:</b>
The Lorentzian 
distance s between two points (a,b,c,d;e) and (a',b',c',d';e')
of de Sitter spacetime (in the mother system) 
is
</p><center>
s = 
2H<sup>-1</sup>arcsinh(
[H/2]
[&#916;e<sup>2</sup>-&#916;a<sup>2</sup>-&#916;b<sup>2</sup>-&#916;c<sup>2</sup>-&#916;d<sup>2</sup>]<sup>1/2</sup>
)
= H<sup>-1</sup>arccosh( [aa'+bb'+cc'+dd'-ee']H<sup>2</sup> )
</center><p>
which as usual is real for timelike and imaginary for spacelike distances.
(These formulas are analogous to the usual Euclidean-space formulas for 
geodetic distance on a sphere of radius H<sup>-1</sup>.  To prove them, it suffices to
show they work when 
c=d=e=0 <!--checked since arccos(x)*i=arccosh(x) if |x|<1 and x real-->
or when 
b=c=d=0  <!--checked-->
because then the symmetries of the metric and the formula
show it for every other case.   The distances in these special cases
are easily found simply by integrating d<i>s</i>.)  
Note that de Sitter space is <i>spatially bounded</i> e.g. it is not possible
for two points both with e=0 to have imaginary (spacelike) distance s with |s|&gt;&#960;/H,
since the distance between 
(cos<i>s</i>,sin<i>s</i>,0,0;0)/H
and
(1,0,0,0;0)/H
is 
i<i>s</i>/H.
The only way that |s|=&#960;/H can happen is if s=i&#960;/H arose from "antipodal"
points such as 
<nobr>(+1,0,0,0;0)</nobr> and 
<nobr>(-1,0,0,0;0).</nobr>
</p><p>
<b>Warning:</b>
One might wrongly conclude from the perception that "de Sitter space is expanding" that
the maximum spatial distance depends on e and grows unboundedly large when |e| does,
so the bound &#960;/H is only of interest if e=0.
<i>Wrong!</i>  Actually, all spacelike geodesics are <i>equivalent</i>
under a symmetry.   Therefore, counterintuitively, the maximum spacelike
distance <i>remains constant</i>.  The source of the misperception: the
intersection of the hyperplane e=E 
with the de Sitter hyperboloid does <i>not</i> contain any geodesics except if E=0.
Incidentally, exactly <i>half</i> the maximum spatial distance, |s|=&#960;/(2H), 
is the <b>distance to the horizon.</b>
That is, two light flashes emitted from
max-distance-separated points such as (±1,0,0,0;0) will never be able to
collide, although they will 
in the limit e&#8594;&#8734;.  Thus an observer initially
located exactly 
midway between the two flashes will view each as lying on her "horizon" of visibility.
</p><p>
In contrast, timelike distances s
can be arbitrarily large, such as between
(cosh<i>k</i>,0,0,0;+sinh<i>k</i>)/H and
(cosh<i>k</i>,0,0,0;-sinh<i>k</i>)/H
where
k=sH/2.
<!-- s=2k/H. This again makes it
clear how in the limits &Lambda;,H&rarr;0 we get ordinary Minkowski spacetime. -->
</p><p>
A distance formula for any daughter system can be produced by computing the mother coordinates
as functions of the daughter coordinates, then using one of the mother distance formulas.
</p><p>
<b>What are the symmetries of de Sitter spacetime?</b>
In the 5-dimensional mother 
picture, any Lorentz transformation, i.e. element of SO(4,1), preserves 
the de Sitter metric. [This is the Minkowski analogue of the group SO(5) of rotation matrices
in Euclidean 5-space.]
This group has the same dimensionality, 10, as the Lorentz-Poincare group
in Minkowski 3+1 space, but it is nicer.  That is because
in Minkowski space there are on the one hand the time and space translations
("Poincare" part) and on the other the boosts and rotations ("Lorentz" part);
in de Sitter space these two kinds of symmetries are merged into only
one kind.
</p><p>
One can also change the sign of any of the 5 coordinates.
</p><p>
Further, any Cunningham transformation in the Minkowski 4+1 space  
will conformally
transform de Sitter space to a (perhaps rescaled and translated) version of itself
–
analogously to how "spherical inversion" conformal maps convert spheres in Euclidean spaces
to other spheres
–
and the restriction to the subgroup which preserves the scale factor yields a 
5-parameter group
(or 15 if we also put in the Lorentzes; see the "subgroup of Cunningham" line of 
<a href="#tabclassicalgroup">table 10</a>),
the same count as for the Cunningham group
in Minkowski 3+1 space.  
The latter (recall from <a href="#badsyms">§32</a>)
can be used to convert
solutions of Maxwell's vacuum equations into other solutions, and indeed preserves the
validity of all of old-style <i>massless</i> particle physics (provided the fields
are appropriately altered as we map).
</p><p>
<b>Close look at the conformal halfspace model system:</b>
This system is highly analogous to the 
Liouville/Beltrami conformal halfspace model of hyperbolic 4D geometry.
(The most important difference is that in the latter, there is only one halfspace
which represents the entirety of hyperbolic geometry; for de Sitter, each halfspace only
represents half of de Sitter spacetime.)
</p><p><small>
<b>Historical note:</b>
J.Liouville recognized in 1850 that the abstract
metric ds²=(dx²+dy²)/y²
(for y&gt;0) had constant negative curvature.  
But it was 
Eugenio Beltrami (1835-1900)
who recognized both 
conformality, and that its geodesics were semicircles with center on the x-axis,
and hence it represented the entirety of hyperbolic nonEuclidean geometry.   Beltrami also
showed that Euclidean plane geometry arose on the "sphere at infinity" of hyperbolic 3-space
thus proving that Euclidean and Hyperbolic geometries both are
logically consistent (or both not).
This then became widely called (entirely unfairly) "H.Poincare's halfspace model"
(or sometimes F.Klein is brought in).  See Stillwell 1996 for reprinted and annotated early papers.
In 1865 Beltrami proved that the <i>only</i> surfaces which can be mapped to the Euclidean plane
in such a way that geodesics map to geodesics, are of constant curvature.
Beltrami's argument (just changing certain quantities from real to imaginary)
can also be redone to show 
<br>
<b>Theorem:</b> The <i>only</i> spacetime metrics which
can be mapped to Minkowski spacetime
in such a way that geodesic-segments map to geodesic-segments, 
are the de Sitter spaces parameterized by
real or imaginary H (but we'll only be interested in real H since only it
is consonant with astronomical observations).
<br>
&nbsp;&nbsp;
I can also prove that in space+time dimension&#8805;3
<br>
<b>Theorem:</b> These de Sitter metrics are
the only metrics with (1) everywhere-isotropic Ricci curvature tensor
which (2) can be mapped conformally to Minkowski spacetime.
<br>
&nbsp;&nbsp;
These uniqueness theorems grant us especially strong leverage to try to 
determine the equations of physics in de Sitter spaces by demanding the ability to
use the Minkowski&#8596;de Sitter map to convert equation-solutions in one space 
(in the massless limit) into the other.
That is the approach we shall follow here (apparently for the first time?).
Such leverage is unavailable for general spacetime metrics and hence I am dubious of most
previous authors' attempts to determine the equations of mathematical physics in
general spacetime metrics.  We outline an approach which might be able to yield much more confident
solutions to that problem, in open problem 10 in 
<a href="#openprobs">§43</a>.
</small>
</p><p>
The <b>symmetries</b> that preserve its distance function include (and are generated by):
</p><ul>
<li>
Any 3×3 rotation matrix may be applied to (x,y,z)
</li><li>
Any translation – adding three constants to (x,y,z) 
</li><li>
Any overall scaling, i.e. multiplying  (t;x,y,z) by a positive constant
</li><li>
One may also change the sign of any subset of {t,x,y,z}.
</li><li>
(3+1)D Cunningham pseudo-inversions about the pseudospheres corresponding to rotated versions
of the hyperbola-geodesics in the (t;x) plane (see below and in
<a href="#badsyms">§33</a>; note these are
<i>not</i> the same thing as the 5D Cunningham transformations which map de Sitter 4-spaces
embedded in 5 dimensions using "mother" coordinates
to other embedded de Sitters – possibly having different H – 
and preserve the validity of massless physics on
those de Sitters; the present transformations concern only <i>one</i> de Sitter 
universe in conformal daughter coordinates and
since they are <i>isometries</i> preserve <i>all</i> physics).
</li><li>
Time-transformations of the form  t&#8596;K/t for any real constant K
(this is a Cunningham limit-case).
</li></ul>
<p>
The <b>geodesics</b> in the (t;x) plane
are hyperbolas &nbsp;
t<sup>2</sup>+K=(x-J)<sup>2</sup>
&nbsp; where
</p><ul><li>
K&gt;0 for a timelike geodesic.
(Lines x=J=constant also are timelike geodesics arising as a limit case.)
</li><li>
K=0 for a lightlike geodesic.
(These "hyperbolas" actually are degenerate, they are lines x=J±t.)
</li><li>
K&lt;0 for a spacelike geodesic.
(Lines t=constant are <i>not</i> spacelike geodesics.)
</li></ul>
<p>
(Since the formula describes a double hyperbola; please only use one of the two curves.)
All other geodesics are obtained from these by using the symmetries listed above
to convert the (t;x) plane into any other plane that contains lines in the t-direction.
The formula giving the <b>distance</b> s between (t;x,y,z) and (t';x',y',z') is
</p><center>
s = 
H<sup>-1</sup>
arccosh(
1 + 
[&#916;t<sup>2</sup> - &#916;x<sup>2</sup> - &#916;y<sup>2</sup> - &#916;z<sup>2</sup>]
/ [2 t t']
)
</center><p>
This simplifies to s=|ln(t/t')| if 
&#916;x=&#916;y=&#916;z=0.
If t=t' then it approximately simplifies to 
the Euclidean formula
s<sup>2</sup>=-(&#916;x<sup>2</sup>+&#916;y<sup>2</sup>+&#916;z<sup>2</sup>)/(Ht)<sup>2</sup>
when |s| is small (the formula for s has additive error of order 
H<sup>2</sup>|s|<sup>3</sup>).  That shows how
this metric converts into Minkowski in the limit H&#8594;0 with Ht held fixed.
If Ht=-1 then we get the usual Minkowski metric; if Ht is fixed at any other negative value
we get a scaled version of it.
</p><p>
<b>A curve "equidistant" from a geodesic</b> 
&nbsp;
t<sup>2</sup>+K=(x-J)<sup>2</sup>
&nbsp; 
is a shifted hyperbola 
&nbsp;
(t-B)<sup>2</sup>+A=(x-J)<sup>2</sup>
&nbsp; 
where A,B are chosen to cause B<sup>2</sup>+A=K
so that the two curves meet when t=0.   
<b>Meaning of "equidistant":</b>
For each point P on the geodesic there is a point Q(P) on the equidistant curve whose
distance from P is stationarized as a function of Q's location along its curve;
and dist(P, Q(P)) remains constant as P slides along its geodesic.
A particle moving along such an equidistant curve would
experience a constant acceleration (since this curve has constant geodetic curvature).
An important special limit case of this is that the lines x=±t,
which are lightlike geodesics, are equidistant 
from each other with the maximum possible spacelike distance (s=i&#960;/H)
and also equidistant (with half the distance) from the line x=0. 
This corresponds to the fact that the "horizon" remains at constant spatial distance
|s|=&#960;/(2H)
from any observer, and a sheet of photons starting at that horizon aimed at the observer will
(a) never quite reach her and (b) continue to define the location of the horizon.
</p><p><small>
Note that this horizon-distance i&#960;/(2H) 
<i>differs</i> from i/H which is the distance formula
one would naively obtain by integrating ds from x=0 to x=t along a line with t=constant
in the (t;x) plane.
It also differs from
arccosh(1/2)/H=i&#960;/(3H)&#8776;1.047i/H
which is what one naively gets by using the distance formula to compute the distance
between (t;t) and (t;0).
The reasons for these discrepancies are that that line is <i>not</i> a spacelike geodesic,
and the point (t;0) was not the right one to use, instead use (&#949;t;0) in the limit
&#949;&#8594;0+.
</small>
</p><a name="desittetlem"></a><p>
The <a href="#tetlemma">tetrahedron lemma</a> still works in de Sitter space
provided all interpoint |distances| are less than the 
de Sitter spacelike horizon distance &#960;/(2H).  
</p><p>
<b>The meaning of the Hubble constant H</b> can now be explained.
The usual astronomical interpretation of H is that it governs the rate
at which objects naturally separate.  In order to cause two
classical point particles
to remain a constant spatial=imaginary 
distance s apart despite the expansion of the de Sitter universe, 
we need to apply a force accelerating each toward the other, each with
a constant acceleration  
</p><center>
a(s) = (H/2) sinh(Hs) = (H/2) sin(H |s|).
</center><p>
<!--
The expansion scalar measures the fractional rate at which the volume
of a small ball of matter changes with respect to time as measured by
a central comoving observer (and so it may take negative values).
  John C. Baez + Emory F. Bunn: "meaning of Einstein" 2006
http://arxiv.org/abs/gr-qc /0103044
http://math.ucr.edu/home/baez/einstein/node10.html
gives
3*dist''/dist = vol'' / vol = -R_ab t^a t^b = -R_{00}   if in local inertial coordinates.
for volume of a small ball of test particles (in small=0 limit).
...based on this I agree with my constant 1/2.
-->
</p>
<a name="binden"></a>
<p>
<b>Extra mass acquired by non-point masses in de Sitter universe:</b>
If we regard the two particles having mass M/2 each and linked by a massless spring,
whose force law is such that it always provides exactly force=a(s)M/2 at each end,
that will suffice to maintain the whole particle+spring+particle mass=M
assembly in a time-invariant state,
but there will be an <i>extra</i> mass-energy
</p><center>
E(s) = M&#8747;a(s)ds = [1-cos(H|s|)]M/2 = sin(H|s|/2)<sup>2</sup>M
</center><p>
arising from stretching the spring.  
Any particle of mass=M can be regarded as
delocalized, thanks to quantum effects, by a distance of order&#8805;1 Compton wavelengths
(but it cannot be delocalized to greater than the maximum possible 
de Sitter spatial distance), so that &nbsp;
O(1)/M&#8804;|s|&#8804;O(1)/H.&nbsp;
It seems pointless to try to determine the exact constants inside the O's because
our model of a delocalized particle as two halves joined by a spring only approximately
describes the idea of the extra energy required to allow the particle to remain
in a time-invariant state.
Hence any particle with "bare rest mass" M, when placed in de Sitter space will
acquire an effective extra mass &#916;M&#8776;min(H<sup>2</sup>/M, M).
Also, any particle with extremely small bare mass M&lt;&lt;H
will, by being constrained in the
de Sitter spatially-bounded "box," acquire a quantum confinement energy of order H.
The combined effect of both of these
is equivalent at our level of approximation to:
</p><p>
<b>Any particle which would in flat space have squared mass M<sup>2</sup>,
when placed in de Sitter space will
effectively acquire extra mass-energy (in Planck units)
of the same order as you'd get by adding
something of order H<sup>2</sup> to M<sup>2</sup>.
</b>
</p><p>
This effect can be considered somewhat like a <b>surface-binding energy.</b>
If a molecule is placed on a tabletop
it sticks to its surface with some binding energy independent of
horizontal location.  Similarly, mass-M particles
"stick" to de Sitter space with characteristic energy &#916;M.
However, the two concepts differ:
</p><ol><li>
&#916;M has the opposite sign to "stickiness" and hence actually is 
an energy  "<i>penalty</i>," like
a water molecule on a hydrophobic surface.
</li><li>
This penalty can be regarded as adding an approximately constant amount not to M,
but rather to M<sup>2</sup>.
</li></ol>
<p>
<b>Buildable toy model of an "extra mass" effect:</b>
Consider a stiff wire bent into the shape of a concave-up
<a href="http://en.wikipedia.org/wiki/Cycloid">cycloid</a>, albeit
for our purposes better words for that same
curve would be 
<a href="http://en.wikipedia.org/wiki/Tautochrone_curve">isochrone</a> or
<a href="http://en.wikipedia.org/wiki/Brachistochrone_curve">brachistochrone</a>.
A mass=M bead sliding frictionlessly along this wire in Earth's gravity
effectively is traveling in a simple harmonic oscillator
potential V(s)=H<sup>2</sup>Ms<sup>2</sup>
where H is a constant proportional to Earth's gravitational acceleration g
times the length of the cycloid, and s is bead-position measured as arclength along the wire.
If we apply nonrelativistic (Schrödinger equation) quantum mechanics, we find that 
the ground-state energy of the bead is raised
&#8463;H&#8730;2 above the classical potential minimum.
The same energy-raising effect would arise purely classically 
if the bead were actually two separated half-beads 
kept distance Q apart by joining them with a compression-resistant spring
with spring constant 2H<sup>2</sup>M (provided we choose Q appropriately).
The half-beads classically would accelerate toward each other with relative
acceleration 2H<sup>2</sup>Q where Q was their arclength-separation
– but could be kept constantly separated by such a spring.
However, note that the rise in gravitational energy caused by this separation would
be exactly canceled by the negative spring-stretching energy (versus zero separation),
if that were counted.
Thus the effect of quantum mechanics on a point particle in such an environment, is 
similar to having a nonpoint classical particle with
the same constant RMS separation as the quantum wavefunction, and also similar to having
a "wire-to-bead binding energy penalty" of 
&#8463;H&#8730;2.
But now even 
if we consider a <i>heavy</i> well-localized 
bead traveling on a <i>long</i> such wire at relativistic speeds (so that one would
expect quantum effects to become irrelevant and the classical relativistic 
treatment to be fine)
we still see that <i>relativistic</i> quantum mechanics 
ought to still force an effective self-separation of order &#8805;1 Compton wavelength
&#8463;/(Mc), thus causing a classical energy rise (now returning to units with
&#8463;=c=1) of order &#916;M&#8776;H<sup>2</sup>/M, which is like
increasing M<sup>2</sup> by an additive amount of order H<sup>2</sup>.
</p><p>
So far, we've only discussed this &#916;M idea in a crude and approximate manner, but 
soon will provide what I believe is an exact understanding for quantum "point particles."
</p><p>
<b>What are the equations of physics in de Sitter spacetime?</b>
I have not previously seen a good answer to this question.
Dirac 1935 attempted in a beautiful way to provide  some,
but I believe got wrong answers (we'll discuss his approaches and their demerits 
<a href="#diracwronghist">later</a>).
Some other answers (some right, some wrong) summarizing work by a large number of authors
are in Birrell &amp; Davies 1984.  (We'll also discuss their problems.)
</p><p>
I have some new, and hopefully correct this time, answers to propose.   
A major difficulty is this.
When you try to transplant the equations of physics into curved spacetimes,
opportunities arise to incorporate terms depending on the local spacetime curvature tensor.
These terms can be designed in an <i>infinite</i> set of possible ways to both
</p><ol type="a">
<li>
satisfy the usual general relativistic desiderata of Einstein, and 
</li><li>
leave the equations in their usual form in the Minkowski flat-universe case
where the curvature tensor is identically 0.
</li></ol>
<p>
Because of (b), the resulting equations will be compatible with all particle-physics
experiments so far.  Fortunately, de Sitter spacetime is much simpler than general
curved spacetimes, since all points of de Sitter space are identical (under a symmetry) 
– de Sitter space, like a sphere, has "constant curvature" – and
the whole metric is characterized only a single real parameter H.
So while we may be infinitely confused about what the laws of physics should be
in curved spacetime, for de Sitter spacetime it fortunately is only a 1-parameter infinity!
And we shall now suggest a way to determine that parameter uniquely.
</p><p>
I propose that the <b>Klein-Gordon equation</b>
(Peskin &amp; Schroeder EQ 2.7)
satisfied by the scalar-valued wavefunction &#934; of
a free mass=M spinless particle
be the following in de Sitter space
(in Planck units):
</p><center>
[&#8706;<sup>&#956;</sup>&#8706;<sub>&#956;</sub> 
+ M<sup>2</sup> + 2H<sup>2</sup>] &#934; = 0
</center><p>
Fully written out in the conformal halfspace model (t;x,y,z) system this is
</p><center>
(Ht)<sup>2</sup>(&#934;<sub>tt</sub> - &#934;<sub>xx</sub> - &#934;<sub>yy</sub> - &#934;<sub>zz</sub>)
- 2tH<sup>2</sup>&#934;<sub>t</sub> 
+ (M<sup>2</sup> + 2H<sup>2</sup>)&#934; = 0
</center><p>
where the subscripts denote partial derivatives.
This choice has the following virtues:
</p><ol>
<li>
In Minkowski spacetime, i.e. when H,&#923;&#8594;0 with Ht=-1 held fixed, 
it reduces to the usual
Klein-Gordon equation.
</li><li>
It is invariant under de Sitter space Lorentz transformations SO(4,1).
</li><li>
In the massless limit M&#8594;0, solutions &#934;(t;x,y,z)
are mapped by 5-dimensional Cunningham conformal maps
(these always map de Sitter 4-manifolds inside the 5-space into other de Sitter 4-manifolds)
into other solutions provided the field is rescaled so that 
&#934;<sub>new</sub>=C<sup>-1</sup>&#934;<sub>old</sub>
where C(t;x,y,z) is the local length-scaling factor of the map.
(Consequence of the theorem on the last page of chapter 6 of Fulling 1989,
which gives the generalization of this to arbitrary
n-dimensional metrics.)
</li><li>
The equation is second-order linear, and arises from a Lagrangian density 
[namely
L=|&#8706;<sub>&#956;</sub>&#934;|<sup>2</sup>-(M<sup>2</sup>+2H<sup>2</sup>)|&#934;|<sup>2</sup>]
of first order.
</li></ol>
<p>
And apparently this equation is the essentially <i>unique</i> one
combining all these virtues.   
</p><p>
<!--
<b>Dirac 1935</b> had
only discussed the M=0 case of Klein-Gordon and actually derived a 1-parameter 
<i>family</i>
of proposed equations.  His elegant approach was to extend the domain of defintion of
&Phi; beyond merely the de Sitter hyperboloid into
the whole of (a,b,c,d;e) 5-space
(in the mother coordinate system).   
Dirac did that by assuming &Phi; depended on H
proportionally to |H|<sup>-&eta;</sup>, where &eta; is called the "degree."
This assumption is analogous to (in Euclidean 5-space) assuming that &Phi; is spherically
symmetric and depends on the radius r proportionally to r<sup>&eta;</sup>.
This approach allowed Dirac to write the equations of mathematical physics in de Sitter
space in elegant ways symmetrical in the <i>five</i> mother coordinates.
Dirac then observed that for his Klein-Gordon equation the two degrees &eta;=0 or &eta;=-3 
are special in that they yield the simplest equations, 
and apparently Dirac liked the latter choice.  
However, Dirac did not
recognize the virtue of <i>conformal</i> invariance in the massless case.   If he had, he
would instead have identified degrees &eta;=-1 and &eta;=-2 as the specially favored ones.
--yes, verified by considering Phi=1.--
[Further, &eta;=-1 is the natural choice in the sense that only it causes the Klein-Gordon 
action integral to be dimensionless and independent of rescaling and hence of H. 
See Peskin &amp; Schroeder EQ 2.6
regarding m as an inverse length.]
Either &eta;=-1 and &eta;=-2 leads to our form of Klein-Gordon.
With &eta;=-1 we want to interpret 
&Phi;<sup>2</sup>H
as proportional to a probability density [since this then has degree &eta;=-3
and hence yields correct normalization behavior].
An interpreter would thus find it more convenient to regard &phi;=H<sup>1/2</sup>&Phi;
(not &Phi;) as "the wavefunction" &ndash; which then
would have degree &eta;=-3/2.
</P><p>
<b>Returning to the present paper (rather than Dirac 1935),</b>
-->
The reader may object: 
"what is so sacred about Cunningham symmetries?".
We'll now provide an extensive discussion to
provide a clear understanding of why this must be the
correct form of the Klein-Gordon equation in the de Sitter universe.
</p><p>
We first must enquire:
"what exactly <i>is</i> 'mass'?"
In the de Sitter universe, this is not so obvious.
In Minkowski space, mass M had a meaning because energy E and momentum p
had meanings (since they were <i>conserved</i>) and due to Einstein's
</p><p></p><center>
E<sup>2</sup> = M<sup>2</sup> +
(p<sub>x</sub>)<sup>2</sup> + (p<sub>y</sub>)<sup>2</sup> + (p<sub>z</sub>)<sup>2</sup>.
</center><p>
However, in de Sitter space, the momentum and energy
of a free particle are <i>not</i> conserved.  
For example, a free photon, traveling in de Sitter space, will redshift with time
as the universe "expands underneath it" – and this has been 
well confirmed experimentally.   It is not so clear, indeed, what "momentum" <i>is</i>
anymore – 
although it <i>is</i> clear if we are speaking of momenta much larger than the de Sitter
momentum scale 
&#8463;H/c 
because then the deBroglie wavelength of the particle we speak of is much smaller
than the de Sitter length scale and hence 
flatspace notions can be used because the region of spacetime we are concerned with, 
being small, is approximately flat.
</p><p>
<b>Proposed Definitions of "mass":</b> There are <i>two</i> natural notions of mass
in de Sitter space.  (And later we'll encounter a third!)
First, the quantity we denote M, will be such that when M=0
we have lightspeed propagation and conformal invariance properties
for wave functions.  (That will be discussed more below.)  For precision we shall call M
the <b>"conformal mass"</b> or just "mass" for short.
Second, in the limit |&#8706;<sub>u</sub>&#934;|/|&#934;|&#8594;0, the
Klein-Gordon operator will reduce to multiplying &#934; by a constant, and this constant
is a second kind of "mass" (actually, since Klein-Gordon is a second-order operator, 
<i>squared</i> mass),
which we shall call <b>"naive mass."</b>
</p><p>
</p><p>
Our proposed equation sets a lower bound 2<sup>1/2</sup>H
on the naive mass of a scalar boson.
Numerically, this in our universe 
(i.e. using <nobr>H&#8776;2.3×10<sup>-18</sup>second<sup>-1</sup></nobr>
and converting back to SI units) 
is roughly M<sub>min</sub>&#8776;4×10<sup>-69</sup>kg.
<!--That is, the term that used to be
the squared mass M<sup>2</sup> in Minkowski space, cannot be brought below
2H<sup>2</sup>&gt;0, although it used to be that it could be made 0.-->
</p><p>
Dirac 1935 pointed out that the 10 components of the
"<i>five</i>-dimensional angular momentum" antisymmetric tensor
</p><center>
J<sub>&#956;&#957;</sub> =
x<sub>&#956;</sub>p<sub>&#957;</sub> – x<sub>&#957;</sub>p<sub>&#956;</sub>,
&nbsp;&nbsp;
which as an operator is
&nbsp;&nbsp;
J<sub>&#956;&#957;</sub> =
-i&#8463; [ x<sub>&#956;</sub>&#8706;<sub>&#957;</sub> – x<sub>&#957;</sub>&#8706;<sub>&#956;</sub> ],
</center><p>
(in the mother coordinate system x<sub>&#956;</sub>), encompass
the natural analogue of what in flat-space we call
"energy," "momentum", and "angular momentum."
At the prototypical point (H<sup>-1</sup>,0,0,0;0)
only the 4 components 
J<sub>ab</sub>,
J<sub>ac</sub>,
J<sub>ad</sub>,
J<sub>ae</sub>
of 
J<sub>&#956;&#957;</sub>
can be nonzero, corresponding respectively (up to a multiplicative factor of H<sup>-1</sup>)
to "local momentum" and "local energy."
The de Sitter analogue of 
Einstein's classical energy-momentum-mass relation 
would be
</p><center>
(J<sub>ab</sub>)<sup>2</sup> +
(J<sub>ac</sub>)<sup>2</sup> +
(J<sub>ad</sub>)<sup>2</sup> +
(J<sub>bc</sub>)<sup>2</sup> +
(J<sub>bd</sub>)<sup>2</sup> +
(J<sub>cd</sub>)<sup>2</sup> +
H<sup>-2</sup>M<sup>2</sup> + 2
<!-- note J has degree=0 and so M/H needs to be regarded as degree=0 too -->
=
(J<sub>ae</sub>)<sup>2</sup> +
(J<sub>be</sub>)<sup>2</sup> +
(J<sub>ce</sub>)<sup>2</sup> +
(J<sub>de</sub>)<sup>2</sup>.
</center><p>
I've never seen this explicitly written before, but nevertheless
it is useful to give it a name; call it the <b>Einstein relation with binding energy (ERBE).</b>
Here I have added the nonobvious term "+2" to the left hand side to 
</p><ol><li>
Make 
the quantum version of this equation correspond exactly to our proposed Klein-Gordon equation.
[Dirac 1935 derived a (wrong) Klein-Gordon equation, but his derivation would
yield ours if the starting point is instead taken to be the ERBE. The ERBE in fact <i>is</i>
our Klein-Gordon equation if the J's are interpreted as differential operators.]
</li><li>
Account for the "de Sitter 
<a href="#binden">binding energy</a>" penalty, which we'd previously 
derived only approximately, but the constant represented by our previous "big O"
now is postulated to be exactly 2 because only this value leads to Cunningham invariance
in the M&#8594;0 limit.
</li></ol>
<p>
One might naively imagine (based on flat-space "hamiltonian" notions 
unjustifiably extrapolated into curved spacetime)
that the naive mass is the sort of mass that tends to
get conserved when everything is at rest.   
But actually we'll now argue that <b>conformal mass M</b> then is
what should be conserved, and hence <i>non</i>naively should
correspond to "rest mass" of a scalar boson in de Sitter space.
Consider the wavefunction &#934; in the 
conformal halfspace model behaving proportional to either t<sup>1</sup> or t<sup>2</sup>.
In that case the 
Klein-Gordon operator reduces to multiplying the wavefunction
&#934; by a constant, and this constant
is exactly M<sup>2</sup>.
In other words for this wavefunction the Klein-Gordon operator effectively
is independent of x,y,z,t, and H.
This wavefunction is the least-varying possible (i.e. best corresponds to "resting")
in the sense that it is independent
of (x,y,z).  But it <i>must</i> depend on t, in order for the total probability
integral to remain normalized as the universe expands.  In view of the fact that the
charge-density and current-density
4-vector for Klein-Gordon is (Peskin &amp; Schroeder EQ 2.16)
proportional to
</p><center>
i&#934;&#8706;<sub>&#956;</sub>&#934;<sup>†</sup>
–
i&#934;<sup>†</sup>&#8706;<sub>&#956;</sub>&#934;
</center><p>
and the length-scale for  (t;x,y,z)  de Sitter space 
is proportional to t<sup>-1</sup>, this forces &#934; to behave 
proportionally to |t|<sup>1</sup>.
</p><p><small>
One could argue vaguely that quantum uncertainty principles 
ought to prevent a particle of
mass M from being localized more precisely
than order half its reduced Compton wavelength, i.e. 1/(2M) in Planck length units.
This, if still true in de Sitter space, would give a vague meaning to "mass."
But observe that in de Sitter space, any object we can 
perceive necessarily lies within the de Sitter horizon and hence <i>is</i> localized
within a sphere of radius H<sup>-1</sup>, which reassuringly
again vaguely suggests that the smallest possible mass-energy ought to
be of order H.
</small></p><p>
It is well known that in flat-space QFTs all IR infinities go away if we artificially make the
photon, gluon, etc have slightly positive "masses"  (since there is no infrared limit anymore).
So it initially seems highly promising that de Sitter space causes all particles to
have naive mass bounded below by a positive constant.
</p><p>
What does "massless" mean?
In the absence of mass,
and provided we are speaking of <i>non</i>gravitational <i>local</i> physics,
it makes sense that the laws of physics ought to be scale invariant because
<i>only</i> in the presence of mass M do the available physical constants 
&#8463; and c (<i>without</i> G and without the nonlocal information arising from H and &#923;) 
allow us to set a length scale, 
&#8463;c<sup>-1</sup>M<sup>-1</sup>.
So we expect that de Sitter space nongravitational physical
laws of massless particles in the high-energy and local (compared to the de Sitter scale) limits
ought to be scaling invariant; and conformal transformations (i.e. Cunningham maps)
are exactly the selfmaps of de Sitter space
which locally everywhere are scalings and Lorentz transformations.
So it makes
sense that we should define our laws in such a way that for "massless" particles,
such as photons, we get Cunningham symmetry.
</p><p>
The definition of "massless" I like best is
<b>massless particles travel at the speed of light.</b>
Our proposal satisfies that desideratum when M=0. <b>Proof:</b>
The Green's function for the 
<a href="http://en.wikipedia.org/wiki/Wave_equation#Solution_of_a_general_initial-value_problem">wave 
equation</a>
(i.e. massless Klein-Gordon equation)
is 
</p><center>
(4&#960;)<sup>-1</sup>
(x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>)<sup>-1/2</sup>
&#948;( t - [x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>]<sup>1/2</sup> )
</center><p>
in 1+3 dimensional flat spacetime (similar formulas work in every odd-spatial dimension)
indicating lightspeed is the only possible propagation velocity.  (Note:
"Green's function" and "propagator" are different things. The latter involves a source.
By historical accident, poor names for these concepts were chosen.)
Because the de Sitter metric system is conformal to Minkowski spacetime, and
because conformal maps preserve solutions (up to a position-dependent scaling factor)
<i>of our proposed form</i> of the Klein-Gordon equation when M&#8594;0, 
we see that the massless Klein-Gordon
Green's function
in the conformal model of de Sitter space is
</p><center>
-Ht (4&#960;)<sup>-1</sup>
(x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>)<sup>-1/2</sup>
&#948;( t - [x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>]<sup>1/2</sup> )
</center><p>
which is supported only on the light cone.
<b>Q.E.D.</b>
</p><p>
Essentially our same list of 4
desiderata now allow one to define, uniquely, the de Sitter
analogues of other equations.
<b>Maxwell's</b> equations were already believed (although the reader may now with some
justification be losing confidence in all such "beliefs") 
to be known in general curved spacetimes,
e.g. see Misner, Thorne, Wheeler 1973 
§22.4 and box 16.1; e.g. their EQ 22.19b'.  
</p><p><small>
Rainich 1925, then Misner &amp; Wheeler 1957 demonstrated
that Maxwell's equations in their=our form in combination with Einstein's gravity-metric
equations constitute an "already unified" purely-geometric gravity+electromagnetism theory
in the sense that the classical gravity and electromagnetism are deducible purely from the metric,
and their time-evolution is purely geometrically determined, without any need for
direct reference to the electromagnetic fields.  
These equations imply charge conservation and
energy conservation.  Thus we get "charge without charge" and "energy without energy";
and then by using the ideas started by Einstein &amp; Infeld 1940 we also have "mass without
mass" in the sense that the nonlinearities in the classical Einstein field equations force
point singularities of the metric to travel along geodesics.
Later Friedman &amp; Sorkin 1980 even showed how to get "(quantized) spin without spin."
This set of ideas has been called "Geometrodynamics", see Wheeler 1962.
It is unfortunate that Albert <b>Einstein</b> (1879-1955)
failed to appreciate Rainich's work and therefore devoted a large fraction of his life to 
searching for a unified theory, not realizing he already had one; this only became well
known with Misner &amp; Wheeler 1957.
I am unaware of any proof that the particular way they chose to implement 
Maxwell's equations in curved space <i>uniquely</i> enjoys these virtues,
but it at least plausibly is the simplest one that does.
</small></p><p>
In the special case of de Sitter
metric the curvedspace Maxwell's equations are
</p><center>
A<sup>&#956;</sup><sub>;&#956;;&#946;</sub>
- A<sub>&#946;;&#956;</sub><sup>;&#956;</sup>
+ 3H<sup>2</sup>A<sub>&#946;</sub>
= 
4&#960;J<sub>&#946;</sub>
</center><p>
where A<sup>&#956;</sup> is the 4-vector potential, the semicolons denote covariant derivatives
(see Misner et al. 1973 re that notation),
we employ Einstein summation convention for repeated indices, and
J<sup>&#956;</sup> is the 4-vector current density.   Again, note the presence of a nontrivial 
H term.   However, the equation may be rewritten in an equivalent form
(Misner et al 1973 EQ 22.19b) which does not explicitly involve H:
</p><center>
A<sub>&#956;;&#946;</sub><sup>;&#956;</sup>
- A<sub>&#946;;&#956;</sub><sup>;&#956;</sup>
= 
4&#960;J<sub>&#946;</sub>.
</center><p>
If one adopts the "Lorenz gauge" 
<nobr>A<sup>&#956;</sup><sub>;&#956;</sub>=0</nobr>
(which is the natural gauge if the photon is 
regarded as the M&#8594;0 limit of a mass=M Proca
boson, see §12.9 of Jackson) 
the first term in both our forms of the equation
vanishes.  (Lorenz gauge is often incorrectly called "Lorentz," and
in QED books it often is, even less correctly, called "Landau.")
<!--
In the "Feynman gauge" 
<nobr>A<sub>&beta;;&mu;</sub><sup>;&mu;</sup>=0<sub>&beta;</sub>,</nobr>
which is more convenient for QED calculations, instead
the <i>second</i> term vanishes???
</p><p>
<small>
I have not previously seen this simple description of Feynman gauge. 
CANNOT BE RIGHT, NEED 1 NOT 4 EQUATIONS!
To confirm it, note that any component of the Feynman photon propagator
(off the light cone)
is proportional to P=(t<sup>2</sup>-x<sup>2</sup>-y<sup>2</sup>-z<sup>2</sup>)<sup>-1</sup>;
and then it is a simple matter to verify
P<sub>tt</sub>-P<sub>xx</sub>-P<sub>yy</sub>-P<sub>zz</sub>=0.
</small>
</p><p>
Fully written out in terms of de Sitter conformal 
coordinates this is, in the Feynman gauge with 
<nobr>J<sup>&mu;</sup>=0<sup>&mu;</sup>,</nobr>
that the following 4 expressions all equal 0 (the subscripts here
denote partial derivatives):
</p><center>
-- in t,x,y,z order: --
A<sup>0</sup> 
+ t<sup>2</sup> A<sup>0</sup><sub>tt</sub> 
- 2t A<sup>1</sup><sub>x</sub> 
- 2t A<sup>2</sup><sub>y</sub> 
- 2t A<sup>3</sup><sub>z</sub> 
- t<sup>2</sup> A<sup>1</sup><sub>xt</sub> 
- t<sup>2</sup> A<sup>2</sup><sub>yt</sub> 
- t<sup>2</sup> A<sup>3</sup><sub>zt</sub> 
,<br>
3 A<sup>1</sup> 
- 2t A<sup>0</sup><sub>x</sub> 
+ t<sup>2</sup> A<sup>0</sup><sub>xt</sub> 
- t<sup>2</sup> A<sup>1</sup><sub>xx</sub> 
- t<sup>2</sup> A<sup>2</sup><sub>xy</sub> 
- t<sup>2</sup> A<sup>3</sup><sub>xz</sub> 
,<br>
 3 A<sup>2</sup> 
- 2t A<sup>0</sup><sub>y</sub> 
+ t<sup>2</sup> A<sup>0</sup><sub>yt</sub> 
- t<sup>2</sup> A<sup>1</sup><sub>yx</sub> 
- t<sup>2</sup> A<sup>2</sup><sub>yy</sub> 
- t<sup>2</sup> A<sup>3</sup><sub>yz</sub> 
,<br>
 3 A<sup>3</sup> 
- 2t A<sup>0</sup><sub>z</sub> 
+ t<sup>2</sup> A<sup>0</sup><sub>zt</sub> 
- t<sup>2</sup> A<sup>1</sup><sub>zx</sub> 
- t<sup>2</sup> A<sup>2</sup><sub>zy</sub> 
- t<sup>2</sup> A<sup>3</sup><sub>zz</sub> 
</center><p><small>
-->
Dirac 1935 independently derived Maxwell's A-equations in de Sitter space 
(in the mother coordinate system in Lorenz gauge) by
using a "degree" technique, where he argued degree &#951;=-1 was necessary "otherwise
we would have a quite different state of affairs from the usual Maxwell theory."
[This also is the unique natural degree in the sense it makes the Maxwell action integral
dimensionless, i.e. independent of H; Dirac did not utilize this principle. 
See Peskin &amp; Schroeder EQ 3.7.]
I believe Dirac's Maxwell-equations are correct
and ought to be equivalent to mine, but his are nicer because they are fully written out
in terms of coordinates and ordinary partial (not covariant) derivatives, while
remaining simple due to the built-in
symmetry of the mother coordinate system.   
Again, the acid test needs to be that 5D Cunningham transformations can be used to
map Maxwell solutions in one de Sitter space, to solutions in another (and in particular
as a limit case we can map flatspace Maxwell solutions to de Sitter solutions); this 
and more was shown in §C5 of
Fulton, Rohrlich, Witten 1962.
<!-- The corresponding equations to FRW's EQs 5.1, 5.2
are MTW EQ 22.17b, 22.17a.-->
Dirac's vector potential A<sub>&#956;</sub>
and field tensor F<sub>&#956;&#957;</sub> both have 5-valued indices but only have the usual numbers,
namely 4 and 6, degrees of freedom respectively, due to certain constraints Dirac
imposes as well as the antisymmetry of F<sub>&#956;&#957;</sub>.
</p><p>
<b>Dirac</b> had originally
derived his electron  spinor-wave equation (in flat space)
by factoring the Klein-Gordon operator.
<!-- &part;<sup>&mu;</sup>&part;<sub>&mu;</sub>. -->
Let us duplicate Dirac's 1920's flat-space work 
in the de Sitter metric (t;x,y,z) coordinates. 
Observe the 1-parameter family of factorizations
</p><center>
<b>–</b>
[iHt&#947;<sup>0</sup>&#8706;<sub>t</sub> +
iHt&#947;<sup>1</sup>&#8706;<sub>x</sub> +
iHt&#947;<sup>2</sup>&#8706;<sub>y</sub> +
iHt&#947;<sup>3</sup>&#8706;<sub>z</sub>
- i(3/2)H&#947;<sup>0</sup>
+ A + B&#947;<sup>5</sup>]
<br>
· 
[iHt&#947;<sup>0</sup>&#8706;<sub>t</sub> +
iHt&#947;<sup>1</sup>&#8706;<sub>x</sub> +
iHt&#947;<sup>2</sup>&#8706;<sub>y</sub> +
iHt&#947;<sup>3</sup>&#8706;<sub>z</sub>
- i(3/2)H&#947;<sup>0</sup>
- A + B&#947;<sup>5</sup>]
&#936;
<br>
=
(Ht)<sup>2</sup>(&#936;<sub>tt</sub> - &#936;<sub>xx</sub> - &#936;<sub>yy</sub> - &#936;<sub>zz</sub>)
- 2tH<sup>2</sup>&#936;<sub>t</sub> 
+ (M<sup>2</sup> + 2H<sup>2</sup>)&#936; 
</center><p>
<!-- 
- HHt dt - 2KHHt dt - KKHH + QQ
K=-3/2
-->
where 
&#947;<sup>0</sup>,
&#947;<sup>1</sup>,
&#947;<sup>2</sup>,
&#947;<sup>3</sup>,
and
&#947;<sup>5</sup>
are the four (plus one extra)
<a href="http://en.wikipedia.org/wiki/Gamma_matrices">Dirac gamma 4×4 matrices</a>,
the wavefunction &#936; has 4 components, 
and A and B are any numbers obeying
</p><center>
A<sup>2</sup> + B<sup>2</sup> 
= 
M<sup>2</sup> - H<sup>2</sup>/4.
</center><p>
The right hand side of our factorization is the same as the left hand side
of the Klein-Gordon equation 
in de Sitter conformal halfspace model coordinates. 
Aside from the ability to vary B, our factorization seems
essentially <b>unique</b> once we demand that the &#8706;'s be multiplied
by Dirac gamma matrices times the conformal factor Ht (times a constant) only, i.e. once
we demand that the de Sitter and Minkowski space Dirac operators agree in their
first-order terms, under the conformal Cunningham limit-map that maps each to the other;
and if we demand that the remaining terms be linear combinations of the five Dirac and 
scalar matrices.
[Proof sketch: once these order-1 terms are selected, continue by trying to write suitable
zeroth-order terms, and it will become clear your remaining choices are forced.]
</p><p>
How should we resolve this non-uniqueness?
To get the right behavior in the Minkowski limit we want
|B|<sup>2</sup> = O(H<sup>2</sup>) when H&#8594;0, but this still leaves 
considerable freedom.  If B&#8800;0 then the naive mass of our fermion becomes
<i>matrix-valued</i>
rather than a number, although the <i>squared</i> mass is still scalar unless AB&#8800;0.
This matrix is <b>Hermitian</b> if (and only if) A and B are both <i>real</i>,
and Hermiticity seems necessary unless we are willing to contemplate an enormous change in
the structure of quantum mechanics.
Matrix-valued mass, besides being peculiar, also would engender such mysteries as:
</p><ol><li>
How does a matrix-valued mass gravitate?
</li><li>
If an equal-mass positron and electron annihilate into photons, how do we conserve mass-energy
(since the photon energies are pure real)?
</li><li>
If AB&#8800;0 how can we interpret the ERBE (or for that matter, the Einstein relation without
any binding energy correction) in <i>classical</i> physics where the J-components
all are real numbers and therefore M<sup>2</sup> must be real also?  It seems as though
this would sacrifice the ability to treat fermions classically.
</li></ol>
<p>
It certainly would be a huge leap to resolve these mysteries, 
which suggests they probably cannot be resolved.  If that is true,
then we're restricted to a unique choice:
</p><center>
A = (M<sup>2</sup> - H<sup>2</sup>/4)<sup>1/2</sup> 
&nbsp;&nbsp; and &nbsp;&nbsp; B = 0
</center><p>
<!-- and if we permitted matrix-valued mass but insisted on hermiticity then
we would also permit
B=(M<sup>2</sup> - H<sup>2</sup>/4)<sup>1/2</sup> and A=0
but that would violate the B^2=O(H^2) condition -->
<a name="correctdiracdesitt"></a>
Hence I conclude that
the <b>Dirac electron spinor wave equation</b> is
</p><center>
<b>–</b>
[iHt&#947;<sup>0</sup>&#8706;<sub>t</sub> +
iHt&#947;<sup>1</sup>&#8706;<sub>x</sub> +
iHt&#947;<sup>2</sup>&#8706;<sub>y</sub> +
iHt&#947;<sup>3</sup>&#8706;<sub>z</sub>
- i(3/2)H&#947;<sup>0</sup>
+ (M<sup>2</sup> - H<sup>2</sup>/4)<sup>1/2</sup>] 
&#936;
= 
0.
</center><p>
This duly reduces to the usual Minkowski-space
Dirac equation (Peskin &amp; Schroeder's EQ 3.31)
when H&#8594;0 with
Ht=-1 held fixed, and it has the property that any Dirac solution automatically
also is a Klein-Gordon solution.   
</p><p>
Our approach of factoring the (correct) Klein-Gordon operator  –
and noting our two factors are identical up to an overall sign and a sign change for the
imaginary unit i –
immediately forces various desirable properties upon our Dirac equation:
</p><ul><li>
unitarity of time-evolution
</li><li>
SO(4,1) (de Sitter version of Poincare/Lorentz) invariance
and invariance under the Cunningham group in the M&#8594;0 limit
(essentially, all the symmetries of our Klein-Gordon equation are automatically inherited)
</li><li>
Lightspeed propagation, due to a discussion that will shortly arrive.
</li></ul>
<p>
The "naive mass" is
</p><center>
(M<sup>2</sup> - H<sup>2</sup>/4)<sup>1/2</sup> - i(3/2)H&#947;<sup>0</sup>
&nbsp;
(matrix valued!)
</center><p>
albeit the squared Frobenius norm of this matrix is M<sup>2</sup>+H<sup>2</sup>
which is the Klein-Gordon squared naive mass.
We can redo our same flaky argument from before to enquire what sort of mass tends
to be conserved when things are at rest.   A wavefunction &#936;  that does not depend on
x,y,z and is proportional to t<sup>3/2</sup> is now the "most nearly resting"
possible wavefunction obeying probability-normalization
(after consulting Peskin &amp; Schroeder EQ 3.73 for
the Dirac probability density and current 4-vector),
and upon it the Dirac operator has the 
same effect as multiplying &#936; by
<b>(M<sup>2</sup>-H<sup>2</sup>/4)<sup>1/2</sup>.</b>
This suggests that it is <i>this</i> quantity which is conserved for nearly-resting fermions
(which is <i>neither</i>
the naive nor the conformal mass)!   Call it then, the <b>effective fermion mass.</b>
</p><p>
Although a Klein-Gordon solution &#934; is
not necessarily a Dirac solution,
thanks to the fact Dirac's two factors commute, we know that
</p><center>
&#936;
=
[iHt&#947;<sup>0</sup>&#8706;<sub>t</sub> +
iHt&#947;<sup>1</sup>&#8706;<sub>x</sub> +
iHt&#947;<sup>2</sup>&#8706;<sub>y</sub> +
iHt&#947;<sup>3</sup>&#8706;<sub>z</sub>
- i(3/2)H&#947;<sup>0</sup>
+ (M<sup>2</sup> + H<sup>2</sup>/4)<sup>1/2</sup>] 
&#934;
</center><p>
will be 
– although it could just yield the trivial solution &#936;=0
–
<i>and</i> this &#936; (if nontrivial) works even if the Klein-Gordon and Dirac equation
right hand sides "0"
are replaced by any other function of position, aka "source term."
The benefit of this is
that it allows us <b>instantly to obtain the Dirac propagator</b> once we know
the Klein-Gordon propagator.  (This equation reduces to
Peskin &amp; Schroeder's EQ 3.117
and Greiner &amp; Reinhardt's EQ 1 in §2.5, 
and Zhang et al's [start of their §3]
all <i>except</i> for an overall minus sign –
which I presume arises from differing definitions –
in the Minkowski limit H&#8594;0 with Ht=-1 held fixed.)
</p><a name="neutmass"></a><p>
Again in our proposed Dirac equation note the presence of some 
<b>nontrivial H-dependent terms.</b>
We now have very peculiar behavior if M lies in the range 0&#8804;M&lt;H/2,
corresponding to <i>imaginary</i> effective fermion mass!
(And hence, very seriously, the Dirac operator
in this range gives us non-Hermitian behavior!!)
However, for M&#8805;H/2 behavior seems normal.
This suggests to me that we must <b>forbid</b> fermion M
in this range.   Thus if I had been alive in 1950 I could
have <i>predicted</i>
the surprising and otherwise unexplained
(but experimentally true!) fact that 
<b>in (3+1)-dimensional de Sitter spacetime,
all spin=1/2 fermions, including quarks and neutrinos, 
must have <i>nonzero</i> masses M&#8805;H/2&#8776;1.4×10<sup>-69</sup>kg.</b>
This fact contradicts the "standard model" which involves massless neutrinos;
that standard model picture 
was disproven by both
Super-Kamiokande in 1998
and the Sudbury Neutrino Observatory in 2001.
</p><blockquote>
It is known that the three neutrino masses sum to at least 0.04eV and 
at most 0.3eV, which in kilograms is 7.1 to 54 times <nobr>10<sup>-38</sup></nobr> kg.
The lower bounds are from flavor-change observations, while the upper bounds are
from astronomical measurements and models of neutrino gravitation effects.
This lower bound on M using H as input can <i>instead</i> be regarded, by using
54×10<sup>-38</sup>/3 kg as input,  as an upper bound on H
and hence on &#923;<sub>Einstein</sub>. It finds
&#923;<sub>Einstein</sub>&lt;2.0×10<sup>-58</sup> in Planck units.
<!-- 3 * (54*10^(-38) * kg)^2 / (3*mplanck)^2 = 2.0 * 10^(-58). -->
In contrast, Weinberg 1989 argues that one would naively expect
&#923;<sub>Einstein</sub>&#8776;1 in Planck units, contradicting reality by
&#8805;121 orders of magnitude 
("the <b>worst theoretical prediction in the history of physics!</b>").
Even if we were to postulate a mode cutoff much smaller than the Planck energy postulated
by Weinberg, namely at the energy 
<nobr>
3.2×10<sup>20</sup>eV&#8776;2.7×10<sup>-8</sup>M<sub>pl</sub>
</nobr>
of the highest-energy cosmic ray
yet observed (Bird et al 1993) – i.e. the least possibly 
permitted by experimental evidence –
then we'd still have a &#8805;91-order of magnitude discrepancy.
Although there still is a large gap,
our H upper bound here has
<b>greatly improved Weinberg's "cosmical constant crisis";</b>
the Weinberg/Planck dsicrepancy being improved by 64 orders of magnitude
and the Weinberg/CosmicRay discrepancy by 35 orders of magnitude.  
(And if one of the neutrino flavors is especially light
this improvement would be larger.)
</blockquote>
<a name="susyforbid"></a>
<p>
Also note that this <b>forbids supersymmetry</b> – at least with any strict interpretation
of SUSY in which massless bosons always have massless fermion superpartners.
</p><p>
Let us discuss that in more detail.
Of course, the supersymmetrists were aware that if bosons with the same mass and charge as
an electron existed, we would have noticed them by now!  Therefore, supersymmetry (if it exists)
must exist in a "broken" manner which allows superpartner masses to differ
from the original particle masses.  Indeed is SUSY is true it <i>must</i>
be in a form which forces 
naively-massless fermion superparticles such as photinos and gluinos, to have masses
comparable or exceeding contemporary collider energies
(since otherwise they'd have already been observed).
But the question is <i>how</i> this symmetry-breaking
occurs.  I am not competent in that subject.   Let me just say this: 
</p><ol><li>
As we just saw, (1+3)-dimensional de Sitter spacetime forbids massless fermions.
</li><li>
Perfect supersymmetry which forces massless fermions ("photinos," "gluinos") to exist,
would be ruled out, both by de Sitter rain of bricks, and by experimental
evidence we already have.
</li><li>
Although broken supersymmetry must (if it is genuine physics)
add mass to naively-massless fermions,
at least some possible mechanisms for granting them such mass would seem ruled out.
For example, the naively-massless W and Z gauge bosons
experimentally have mass, caused by an interaction between
them and the "Higgs boson." 
If, however, some geometrical fact had prevented massless gauge bosons of the W and Z sort from
existing in the first place, then it seems to me the Higgs mechanism would not be 
available to save the situation.   Similarly, in de Sitter space
it should be illegal to postulate that the
photino or gluino are massless but acquire an effective mass through interaction with
some Higgslike boson (or with any other field postulated 
to have nonzero vacuum expectation value).  Note in rain of bricks, "bare mass" is a meaningful
concept, even if Higgs effects cause a greater "effective mass"; hence this <i>logically</i>
forbids Higgs-broken supersymmetry.  
<b>Rain of bricks in de Sitter background, and Higgs-broken supersymmetry, are logically
incompatible.</b>
</li><li>
Aitchison 2007 claims there are two possible ways SUSY-breaking could happen,
with the one he prefers being precisely the Higgs approach we just ruled out; 
the other is to add  explicit symmetry-breaking
terms to the lagrangian, which Aitchison says would be less preferred since it
would "destroy renormalizability."  
(It also just seems uglier; the "beauty" of supersymmetry would be jettisoned by 
adding explicitly non-supersymmetric terms, which seems to sacrifice the whole point of
the idea.)
</li><li>
Bilal 2000 remarks in §6.2: "If supersymmetry is spontaneously broken there is a massless 
spin=1/2 particle... sometimes called the 
<a href="http://en.wikipedia.org/wiki/Goldstino">Goldstino</a>."
If so, we would conclude that <i>spontaneously broken supersymmetry is 
disallowed in (1+3)-dimensional de Sitter spacetime.</i>
But presumably Bilal was wrong because if Goldstinos existed this would already have been
experimentally obvious.
</li><li>
Wikipedia
<a href="http://en.wikipedia.org/wiki/Minimal_Supersymmetric_Standard_Model">
Minimal Supersymmetric Standard Model</a>
says MSSM features "explicit soft supersymmetry breaking"
via "introducing operators into the Lagrangian communicated
by some unknown (and unspecified) dynamics."
This might be permitted by de Sitter rain of bricks, 
although the crux issue would seem to lie exactly in the
"unknown (and unspecified) dynamics" or (with Aichison's way of phrasing it)
what exactly the magic new Lagrangian terms are.
</li><li>
Another problem is this: recent analyses of experimental data from LHC
(Large Hadron Collider at CERN, see Eberhardt et al 2012)
have indicated that all fermions have been found –
there are no further to be discovered beyond the ones in the "standard model"!
Supposedly they've ruled out
the existence of further fermions with a probability of
99.99999% (5.3&#963;).
If so, this would for example refute "supersymmetry"
(the fermion superpartners of the W and Z bosons could not exist) and hence 
"superstring theory."
<br>&nbsp;&nbsp;&nbsp;
How can Eberhardt et al possibly infer the <i>non</i>existence of new high-energy particles
from data gathered from the comparatively low energies accessible by the LHC?
Apparently their reasoning is roughly the following.  They presume
that a large fraction of the "mass" of the new particles is not mass
per se, but rather is effective mass due to interaction-energy with
the all-pervading Higgs scalar field.  (It is this "Higgs mechanism"
that allows the W and Z vector bosons to have effective mass even
though in Yang-Mills theory they necessarily have zero mass, thus
rescuing and enabling Yang-Mills theory
to be physically real.)
<br>&nbsp;&nbsp;&nbsp;
Their point:
given this is the cause of a goodly fraction of the the new particles'
huge masses (and it presumably necessarily would be, under supersymmetry),
the Higgs is going to have something to say about that.
If the Higgs boson were assumed to have enormous interactions, then it
ought to have enough-altered physics that the LHC would have seen that, which it
did not.  On the other hand,
if the new particles had <i>small</i>
masses, then the LHC would have seen them directly.
Either way, we conclude there are no more kinds of new
particles.
</li></ol>
<p>
Given that at least Aichison's most-favored kind of supersymmetry
is incompatible with de Sitter rain of bricks, one has to ask which if the two is 
more favored by evidence.  So I find it quite interesting that 
the evidence in our 
<a href="#runcoupconstplot">running coupling constant plot</a>
seems also to favor the non-supersymmetric plain standard model, despite many 
having previously proffered that plot as among the best evidence <i>for</i> supersymmetry!
</p><p>
<b>Two warnings about wrong Dirac equations:</b>
Birrell &amp; Davies 1984 used vierbein formalism but did <i>not</i>
alter M via an H (curvature coupling) term, and hence they
and all the papers they cited obtained (their
EQ 3.179) what I believe is the <i>wrong</i> Dirac equation in curved
spacetime, in particular it is wrong in de Sitter spacetime.
But oddly enough Birrell &amp; Davies were aware of the possibility of curvature-coupling
within the <b>Klein-Gordon</b> equation: as they pointed out immediately
after obtaining the (in our view wrong)
Klein-Gordon generalization to curved space in their EQ 3.174,
there is a more-general possibility, namely their EQs 3.24 and 3.26 – and they duly
get our "correct" Klein-Gordon equation by using n=4 in their "conformal coupling"
EQ 3.27 (using the fact that
&#923;=nR whenever Einstein's vacuum gravity equations with &#923; are satisfied
in n-dimensional spacetime with scalar curvature R=R<sup>&#956;&#957;</sup><sub>&#956;&#957;</sub>).
Birrell &amp; Davies' (wrong)
Dirac operator
is not a factor of their (correct) conformally-coupled Klein-Gordon operator.
Birrell &amp; Davies and a host of other authors all have the <i>wrong</i>
Dirac equation in curved spacetimes generally. 
<!-- the right one presumably involves 
replacing their mass M by (M<sup>2</sup>+R/6)<sup>1/2</sup>. -->
</p><a name="diracwronghist"></a><p>
<b>Partial understanding of how Dirac 1935 got the wrong Dirac equation(!):</b>
Dirac 1935 attempted to derive the Dirac electron spinor-wave equation in de Sitter space.
Although he did so in a very beautiful way, I still claim it is wrong.
<!--Oddly enough Dirac 1935 <i>departed</i> 
from his famous original flat-space strategy of trying to factor the Klein-Gordon operator,
and also abstained from his "degree &eta;" trick!
[The natural degree, to make the action-integral dimensionless, would have been &eta;=-3/2.
See Peskin &amp; Schroeder EQ 3.34 
regarding m as an inverse length.  This fits
perfectly with our approach of factoring the 
Klein-Gordon operator because as we discussed, 
the Klein-Gordon "wavefunction" &phi;=H<sup>1/2</sup>&Phi;
which is the natural one from the standpoint of an interpreter 
since it has the right normalization behavior, also has degree &eta;=-3/2.]
He instead employed a beautiful but somewhat ad hoc approach
of trying to base things on the 5-dimensional "angular momentum" J<sub>&mu;&nu;</sub>
(which he called m<sub>&mu;&nu;</sub>)
in de Sitter
space by factoring our Einstein relation between J<sub>&mu;&nu;</sub> and M/H.-->
Dirac used the wrong Klein-Gordon operator and 
insisted for no stated reason on a specific form of its factorization,
and since he was able to find one meeting his ansatz, he thought about it no further,
not realizing there is a 1-parameter family of factorizations.
We demanded a conformal connection between our and the flatspace Dirac equation,
which gave us a unique result.  Dirac 1935, however, nowhere thought about conformal 
maps or invariance and thus had no tool to force uniqueness (aside from ad hoc unjustified
ansatzes).  His procrustean ansatz forced the naive mass to be exactly M, allowing no 
H-caused modification.  Another issue was that Dirac's approach started by
writing what seemed like a reasonable "Hamiltonian" expressed in terms of the 
J<sub>&#956;&#957;</sub>.  However, the whole <i>concept</i> of a Hamiltonian is based on the
idea that things that commute with the Hamiltonian are conserved as time advances –
which is fine in flat space where we have a clear notion of what "time" is, but in
curved spacetime may not make sense.  (For Dirac
the mother coordinate e was "the" notion of time.)
That whole issue was central to,
but ignored by, Dirac.  In contrast, our approach bypasses it.
</p><p>
Dirac then derived an elegant wave equation (his EQ 25)
symmetric in the 5 mother coordinates.
<!--
with the aid of the fact 
[which he attributes to A.S.Eddington, Proc.Royal Soc. London A 121 (1928) page 524]
that <i>five</i>  
self-inverse anticommuting hermitian 4&times;4 "Dirac matrices" can be constructed.
-->
Dirac then noted that his resulting claimed fermion wave equation
could be interpreted as arising from a hermitian Hamiltonian operator
<i>provided</i> M is
<i>complex</i> with
constant imaginary part=-2i&#8463;H
and arbitrary real part!
This part of Dirac 1935's thinking lost the wonderful clarity of the rest of
his paper because he did not define precisely
what his Hermiticity desiderata were, nor justify them,
nor prove there was only one way to satisfy them.  (Again, I find them suspect because the
whole idea of a hermitian Hamiltonian depends on having <i>a</i> Hamiltonian, a notion
of at best unclear validity in a curved spacetime.)  It could certainly be argued, for example,
that having any non-real mass instantly demolishes hermiticity.
Dirac's conclusion, in any case, seems incompatible with 
local conservation of mass-energy when a pair of equal-mass
electrons and positrons
annihilate into a photon-pair (since the photon's energies are pure-real)!
And what would a complex mass <i>mean</i>, e.g. gravitationally?
Similarly in the classical Einstein relation (which was Dirac's starting point!), 
only pure-real squared-masses M are permissible.
Finally – and most-damning –
with the real part of the mass taken as 0
Dirac would get fermions naturally propagating
<i>faster than lightspeed!</i>   
</p><p>
<b>What are the (unsmeared) propagator formulas?</b>
In 
<a href="#propformulas">§15</a> 
we outlined Greiner &amp; Reinhardt's derivation (which employed
momentum and Fourier transforms) of the 
Minkowski-space position-basis propagator formulas.   Their kind of derivation no longer seems
possible in de Sitter land, 
because there is no longer any clear notion of what "momentum" is, nor does
the Fourier transform have much importance anymore.
A better approach seems to be:
</p><ol>
<li>
Simply write down the answer!
</li><li>
Verify that your formula
satisfies the appropriate partial differential equation (e.g. the Klein-Gordon equation, if we are
speaking of spin=0 particles) and has the right asymptotic behaviors at infinity and
near/at the origin.  
</li></ol>
<p>
We can use the analogy between de Sitter space and
a sphere to accomplish step 1.
For a (D-1)-dimensional spherical surface with radius=R in D-dimensional Euclidean space,
the laplacian (on the surface) of a function F 
depending only on &#952; (the geodetic angular distance to
a fixed "North pole" point on the sphere) is known to be
</p><center>
&#8706;<sub>&#956;</sub>&#8706;<sup>&#956;</sup>F
= 
R<sup>-2</sup> (sin&#952;)<sup>2-D</sup> (d/d&#952;) [(sin&#952;)<sup>D-2</sup> dF/d&#952;].
</center><p>
For such an F,
the Euclidean version 
<nobr>
&#8706;<sub>&#956;</sub>&#8706;<sup>&#956;</sup>F=K<sup>2</sup>F
</nobr>
of the Klein-Gordon equation (aka the "Helmholtz equation")
on the sphere,  is thus
</p><center>
(sin&#952;)<sup>2-D</sup> (d/d&#952;) [(sin&#952;)<sup>D-2</sup> dF/d&#952;]
= (KR)<sup>2</sup>F.
</center><p>
The general solution of this equation is 
</p><center>
F(&#952;) =
 C<sub>1</sub>
 (sin&#952;)<sup>(3-D)/2</sup>
 LegendreP( [([D-2]<sup>2</sup>-4[RK]<sup>2</sup>)<sup>1/2</sup>-1)/2, [D-3]/2, cos&#952; )
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
+  C<sub>2</sub>
 (sin&#952;)<sup>(3-D)/2</sup>
 LegendreQ( [([D-2]<sup>2</sup>-4[RK]<sup>2</sup>)<sup>1/2</sup>-1)/2, [D-3]/2, cos&#952; )
</center><p>
See ch.8 of Abramowitz &amp; Stegun for
<b>associated Legendre functions.</b>
These also can be 
expressed in terms of the Gaussian hypergeometric function,
but seem most naturally expressed in terms of
Gegenbauer functions (see the papers by Durand for those).
<!--
GegenbauerD( [([D-2]<sup>2</sup>-4[RK]<sup>2</sup>)<sup>1/2</sup>+D-4)/2, [4-D]/2, cos&theta; )
-->
Here 
C<sub>1</sub> and C<sub>2</sub>
are two arbitrary complex numbers; they may be chosen to force F to
have desired behaviors when &#952;&#8594;0+ or &#952;&#8594;&#960;-.
For example if C<sub>2</sub>=0 and D=3 we get the well known Legendre polynomials of 
<nobr>degree=[([D-2]<sup>2</sup>-4[RK]<sup>2</sup>)<sup>1/2</sup>-1)/2</nobr>
(and argument cos&#952;)
in cases where this degree is a nonnegative integer.
<!--
MAPLE:
lapF := sin(t)^(2-N) * diff( sin(t)^(N-2) * diff(F(t),t), t ) ;
dsolve( lapF - M^2 * F(t), F(t) );
F(t) = C1 * sin(t)^((3-D)/2) * LegendreP([((D-2)^2-4*M^2)^(1/2)-1]/2, (D-3)/2, cos(t))
     + C2 * sin(t)^((3-D)/2) * LegendreQ([((D-2)^2-4*M^2)^(1/2)-1]/2, (D-3)/2, cos(t))
-->
</p><p>
If we now convert all this to work in de Sitter space with (D-1) spacetime dimensions, 
remembering to replace K<sup>2</sup> by
<nobr>M<sup>2</sup>+(D-3)(D-1)H<sup>2</sup>/4</nobr>
which is the D-dimensional generalization of the conformal-invariance discussion above
(we only discussed the physical case D=5 before, when this is
<nobr>M<sup>2</sup>+2H<sup>2</sup>)</nobr>
<!-- Fulling p133 gives the conformal term in terms of R and D;
http://en.wikipedia.org/wiki/De_Sitter_space
"Properties" gives R in terms of Lambda and H -->
we find
that the most general solution of the Klein-Gordon equation that depends only on the distance 
s to a fixed point (s=imaginary for spacelike and s=real for timelike distances), is
</p><center>
F(s) =
 C<sub>1</sub>
 [sinh(sH)]<sup>(3-D)/2</sup>
 LegendreP( [([D-2]<sup>2</sup>-4[M/H]<sup>2</sup>-[D-3][D-1])<sup>1/2</sup>-1]/2, [D-3]/2, cosh(sH) )
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
+  C<sub>2</sub>
 [sinh(sH)]<sup>(3-D)/2</sup>
 LegendreQ( [([D-2]<sup>2</sup>-4[M/H]<sup>2</sup>-[D-3][D-1])<sup>1/2</sup>-1]/2, [D-3]/2, cosh(sH) )
</center><p>
<!-- MAPLE:
evalf( LegendreP((((5-2)^2-4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(99.0)) );   +1.4*10^(+26)
evalf( LegendreQ((((5-2)^2-4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(99.0)) );   -6.6*10^(-70)
evalf( LegendreP((((5-2)^2+4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(99.0)) );   +6.0*10^(+55)
evalf( LegendreQ((((5-2)^2+4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(99.0)) );   -2.8*10^(-99)

evalf( LegendreP((((5-2)^2-4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(0.01)) );   +0.005
evalf( LegendreQ((((5-2)^2-4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(0.01)) );   -99.97
evalf( LegendreP((((5-2)^2+4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(0.01)) );   +0.015
evalf( LegendreQ((((5-2)^2+4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(0.01)) );   -99.93

evalf( LegendreP((((5-2)^2-4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(0.01*I)) );   +0.005*I
evalf( LegendreQ((((5-2)^2-4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(0.01*I)) );   0.008+100.0*I
evalf( LegendreP((((5-2)^2+4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(0.01*I)) );   +0.015*I
evalf( LegendreQ((((5-2)^2+4*1.0^2)^(1/2)-1)/2, (5-3)/2, cosh(0.01*I)) );   0.024+100.1*I

The degree reflection formulas (A&S 8.2.1 and 8.2.2) which change degree v to degree -v-1,
reflecting about v=-1/2, indicate there is something special about v=-1/2.

evalf( LegendreP( 1/2, 1, cosh(99.0)) );   +1.0*10^21
evalf( LegendreQ( 1/2, 1, cosh(99.0)) );   -7.5*10^(-65)
evalf( LegendreP( 1/2, 1, cosh(0.01)) );   +0.0037
evalf( LegendreQ( 1/2, 1, cosh(0.01)) );   -99.98
evalf( LegendreP( 1/2, 1, cosh(0.01*I)) );  0.0037*I
evalf( LegendreQ( 1/2, 1, cosh(0.01*I)) );  0.0059+100.0*I

evalf( LegendreP( 1/2 + 1*I, 1, cosh(99.0)) );   +1.1*10^21 - 1.4*10^21*I
evalf( LegendreP( 1/2 + 2*I, 1, cosh(99.0)) );   -1.7*10^21 - 1.8*10^21*I
evalf( LegendreP( 1/2 + 4*I, 1, cosh(99.0)) );   +2.2*10^21 - 2.8*10^21*I

evalf( LegendreQ( 1/2 + 1*I, 1, cosh(99.0)) );  all these are complex with 
evalf( LegendreQ( 1/2 + 2*I, 1, cosh(99.0)) );  absolute value around 10^(-63) or below
evalf( LegendreQ( 1/2 + 4*I, 1, cosh(99.0)) );  

Evidently from 8.2.2 the Q's become bad (growing like P's at infinity) if v<1/2.
The P's stay the same behavior since P[-v-1]=P[v].

LegendreQ is the one we want. 
It is best behaved at large timelike distances.    There are no large spacelike
distances.   LegendreQ also gets large at small spacelike and timelike distances.
-->
<b>Behaviors at <i>large</i> timelike distances:</b>
|LegendreP(v,1,cosh<i>s</i>)| grows ultimately exponentially to &#8734; 
when s&#8594;±&#8734;
with v held fixed if 
|Re(v+1/2)|&gt;1/2.
But if
|Re(v+1/2)|&lt;1/2, 
then we get exponential falloff 
toward 0 instead, ultimately roughly like exp(vs)
[up to a factor that behaves like a v-dependent power of s]
if -1/2&#8804;Re(v)&lt;0.
<!--
|LegendreP(-1/4,1,cosh<i>s</i>)| drops like  exp(-s/4).
|LegendreP(-1/5,1,cosh<i>s</i>)| drops like  exp(-s/5).
|LegendreP(-2/5,1,cosh<i>s</i>)| drops like  exp(-2s/5).
|LegendreP(-1/2,1,cosh<i>s</i>)| drops like  exp(-s/2).

LegendreP(-1/2, 1, x) * sqrt(x)/ln(x)
appears to approach the limit -0.2250810
as x->infinity.

LegendreP(-1/4, 1, x) * x^(1/4)
appears to approach the limit -0.3509173596
as x->infinity.
-->
</p><p>
Meanwhile
|LegendreQ(v,1,cosh<i>s</i>)| 
<i>falls</i> toward 0 ultimately like exp(-|s|/2) or faster
as s&#8594;±&#8734;
with v held fixed with Re(v)&#8805;-1/2.
This is tight since
LegendreQ(-1/2,1,x<sup>2</sup>)=-2<sup>1/2</sup>&#960;/(4x)+O(|x|<sup>-5</sup>) 
when |x|&#8594;&#8734;.
Indeed
|x|<sup>v+1</sup>LegendreQ(v,1,x)
remains bounded as 
|x|&#8594;&#8734;
for each fixed v with Re(v)&#8805;-1/2.
</p><p>
Hence |LegendreQ(v,1,cosh<i>s</i>)| 
is much smaller than 
|LegendreP(v,1,cosh<i>s</i>)| 
when s&#8594;±&#8734; for any real fixed v&gt;-1/2.
But if v=1/2+iy with y&#8800;0 real and fixed, then the story is different.
Now 
LegendreP(v,1,cosh<i>s</i>)
is pure-real
and behaves, when s&#8594;±&#8734;,
like an oscillating factor times
exp(-s/2)/s.
<!-- This fact is useful because the Minkowski Feynman 
<a href="#FeynMinkKGprop">propagator</a> 
is pure-imaginary when s is pure-imaginary? No: we never use it.-->
Meanwhile
LegendreQ(v,1,cosh<i>s</i>)
behaves like an oscillating complex factor times
exp(-s/2).
</p><p>
<b>Behaviors at <i>small</i> spacelike or timelike distances:</b>
|LegendreP(v,1,cosh<i>s</i>)|=(v+1)vs/2+O(s<sup>3</sup>)
when |s|&#8594;0 with v held fixed.
But 
LegendreQ(v,1,cosh<i>s</i>)=-1/s+O(s) when
|s|&#8594;0 with v held fixed with Re(v)&#8805;-1/2.
<!-- the coefficient of s in the O(s) term seems to be about 11*(v*v-v-1). -->
</p><p>
<b>Behaviors at large spacelike distances:</b>
Remember, in the de Sitter metric, unboundedly large spacelike distances are impossible.
Ignoring that, the formulas make it obvious that all these functions are periodic in s with 
period=2&#960;i/H.
The horizon distance is thus a quarter-period.
</p><p>
<b>Behavior on the de Sitter horizon and antipode:</b>
LegendreP(v,1,0)=0 if v&#8800;-1 is a positive even or negative odd integer.
Re[LegendreP(v,1,0)]=0 if v is real.
Re[LegendreQ(v,1,0)]=0 if v is a non-negative even integer or v=-1.
Im[LegendreQ(v,1,0)]=0 if v is a positive odd integer or v=-1.
LegendreP(v,1,-1)=0 if v&#8805;0.
But |LegendreQ(v,1,x)| becomes logarithmically infinite 
when x approaches ±1
from inside the interval [-1,1],
for any fixed v&#8805;0.
</p><p>
<b>Simplifications when D=5 and M=0:</b>
LegendreQ(0,1,cosh<i>s</i>)·sinh<i>s</i>=-1, &nbsp;
LegendreP(0,1,z)=0 for all complex z.
</p><p>
In the physical dimension D=5,
to match the behavior of the Minkowski-space Klein-Gordon propagator when H&#8594;0 
in the M=0 case,
we need to choose 
C<sub>2</sub>=-iH<sup>2</sup>(2&#960;)<sup>-2</sup>.
With this choice and 
C<sub>1</sub>=0,
we indeed match [up to an O(H<sup>2</sup>) additive error]
the behavior of the Minkowski-space Klein-Gordon 
<a href="#FeynMinkKGprop">propagator</a>
when H&#8594;0 
for any fixed M&gt;0 and fixed real s.
But with fixed <i>imaginary</i> s,
with the above choice of 
(C<sub>1</sub>, C<sub>2</sub>)
we match the imaginary part of the
Minkowski-space Klein-Gordon 
<a href="#FeynMinkKGprop">propagator</a>
but not its real part (which is exactly 0).
</p><p>
<!--
The choice
C<sub>1</sub>=0 
has the advantage of 
keeping the |propagator| as small as possible when s&rarr;&plusmn;&infin; 
for sufficiently small fixed M, but I do not see that it is forced???
-->
</p><p>
In view of these observations,
and inserting an appropriate Dirac delta function term
[and noting k&#948;(kx)=&#948;(x) if k&gt;0],
we conclude that the <b>Feynman Klein-Gordon propagator</b> in dS<sup>3,1</sup> is
</p><center>
G<sub>Feyn</sub>(t;x,y,z)
= 
-(4&#960;)<sup>-1</sup> &#948;(s<sup>2</sup>) 
- CZRP<sub>s</sub>( (2&#960;)<sup>-2</sup>H<sup>2</sup>i
[sinh(sH)]<sup>-1</sup>
LegendreQ(
-2[M/H]<sup>2</sup>/[(1-4[M/H]<sup>2</sup>)<sup>1/2</sup>+1], 1, cosh(sH) ) )
<br>
where &nbsp;&nbsp; 
CZRP<sub>s</sub>(a+ib) = a+ib 
&nbsp;
if s is real, but 
&nbsp;
ib 
&nbsp;
if s is imaginary.
<br>
(CZRP stands for "conditionally zero the real part.")
</center><p>
The reason it is legitimate to employ the strange-looking CZRP function is:
</p><p>
<b>Lemma:</b>
Given any complex solution F(s) of a linear differential equation with only
real coefficients (and we can write ours in such a manner), 
both its real and its imaginary parts are
solutions by themselves along either the real s-axis or imaginary s-axis.  
(<b>Proof:</b> Trivial.)
</p><p>
Just as in the Minkowski metric (<a href="#propformulas">§15</a>) case, G<sub>Feyn</sub>
is <i>not</i> an analytic function of s,
but can be regarded as two
analytic functions, one
used when s is real and the other when s is imaginary.
Both functions arise naturally by choosing
weights in linear combinations
to match the behavior of the Minkowski propagator when H&#8594;0.
<!-- 
note the radical trick
[(1-4[M/H]<sup>2</sup>)<sup>1/2</sup>-1]/2 =
-2[M/H]<sup>2</sup>/[(1-4[M/H]<sup>2</sup>)<sup>1/2</sup>+1]

MAPLE:
FKG := (s,m) -> -sign(s*s) * (m/s) * (2*Pi)^(-2) * BesselK(1, sign(s*s)*I*m*s);   #checked. 
GD := (s,m,H) -> (-I)*
(2*Pi)^(-2)*H^2*sinh(s*H)^(-1)*LegendreQ( -2*(m/H)^2/((1-4*(m/H)^2)^(1/2) + 1),
 1, cosh(s*H));
QQ := (s,m,H) -> GD(s,m,H) - FKG(s,m);
GI := (s,m,H) -> (-I)*
(2*Pi)^(-2)*H^2*sinh(s*H)^(-1)*LegendreP( -2*(m/H)^2/((1-4*(m/H)^2)^(1/2) + 1),
 1, cosh(s*H));
QI := (s,m,H) -> GI(s,m,H) - FKG(s,m);
Q2 := (s,m,H) -> GI(s,m,H) + GD(s,m,H) - FKG(s,m);
GD(1,1,1); #0.013652309809821708527380207386150495011085632993668
           # + 0.024952364069301244182449865497319293707492362518392 * I
GD(I,1,1); #0.058139165227499785817376883177968240187898648487560 
           # - 0.019037938274561180020131342996184140344182689628394 * I
GD(I,10,1); #2755.7668643285984500348383320169709118480931956078 
            # - 0.0000061131335034959656977709219919327174677438175165032 * I
evalf( QQ(1,1,0.1), 30 ); 
evalf( QQ(1,1,0.01), 30 ); 
evalf( QQ(1,1,0.001), 30 ); 
evalf( QQ(1,1,0.0001), 30 ); #tiny
evalf( QQ(I,1,0.1), 30 ); 
evalf( QQ(I,1,0.01), 30 ); 
evalf( QQ(I,1,0.001), 30 ); 
evalf( QQ(I,1,0.0001), 50 ); #real part not tiny!! Re(FKG)=0. Re(GD)=0.04497.
evalf( QI(I,1,0.0001), 50 ); #imag part not tiny!! Re(FKG)=0. Re(GD)=0.04497.
evalf( Q2(I,1,0.0001), 50 ); #both parts not tiny
evalf( QQ(0.1,1,0.1), 30 ); 
evalf( QQ(0.1,1,0.01), 30 ); 
evalf( QQ(0.1,1,0.001), 30 ); 
evalf( QQ(0.1,1,0.0001), 30 ); #tiny
evalf( QQ(0.1*I,1,0.1), 30 ); 
evalf( QQ(0.1*I,1,0.01), 30 ); 
evalf( QQ(0.1*I,1,0.001), 30 ); 
evalf( QQ(0.1*I,1,0.0001), 50 ); #real part not tiny!!  Re(FKG)=0. Re(GD)=0.03984.
evalf( QQ(1,2,0.1), 30 ); 
evalf( QQ(1,2,0.01), 30 ); 
evalf( QQ(1,2,0.001), 30 ); 
evalf( QQ(1,2,0.0001), 30 );  #tiny
evalf( QQ(1,0.12,0.1), 30 ); 
evalf( QQ(1,0.12,0.01), 30 ); 
evalf( QQ(1,0.12,0.001), 30 ); 
evalf( QQ(1,0.12,0.0001), 30 );   #tiny
evalf( QQ(10,2,0.1), 30 ); 
evalf( QQ(10,2,0.01), 30 ); 
evalf( QQ(10,2,0.001), 30 ); 
evalf( QQ(10,2,0.0001), 30 );  #tiny
evalf( QQ(10,0.12,0.1), 30 ); 
evalf( QQ(10,0.12,0.01), 30 ); 
evalf( QQ(10,0.12,0.001), 30 );  #tiny
#Yes: matches in limit H to 0+ if we do the real part hack.
lim GD(s,M,H)*s^2 = I/(2*Pi)^2   as s->0   is a numerical fact true regardless of M and H.
-->
Here s is the de Sitter distance from the source point to (t;x,y,z)
and is positive real for timelike and positive imaginary for spacelike distances.
As far as I know, this propagator formula is new (although various previous authors 
have provided wrong answers!).
In the <b>massless</b> case M=0 it simplifies to
</p><center>
G<sub>Feyn</sub>(t;x,y,z)
= 
-(4&#960;)<sup>-1</sup> &#948;(s<sup>2</sup>) 
+ H<sup>2</sup>i
[2&#960;sinh(sH)]<sup>-2</sup>
<!--checked!-->
</center><p>
The Dirac propagator may also be obtained via differentiation as described above.
In the limit H&#8594;0 
the de Sitter formula 
becomes the Minkowski-space propagator
[the additive error is O(H<sup>2</sup>)].
Both also agree when |s|&#8594;0.
However, the de Sitter formula is considerably better behaved when s&#8594;±&#8734;:
It falls off exponentially with characteristic e-folding length&#8804;2/H.
Meanwhile the Minkowski version 
only falls off like a power law, times, if M&gt;0, an oscillatory factor.
To make that clear: 
Improving on
Greiner &amp; Reinhardt EQ 40a and 40b
in their §2.5, we can use known Bessel function asymptotic formulae to deduce that
as s<sup>2</sup>&#8594;+&#8734; (timelike infinity) the Feynman Klein-Gordon 
<a href="#FeynMinkKGprop">propagator</a>
in Minkowski metric is asymptotic to
<!--MAPLE verification of Minkowski asymptotics:
FKG := (s,m) -> -sign(s*s) * (m/s) * (2*Pi)^(-2) * BesselK(1, sign(s*s)*I*m*s);   #checked. 
AST := (s,m) -> 2^(-5/2)*Pi^(-3/2) * (m)^(1/2) * (I*s)^(-3/2) * exp(-I*m*s);   #timelike asymp
RAT := (s,m) -> FKG(s,m)/AST(s,m);
evalf(abs(RAT(999999999,1)),80);
evalf(abs(RAT(999999999,2)),80);
evalf(abs(RAT(999999999,3)),80);
ASS := (s,m) -> -I*2^(-5/2)*Pi^(-3/2) * (m)^(1/2) * (s)^(-3/2) * exp(-m*s);   #spacelike asymp
RST := (s,m) -> FKG(I*s,m)/ASS(s,m);
evalf(RST(99999,1));
evalf(RST(99999,2));
evalf(RST(99999,3));
-->
</p><p></p><center>
2<sup>-5/2</sup>&#960;<sup>-3/2</sup>M<sup>1/2</sup>(is)<sup>-3/2</sup> exp(-iMs)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
(oscillatory times power law if M&gt;0)
</center><p>
while as s<sup>2</sup>&#8594;-&#8734; (spacelike infinity) it
is asymptotic to
</p><p></p><center>
-2<sup>-5/2</sup>&#960;<sup>-3/2</sup>M<sup>1/2</sup>s<sup>-3/2</sup>i exp(-M|s|)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
(exponential falloff).
</center><p>
This all is a tremendous <b>improvement as far as "infrared infinities" of QFTs are concerned:</b>
</p><ol>
<li>
If we go to large timelike distances,
de Sitter gives us exponential falloff instead of power law.
</li><li>
If we try to go to large spacelike distances,
instead of exponential falloff,
de Sitter gives us something even better – 
spacelike |distances| longer than &#960;/H
simply do not <i>exist.</i>
</li><li>
Some critic might complain that the "faster falloff" with distance in de Sitter space
is misleading because "there is more de Sitter than Minkowski volume at large distances."
This complaint is simply wrong:
The measure of the de Sitter metric at spatial distance s
obeying L&lt;|s|&lt;U from a given point,
is finite if restricted to a constant-time hyperplane, infinite otherwise – exactly the same
situation as in Minkowski metric but with the improvement that the finiteness remains valid 
for de Sitter even when U&#8594;&#8734;, and the de Sitter finite quantity is always
smaller than the Minkowski finite quantity with same L and U
if 0&#8804;L&lt;U.
The measure of the de Sitter metric at timelike distance s with L&lt;s&lt;U from a given point,
is infinite (just as it is infinite for Minkowski) for any L and U with 0&lt;L&lt;U.
</li></ol>
<p>
De Sitterizing has, however, absolutely no effect on the ultraviolet situation.
</p><p>
<b>Intuitive argument that all infrared infinities in QFTs are cured in de Sitter space:</b>
In flat space, it is possible to have an infinite set of photons 
(or any other massless particle) with finite summed mass-energy.
This is the whole reason IR infinities can pose a problem.
But in de Sitter spacetime, this is <i>im</i>possible.
Massless bosons can still exist, but in order to make a massless boson's
<i>energy</i> be arbitrarily small, it is necessary to make its de Broglie 
wavelength arbitrarily large,
i.e. the particle must be arbitrarily delocalized.  But this in de Sitter space is simply 
not possible – because the maximum spatial distance is &#960;/H.
<b>Q.E.D.</b>
</p><p>
Finally, we must describe the <b>Poisson process</b>.
It needs to be with respect to the natural 
de Sitter 4-volume measure, whose infinitesimal element is
|det(g)|<sup>1/2</sup>dxdydzdt where g is the metric tensor.
This in the conformal model is (Ht)<sup>-4</sup>dxdydzdt.
</p>
<!--<p>
"???simply add 1"
</p>-->

<hr>

<a name="philosource"></a>
<h3>36. Philosophical interlude: what was the source of all QED &amp; QCD's
infinities and nonanalyticities?</h3>

<p>
I believe there really are only two logical-sources of all of these bad phenomena in 
pre-rain-of-bricks QFTs:
</p><ol><li>
<i>infinite dimensionality</i> of the state space
and access to <i>unboundedly small length scales</i>
(which amount to the same thing if we are speaking of states that live 
within a bounded region of the universe).
With rain of bricks, via the entropy bound, we get only <i>finite</i> dimensionality and 
cut off ultrashort length scales, eliminating all those problems in one fell swoop.
</li><li>
The rapid (factorial-style)
growth with N of the number of combinatorially different
Feynman diagrams of order N, combined with the not-fast-enough drop in their typical
complex probability amplitudes.
It is rather subtle, but nonrigorous analysis we shall 
<a href="#byeinfin">soon</a>
<a href="#convser">do</a>
will indicate that if (and only if) rain of bricks is set up in <i>de Sitter</i> rather
than Minkowski space, then the interplay between the geometry, the propagator
formula asymptotics, and the combinatorics is such that this problem is cured.
</li></ol>
<p></p><p>
Re (1), let me elaborate. Consider some quantum mechanical Hamiltonian operator H<sub>1</sub>.
Suppose you want to perturb it to become H=H<sub>1</sub>+&#945;H<sub>2</sub>.
If H is a Hermitian matrix of <i>finite order</i> then
all its eigenvalues, eigenvectors, determinant, and H<sup>-1</sup>
are <i>analytic</i> functions of &#945; 
everywhere in the &#945; complex plane
except perhaps for a finite number of
algebraic branch points.
Hence we could
expand them in a Taylor series in &#945; about any generic basepoint
whereupon the series would converge for all sufficiently small |&#945;|
and could be analytically continued everywhere else.
</p><p>
The situation changes drastically if we allow H to become an operator, e.g. matrix 
of infinite dimensionality.
If, say, H<sub>1</sub>=diag(1,1,1,1,...) is the 
&#8734;×&#8734; identity matrix and
H<sub>2</sub>=diag(1,2,3,4,...) then 
det(H)<sup>-1</sup> 
and
det(H)
have <i>essential</i> singularities at &#945;=0,
their Maclaurin series diverge for every &#945;&#8800;0,
and we get unboundedly large energy eigenvalues with arbitrarily small |&#945;|.
</p><p>
To oversimplify it onto a bumper sticker: Finite-order matrices good. Operators bad.
</p>

<a name="byeinfin"></a>
<h3>37. What happens to these infinities with rain of bricks? (First 
look – and first hint of a stunning salvage of analyticity by setting in
 de Sitter space) </h3>
<p>
There are two kinds of infinities:
</p><ol>
<li>
UV infinities (which old-style QEDers abolished by renormalization; but rain of bricks
abolishes renormalization)
</li><li>
Divergence of QED-output series (whose individual terms were all made finite thanks
to renormalization)
</li></ol>
<a name="nouvinf"></a><p>
<b>1. Individual (unrenormalized) Feynman integrals:</b>
I'll regard it as well-known (and the latter two were proven by Zimmermann 1968) that 
</p><ol type="a"><li>
QED<sub>N</sub> Feynman diagrams (and indeed in
any IRFrQFT) have either finite, or
infinite like a power of a logarithm, values.
</li><li>
The infinities arise from short-distance behavior (high momentum, if we are working
in momentum space); at large distances (low momenta) they are finite.
For the purposes of this section "infrared infinities" will be ignored or 
assumed to be dealt with by some
other method (e.g. see <a href="#infrarednote">§14</a> 
and <a href="#desit">§34</a>).
</li><li>
Those logarithmic infinities, 
while present in 3+1 dimensions, are absent
in any lower dimension, including absent in "3+1-&#949;" dimensions
and "4-&#949;" dimensions for any positive &#949; according to the
nowadays-usual lore of "dimensional regularization."
(Logarithmic infinities by their very nature are very fragile.)
Indeed, in the latter case if the particle masses all are positive
then the integrals are "absolutely convergent" (Zimmermann 1968 using
his &#949;-convention).
</li></ol>
<p>
<b>Finiteness claim:</b>
Rain of bricks employs 3-dimensional (or lower) bricks.  Therefore, the integrals
representing any particular Feynman diagram
arising in rain of bricks all automatically are finitely bounded.   [Another way to view this 
would not be dimension-reduction, but rather that 
the propagators are replaced with the much better behaved "brick-smeared propagators" 
(<a href="https://dl.dropboxusercontent.com/u/3507527/propformulas">§15</a>).]
</p><p>
<b>Because:</b>
The value of a rain of bricks 
QED<sub>N</sub> Feynman diagram is expressible as a sum with the summands being integrals over
bricks.
The sum itself is approximately a Monte-Carlo approximation of the old-style-QED Feynman
integral (although since the integrand is modified this is not exactly true).
The sum is better behaved that the original integral because bricks are far apart, and even if
not, the lower-dimensional brick-integration <a href="#betterbehaveddelta">cuts off</a>
any short-distance infinities
of the old-QED integrand, replacing them with finitely-bounded quantities.
It is not possible for an integral over various distances to be infinite if its |integrand|
is well-behaved (leading to absolute convergence) at large distances, 
and finitely bounded at small ones.
Actually, sorry; the proof is not quite that easy.  We do not always enjoy "boundedness"
for all brick-smeared propagators, but we had shown that brick-smearing always reduces their
singularity-degree.  In view of the fragility of logarithmic infinities
this is good enough to finitize them.
<b>Q.E.D.</b>
</p>
<!--
<p>
Is it possible to deduce <b>explicit bounds</b> from the equalities and inequalities 
of Zimmermann 1968???
</p>
-->
<p>
<b>2.</b>
Some of the usual arguments (§<a href="#dyson">5</a> &amp; <a href="#ratediv">6</a>)
for <b>series-divergence</b> in QED and QCD
no longer work with rain of bricks, but a form of Dyson's collapse argument
still does.  We'll now review those series-divergence arguments seeing for each what happens
with rain of bricks.
<small>The reader may find it helpful at this point
to recall our preceding discussions of 
<a href="#dysonH">Dyson failure in hyperbolic geometry</a>
from
<a href="#ratediv">§6</a>
and de Sitter space slicing into hyperbolic geometries
from
<a href="#desit">§35</a>; these served as inspiration to the author.
</small>
</p><ol><li>
<a name="dysondesit"></a>
<b>Dyson 1952's electron-cloud "collapse" argument</b>
(with infinite energy release):
At first inspection, it appears Dyson's collapse no longer works
due to the rain of bricks postulate that no two electrons can coexist 
within the same brick, making
unboundedly high electron number densities now 
effectively impossible.  Hence at worst, 
only finite energy (per volume) could 
be released in the collapse of a cloud with any finite fixed number of electrons, 
and the smaller |&#945;| the smaller the possible 
energy release.   
<br> &nbsp;&nbsp;&nbsp;
However, at second inspection, we realize an altered form
of Dyson's (<a href="#dysoncorrect">repaired</a>) argument will still yield
collapse even in rain of bricks QED.
Specifically,
lots of e<sup>-</sup>e<sup>+</sup> pairs would spontaneously create, segregate,
and then fall into disjoint balls of electrons (or balls of positrons) packed at
some finite number-density within each such ball, each ball containing
N mutually-attracting electrons.  
Specifically, imagine a ball of volume&#8776;N<sup>2</sup> Planck units
containing N electrons.
(The reason I chose volume&#8776;N<sup>2</sup> was to permit N<sup>2</sup> force-carrier photons
to happily exist so that all the electrons can mutually attract.)
Provided N is large enough,
the ball's Coulombic energy (which would be negative with &#945;&lt;0)
of order N<sup>4/3</sup>&#945; would
far outweigh the Fermi confinement energy and mass-energy, each of order&#8804;N.
Hence unboundedly negative 
energy densities would be achieved.  
This argument indicates that
rain of bricks QED vacuum would be unstable for
any &#945;&lt;0 (including only arbitrarily slightly negative &#945;)
and the instability would release infinite energy density –
just like old-style QED.
And that disaster in turn (via Dyson's usual recourse to standard complex analysis theorems
about convergence regions for power series) indicates to us that
the &#945;-power series we get from Feynman diagram technique in rain of bricks QED
in Minkowski space <i>still</i> will be expected to
have <b>zero radius of convergence.</b>
<br>&nbsp;&nbsp;&nbsp;
We shall also present other independent arguments 
in <a href="#convser">§38</a> (based on
directly counting and estimating the values of Feynman diagrams)
also suggesting that this divergence still happens 
and still is severe enough that it cannot be remediated with Borel summation.
But there is a remarkable saving grace.  All those divergence arguments were in Minkowski space.
Suppose we instead set up rain of bricks in <b>de Sitter space.</b> 
It will then turn out in 
<a href="#convser">§38</a> that 
direct Feynman-diagram estimates suggest that 
there will be a <i>positive</i> radius of convergence of QED Maclaurin
series in the &#945; plane.  In the Dysonian collapse argument we just gave
in MInkowski spacetime for negative &#945;,
note that if |&#945;| is very small, then we only get collapse for
N very large (because the threshold N behaves like a negative power of |&#945;|)
i.e. very large-radius balls of charge each containing very large number of
leptons.  But in de Sitter space, radius greater than
the Hubble length scale is <i>impossible</i>.  (Any two 
electrons separated by further than that would be unable to communicate.)
Hence Dysonian collapse would not be possible unless &#945; got <i>sufficiently</i>
negative (just infinitesimal negativity would no longer suffice).
Hence with rain of bricks QED set in de Sitter space,
a small-enough <i>positive</i> radius of series-convergence would still be possible
as far as Dyson's collapse arguments are concerned.  And that in turn would allow 
defining QED for any &#945; reachable via analytic continuation. This &#945; set
presumably is
far larger than merely the convergence disk and presumably includes the physical value 
&#945;&#8776;1/137.
</li><li>
For QCD, the <b>Khuri-Ren "separatrix"</b> 
no longer exists because "asymptotic freedom" in the limit of distance&#8594;0
no longer exists because this whole asymptotic regime no longer exists.
Similarly my own arguments about asymptotic freedom in QED(&#945;&lt;0) and in QCD
no longer have
any applicability with rain of bricks.
</li><li>
What about my QCD series-divergence
<a href="https://dl.dropboxusercontent.com/u/3507527/quarkfreeing">argument</a>
based on perturbing &#945;<sub>s</sub> slightly
off the real axis into the complex plane, which (I had argued) might
allow <b>unconfined quarks</b> – 
a drastic change in physics?
In de Sitter space,
again I claim this argument now fails.  The point is, one must perturb
&#945;<sub>s</sub> <i>non</i>infinitesimally off the real axis to cause the freeing
effect to kick in at a length scale shorter than the Hubble scale.
(It is useless if it happens at any much-larger-than-Hubble scale, 
because lengths larger than Hubble do
not exist in de Sitter space.)
</li><li>
<b>Lautrup 1977's</b> construction of an infinite sequence of Feynman diagrams with
values growing roughly like N! for the QED<sub>N</sub> diagram:
This no longer works in rain of bricks QED.
There is no longer any "renormalization" and we'll see below that the unrenormalized 
expected value
for Lautrup's Nth diagram now is bounded by
C<sup>N</sup> for some positive constant C.
This by 
itself is not enough to force series divergence
for all &#945;&#8800;0.  Instead it would cause <i>con</i>vergence of the Lautrup subseries for
all complex &#945; in a sufficiently small disk; and outside that disk the series sum 
hopefully still would be defined by analytic continuation.  (Actually the physical value 
&#945;&#8776;1/137 is small enough that I believe it lies within this convergence disk 
so that no analytic continuation would be required to make Lautrup's sub-series converge.)
<!-- 
</li><li>
The attempts by Balian et al and by Bogomolny and Fateyev to 
integrate over all posible quantum field configurations to deduce asymptotic growth 
of the Nth series |coefficient| roughly like (N/2)! no longer apply to rain of bricks QED
because ultrashort length scales and unboundedly high dimensionality are no longer available.
-->
</li></ol>
<hr>
<blockquote>
More generally, it is possible to show that the so-called <i>renormalon</i>
contributions are connected with the first coefficient &#946;<sub>2</sub>
of the Renormalization Group (RG) beta-function
<center>
&#946;(g) = 
&#946;<sub>2</sub>g<sup>2</sup>
+ O(g<sup>3</sup>)
</center>
and are proportional to
(&#946;<sub>2</sub>/2)<sup>k</sup>k!.
Therefore, all non-asymptotically-free theories (this includes QED) are not Borel summable.
<br> &nbsp;&nbsp;&nbsp; –
J.C.Le Guillou &amp; J.Zinn-Justin,
pages 23-24 of their book
<i>Large order behavior in perturbation theory</i> (North-Holland 1990).
Incidentally the paper by Lautrup 1977 is reprinted pages 438-440 of this book.
</blockquote>
<hr>
<a name="lautrupdiaganal"></a>
<a name="FIG5lautrupdiag">
<img src="WarrenSmithQED131123_files/LautrupNdiagram.png" alt="fig5" align="right" width="35%"></a>
<p><b>Analysis of Lautrup 1977's diagrams:</b>
There are <b>three reasons</b> we'll soon contend that the Lautrup<sub>N</sub> diagrams (pictured)
have |values| in rain of bricks QED which, unlike in ordinary QED, do <i>not</i>
grow factorially with N, but rather are bounded by C<sup>N</sup> for
some positive constant C.  This causes the series defined by
these diagrams to have positive radius of convergence.
</p><a name="historylautrup"></a><p>
<b>Remark on history of Lautrup's "renormalon" diagram:</b>
Lautrup 1977 wrote a very nice clean paper explaining why his diagram |value|
in renormalized QED grew superexponentially with N when N&#8594;&#8734;.  
However, it turns out
that only the simpler "N-bubble chain" subdiagram is needed to 
demonstrate such superexponential growth.  Although that is simpler,
it unfortunately was pointed out in later
literature that was considerably messier than Lautrup's paper and which was never 
"cleaned up."  Specifically, that subdiagram
was examined by Coquereaux 1981 in a paper whose renormalization procedure
unfortunately was contaminated by errors.  Those errors were pointed out and 
to a considerable degree corrected by  Kawai, Kinoshita, Okamoto 1991.
KKO 1991 denote Coquereaux quantities with a tilde (the tilde means "incorrect")
and explain how the no-tilde'd correct quantities arise by adding correction
terms to Coquereaux's.  The tilde'd value of the (N-1)-bubble chain is
</p><center>
N! · 
[(-1)<sup>N</sup> (2/3)<sup>N+1</sup> + 4 exp(-5/3) (1/3)<sup>N+1</sup>] 
· [1+O(1/n)]
</center><p>
Interestingly, they identify <i>two</i> factorially-growing contributions,  one
with alternating sign and the other (which is exponentially smaller) 
with positive sign.   The alternating-sign contribution is Borel-summable but
the whole expression is not because the positive contribution
is enough (despite its exponential relative smallness) to kill Borel summability.
KKO then say on their last page that "of course" their no-tilde corrected version of this still
is factorially growing and still not Borel summable. (End of historical note.)
</p><a name="firstlautrup"></a><p>
<b>First</b>, the reason for the factorial-style growth found by Lautrup, was renormalization.
When I redo Lautrup's arguments in <i>un</i>renormalized QED with energy cutoff &#923;
(in units of m<sub>e</sub>c<sup>2</sup>), I find that 
his diagram has |value| upper-bounded by 
K<sup>N</sup>log(&#923;)<sup>N</sup>
for some positive constant K, 
for all sufficiently large N.
(Indeed, from Lautrup it seems one can even deduce K&#8804;1.)
In fact, the value of the CKKO subdiagram is exactly the Nth power of the
"single bubble" (N=1) version of that subdiagram, and that "vacuum polarization"
diagram is examined by Bjorken &amp; Drell 1964 in their chapter 8.  On pages 154-157
they found (using Pauli-Villars regularization) it diverges like log(&#923;)
as a function of the energy cutoff &#923;.  The same diagram also is examined, but 
now instead using dimensional regularization, by Peskin &amp; Schroeder §7.5, again
finding (on p.252) divergence like a single power of a logarithm.
If we believe that rain of bricks effectively imposes a UV-cutoff energy of
the same order as the Planck energy, then log(&#923;) is effectively 
bounded hence the convergence claim follows.
</p><p>
<b>Second</b>, we shall 
<a href="#minkdiv">soon</a> give a general purpose Minkowski space divergence argument
for rain of bricks' QED perturbation series.
However, that argument concerns the sum of <i>all</i> diagrams, not merely Lautrup's
diagrams.  If this argument is redone using Lautrup's diagrams only, then
the "count of embedded diagrams " used there is not (N/2)!<sup>3</sup> but rather
approximately N!.  As a result the diagram-sum estimate (for Nth order diagrams)
instead of being
<nobr>
C<sup>N</sup>(N/2)!<sup>3/2</sup>N<sup>-9N/16</sup>
</nobr>
becomes
<nobr>
C<sup>N</sup>N!<sup>1/2</sup>N<sup>-9N/16</sup>.
</nobr>
Since 9/16&gt;1/2 this latter expression actually shrinks superexponentially
toward zero when N&#8594;&#8734;, which would
cause the Lautrup-diagram series to enjoy infinite radius of convergence.
Now actually this whole estimate depended on a particular geometric model scenario;
others also are possible.  Nevertheless, all others I tried were unable to
alter the estimate by much, leaving the convergence-conclusion unaffected.
</p><p>
<b>Third</b>, that was in Minkowski space and was not even trying to accrue the further benefits of
using de Sitter space, where we shall 
<a href="#moregendsconv">later</a>
produce a general purpose convergence argument.
</p><p>
<b>Side-Warning about Feynman integral "formulas" in books:</b>
My initial attempt (before I wrote the above) 
was to look up Lautrup-appropriate Feynman diagrams in books about
Feynman integrals.
<!--
E.g. the "chain of bubbles" part of Lautrup's diagram 
(<a href="#FIG5lautrupdiag">fig.5</a>) that depends on N
can be written (up to constant times C<sup>N</sup> multiplicative factors)
as a Feynman integral of the form in EQ 3.85 and EQ S.4
in problem and solution
3.6 of Smirnov 2006, with 
&lambda;<sub>1</sub>=&lambda;<sub>2</sub>=&lambda;<sub>3</sub>=N
and &lambda;<sub>4</sub>=0.
</p><p>
In Smirnov's tables of integrals and also, e
--> 
E.g, in the small table in 
Peskin &amp; Schroeder's appendix A4 – focus on their EQ A44 as the simplest
exemplar – essentially all their integrals are obviously enormously <i>wrong</i>
for 3 reasons:
(i) Imaginary values of integrals whose integrands are all-real, (ii) when n=0
their integral is &#8747;1d<sup>d</sup>L=&#8734; but their formula returns a finite answer,
(iii) when n=8 their integral plainly is infinite due to behavior near
where their denominator is 0, but their formula returns a finite answer.
I believe the explanation of these massive errors is this: (i &amp; iii) are
solved by assuming that what Peskin &amp; Schroeder <i>meant</i> but did not <i>say</i>, was that
the (L<sup>2</sup>-&#916;) terms in their denominators were really
(L<sup>2</sup>-&#916;+i&#948;) and they intended to have an unstated 
lim<sub>&#948;&#8594;0+</sub> outside the integral;
(ii) would be solved by demanding n&gt;d/2; 
Peskin &amp; Schroeder's integrals 
are only valid in certain parameter regimes which they did not state.  Smirnov has his own
crazy conventions of a similar ilk, some stated (albeit many pages distant) and others not.
(Incidentally,
Zimmermann 1968 has a better &#949;-convention which he used to prove convergence theorems;
I have no idea why the others do not adopt it.)
In my opinion, there is no excuse for this sort of behavior, e.g. it cannot be justified 
on the grounds of brevity because their formulae are so long that doing them right would 
increase their length negligibly.  
</p><p>
QED even done the best that is currently possible is like dancing on a flying
trapeze while juggling knives; this sort of behavior (in a pedagogical text, no less!)
is like hurling those knives at the audience!
(End of warning.)
</p>
<!--
Smirnov EQ S.4 p264 soln to prob 3.6

int int d^d vec(k) d^d vec(L) /
{ (m^2-k^2)^v * (m^2-L^2)^w * [-(k+L)^2]^x * [-2*v*(k+L)]^(2*y) }
=
[i*pi^(d/2)]^2     {comment this term is -1*pi^d, right?}
* GAMMA( v + x + y + eps - 2)
* GAMMA( w+x+y+eps-2 )
* GAMMA(y)
* GAMMA(2-eps - x - y)
* GAMMA(v+w+x+y+2*eps-4)
/ [2 * (m^2)^(v+w+x+y+2*eps-4)
* GAMMA(v)
* GAMMA(w)
* GAMMA(2*y)
* GAMMA(2-eps-y)
* GAMMA(v+w+2*x+2*y+2*eps-4)
]

where eps = (4-d)/2 is dimensional regularization parameter.
end.

 (admittedly sketchy) REDO -- THIS ALL MAY BE FUCKED UP 
BECAUSE OF CONFUSION BY ME RE VIS-A-VIS THE
ERRORS IN THOSE TEXTBOOKS -- OR JUST ABANDON THIS LAUTRUP-STUFF ENTIRELY???:</b>
The "chain of bubbles" part of Lautrup's diagram 
(<a href="#FIG5lautrupdiag">fig.5</a>) that depends on N
can be written (up to constant times C<sup>N</sup> multiplicative factors)
as a Feynman integral of the form in EQ 3.85 and EQ S.4
in problem and solution
3.6 of Smirnov 2006, with 
&lambda;<sub>1</sub>=&lambda;<sub>2</sub>=&lambda;<sub>3</sub>=N
and &lambda;<sub>4</sub>=0.
Lautrup's diagram is logarithmically infinite in 1+3 dimensions without renormalization
due to short distance behavior (an infinity Lautrup had removed via renormalization); but
in rain of bricks we only integrate within bricks, which are lower-dimensional, and hence
by the lore of "dimensional regularization"
any such integrals must remain finitely bounded even if we allowed the brick-sizes 
to grow unboundedly.   
</p><p>
Smirnov's exact formula
as a ratio of products of
Gamma functions then shows
a C<sup>N</sup> bound on the Feynman |integral| would hold 
<i>regardless</i> of the Minkowski-space distances
between the bricks' raindrop locations &ndash; indeed even if
all bricks were co-located &ndash; if we use 1- or 3-dimensional bricks.
To see that, we shall employ this
</p><p>
<b>Gamma-product-ratio lemma:</b>
Consider a ratio R(N) of products of &Gamma;-functions of linear 
functions of N, with rational coefficients, for example, 
&Gamma;(5N+3/4)&Gamma;(1/3-3N/2)/&Gamma;(7N/2).
Suppose the sum of the coeffcients of N inside the numerator's &Gamma;-functions
(here 5-3/2) <i>equals</i> the corresponding sum in the denominator (here 7/2).
Then: <i>Either</i>
the asymptotic behavior of R, when N&rarr;&infin; within the positive integers, is
upper bounded by C<sup>N</sup> for some positive constant C, <i>or</i>
R=&infin; at an infinite set of integers N.
</p><p>
<b>Proof:</b>
Use the
Gamma-function reflection formula
<nobr>
&Gamma;(z)&Gamma;(1-z)=&pi;csc(&pi;z).
</nobr>
to make all the Gamma functions have arguments that are positive for large positive N,
then apply
Stirling's formula to find the asymptotic behavior of R
(EQ 6.1.39 and 6.1.17 in Abramowitz &amp; Stegun).
If all the poles (at nonpositive integer arguments) of &Gamma;(z) are avoided, then
since the coefficients of N were assumed rational the avoidance is by at least 
some constant additive amount; and since we assumed the numerator and denominator 
coefficient sums are equal this causes the N<sup>KN</sup> behavior in 
Stirling formula to cancel out,
i.e. to yield K=0, leaving only C<sup>N</sup> and lower-order growth behavior (which 
we can absorb by changing C).
On the other hand if they are not avoided then there must be an infinite number of such poles
(since consider shifting N by the LCM of the rational-coefficients' denominators).
<b>Q.E.D.</b>
</p><p>
In the case of Smirnov's formula, the numerator and denominator &Gamma;-argument-sums 
are respectively 
6N+3&epsilon;-8
and
6N+&epsilon;-2
where (as is stated over 200 pages away in Smirnov's book)
&epsilon;=(4-d)/2
in d dimensions.
The physical dimension to Smirnov is d=4 whereupon &epsilon;=0, but for us 0&lt;d&lt;4
since d is the dimension of the bricks,
whereupon &epsilon;=3/2, 1, and 1/2 for d=1, 2, 3 respectively.
(Smirnov's formula also involves some constant multiplicative factors and a factor of
form C<sup>N</sup>, but those do not matter for our present purposes.)
Smirnov has only one &Gamma; with a negative argument, &Gamma;(2-&epsilon;-N),
but for any <i>odd</i> d, we have that &epsilon; is distance 1/2 away from
the integers and thus its poles are avoided.
Hence, Smirnov's |formula| indeed is always bounded by C<sup>N</sup> for
some computable positive constant C, provided d=1 or d=3.  When d=2 it
yields &infin; for every integer N&ge;0, but I presume that this infinity is
not genuine and in the <i>full</i> Feynman integral
(we have omitted a small N-independent part of the diagram in ours!)
would return to finiteness (and if it does, it is clear this also will then
obey a C<sup>N</sup> upper bound).
</p><p>
<b>Warning about Feynman integral "formulas" in both Smirnov and Peskin &amp; Schroeder:</b>
<b>Returning to Smirnov's formula and the Lautrup diagram analysis:</b>
The situation is as follows.  In rain of bricks QED, the Lautrup<sub>N</sub>
diagram's value arises as a certain N-fold sum (which acts like a Riemann-sum Monte Carlo
approximation of
an integral) where the summands are expressible as integrals over bricks.   If the locations
in the sum all are on <i>far-apart</i> bricks, the situation is roughly like Lautrup's
original unrenormalized d=4 integral <i>but</i> with a Planck-scale UV momentum-cutoff imposed
[and the resulting |integral| would then be finite and enjoying a C<sup>N</sup>log(Planck) bound].
If the locations of bricks are allowed to become closer than Planck-scale, then 
with rain of bricks thanks to the lower-dimensionality of bricks via Smirnov's formula
we still have
a C<sup>N</sup>-style upper bound on each |summand|,
which also has a UV-cutoff effect.   This situation is like
integrating K&int;<sub>0&lt;x</sub>dx/(x+x<sup>2</sup>) 
(whose value is logarithmically infinite)
using Monte-Carlo integration, <i>but</i> where each integrand-value is magically prevented
from exceeding C<sup>N</sup>, and where K also enjoys a C<sup>N</sup> upper bound.
The result is bounded by C<sup>N</sup> for some (other) positive constant C.
<b>Q.E.D.</b>
</p>
-->
<p>
<b>Unfortunately</b>, 
just because the four biggest arguments for series-divergence no longer work, 
does not prove the series converge!    But it cannot hurt.  
The next section will take a deeper look.
</p>


<!--
<h3>X. Brief review of "Borel summation"??? </h3>

<p>
L.Euler ??? responded to a challenge to "sum" the divergent series
</p><center>
1 - 1!z + 2!z<sup>2</sup> - 3!z<sup>3</sup> + 4!z<sup>4</sup> - ...
&nbsp;&nbsp; and &nbsp;&nbsp;
1 - 1z<sup>2</sup> + 1&middot;3z<sup>4</sup> - 1&middot;3&middot;5z<sup>6</sup> + 1&middot;3&middot;5&middot;7z<sup>8</sup> - ...
</center><p>
with the answers 
</p><center>
z<sup>-1</sup>exp(z<sup>-1</sup>) &int;<sub>0&lt;t&lt;z</sub> exp(-t<sup>-1</sup>) t<sup>-1</sup> dt
&nbsp;&nbsp; and &nbsp;&nbsp;
z<sup>-1</sup>exp(z<sup>-2</sup>/2) &int;<sub>0&lt;t&lt;z</sub> exp(-t<sup>-2</sup>/2) t<sup>-2</sup> dt
</center><p>
which also may be written as (convergent) continued fractions
</p>
<pre>
    1   z   z   2z  2z  3z  3z  4z  4z              1  1z&sup2; 2z&sup2; 3z&sup2; 4z&sup2; 5z&sup2; 6z&sup2;
   --- --- --- --- --- --- --- --- ---       and   --- --- --- --- --- --- --- 
    1+  1+  1+  1+  1+  1+  1+  1+  1+ ...          1+  1+  1+  1+  1+  1+  1+ ...
</pre>
<p>
Both these continued fraction and integral expressions converge to a well defined answer for 
every complex z except for negative real z.  The original series diverge for every complex z 
besides the single point z=0.
</p><p>
Euler's "solutions" are valid in the sense that any truncation of the original power series
(removing, say, the z<sup>N</sup> term and all higher orders, for any given N&ge;1)
is equal to Euler's expression up to an additive error bounded by
K(&delta;) times the absolute value of the first omitted term,
in the asymptotic regime
|z|&rarr;0 
restricted within the wedge
|arg(z)|&le;&pi;-&delta;,
where
K(&delta;) is some positive constant depending on &delta;&gt;0 
but <i>independent</I> of N and z.
A famous lemma of
G.N.Watson proved that Euler's solutions were the <i>unique</i> functions of z analytic
in the wedge
|arg(z)|&lt;&pi;-&delta;
and obeying such an asymptotic eror bound.
</p>
<p>
Much later (about 1899) Emile Borel
generalized Euler's ideas &ndash; and we shall now generalize them further &ndash;
by defining the notion of the 
<b><i>K</i>th-level Borel sum"</b>
(for any positive integer K) of a (possibly divergent) power series
</p><center>
f(z) &nbsp;&nbsp; "=" &nbsp;&nbsp; &sum;<sub>0&le;N</sub> A<sub>N</sub> z<sup>N</sup> 
</center><p>
to be
</p><center>
f(z) = 
z<sup>-1</sup>K &int;<sub>0&lt;t&lt;&infin;</sub> exp(-t<sup>K</sup>/z<sup>K</sup>) 
&sum;<sub>0&le;N</sub> A<sub>N</sub> t<sup>N</sup> / &Gamma;((N+1)/K) dt.
</center><p>
Note that the sum here, since it includes a division of the t<sup>N</sup> term by N!,
converges better than the original series.  If it converges for all positive real t,
then the integrand has a meaning.  If the integral also converges for certain z, then
the Borel sum yields, at that z, a unique value f(z).
Some famous standard theorems:
</p><ol>
<li>
The Borel sum and ordinary sum are formally identical in the sense that if the sum and
integral were interchanged, we would have an exact equality &ndash;
thanks to Euler's integral 
<center>
&int;<sub>0&lt;t</sub> exp(-t<sup>K</sup>/z) t<sup>KN</sup>dt =  z<sup>N+1/K</sup>&Gamma;(N+1/K)/K
</center>
</li><li>
Power series always converge throughout the interior of some disk centered
at z=0 in the complex z-plane (the radius of the disk is called the "radius of convergence")
and always diverge throughout the exterior of that disk.
</li><li>
Borel sums always converge throughout a "star shaped" region, i.e. if it converges for some
z=X, then it automatically also converges for every point z=kX with 0&le;k&le;1.
Indeed, further, it automatically converges for every z with |z-X/2|&lt;|X|/2.
(Theorems 129 and 132 in Hardy.)
This star shaped 
region includes the convergence disk-interior for the original series, and the Borel sum
and ordinary sum agree everywhere within that disk-interior.
</li><li>
If f(z) is analytic at z=0, then the convergence region of the Borel sum is understood.   
For each point z=P of nonanalyticity 
of f(z), draw a line through that point and perpendicular
to the line segment 0P.   
These lines cut the complex z-plane into convex polygonal regions.
The polygon containing 0 in its interior is the "Borel polygon."  Theorem:
the Borel sum converges throughout the interior of this polygon,
and diverges throughout its exterior.
</li></ol>
<p>
One immediate corollary  of all the above is this:
</p>
<p>
<b>Borel Convergence Lemma:</b>
If there exists a constant c with 0&le;c&lt;1 such that
lim<sub>N&rarr;&infin;</sub> N<sup>cN</sup>|A<sub>N</sub>| = 0,
then the Borel sum "f(z)" above converges for every complex z.???
</p><p>
<b>Proof.</b>
To make the notion of c unique, assume wlog that c is infemum of all the
c&gt;0 valid for that function.  Then automatically 
F(t)=&sum;<sub>0&le;N</sub> (A<sub>N</sub> t<sup>N</sup> / N!)
is an "entire function of order&ge;1/(1-c)," meaning for any constant
u&gt;1/(1-c), the series defining F(t)
converges for all complex t and F(t) has the property that
|F(t)|&lt;exp(R<sup>u</sup>) for all complex t in the disk |t|&le;R for all
sufficiently large R.

(1-c)klnk
-->

<a name="convser"></a>
<h3>38. The apparent convergence of rain of bricks QED power series in de Sitter, and
divergence in Minkowski, space</h3>

<a name="limorder"></a>
<p>
<b>Remark about limit order:</b>
In QED<sub>N</sub> each Feynman diagram corresponds  to an integral over all of 
some finite-dimensional space.  With rain of bricks, the integration is replaced by
something else involving both summation and integration but that distinction
will not matter for the point we are about to make.
It can be easy to forget, when writing
integrals "from -&#8734; to +&#8734;" that
all these integrals are <i>really</i> only defined as <i>limits</i> when
R&#8594;&#8734; of, say, integrals from -R to +R.
That fact does not particularly matter for QED<sub>N</sub> for any particular finite N.
However, when we attempt to construct full QED, that is 
QED<sub>&#8734;</sub>, we also have a different limiting process, namely the
power-series summation limit N&#8594;&#8734;.
That leads to the possibility that the <i>order</i> in which we apply
these limiting processes, may matter.  
</p><ol><li>
It will soon be clear that if we do rain of bricks
"cavity QED," i.e. artificially demand that
the process we are examining cannot be affected in any way by anything happening
outside of a spacetime <b>"pillbox"</b> such as 
<nobr>x<sup>2</sup>+y<sup>2</sup>+z<sup>2</sup>&#8804;R<sup>2</sup></nobr>
and <nobr>|t|&lt;R</nobr>, then 
QED power series will always converge (in both Minkowski or de Sitter space,
and optionally also in expectation over the raindrop-generating random process)
for any fixed R&gt;0.
[This is basically because any such pillbox contains only a finite set of raindrops, hence 
there can be only a finite number of Feynman diagrams yielding a series with only a finite
number of terms.  In expectation over the Poisson process the number Q of raindrops
can be unbounded 
<a href="#thepoissonlaw">but</a>
with Q!<sup>-1</sup> probability-factor dropoff when Q&#8594;&#8734;, which 
is well more than strong enough to handle the believed rates of divergency from
the top two lines of <a href="#tabpertdivrates">table 3</a>.]
The difficulty is that the
limit when R&#8594;&#8734; might fail to exist.
</li><li>
It also will be clear that if we do finite order QED, i.e.
rain of bricks QED<sub>N</sub> for some fixed integer N&gt;0, then again 
everything is finite even though R=&#8734;.  [Again, because with finite N
the series has only a finite number of terms; and we have already argued
rain of bricks finitizes the terms themselves by creating UV-cutoff effects.]
The difficulty is that the
limit when N&#8594;&#8734; might fail to exist.
</li></ol><p>
Of course, other regions besides the "pillbox" could also be considered –
the pillbox per se is not Lorentz-invariant before reaching the R&#8594;&#8734; limit
so it might be more desirable to 
(and we <a href="#moregendsconv">later</a> shall)
consider regions that are –
and besides just the two possible <i>orderings</i> of our two kinds of limits,
one could also consider more complicated 2-variable limiting processes.
I suspect/assume that all our main (nonrigorous) convergence and divergence
results/claims in this section hold <i>regardless</i> of which limit-order is used
and are insensitive about the precise nature of the limiting process.
</p><a name="poincarejoke"></a><p>
<b>Poincare's joke:</b>
Concerning convergence, it is worthwhile to recall the following joke by H.Poincare:
</p><blockquote>
"Consider the two series
<br><center>
&#8721;<sub>N&#8805;0</sub> N!/(-1000)<sup>N</sup>
&nbsp;&nbsp;&nbsp; and &nbsp;&nbsp;&nbsp; 
&#8721;<sub>N&#8805;0</sub> (-1000)<sup>N</sup>/N!.
</center><br>
Mathematicians say that the series on the
left is divergent and the one on the right is convergent.
Astronomers contend the opposite."
</blockquote>
<p>
We of course agree with the "mathematician's" definition; the righthand
series indeed converges to <nobr>exp(-1000).</nobr>
However we must also concede that it
is much quicker and more enjoyable to "compute" the value of the lefthand series 
than the righthand one.  
The early partial sums of the lefthand series 
offer excellent approximations to
</p><center>
&#8747;<sub>0&lt;u&lt;&#8734;</sub> 1000exp(-u)du/(u+1000)
&#8776; 0.99900199402388071499996070935606047039
</center><p>
In the below, we shall propose rain of bricks methods for "saving" QED from
divergent series which on toy problems had behaved like
the lefthand one. But unfortunately this "save" will
act much like "removing the divergent tail of the lefthand series and replacing it by
the righthand series."   Poincare's astronomers would not be greatly impressed
by that – but QCD series, and QED series on <a href="#dnaexample">non-toy</a>
problems,  diverge so severely
that even the brain-deadest astronomers must concede this is a genuine save.
Nevertheless, they still justifiably should complain about the poor computational efficiency.
</p><p>
The number of QED<sub>N</sub> Feynman diagrams has been estimated by 
Riddell 1953 and Cvitanovic-Lautrup-Pearson 1978.
We shall not need estimates as
precise as theirs.  For us, the following cruder estimate will suffice:
</p><p>
<b>Diagram-Counting Lemma:</b>
The number of 
QED<sub>N</sub> Feynman diagrams lies between 
A<sup>-N</sup>N!
and 
A<sup>N</sup>N!
for each N&#8805;1 and some computable positive real constant A.
</p><p>
A technique we will sometimes use (or at least <i>consider</i> using) is
</p><a name="sqrtsum"></a><p>
<b>Square-root-sum estimates:</b>
In some, but not all, cases it is possible to estimate a sum 
<nobr>S(N)=&#8721;<sub>1&#8804;k&#8804;N</sub>A<sub>k</sub>,</nobr>
via <nobr>|S(N)|&#8776;A<sub>rms</sub>&#8730;N</nobr>
where A<sub>rms</sub> is the root mean
square of the absolute value
of the summands.
In some cases replacing A<sub>rms</sub> by
a slight overestimate yields a rigorous upper bound on |S(N)|
valid in the limit N&#8594;&#8734; with probability&#8594;1.
</p><p>
Such estimates are clearly wrong when, for example, A<sub>k</sub>=1
for all k.  However, they work well when the A<sub>k</sub>
have independent random signs (or random complex phase angles).
A safer upper bound could involve using, e.g, N<sup>3/4</sup> instead of &#8730;N.
</p><p>
As an example where one can rigorously justify this kind
of estimate, consider 
<nobr>S(N)=&#8721;<sub>1&#8804;k&#8804;N</sub> (±A<sub>k</sub>)</nobr>
where the ±1's are independent random coin-flip signs
and the A<sub>k</sub> are any non-increasing sequence of non-negative reals
with 
<nobr>&#8721;<sub>1&#8804;k&lt;&#8734;</sub> |A<sub>k</sub>|<sup>2</sup>&lt;&#8734;.</nobr>
Then it may be proven (Montgomery &amp; Odlyzko 1988) that
|S(N)|&lt;lnlnlnlnN
with probability&#8594;1 when N&#8594;&#8734;.
Actually, any function of N that increases
unboundedly may be used in place of lnlnlnlnN.
</p><p>
When the signs obey weaker independence properties
it still sometimes can be possible to get rigorous 
bounds using probabilistic techniques appropriate for 
whatever sort of random variables it is.   We, however, are often going to use 
square-root-sum estimates in <i>nonrigorous</i> ways "justified" mainly by intuition.
(Such intuition enjoys
rain of bricks' explicit postulation that all raindrop locations 
are independent random.)
More precisely we will need to <i>assume</i>
that certain quantities behave enough like independent
random-uniform phase angles.  The fact that 
<nobr>&#8721;<sub>1&#8804;k&lt;&#8734;</sub> |A<sub>k</sub>|<sup>2</sup>&lt;&#8734;</nobr>
is going to be easy in our applications.
</p><p>
The square-root-sum estimate also is useful for getting <b>lower bounds</b> on |S(N)|,
which ought to be valid unless there is miraculous cancellation within the sum allowing
|S(N)| to be smaller than the estimate.  I feel such lower bounds are highly
reliable even if nonrigorous, and have much more confidence in them than in
the upper bounds.
</p><p>
<b>The "Poisson cheat" trick:</b>
It is convenient to <i>pretend</i> that Poison random (unit density) points
in fact <i>have</i> bounded density, i.e. O(N) occur in a ball of volume N centered
at the origin.  However, actually, more could occur, say kN
for arbitrarily large k... it is just that the <a href="#thepoissonlaw">law</a>
of the Poisson distribution makes that 
<a href="#thepoissonlaw">factorially</a>
unlikely.  The expectation over the Poisson randomness introduces a 1/(kN)! 
convergence factor if more are considered, and this is enough in many situations
to ensure convergence (in expectation, and/or with probability&#8594;1)
provided we had convergence under the pretense.
</p><a name="#eucserconv"></a><p>
<b>Euclidean space <i>conv</i>ergence and analyticity argument:</b>
In "Euclidean" space, i.e. where (unphysically)
all 4 coordinates are spacelike and "time" does not exist,
the analyst's life is considerably simpler.  
We will now argue that rain of bricks suffices 
in such a space to make QED power series (for Feynman diagrams including
a fixed raindrop at the origin)
always converge provided the absolute value |&#945;|&gt;0 of the coupling constant 
is small enough.
The key ideas are these:
</p><ol>
<li>
At large distances, the
<a href="#euclideanpot">propagator</a> for a positive-mass particle
falls off exponentially ("Yukawa potential").  In QED, each Feynman
vertex has two electron-lines and one photon-line emanating from it, and
electrons have positive mass.
</li><li>
We may and shall ignore constant multiplicative factors and indeed for N-edge Feynman 
diagrams we may ignore factors of order C<sup>N</sup> for any real constant C&gt;0,
because the worst that they can do is multiply or divide the radius of
convergence of our &#945;-power-series by C.
</li><li>
At small distances, we have no problems with huge |propagator|
values because of "brick smearing." Hence since
we don't care about constant multiplicative factors, wlog
we can act as though all inter-raindrop distances are &#8805;1 in Planck units. 
[More precisely, nearby points are unlikely, with the unlikeliness
getting severe the closer they are, which is enough to overcome the brick-smearing-weakened
propagator singularites at short distances.]
</li><li>
There are O(N) raindrops within a ball of radius N<sup>1/4</sup>.
Hence an N-vertex Feynman diagram (all vertices located at distinct raindrops)
is not going to fit in any ball of radius below about N<sup>1/4</sup>.
</li></ol>
<p>
A random embedding of an N-vertex Feynman diagram necessarily has average
edge length at least N<sup>1/4</sup>.  Hence the |amplitude| for that embedded diagram is
going to be upper bounded by an <i>exponential function of -N<sup>5/4</sup>.</i>
This is far more than enough to outweigh the factorial-style growth of
the number of such Feynman diagrams and embeddings.
But life is not quite that simple
because there can be Feynman-graph embeddings that are much shorter then average, for example
with all O(N) edges each O(1) long.   Fortunately such embeddings are rare –
there are only C<sup>N</sup> of them – hence don't matter.
More generally, each time you draw a Feynman edge of length&#8804;L, there are 
O(L<sup>4</sup>) possible choices, hence the number of embedded Feynman diagrams with
N vertices and total length&#8804;LN for some L&#8805;1, is 
O(L)<sup>4N</sup> by using Jensen's inequality 
(or the inequality of arithmetic and geometric means, either suffices).
Each contributes |amplitude|&lt;exp(-cLN) for some positive constant c.
In conclusion, the whole QED power series certainly converges if
</p><center>
&#8721;<sub>L=1,2,4,8,16,...</sub>  &#8721;<sub>N&#8805;1</sub>  
&#945;<sup>N</sup> exp(-cLN) (kL)<sup>4N</sup>
<!-- MAPLE:
sum( exp(-c*L*N) * (k*L)^(4*N), N=1..infinity );
#get   (k*L)^4 / (exp(c*L)-(k*L)^4).  And that, summed over L=1,2,3,4...
#converges except if c is unlucky so that we divide by 0.
#Bad luck aint happening if c and k rational since exp(nonzero rational) is irrational.
-->
</center><p>
converges provided c and k are <i>generic</i> positive constants.
</p><blockquote>
It does, indeed even if L is summed over <i>any subset</i> of the positive integers,
e.g. I can prove convergence if c and k are any two  positive rational numbers.
<b>Proof:</b>
The inner sum obviously converges (indeed absolutely) 
if L is any fixed sufficiently-positive real, and according to MAPLE9
the inner sum has closed formula
(kL)<sup>4</sup>/[exp(cL)-(kL)<sup>4</sup>].  
If L, c and k are positive rationals then the denominator
cannot be 0 (since e is transcendental) – also if c and k are
<i>generic</i> reals then
0 still cannot happen for any integer L – and then this expression clearly
converges (indeed absolutely) if summed over all (or any subset of the)
positive integers L.
Finally, the proviso "sufficiently-positive" does not matter since back in the original problem
obviously any sum over a finite set of embedded Feynman diagrams yields a finite result so
we can ignore all L smaller than whatever constant we like.
<b>Q.E.D.</b>
</blockquote><p>
Hence the &#945;-power series converges for a nonempty set of positive &#945; (or indeed
for all complex &#945;  in a sufficiently small disk centered
at 0), and within that set the sum is analytic as a function of &#945;.
</p><p>
Finally, we can use the "Poisson cheat" trick above to resolve the objection
that denser raindrops could (albeit this is unlikely) occur.
<b>Q.E.D.</b>
</p>
<a name="FIG6badfeyn">
<img alt="fig6" src="WarrenSmithQED131123_files/BadFeynmanExample.png" align="left" width="35%"></a>
<a name="minkdiv"></a>
<p>
<b>Minkowski space <i>div</i>ergence argument:</b>
A fairly convincing, if nonrigorous, demonstration of generic QED-series
divergence in Minkowski (3+1)-space with rain of bricks, is as follows.
Consider the set of Feynman diagrams in <a href="#FIG6badfeyn">figure 10</a>.
It illustrates two blobs of spacetime A and B, each containing &#8805;N/2 raindrops, and each blob
having both spatial and time-widths of order N<sup>1/4</sup>, and finally the two
blob-centers are separated in time by a time interval also of order
N<sup>1/4</sup>.  We consider only "bipartite" Feynman diagrams in which all
3N/2 edges join a raindrop in A to one in B.  There clearly are
at least (N/2)!<sup>3</sup> such embedded diagrams.
Due to the fact the Minkowski space |propagator| falls off
slowly at large time-separations T (despite falling exponentially with space-separation!),
specifically like T<sup>-3/2</sup>, we see that the |sum| of the diagrams illustrated ought
to behave, under the square-root-sum assumption, like
</p><center>
C<sup>N</sup>(N/2)!<sup>3/2</sup>N<sup>-9N/16</sup>
</center><p>
for some positive constant C when N&#8594;&#8734;. 
Here 9/16=(3/2)(1/4)(3/2)
with the first 3/2 coming from T<sup>-3/2</sup>, the 1/4 coming from
N<sup>1/4</sup>, and the second 3/2 coming from 3N/2.
Note from a crude form of
Stirling's approximation that <nobr>(N/2)!&#8776;(N/(2e))<sup>N/2</sup>.</nobr>
Because 3/4&gt;9/16, this Feynman |sum| rapidly goes to infinity when N&#8594;&#8734;,
indeed so quickly that even if multiplied by &#945;<sup>N</sup> it still goes 
ultimately-superexponentially to
infinity, no matter how small (if fixed at a positive value) &#945; is.
In other words, we expect the radius of convergence in the &#945; complex plane 
to be 0.
<b>Q.E.D.</b>
</p><p>
The reason for the difference between Euclidean 
and Minkowski space is that in Euclidean space, |propagators| fall off exponentially
for positive-mass particles at large distances.  But in Minkowski space 
in timelike propagation directions, the far-time falloff is
only power law.  That still would have been ok if it were a fast-enough
power law such as T<sup>-99</sup>, but it isn't.
</p><p>
<b>What about hyperbolic nonEuclidean (still without time) geometry?</b>
In the hyperbolic plane (similar remarks are also true in each higher dimension,
but the plane is the simplest to think about), the area of
a disk of radius R is &#960;[cosh(R)-1]
and its perimeter is 2&#960;sinh(R).
Both of these grow ultimately exponentially with R, with the
<i>same</i> ultimate growth rates.
This is quite unlike the Euclidean plane where the formulae are &#960;R<sup>2</sup>
and 2&#960;R respectively.
For large R, a fraction asymptotic to exp(-1)&#8776;37%
of the area of the hyperbolic radius-R disk, is contained in its outer
1-thick annular region (i.e. the part at distance&#8804;1 from the boundary).
Then 37% of the remaining 67% is contained within distance&#8804;1 of <i>its</i> boundary,
and so on.
This again is quite unlike 
the Euclidean plane where the fraction within distance&#8804;1 of
a disk's boundary, is zero when R&#8594;&#8734;.
It is possible to write the Klein-Gordon propagator (aka Helmholtz equation's Green function)
in closed form in spherical (and hence hyperbolic nonEuclidean, and hence also Euclidean)
arbitary-dimensional geometry using Gegenbauer functions, see
Szmytkowski 2007.
I'm mentioning all this as mental preparation for considering de Sitter space, which bears
some resemblance to both Minkowski space and to
hyperbolic geometry.
</p><p>
<b>In de Sitter space, that Minkowski-divergence scenario yields convergence:</b>
What seems to be the worst possible (from the point of
view of series convergence) situation in de Sitter space
among scenarios just like the figure we just explored
(for the Minkowski divergence argument),
albeit with altered parameters, is this.
Now let the two blobs A and B be such that the time-separation between them is
of order logN. As before, each blob contains order N raindrops, of which N/2
(each) are used as Feynman vertices for a bipartite Feynman graph. 
The fact that the B blob can be large enough to contain order N raindrops despite being in
the future light cone of a single point and only time-separated from it
by order logN, is due to the exponential expansion with time of the de Sitter universe.
(The value of the Hubble constant will be immaterial for us
since it is merely a constant – very large in Planck units, but still just a constant.)
The de Sitter propagator formulas then automatically have amplitude that
falls off exponentially in time (with oscillation) in such a way
that each Feynman edge has propagation |amplitude|&#8804;O(N<sup>-1/2</sup>).
The total |sum| of all the (N/2)!<sup>3</sup>
embedded Feynman diagrams each as usual weighted
by its product of its 3N/2 edge-propagation amplitudes, 
is then, under the square-root-sum
estimate, 
</p><center>
C<sup>N</sup>(N/2)!<sup>3/2</sup>N<sup>-3N/2</sup>
</center><p>
for some positive constant C.
Because 3/2&gt;3/4, this |sum| superexponentially rapidly drops toward 0
when N&#8594;&#8734; indeed so rapidly that even if multiplied by &#945;<sup>N</sup>
for some positive real &#945; it still would fall ultimately-superexponentially
no matter how large &#945; was (if fixed).
</p><p>
Indeed even if we refuse to accept the
square-root-sum estimate, and just use the brute-force estimate that a |sum|
is bounded by its number of terms times its maximum |summand|, then still we would have
</p><center>
C<sup>N</sup>(N/2)!<sup>3</sup>N<sup>-3N/2</sup>
</center><p>
which still would drop ultimately geometrically provided |C|&gt;0 was small enough.
</p><p>
This indicates rain of bricks QED &#945;-power series should converge
for all complex &#945; ("entire analytic function"), 
or perhaps merely for &#945; within some positive-radius
disk.  Either way, we've achieved our goal: convergence plus good behavior.
However, this was only in the one particular type of
scenario, illustrated in the <a href="#FIG6badfeyn">picture</a>; it was not general.
</p>
<a name="moregendsconv"></a>
<p>
<b>More general examination of series convergence for rain of bricks QED in de Sitter space:</b>
We would now like to argue, similarly to the previous Euclidean-space argument, that
rain of bricks QED series in de Sitter space <i>always</i> enjoy convergence
if <nobr>|&#945;|&gt;0</nobr> is small enough.
In the Euclidean argument we needed the fact that a ball of
radius R about the origin, was expected to contain O(R<sup>4</sup>) raindrops.
In de Sitter space's indefinite metric, there is no such result, indeed 
we expect an <i>infinite</i>
set of raindrops to exist, each of whose |distances| 
to the origin is &lt;R for any given R&gt;0.
We intend instead to rely on
</p><ol><li>
The <a href="#tetlemma">tetrahedron lemma</a>
that, given any 4 points A,B,C,D in <i>general position</i> 
<a href="#desittetlem">in</a> de Sitter space
(and all mutually closer than the distance to the de Sitter horizon)
the expected number of raindrops at |distance|&lt;R to all of them <i>simultaneously</i>, is
finite for finite R.
</li><li>
This <b>fancy limiting process:</b>  Consider only Feynman diagrams
in which each pair of vertices has |distance|&lt;R,
and then later take the R&#8594;&#8734; limit.
</li></ol><p>
We begin with &#8805;4 fixed suitable points in general position
(e.g. collinear points not allowed!)
and we are <i>only</i> going to consider/allow Feynman diagrams 
which include all four.
</p><p>
Under those rules, each time we add a new Feynman vertex raindrop to our current set,
the number of choices Q we have is finite, and the diagram |amplitude| 
gets multiplied by (at most) the largest |amplitude|<sup>E</sup> 
for propagation |distance|&#8804;R, assuming the new vertex is joined by E Feynman
edges to old ones.
Because of the exponential expansion of de Sitter space and the far-falloff properties
of de Sitter propagators, one can argue that the propagator |amplitude| actually
is at most of the same order as
Q<sup>-1/2</sup> where Q is the number of choices restricted to
those within a ±1 wide length range of the given Feynman edge |lengths|.
Hence, using the square-root-sum estimate (and summing over all integer |length| ranges
similarly to our previous <a href="#eucserconv">Euclidean</a>
estimate...) we would conclude that the total contribution
from all N-vertex Feynman diagrams ought to be bounded by
order C<sup>N</sup> (for some positive constant C) even if we allow
arbitrarily large N
and/or arbitrarily large R, which would with all E=1 
yield the conclusion that the &#945;-power series ought to converge for all 
sufficiently small &#945;&gt;0.
And with E=3/2, i.e. average Feynman valency=3, we would – if
still buying the square-root-sum estimate –
reach the stronger
conclusion that the radius of convergence ought
to be, not merely positive, but actually infinite.
If, however we use a "safer" weakened form of this estimate, e.g. based on
some power P with 1/2&lt;P&#8804;3/4, we would conclude that the convergence radius
should be positive, but not necessarily infinite.
<b>Q.E.D.</b>
</p>
<p><b>Is this crazy? Dialogue between Simplicio and Sagredo:</b>
</p><blockquote>
<br><b>Simplicio:</b> It is obviously ridiculous that the finiteness of QED series for
quantities like the electron mass, charge, and magnetic moment
depend upon the fact that the universe is expanding, i.e. that
the rain of bricks is set in de Sitter rather than Minkowski space.
<br><b>Sagredo:</b> Why do you find this "obviously ridiculous"?
<br><b>Simplicio:</b> Well, for one thing, if I have (A) an electron, or (B) this electron 
in the middle of a
10cm-wide cavity inside a 10-meter thick ball of copper and mu-metal as a shield,
then my equipment is unable to detect any alteration in the
electron's mass, charge, or magnetic moment with 8 significant digits of measuring accuracy.
From this comparison of (A) versus (B) I conclude
electron mass, charge, and moment are virtually unaffected by big <i>real</i> objects a few
centimeters to meters away plus shielding of everything further away.  
But now I am asked to believe that the electron mass, charge,
and moment might have actually been <i>infinite</i> except for the right <i>virtual</i>
stuff happening in the universe 10<sup>9</sup> <i>light years</i> away in distance and/or 
±10<sup>9</sup> years away in time? (Since de Sitter and Minkowski space are
pretty much the same until you walk 10<sup>9</sup> light years.)
I don't think so!
<br><b>Sagredo:</b> My infinite subcollection of Feynman diagrams that, in Minkowski space, 
cause us to worry
these electron quantities could be infinite or undefined,
are there whether or not your copper shield is there.
The photon lines in my diagrams go right through your shield without even noticing it.
There also would be other diagrams in which there is interaction with particles
within your copper shield, but those are irrelevant to the case.  So your experiment tells 
us nothing about this question.
<br><b>Simplicio:</b> WHAT?!
<br><b>Sagredo:</b> Meanwhile, in de Sitter space, my analogous infinite subcollection of
diagrams appears to yield
finite and convergent results.  Further, a very independent chain of reasoning 
based on Dyson 1952's (discussed <a href="https://dl.dropboxusercontent.com/u/3507527/dysondesit">above</a>) 
also finds series divergence in Minkowski
but not de Sitter space, due to a negative-&#945;
vacuum instability which, for small enough
|&#945;| to give convergence would necessarily
happen over an almost <i>cosmologically</i> large length scale.
<br><b>Simplicio:</b> WHAT?! I've never seen anything like this before...
<br><b>Sagredo:</b> Furthermore, the question of series convergence/divergence is
<i>inherently</i> a question about an <i>infinite</i> 
set of diagrams which means in the rain of bricks framework it necessarily 
<i>must</i> involve phenomena extending (in the view of any observer) over
an infinite span of space and/or time.   Your intuition that 
only <i>local</i> physics matters must be
incorrect at least as far as this question is concerned.
<br><b>Simplicio:</b> (this must be true... mind reels)
<br><b>Sagredo:</b> Another reason your experiment is irrelevant is that the convergence radius of
rain of bricks &#945;-power series in de Sitter space, while expected to be <i>positive,</i> 
also is expected to be very <i>small,</i>
much smaller than the physical value of &#945; namely &#945;&#8776;1/137.
(Note, this is the expectation from the Dysonian argument;
our direct diagram-counting and propagator-estimating argument disagrees and thinks
large, even infinite, convergence radius is possible.   It seems best to take
the more conservative of these two...
Meanwhile in Minkowski space, both arguments agree this radius would be exactly 0.
By the way, I haven't carefully worked out how small is "small," but 10<sup>-500</sup>
certainly should be safe.)
Hence computation of F(&#945;) at the physical &#945; value likely cannot be accomplished
by merely summing F's series directly;
it only can be accomplished via an analytic continuation, see
Henrici 1974 about computational techniques for accomplishing that to arbitrary accuracy.
(If it <i>could</i> have been directly summed, then there really might have been an
insuperable conflict with your physical intuition.  But as matters stand I see no conflict.)
So all that your amazement actually indicates, is that the mathematics involved
here acts in a way that is highly peculiar based on your old-style-physics
intuition.  Hence: That intuition must be regarded as suspect.
<br><b>Simplicio:</b> Ok, but I'm still not very happy.
I want a useful replacement for my old non-useful
intuition, please.  Also, it seems likely that the algorithm you sketch
for computing F(&#945;) is likely to be be <i>very</i> laborious, although
I agree that it is an algorithm. 
And finally I am bothered by the disagreement between 
the Dysonian and convergence arguments.
<a name="FIG7ccvenn">
<img alt="fig7" src="WarrenSmithQED131123_files/ComputClassVenn.png" align="right" width="40%"></a>
<br><b>Sagredo:</b> I agree a replacement-intuition would be nice.
Some of our comments and analysis go in the right direction toward developing one.
The algorithm indeed is very laborious.  
But let's put that in perspective by comparing it with the old-style QED
situation using our previous
<a href="#dnaexample">virus DNA</a> example.  That molecule had
about 6.4×10<sup>7</sup> electrons and 
1.2×10<sup>7</sup> atomic nuclei of 5 different kinds.
The <a href="http://oeis.org/A000055">number</a>
<a href="http://en.wikipedia.org/wiki/Tree_%28graph_theory%29">of</a>
ways to draw an unrooted tree with N=7.5×10<sup>7</sup>
unlabeled nodes is about exp(1.08N), and various labeling possibilities can only increase this
already-unreachably-enormous number. (See Otter 1948 for the asymptotic formula counting trees.)
Then as we'd previously noted, the number of one-loop diagrams exceeds the number of tree-diagrams
by a factor of order&#8805;10<sup>10</sup> so that &#945;-power series generated by
old-style QED "begin to diverge immediately."  With rain of bricks, we expect comparably
or even more enormous
diagram counts, and also expect immediate apparent series-divergence, 
with convergence only setting in much later just
like in <a href="#poincarejoke">Poincare's joke</a>.
<p></p><blockquote><small>
Actually, the situation is quite amusing: the series first appear to converge
(at least for simple physical scenarios) in the sense
of being asymptotic series valid for small |&#945;|, then appear to diverge in the sense
the |terms| get larger and larger, but
then finally converge, as you take more and more terms.  The final stage
only seems to happen after an immense number of series terms.  It is rather like
adding an extra level or two of insanity on top of Poincare's joke.
</small></blockquote><p>
So the computational situation, even though it <i>is</i>
algorithmic, is clearly a disaster until and
unless somebody devises much better algorithms.
I believe the naive algorithm is in the computational complexity class
PSPACE and indeed in its subclass
P<sup>#P</sup> or perhaps even BQP (albeit it cannot be a strict
subclass of BQP)... but PSPACE is not exactly the land
of milk and honey. 
The "rescue" of old style QED by rain of bricks is not as fun as
the sort of rescue you see in Hollywood movies.
<br> &nbsp;&nbsp;&nbsp;
It is not clear the Dyson/convergence "disagreement" is serious.
(It definitely is not in the sense that whichever one you believe, 
rain of bricks still is OK.)
After all,
various argumentation-methods already had disagreed 
about the divergence rate for
old-style (pre rain of bricks) QED,
cf. <a href="#ratediv">§6</a>.
I find Dyson to be a convincing argument for divergence outside
of some – necessarily small –
positive convergence radius (and many others have previously accepted the validity
of the Dyson idea; it is not just me).  I find the opposed convergence argument
less convincing, but its most conservative form seems to predict that the convergence
radius is positive – and that is also the prediction of the most-conservative form of
the non-general de Sitter bound (based on 
<a href="#FIG6badfeyn">fig.6</a>) which would <i>be</i> general if we could
be sure that it represented the worst case.  (I conjecture that.)
It is only its more optimistic/speculative forms which predict infinite
radius of convergence.
<br><b>Simplicio:</b> Is there a way to be more upbeat about this?
<br><b>Sagredo:</b> We apparently have both
<i>finiteness</i> and <i>algorithmicity.</i>
And we have <i>deduced,</i> from the experimental finiteness and assumed good behavior 
(analyticity as function of &#945;, with necessity that
radius of convergence be positive but considerably smaller than |&#945;|&#8776;1/137,
which in turn is considerably smaller than 1) of QED
that the Einstein cosmical constant <i>must</i> be both <i>nonzero</i>,
and <a href="#neutmass">far</a> smaller than 1 in Planck units.
That in turn <a href="#desit">yields</a> 
the <i>deduction</i> that, e.g. the neutrino masses 
(and masses of any spin-1/2 fermion) must be <i>nonzero.</i>
These all constitute <b>remarkable advances</b>
versus every current-rival QFT-type physical theory.
</p></blockquote>
<p><small>
Incidentally, in view of the $1 million Clay
prize for proving the existence of a <b>"mass gap"</b> in 
Yang Mills theory – e.g. that all QCD "glueballs" have mass bounded above
some positive constant –
the reader might naturally ask: do our results that neutrinos must have
positive mass, generalize to glueballs?
The answer appears to be "yes, glueballs must have mass&#8805;1600 MeV, <i>but</i>
this has nothing much to do with de Sitter space."
This is simply because of lattice gauge theory computer
computations of the smallest 13 glueball masses
(Morningstar &amp; Peardon 1999; also there is good experimental evidence that
glueballs exist mixed with nearby mesons, but as of 2012 there
still has not been any clear detection of isolated glueballs).
Presumably rain of bricks would have found essentially the same numerical results.
This is not going to win Morningstar &amp; Pearson
the prize because their computer computations and the whole
underlying theory they are based on, are both nonrigorous.
With rain of bricks we 
would come much closer to rigor, indeed it might within
my lifetime even be feasible to construct
an algorithm that (if run to completion) would prove it and win the prize
(albeit it likely would not be feasible to run it to completion).
</small></p>

<!--
<a name="hawking"></a> 
<h3>X. Low-tech explanation of Bekenstein-Hawking "black hole entropy" II
and a way (perhaps) to
determine the exact value of &rho;<sub>rain</sub>???</h3>
<p>
This section currently
omitted &ndash; will it return??? Maybe move ideas to open problems section or another paper???
</p>
<p>
If the mechanism we propose is the <i>sole</i> source of black hole entropy, then
a unique value of &rho;<sub>rain</sub> should be <i>forced</i>
to cause the entropy of a black hole to match the above formula.
In other words, we can deduce 
&rho;<sub>rain</sub>. 
I have not actually done this computation, however.  It ought to
be feasible, but it is not trivial and will require a lot of computer work.
</p><p>
If this computation is done, and it returns the <i>same</i> 
&rho;<sub>rain</sub> value <i>regardless</i> of the mass, charge, and spin of the black hole, 
that will be impressive evidence rain of bricks is on the right track.
On the other hand, if it returns values that <i>depend</i> on the black hole parameters,
that proves the microscopic mechanism we describe cannot be the sole source of black hole 
entropy, and/or that rain of bricks is wrong.
</p><p>
<b>Microscopic explanation of black hole entropy:</b>
It is well known that black holes "ring like a bell."  That is, any slight perturbation
away from (say) the Schwarzschild black hole metric, or to its electromagnetic field (or other 
fields, e.g. Dirac field, neutrino fields, could also be considered), will
vanish by decreasing oscillations.   
[This has been shown rigorously CITE GREEKS???.]
These are called <b>quasimodes</b> of black holes.
They have characteristic <i>complex</i> frequencies.  The real part of the frequency of a
quasimode governs its oscillations, and the imaginary part (which always is positive) governs
how quickly those oscillations die out.  Each quasimode has time-dependence exactly 
proportional to exp(2&pi;iFt) where F is the complex frequency and t is time.
Known algorithms can compute the full infinite set of quasimodes, and their
characteristic frequencies, for any kind of black hole, but it is not a trivial computation.
</p><p>
Mathematically speaking, the situation is exactly analogous to LRC electrical circuits,
in the sense that such circuits also exhibit characteristic complex frequencies.
Such circuits were examined <i>thermodynamically</i> by Callen &amp; Welton 1951,
with the conclusion (often called by physicists the "fluctuation dissipation theorem,"
which means, as usual in physics, that a better name for it would be "nontheorem")
that the quasimodes in such circuits, at any positive temperature, will exhibit excitations
described by certain formulas given by C&amp;W???
These electrical excitations are experimentally seen and are called "Johnson-Nyquist noise."
By analogy, then, we would predict black hole quasimodes ought to
be thermally excited at the Hawking temperature.   
</p><p><small>
There is one flaw in this analogy, though.
The Hawking temperature happens to be of the <i>same</i> 
order of magnitude (expressed as an energy
by multiplying by Boltzmann's constant k<sub>B</sub>) as
the real part of a large set of quasimode frequencies (if they also are expressed as an energy
by multiplying by Planck's constant h) &ndash;
Motl 2002 showed hRe(F)&sim;ln(3)k<sub>B</sub>T<sub>Hawking</sub> 
for an infinite subset of quasimodes
&ndash;
and for an even larger set of quasimodes,
the thermal energy is much <i>smaller</i> than Re(F).  In other words, 
there is only enough thermal energy available to excite each of the former 
kind of quasimode typically O(1) quantum levels high, while the latter kind of quasimode usually
will not get excited at all.
In contrast, in Johnson-Nyquist noise under the circumstances in which electrical
engineers usually encounter it, we are in the opposite limit: the thermal energy
is far larger that the quantum mode-energy step size, at commonly encountered frequencies.
The frequency corresponding to room temperature is 6&times;10<sup>12</sup> hertz,
while electrical engineers normally only care about frequencies &ge;1000 times
smaller.
</small></p><p>
The resulting quasimode excitations store energy, entropy, and information
in the vacuum in the region nearby any black hole event horizon.
This is an extremely simple mechanism and it could have been (but was not)  heavily
analysed 30 years ago.
</p><p>
It is known 
that the set of quasimodes is infinite;
indeed, an infinite number exist for each (L,m) tuple-value
(Bachelot &amp; Motet-Bachelot 1993).
However,
we will postulate that there effectively are only a <i>finite</i> set of
quasimodes, due to a UV <i>cut off</i>
at the Planck scale.  That is, quasimodes with either |Im(F)| or |Re(F)|
above the Planck scale
are cut off by rain of bricks effects.
Such a cutoff <i>must</i> happen, because otherwise our analysis will make it clear that
the entropy and mass-energy of a black hole would be infinite.  
Furthermore, we shall soon attempt to argue
that only a <i>Planck scale</i> UV cutoff will yield
a total mass-energy, and a total informational entropy, 
both of the same order of magnitude as the black hole's mass and Bekenstein-Hawking entropy.
This result is not at all a triviality.  The mass or entropy could easily have come out
far larger or far smaller, but both do not.  Hence I regard this as evidence in favor of
rain of bricks.
</p><p>
Quasimodes are indexed by three integer quantum numbers, which we shall call N (radial) 
and L and m (angular).  N and L are independent &ndash; each do not constrain the other &ndash;
but |m|&le;L.   (Do not confuse the integer m with the black hole's mass M.)
It is known that quasimode frequencies only depend on N and L, not on m.
It is known (Motl 2002, Nollert 1993) that 
</p><blockquote>
When 
N&rarr;&infin; with L held fixed, 
the quasimode frequency F behaves like
<nobr>
FM = (2N-1)&pi;i &plusmn; ln(3) + O(N<sup>-1/2</sup>)
-- or 2N+1, but guess depends on defn of N 
Thawk = (8piMG)^(-1)  and  Motl&Neitzke say w=RHS*Thawk
Motl&Neitzke say they will use units where 2GM=r[horizon]=1.
--
</nobr>
for scalar-field and gravitational perturbations of a mass=M 
Schwarzschild black hole, and
<nobr>
FM = 2&pi;iN + O(N<sup>-1/2</sup>)
</nobr>
for electromagnetic perturbations.
We claim (more strongly) that
both those expressions are valid even if L and m <i>not</i> held fixed, provided
that |m|&le;L=O(N<sup>1/2</sup>).  
</blockquote><p>
<b>Derivation of Strengthening:</b>
We slightly strengthen Motl's analysis
by replacing Motl's EQ 30 by
</p><center>
nR<sub>n</sub>
&nbsp; = &nbsp;
-(n+2iw)<sup>-1</sup>(L-1)L/2 - (n+2iw) &plusmn; 
 [ 2iw (n+2iw) + (L-1)L (1 + [L-1]L/[n+2iw]) ]<sup>1/2</sup>
</center><p>
where Motl's w=4&pi;FM.
This arises by simplifying the exact expression by keeping only the leading terms when 
n, |w|, and <nobr>(L-1)L</nobr> all are assumed to be large.
This shows that if 
<nobr>(L-1)L=o(|w|)</nobr> and (L-1)L=o(n)
then the terms involving (L-1)L all can be dropped as having relatively negligible
effect on this formula, then the rest of Motl's analysis goes through unchanged
to show his same results provided |m|&le;L=o(N<sup>1/2</sup>).
</p><p>
If the o's are changed to O's 
then the (L-1)L terms will have the same order as the other terms in Motl's formula,
and hence cannot be dropped.  
Nevertheless, the remainder of Motl's argument then still works, because
it depends on considering R<sub>n</sub> in an asymptotic regime 
<nobr>
1&lt;&lt;|n-2iw|&lt;&lt;|w|.
</nobr>
We instead shall employ the (sub)regime
<nobr>
L&lt;&lt;|n-2iw|&lt;&lt;|w|.
</nobr>
In this regime the (L-1)L terms have negligible
effect even if they have the same order as |n| and |w|.
<b>Q.E.D.</b>
</p><p>
<b>Even stronger???</b>
Motl &amp; Neitzke 2003 were able, using a completely different analytic method,
to extend the above result to a wider class of black holes,
as well as to prove its validity when 1&lt;&lt;L&lt;&lt;N???
</p><p>
On the other hand, when 
L&rarr;&infin; with N held fixed
Re(FM) grows proportionally to L; 
-- this estimate agrees with the Schutz-Will WKB estimate when L is made large, which
is the wrong regime for WKB but it still works --
indeed Barreto &amp; Zworski 1997 showed that when 
<nobr>L<sup>2</sup>+N<sup>2</sup>&rarr;&infin;</nobr>
with 
&kappa;<sub>1</sub>&lt;N&lt;&kappa;<sub>2</sub>L
--  &kappa;<sub>1</sub>&lt;|Im(F)|&lt;&kappa;<sub>2</sub>|Re(F)| --
for some positive constants
&kappa;<sub>1</sub>, &kappa;<sub>2</sub>,
</p><center>
FM = 3<sup>-3/2</sup>4&pi; [2L + 1 + i(2N+1)] + o(1).
</center><p>
-- while Im(F) fixed with L but propto N. --
Nonrigorous (but simple) WKB-based estimates by Schutz &amp; Will 
(see Ferrari &amp; Mashhoon 1984, who also consider some rotating and charged black holes)
yield the same results as the more complicated (but rigorous)
techniques of Barreto &amp; Zworski.
[The latter also examined the Schwarzschild de Sitter family of black holes.]
It seems obvious???
-- from the 1D wave equation EQ21 in 
http://relativity.livingreviews.org/Articles/lrr-1999-2/ --
that Re(F) is a monotonic-increasing function of L with N,M
fixed, while Im(F) is a monotonic-increasing  function of N with L,M fixed.
There is a change in behavior of F when L=O(N<sup>1/2</sup>) versus when
N=o(L<sup>2</sup>).
In the latter case,
</p><blockquote>
When L&rarr;&infin; with N=o(L<sup>2</sup>),
there is unbounded growth of Re(FM).
</blockquote><p>
<b>Proof:</b>
The point is that Motl's argument showing Re(FM)&sim;constant
cannot be used if N=o(L<sup>2</sup>).
Motl needed for an asymptotic regime
<nobr>
L&lt;&lt;|n-2iw|&lt;&lt;|w|
</nobr>
to exist.  By choosing
n&asymp;Im(2w)  CONJUGATE???
the middle term is minimized, but even then
it will not be &gt;&gt;L 
if Re(w) is of order L.
???
<b>Q.E.D.</b>
</p><p>
This suggests we need cutoffs for N, L each at about
M/M<sub>pl</sub> ,
the former caused by a demand to keep Im(F) below Planck scale,
the latter arising from the same demand for Re(F).
These cutoffs yield a number of allowed quasimodes proportional to 
<nobr>(M/M<sub>pl</sub>)<sup>3</sup>.</nobr>
</p><p>
If the quasimode fields are only available in the form of
<i>samples</i> at the raindrop points, and we only speak of raindrops within
the region r=O(M) of the Schwarzschild metric (i.e. within a constant
number of Schwarzschild radii), then
that should effectively <i>cause</i> 
UV cutoffs at the Planck scale, via Nyquist-like sampling theorems,
of precisely that sort.
We assume raindrop points are scattered
Poisson-randomly throughout the Schwarzschild spacetime metric at 
Lorentz-invariant 4-density &rho;<sub>rain</sub>.
By a happy mathematical coincidence,
in the 
<a href="http://en.wikipedia.org/wiki/Schwarzschild_metric#The_Schwarzschild_metric">usual</a>
Schwarzschild (t;r,&theta;&phi;) coordinate system with r "circumferential,"
this happens to be exactly the same thing as 
scattering Poisson points at 
constant 4-density &rho;<sub>rain</sub>
in Minkowski (t;r,&theta;&phi;) space using spherical polar coordinates
(r,&theta;&phi;).
</p><p>
It should be possible to compute
<i>exactly</i> how much energy and entropy will be stored in the quasimodes, but I have not done
this exact computation and shall only work with orders of magnitude, 
&ndash; mostly ignoring the exact values of constant factors.
We shall use some
known, and some new, results about
quasimode asymptotics.
[It would also be interesting to examine the full set of Kerr-Newman (rotating and charged)
black holes, not merely the Schwarszchild (nonrotating uncharged) subcase we consider.]
</p><p>
<b>Details.</b>
We shall use Planck units throughout.
Nollert 1993 found a way to express the quasimode frequencies for a Schwarzschild 
black hole using a continued fraction equation.   
A crude and not fully rigorous convergence analysis
of this continued fraction 
(sufficient for his very limited purposes) was performed by
Motl 2002.
We will now perform a genuine convergence analysis to get stronger results
than Motl.   [This also as a side effect
allows rigorizing Motl's results.]
The technique is simple.  One applies 
<a href="http://en.wikipedia.org/wiki/Fundamental_recurrence_formulas">Wallis's recurrence 
relations</a> for continued 
fractions to convert the problem of evaluating an N-deck continued fraction,
to that of evaluating a product of N matrices, each 2&times;2.
(John Wallis, 1616-1703.)
-- EQ 26-29 in
http://mathworld.wolfram.com/ContinuedFraction.html 
some CF formulas are here:
http://www.math.binghamton.edu/dikran/478/Ch7.pdf
--
Convergence or divergence, and understanding of the asymptotic behavior, for the continued
fraction is then established by
consideration of the eigenvalues of the matrices.
</p><p>
Nollert's result
is the following.  In order for a quasimode of a mass=M Schwarzschild black hole
with complex 
frequency F to exist, the continued fraction
</P><center>
<pre>
         a<sub>1</sub>
b<sub>0</sub> + ----------------
               a<sub>2</sub>
         b<sub>1</sub> + ----------
                 b<sub>2</sub> + ...
</pre>
</center>
<p>
must equal <i>zero</i>, where 
<nobr>Q<sub>n</sub>=n+8&pi;iFM</nobr>
and
<center>
<nobr>
b<sub>n</sub> = -2(Q<sub>n</sub>+&frac12;)<sup>2</sup> - (L+1)L + j<sup>2</sup> - &frac12;
</nobr>
&nbsp; and &nbsp;
<nobr>
a<sub>n</sub> = Q<sub>n</sub> n [Q<sub>n</sub><sup>2</sup> - j<sup>2</sup>].
</nobr>
-- Motl EQ 21-23 and 28. --
</center>
<p>
This allows one to solve for FM numerically.
Here j=0 if we are speaking of quasimodes of scalar (Klein-Gordon) fields, 
j=1 for electromagnetic fields, and j=-3 for the gravitational metric.
L and m are integers if we are discussing quasimodes whose angular behavior
as a function of &theta; and &phi; is the spherical harmonic Y<sub>Lm</sub>.
(Note that m turns out to be irrelevant.  Do not confuse the 
integer m with the black hole mass M.)
The eigenvalues 
&lambda;<sub>&plusmn;</sub>
of the Wallis matrix
</p><center>
<pre>
[ b<sub>n</sub>     a<sub>n</sub> ]
[  1     0  ]
</pre>
</center>
are given by
--MAPLE:
W := linalg[matrix](2,2,[bn, an, 1, 0]);
eigenvalues(W);
--
<center>
2&lambda;<sub>&plusmn;</sub> 
&nbsp; = &nbsp;
b<sub>n</sub> &plusmn; [b<sub>n</sub><sup>2</sup>+4a<sub>n</sub>]<sup>1/2</sup>
</center><p>
which for the Nollert continued fraction are
</p><center>
2&lambda;<sub>&plusmn;</sub> 
&nbsp; = &nbsp;
-9/2 
- (L+1)L 
+ j<sup>2</sup> 
&plusmn;
[(j<sup>2</sup>-9/2-[L+1]L)<sup>2</sup> 
+ 4 Q<sub>n</sub> n (Q<sub>n</sub><sup>2</sup>-j<sup>2</sup>)
]<sup>1/2</sup>
</center>
<p>
Define &lambda;<sub>max</sub> to be whichever one of
{&lambda;<sub>&plusmn;</sub>} has greater absolute value.
(For Nollert, &lambda;<sub>max</sub>=&lambda;<sub><b>&ndash;</b></sub>.)
Such continued fractions generically converge if there is an upper bound below 1 on
the absolute value of the "Wallis quotient"
<nobr>
W<sub>n</sub> = a<sub>n</sub>/(&lambda;<sub>max</sub>)<sup>2</sup>
</nobr>
when n&rarr;&infin;.  
but generically diverge if
<nobr>
|W<sub>n</sub>|&gt;1.
</nobr>
</p><p>
If |W<sub>n</sub>|&rarr;1
in the limit (which it does for us if L and j are
held fixed when n&rarr;&infin;) then
the question depends on the behavior of 
<nobr>1-|W<sub>n</sub>|</nobr>.
E.g. if these quantities all are positive and
exceed n<sup>-0.99</sup>
for all sufficiently large n,
then the fraction converges.
If the W<sub>n</sub> are real and
W<sub>n</sub>&rarr;1
from below, then the fraction converges if all the a<sub>n</sub>
(above some n) are positive and &lambda;<sub>max</sub> is always (above some n) real
and
<nobr>&sum;<sub>n</sub>(1-W<sub>n</sub>)=&infin;.</nobr>
</p>
--MAPLE:
Qn := (n+pii8FM);
an := Qn*n*(Qn^2-j^2);
Lam2 := -9/2 - Lm1L+ j^2 - ((-9/2 - Lm1L + j^2)^2 + 4*Qn*n*(Qn^2-j^2))^(1/2);
Wn := an / (Lam2/2)^2; 
series(Wn, n=infinity, 8);
Result 1-(9/2+(L-1)L-j^2)/n^2 + O(n^-3).
RR = ((Qn-1)^2-j^2) / ( -2*(Qn-1/2)^2 - Lm1L + j^2 - 1/2 - n*Qn*RR);
solve(%, RR);
#now keep only leading terms in RR when Lm1L, n, and F all large  (#+- sign really):
RRS := (-Lm1L/2 - (n+pii8FM)^2  + 
 ( pii8FM*(pii8FM+n)^3 + Lm1L*(pii8FM+n)^2 + Lm1L^2 )^(1/2))
 / (n*(pii8FM+n));
#Evidently the Lm1L has negligible effect if it is o(F^2) and if n has same order as F.
--
<p>
For Nollert we have
W<sub>n</sub>=1-[9/2+[L-1]L-j<sup>2</sup>)n<sup>-2</sup>+O(n<sup>-3</sup>).
This implies NONCONVERGENCE!!!!
--
The second criterion shows that Nollert's continued fraction converges for any
fixed j and L?  No, complex Wn.
--
</p><p>
Motl was able to show, by considering truncating the continued fraction after
order |FM| terms, that the quasimode with quantum numbers (N,L,m)
has a frequency F asymptotic as N&rarr;&infin; with L,m,j fixed
to FM=iN/(4&pi;)+const+o(1).
(Do not confuse n with N.)
Our claim is this is still true even if L and m are <i>not</i> held fixed provided 
L=o(N).   Hence we shall consider letting |L| range up to almost as large as N,
e.g. N<sup>0.99</sup>, and all m with |m|&le;L.  
Further, we assume frequencies with either real or imaginary part above order 1
Planck frequency units, are cut off.
As a result we get
a number of quasimodes growing almost cubically (e.g. like N<sup>2.99</sup>)
as a function of the maximum allowed N, which is of order M/M<sub>pl</sub>,
with each of these modes having bounded real part of its frequency [and
the real part of the frequency is of the same order as 
k<sub>B</sub>T<sub>Hawking</sub>=(8&pi;M)<sup>-1</sup>
where T<sub>Hawking</sub> is the Hawking temperature],
but with imaginary part depending approximately linearly on N.

???
</p><p>
When we do this, we conclude the following about the asymptotics of eigenfrequencies.
???
</p><p>
Sampling and reconstruction???
</p><p>
Feynman &amp; Vernon???
</p><p>
The second question &ndash; about whether information dropped into a black hole is lost &ndash;
does not seem answerable by our theory as it currently stands, since it would
also depend on the nature of quantum gravity, comprising new postulations beyond those here.
</p>
-->

<a name="lacunae"></a>
<h3>39. Lacunae </h3>

<p>
The analysis in this paper has fallen short of rigor/desires/perfection in 
numerous ways (sometimes by citing imperfect previous papers, other times 
the imperfections are directly mine).
I believe physics papers should highlight, not conceal, such imperfections, so
to be a role model I attempt to summarize those here.
</p>
<p><b>Old-style QED is flawed?</b>
Although I presented numerous (as well as reviewed many previous) arguments
for this, most fail to constitute a rigorously airtight
demolition of QED, for example the Dysonian divergence (based on "collapse")
argument depends on physical intuition, in place of a proof, that collapse happens;
the "Landau pole" perhaps is not as devastating as it seems; although old-style QED is
currently nonalgorithmic, conceivably algorithms with validity proofs might in future be found, 
etc.  It's hard to rigorously 
prove QED wrong if to do so you need to use QED (which is nonrigorous) as a tool!
</p><p>
I will say, however, that my 
<a href="https://dl.dropboxusercontent.com/u/3507527/delocthm1">refutation</a> of the notion "QED and quantum mechanics imply
spatial-decoherence and that position is the 'pointer basis' " <i>is</i> fully rigorous.
</p><p>
<b>The "saddlepoint method"</b> (aka "method of steepest descent") is often used 
to find asymptotics for integrals.  This method and the asymptotic results it outputs
often can be done rigorously.  But there are many more ways to do it nonrigorously 
than rigorously! –
such as (a) not fully proving your "saddlepoint" really is one, (b) not proving
the distortion of the path of integration to make it go through the saddle
really was justified (e.g. did not move through poles),
(c) not proving your saddlepoint(s) were dominant and all rivals were subdominant,
(d) not proving tail estimates showing only the integration region 
near the saddlepoint asymptotically matters 
while the rest of the integral contributes subdominantly.
We cited many papers in our <a href="#ratediv">§6</a>
which had used the saddlepoint method 
in nonrigorous ways to deduce asymptotics for infinite-dimensional integrals.  
As a rough rule of thumb based on my experience reading physics papers and
(often) their later refutations, every time any physicist (so far in
the history of the world) says they
have used saddlepoint method in D dimensions to deduce some result, then if D&gt;1 they
were lying (i.e. their result was nonrigorous and may or may not be correct) 
while if D=1 they almost always have
a valid answer, but even
when D=1 they usually did not do the whole job required to get a genuine proof.
</p><p>
However, results "proven" nonrigorously can nevertheless sometimes be correct!
Magnen, Nicolo, Rivasseau, Seneor 1987
were able to rigorously prove a weakened version of a "Lipatov style estimate"
(ala <a href="https://dl.dropboxusercontent.com/u/3507527/tab2">table 3</a>)
on series-divergence rate in "(&#966;<sup>4</sup>)<sub>4</sub> Euclidean field theory"
finding the  exact same dominant asymptotic as Lipatov's nonrigorous saddlepoint 
method would have provided.  They find that their
<a href="http://en.wikipedia.org/wiki/Borel_summation">Borel transform</a> converges in
a certain positive-radius disk in the complex plane
but diverges outside another such disk.
They find strong reason to believe (but not a proof because they have not been able to rule out
the possibility of incredible cancellations) in the Borel non-summability of the original
(real) series.
</p><p>
Everything in 
<a href="#quantumgrav">§11</a> rests on the <b>"holographic principle" entropy bound</b>
and then seems pretty rigorous as an "assumption&#8658;result"
statement, <i>but</i> this underlying bound has never really been rigorously justified
and until then <i>must</i> be regarded as an "assumption."
As a rough rule of thumb, the majority of
all General Relativity results claimed by physicists (besides exact solutions)
are nonrigorous because solution existence and uniqueness for the Einstein equations has
never been proven (except under very restrictive assumptions) and no algorithm for
simulating GR has ever been proven to work with arbitrarily small
(or for that matter with essentially any) guaranteed error bounds.
I.e. GR currently remains a non-algorithmic theory of physics.
Rain of bricks QED is, however, the first algorithmic QFT-type theory of physics, if
the issues in the following two paragraphs pan out as hoped.
</p><p>
Our <a href="#nouvinf">arguments</a>
that rain of bricks <b>abolishes ultraviolet infinities</b> in IRFrQFTs
(and hence also abolishes the need for cutoffs and renormalization)
perhaps are not really rigorous, e.g. may depend to some extent on physical intuition
and "power counting" as matters currently stand.
But I think they are very close to rigor and are valid "beyond a reasonable doubt."
They probably could be stated and proven in a fully rigorous manner
with more work and care.  (As an estimate of <i>how much</i> further work would be needed,
note that the papers by Lowenstein, Weinberg, Zimmermann et
al establishing the necessary integral-estimates needed to prove QED renormalizable,
consumed 100-200 pages.)
But our preliminary <a href="#maxstrength">attempts</a>
to argue the same for <i>non</i>renormalizable QFTs such as gravitons
were less rigorous and less convincing, and might be harder to rigorize.
</p><p>
Our rain of bricks <b>&#945;-power series divergence and convergence</b> arguments in
Minkowski and de Sitter space partly depend upon 
"<a href="https://dl.dropboxusercontent.com/u/3507527/sqrtsum">square root sum estimates</a>" or weakened variants
thereof, which seem plausible (and independent plausibility arguments have also
been given, which increases correctness probability) but certainly were not rigorously justified.
However the main results use these estimates only in comparatively convincing ways.
</p><p>
Another rigor problem is the "<a href="#limorder">limit order</a>" issue.
It might be feasible to resolve that
 by making probabilistic arguments about Poisson raindrops combined
with lemmas about the phase-angle behavior of QED propagator formulas at large 
(especially timelike) |distances|, but as matters currently stand I haven't.
</p>

<a name="glossary"></a>
<h3>40. Glossary </h3>

<p>
EM: electromagnetism.
</p><p>
GI-class:
A set of Feynman diagrams for some process whose sum is <i>gauge-invariant.</i>
</p><p>
GUT: "Grand unified theory," meaning that the gauge groups U(1)×SU(2)×SU(3) of
the standard model are hypothesized to 
form a subgroup of some larger group such as SU(5).
GUTs predict new phenomena not present in the plain SM, such as magnetic monopoles
and proton decay, which so far have escaped detection by experimenters.
</p><p>
IRFrQFT:
Infrared-finite renormalizable QFT.
The standard model is believed to be an example.   IRFrQFTs are "input"
to the rain of bricks construction, which outputs a hopefully rigorous version of them.
Some versions of rain of bricks 
look likely to be able to handle some non-renormalizable QFTs such as gravitons.
</p><p>
LHC: The "large hadron collider" at CERN.
</p><p>
LQG: loop quantum gravity, an embryonic theory of quantum gravitational spacetime
developed by C.Rovelli, L.Smolin, and others.
</p><p>
NP:  the set of decision problems whose "yes"-instances have proofs
verifiable in polynomial time by Turing machine. 
It is one of the most famous conjectures in computer science that P&#8800;NP,
i.e. there exist problems whose solutions cannot be found by any
polynomial time algorithm, even though once
found they can be verified in polynomial time.
(A "polynomial time" algorithm means one whose
worst-case runtime for B-bit input, is upper bounded by
a polynomial function of B.)
</p><p>
#P: <a href="http://en.wikipedia.org/wiki/Sharp-P">Another</a>
important computational complexity class; essentially the task is to count the number of 
solutions of some problem, where each such solution is verifiable in polynomial time (P).
Also important is P<sup>#P</sup>; it denotes the class of problems
soluble in polynomial time by a Turing machine with access to
a polynomial-time oracle for solving #P problems.
</p><p>
PDE: partial differential equation.
</p><p>
PSPACE:  the set of all decision problems that can be solved by a 
Turing machine using a polynomially bounded number of bits of memory.
</p><p>
QCD: quantum chromodynamics.  An SU(3) gauge theory of the strong nuclear force
developed by M.Gell-Mann by building on both QED and Yang-Mills gauge theory.
Other important contributors include 
J.Bjorken, R.Feynman, D.Gross, K.Nishijima, G.Zweig.
</p><p>
QED: quantum electrodynamics.  This is an rQFT theory of electrons, positrons, and photons
tracing to P.A.M.Dirac, R.P.Feynman, J.Schwinger, S-I.Tomonaga, F.Dyson, and K.G.Wilson.
</p><p>
QED<sub>N</sub>: 
QED at the Nth degree (and below) of perturbation theory, i.e. including
terms at the (&#8804;N)th power of &#945; in perturbative series expansions, and
which presumably would make asymptotically correct predictions in
the limit &#945;&#8594;0+ as opposed to its true
value &#945;&#8776;1/137.036.
(This presumption unfortunately seems experimentally
untestable since we do not have access to other
universes in which &#945; is smaller.)
Rain of bricks attempts to provide a mathematically and algorithmically defined
meaning for "QED<sub>&#8734;</sub>."
</p><p>
"Quenched" QED or QED<sub>N</sub>
means QED in which we forbid "virtual electrons" aka "lepton loops"
in Feynman diagrams.  
</p><p>
QFT: quantum field theory.
</p><p>
rQFT: renormalizable quantum field theory.  QED, electroweak theory, QCD, 
their union (which is SM), and presumbly also proposed
GUT and SUSY and SUSY-GUT generalizations of SM, all are rQFTs.
</p><p>
Rain of bricks: 
The main proposal of the present work.  Proposes that QFTs
should involve "propagation" and "interaction" only, all interactions occurring only on "bricks"
where the usual Minkowski-spacetime arena of physics is replaced with a new metrical 
structure, also called "rain of bricks," which resembles Minkowski spacetime
at large length scales, but microscopically differs from it.  
It appears that this removes the UV infinities
from IRFrQFTs without needing "renormalization"; and it does so in a way compatible with
"unitarity," "gauge invariance," "causality," and
"doubly (or triply) special relativity" at least if these are interpreted suitably.
It also appears that by setting up rain of bricks in de Sitter, rather than Minkowski, spacetime,
we can define QED<sub>&#8734;</sub>, i.e. the perturbative series QED outputs actually 
should have nonzero radius of convergence.  This turns QFTs into actual science in
the sense it now is an algorithmic task to output physics predictions to arbitrary accuracy.
(It also seems likely that some non-renormalizable QFTs such as gravitons can be handled,
at least with some rain of bricks versions, in which case one could produce a 
"theory of everything.")
All this ought to be rigorously provable for a refreshing change (although the present work 
falls short of that).
</p><p>
RMS: root mean square.
</p><p>
SM: standard model. A large rQFT that includes QED, QCD, and electroweak/Higgs
theory under one umbrella.
It was so-named by S.Weinberg.  Veltman 1994 claims to provide 
a complete description at the end of his book.
When Weinberg christened it the
standard model was the best available theory of
these things, i.e. of all non-gravitational physics, and it remains heavily used.
However, with the later indirect astronomical 
discovery of "dark matter" (which if real, must involve at least one
additional particle beyond those the standard model knows about), 
the direct discovery that neutrinos have mass (the 
standard model had treated them as massless), and the hypothesis of "inflatons" (which
if they exist would be further particles important during the early universe) 
it is now clear that the standard model is incomplete.
It is believed, however, that it will be trivial to modify the standard model to 
incorporate any and all of the these things because the "toolkit"  that comes
with the standard model easily allows inserting new particles.
The only reason this has not yet been done is that the parameters/types of the new
particles have not yet been determined by experimenters (e.g: are neutrinos
their own antiparticles?).
</p><p>
SUSY: supersymmetry, a hypothetical new symmetry (which if present must only be an
approximate symmetry) of nature which interchanges bosons and fermions.
SUSY is a prerequisite for both SUGRA (supergravity) and superstring theory.
The latter is the most ambitious theory of physics yet proposed and 
would be "the theory of everything."  Rain of bricks appears likely to
also yield "theories of everything" which would be greatly superior
to superstrings in the sense that
they would be provably algorithmic and much simpler.  
But they would be inferior to superstrings in the sense that superstrings allegedly would
predict, from "nothing," all known physical constants (such as electron charge and mass)
in Planck units.   
</p><p>
UV: ultraviolet, i.e. at high frequencies, short length scales, high energies.
As opposed to IR=infrared (low frequencies).
</p>

<a name="epitaph"></a>
<h3>41. Comparative summary </h3>

<blockquote>
Philip II sent a message to Sparta saying "If I enter Laconia, I will 
destroy your farms, slay your people, and burn Sparta to the ground."
The Spartans sent back the one-word reply "If."
<br>&nbsp;&nbsp;<b>–</b> 
An anecdote from &#8776;350 BC curiously apposite to
the gap between the ambitions of
string theorists and reality.  (The Macedonians ultimately never invaded Sparta.)
</blockquote>
<blockquote>
Two campers suddenly encountered a bear.
One dropped his pack,
threw off his boots, and laced on a pair of running shoes. 
The other said: "What are you doing? Running shoes aren't going to be enough
for you to outrun that bear!"
The first replied: "I only need to outrun <i>you!</i>"
<br>&nbsp;&nbsp;<b>–</b> 
A joke curiously reminiscent of current situation with rain of bricks versus rival
theories.  I unfortunately have failed to fully  accomplish my fondest dreams
for, e.g. rigorizing rain of bricks QED, or connecting it to
experiment... but it clearly is much closer to both rigor and experiment
(and far simpler) than string theory!  
<!--In fact, this whole paper can be regarded as a "nonrigorous proof
that there exists a rigorous proof"!-->
</blockquote>
<blockquote>
Physics is reliable knowledge of the real world, based on experiment. A new theory of physics 
must establish its reliability first by explaining existing knowledge of the real world. 
New theories of physics build on existing reliable theories. A candidate theory of physics 
obtains credibility first by giving definite explanations of established theories. 
A new theory inherits the reliability of the theories it explains. For example, special 
relativity explained Newtonian mechanics. General relativity explained special relativity and 
Newtonian gravity. Quantum mechanics explained classical mechanics [with gaps]. 
<br> &nbsp;&nbsp;&nbsp;
Present knowledge of the laws of physics is summarized in the combination of 
classical general relativity and the standard model of elementary particles, to the 
extent that the standard model has been confirmed by experiment. [There also is
dark matter...]
A candidate theory of physics must establish its reliability by explaining this 
currently successful theory. It must explain classical general relativity 
and the standard model in detail.
<br>&nbsp;&nbsp;<b>–</b> 
Daniel <a href="http://en.wikipedia.org/wiki/Daniel_Friedan">Friedan</a>
2002 in <a href="http://arxiv.org/pdf/hep-th/0204131v1.pdf">hep-th/0204131</a>, 
but words in [square brackets] added by me.  
<small>I disagree with many of Friedan's statements but like this one.</small>
<!--
<small>He continues: ... The reliability of string theory cannot be evaluated, 
much less established. String theory has no credibility as a candidate theory of physics.</small>
friedan AT physics.rutgers.edu
-->
</blockquote>

<p>
There are almost certainly an infinite number of ways to mathematically construct
self-consistent "theories of everything" (unifying gravity and the other forces) 
in ways compatible with all experimental evidence so far collected.
However, at most one of them can be right.  Further, very few of them are
likely to be simple enough for humans to prove self-consistent, and to use;
and none have yet been found.
</p><a name="cqft"></a><p>
<b>"Constructive quantum field theory"</b>
is the struggle to convert "axiomatic QFT" from a set of abstract demands about
the mathematical structure of a quantum field theory, into
something that actually exists.
It has succeeded in creating a number of mathematically rigorous quantum field theories
satisfying the 
"<a href="http://en.wikipedia.org/wiki/Wightman_axioms">Wightman axioms</a>,"
including some
with renormalization and with Borel-summability, 
and which can be treated nonperturbatively.
But so far, these successes have always
been with unphysical "toy" QFTs in spacetime dimensions 2 or 3
(lower than the physical 4) and/or with simpler
kinds of fields and/or interactions.  The closer these mathematically
successful QFTs got to physical QFTs (which was never very close), the
longer and more difficult the proofs got. It took 10-20 years before the 
initial hurdles were overcome by the efforts of about 30 workers 
(especially
J.Fröhlich, J.Glimm, F.Guerra, A.Jaffe,
E.Nelson, K.Osterwalder, L.Rosen, R.Schrader, B.Simon, T.Spencer, K.Symanzik)
allowing the first CQFT successes.
Axiom sets were formulated by 
L.Gårding, R.Haag, R.Jost, 
K.Osterwalder, R.Schrader, A.Wightman.
Some of the most impressive recent results seem due to 
Tadeusz Ba&#321;aban.
</p>
<a name="tabaxiomqft"></a>
<table bgcolor="CCFFCC"><caption>
<b>Table 11:</b> Successful mathematical constructions of axiomatic quantum field
theories.
</caption>
<tbody><tr bgcolor="pink">
<th>Space</th>
<th>Time</th>
<th>Lagrangian</th>
<th>Who &amp; when</th>
<th>Comments</th>
</tr><tr>
<td>1</td><td>1</td><td>&#955;&#966;<sup>4</sup></td><td>
Nelson 1966; Glimm &amp; Jaffe 1968-1972
<!--(6);15,19,22--></td><td>Renormalization; 1 kind of bosons with interaction</td>
</tr><tr>
<td>1</td><td>1</td><td>polynomial(&#966;)</td><td>
Glimm &amp; Jaffe 1973; Glimm, Jaffe, Spencer 1974; Guerra, Rosen, Simon 1975.
</td><td>
If degree(polynomial)=4 then analyticity as function of coupling constant
in a positive-radius semicircle with center at 0 and positive real part, and Borel summability
of perturbative series:
Eckmann, Magnen, Seneor 1974.
</td>
</tr><tr>
<td>2</td><td>1</td><td>&#966;<sup>4</sup></td><td>Glimm &amp; Jaffe 1973,
Feldman &amp; Osterwalder 1976,
Brydges, Dimock, Hurd 1995.
</td><td>Singularities removable by mass renormalization.
Lattice approximations converge (Park 1977).
Perturbative series is Borel summable (Magnen &amp; Seneor 1977).
</td>
</tr><tr>
<!-- just a polynomial
<td>1</td><td>1</td><td>&lambda;&phi;<sup>4</sup>+(&sigma;/2)&phi;<sup>2</sup>+c</td><td>??</td><td></td>
</tr><tr>
-->
<td>1</td><td>1</td><td>&#936;<sup>*</sup>&#936;&#966;</td>
<td>Glimm<!--1967?-->; Seiler 1975; 
Magnen &amp; Seneor 1976;
Glimm &amp; Jaffe 1987.
</td><td>Yukawa boson-fermion interactions, infinite boson mass renormalization;
also certain generalizations and with supersymmetry 
treated by Jaffe in about 2000.</td>
</tr><tr>
<td>1</td><td>1</td><td>2D Higgs model of abelian gauge field interacting with charged scalar field
</td><td>Brydges, Fröhlich, Seiler 1979-1981</td><td>
Super-renormalizable continuum gauge theory.
A mass gap exists in this model:
Balaban, Brydges, Imbrie, Jaffe 1984</td>
</tr><tr>
<td>1</td><td>1</td><td>QED</td><td>
Weingarten &amp; Challifour 1979 and 1980.
</td><td></td>
</tr>
</tbody></table>
<p>
Note that in 1+1 dimensions, the Coulomb potential is ultraviolet-finite
(so that, e.g, in classical physics the "electron self energy"
does not increase by infinity as we shrink the "electron radius" from one to zero)
and the <a href="#dysonD">Dyson</a>-collapse argument does not force series divergence
either.  Thus QED's two major difficulties are not present in (1+1)D.  This presumably has a lot
to do with why the CQFTers, so far, mostly only have been able to construct rigorous QFTs 
in 1+1 dimensions.  The only successes I know of that go beyond 1+1 dimensions always
have been such that the perturbation series was provably
Borel summable (or convergent), i.e. for these
QFTs,  "perturbation theory works."  
But there are strong reasons to believe that's false for physical (3+1)-dimensional QFTs.
</p><p>
It thus remains unclear as of 2012 
whether quantum mechanics is <b>compatible</b> with 4D special relativity
vis-a-vis the <a href="http://en.wikipedia.org/wiki/Wightman_axioms">Wightman axioms</a>.
This is an open mathematical problem – indeed, the task of CQFTing Yang-Mills theory
is (essentially) one of the $1 million Clay prize 
<a href="http://www.claymath.org/millennium/Yang-Mills_Theory/">problems.</a>
</p><p>
<b>Are the Wightman CQFT axioms physically correct?</b>
The mathematical answer to the above compatibility question
could be "no," in which case that
presumably would be evidence the CQFT axioms were physically wrong.
</p><p>
<b>Theorem:</b> Rain of bricks QED <i>disobeys</i> the 
<a href="http://en.wikipedia.org/wiki/Wightman_axioms">Wightman axioms</a>.
</p><p>
<b>Proof:</b>
The rain of bricks disobey microscopic Lorentz invariance, i.e. "axiom W0."  
I.e. there are two different
Lorentz observers for whom the laws of rain of bricks physics differ.
(Indeed W0 seems to implicitly assume the "arena" is Minkowski spacetime,
contradicting rain of bricks <a href="#post1">postulate 1</a>.)
<b>Q.E.D.</b>
</p><p><small>
I think, however, that superstring theory perhaps still could hope to obey some version of
the CQFT axioms.  Also, if a suitable <a href="#optenvavg">optional environment-averaging</a><a>
postulate were added to rain of bricks (this postulate would need to be very carefully
designed, which I haven't; the vast majority of naive averaging methods will not work) 
a rapprochement with CQFT axioms might become possible.
</a></small></p><p><a>
If rain of bricks with gravitons added to the QFT works and can be made rigorous, 
then it would unify <i>general</i> relativity with quantum field theories, thus more-than-solving 
the compatibility problem – but not within CQFT axiom systems.
</a></p><p><a>
The "bible" of constructive quantum field theory, for lack of
a better alternative, is the book by Glimm &amp; Jaffe 1981/1987;
but you are warned that this is an extremely difficult area and an extremely 
difficult book – CQFT is far deeper and more difficult than
QFT as currently performed by physicists, since the fundamental objects
are operator-valued distributions.
<small>
(Later note: more recent books on this topic are Araki 2000 and Haag 1996.
I have not seen them.)
</small>
And even so, this book's coverage is quite
small compared to what has been accomplished in the CQFT literature.  
See Jaffe's 2000 and 2008
review articles, and the Clay prize 
</a><a href="http://www.claymath.org/millennium/Yang-Mills_Theory/yangmills.pdf">problem description</a>
by Jaffe &amp; E.Witten,
for pointers to that literature.  A large fraction of the CQFT literature is
inside the journal <i>Communications in Mathematical Physics.</i>
Flato, Simon, Taflin 1995 solved the "Cauchy problem," i.e.
proving global solution-existence &amp; uniqueness, for the
coupled Maxwell-Dirac equations for sufficiently small initial data.
("Small" in certain norm-senses they specify.)  This included a way to avoid "IR infinities."
This seems an essential prerequisite for
progress in attempting to rigorize QED.
</p><p>
Working physicists today have paid essentially zero attention to the entire CQFT field.
And the present paper also has made no effort to connect to the constructive QFT
axioms and formalisms.  E.g. we do not need "operator-valued distributions," we simply
use Feymnan rules to calculate things, and argue with rain of bricks the issues
with infinities, non-convergence, and ill-definedness stop being issues.
I think that, as opposed to Wightman axioms,
the alternative desiderata listed in our <a href="#whatisrigor">§7</a> are sufficient 
(if they can be made to work), seem much easier to deal with, 
and are more familiar to more people – plus
I think it is quite likely the CQFT axioms are just plain wrong, physically.
Our treatment instead lies closer to the language and methods of physicists and computer
scientists, and presently still involves a lot of nonrigor.  Nevertheless I feel, or hope,
that I've made substantial progress toward making physical QFTs rigorous.
</p><p>
<b>Folland's list of "bargains with the devil":</b>
One of the things I like about Folland 2008 is that, as he develops QED, 
he explicitly points out, using the name "bargains with the devil," the places where 
departures from rigor appear.  We now go through Folland's list 
explaining how rain of bricks handles each one.
</p><ol><li>
Perturbation theory:  Rain of bricks in de Sitter background 
appears to yield perturbative series featuring <i>positive</i> radius of convergence
(<a href="#byeinfin">§37</a>).
If so, everything is well defined.
</li><li>
Interacting fields:
Rain of bricks "tames" interactions by requiring them to occur only on "bricks," 
a set which is a lower-dimensional domain than ordinary Minkowski 3+1 dimensional spacetime.
</li><li>
Chain approximation's validity p.215-216:
Folland's discussion is a bit vague, I think he is trying 
to worry about the "Landau pole" without 
explicitly mentioning it.  Anyhow, the particular bone Folland picks here is that
an attempt to derive/discuss the "true electron propagator" via a recursion, may not really be
justifiable even though valid formally in perturbation theory.  This whole problem
simply does not arise in rain of bricks because we do not need or use the 
"true electron propagator"; we only use the bare undressed propagator.
</li><li>
Feynman "path integrals": 
Rain of bricks does not ever need them.
The only place we have used them in this paper is in <a href="#ratediv">§6</a>'s
discussion of divergence rate of generic QED series 
(since some of those results were obtained by 
previous authors using nonrigorous functional-integral techniques).
But that discussion was not about rain of bricks trying to succeed, 
it was about old-style QED trying to fail.
With rain of bricks there are only two things happening: propagation (using bare propagators)
between points on two bricks, and "interaction" (i.e. emission or absorption)
occuring at that point on the brick.  These are handled at any order of perturbation
theory using sums and finite-dimensional integrals, which are perfectly well defined, finite,
and convergent (<a href="#byeinfin">§37</a>).
Then if (as we believe and have argued) the full perturbative series converges, everything
is well defined.   Then the particular Feynman sum-over-histories idea rain of bricks
instantiates, would be fully rigorous.  So our opening statement "rain of bricks does not 
ever need them" could be continued "...<i>or</i>,
you can also interpret rain of bricks as implementing
a more-agreeable/valid form of Feynman's idea."
</li></ol>
<p>
<b>Rival attempts to build more advanced and inclusive physical theories:</b>
The rain of bricks approach is distinguished from all the other main attempts
(superstring/M theory, loop quantum gravity, Sakharov, Wheeler/DeWitt/Hawking
all-histories, noncommutative geometry) by
its comparatively tremendous <b>simplicity</b> and "low tech" nature.
I see little hope in the forseeable future that those other approaches
can be made mathematically rigorous and computationally useful, but
I believe that hope does exist for rain of bricks.
</p><p>
<b>Superstring theory</b> indeed appears to have reached the point where it is necessary to 
train for a decade to
become able to make an important contribution in the area 
(except perhaps for a few remarkable geniuses).
I personally am unwilling to invest that many years in education given the considerable probability
that string theory is just wrong so that any
work I succeeded in doing on it would mostly be wasted;
and those who are willing to make that gamble will unavoidably be highly biased
in favor of string theory hence largely incapable of honestly evaluating it.
</p><p>
String theory is the most <b>ambitious</b> of the unification ideas.  It aims to predict
every constant of physics (particle masses, coupling constants etc) in Planck units.
Strings (just like rain of bricks), were hoped to clear up all the infinities plaguing QFTs
without requiring "renormalization" to do it.  
(String theory is widely believed by practitioners to have in some unclear sense removed 
UV infinities, but no proof has ever been offered, whereas rain of bricks currently
comes very close to a proof, and it could probably be genuinely proven with more work.)
It appeared that superstrings were
forced by purely logical desiderata into being a <i>unique</i> theory.   Unfortunately,
strings have, if anything, gone in the opposite direction to "rigorizing QFTs" and
their uniqueness has also evaporated!
Shouldn't the "final theory" be more than perturbative with divergent series (and
the 11th dimension added to string theory to make "M theory" apparently has destroyed the hope
of even having a perturbational theory... which was never
known to exist anyhow beyond "two loops").
Also, string theory computations are so difficult
that there appears to be no hope in the forseeable future of making 
experimentally-falsifiable predictions.
And many (most?) string theorists now believe there is
a <a href="http://en.wikipedia.org/wiki/String_theory_landscape">landscape</a>
of 10<sup>500</sup> possible choices – maybe even an infinite number –
inside string theories, with the right one, e.g. one yielding the observed miraculously small
value of the Einstein cosmical constant,  perhaps 
being favored only by "anthropic" considerations
and almost certainly infeasibly difficult for humanity to determine.
I find that prospect extremely distasteful!
Also, choosing among 10<sup>500</sup> variants is about 
1700 bits of information.   In contrast, simply specifying all known constants of
old-style physics (<a href="http://math.ucr.edu/home/baez/constants.html">John Baez</a>
has counted 26 so far)  to 20 significant figures would also be about 1700 bits.  
It quite likely would
be far more difficult to determine the 1700 bits for string theory 
(see, e.g, Denef &amp; Douglas 2006 for arguments it is "NP-hard")
than to experimentally measure them
for old-style physics.  It's plausible humanity will never be able to do the
former even if string theory is entirely right.   
</p><small><p>
<b>Details about information-content:</b>
Baez reckons there are 25 constants in the "standard model" of nongravitational physics
as enhanced in some unknown way to know about neutrino masses:
</p><pre> 13 particle masses,
 4 entries of CKM matrix,
 4 entries of PMNS matrix,
 1 Higgs field expectation value, and
 3 dimensionless coupling constants.
</pre>
A full description of physics would also include
<pre> 1 the Einstein cosmical constant
 X additional constants concerning the nature of the unknown "dark matter" particles;
   also many "inflationary cosmologies" require an unknown "inflaton field" to exist.
</pre>
The Planck constant, Newton gravitational constant [or some kind of
graviton coupling constant], and speed of light are not counted since
they merely define the Planck unit system.
In total this is 26+X constants in "old style" physics.
String theory (if it lives up to its hopes) would predict all
these from "nothing"... except actually there is a "landscape"
of an infinite set of discrete choices.  It has been guesstimated that there are 10<sup>500</sup>
a priori "reasonable" landscape choices (specifying Calabi-Yau microscopic geometry
plus integer "trapped fluxes"); if so, then specifying which it is could be done using 
1661 bits. 
Which – string theory or old style physics – involves fewer bits of
arbitrary-input information and hence is the "simpler" theory?
<p></p><p>
If we measure the electron mass (say) then the fact our
electron must fit inside the observable universe whose length scale is
set by the Einstein cosmical constant to be of order 10<sup>10</sup> light
years, implies via the energy-time uncertainty principle that the electron mass
is <i>unknowable</i> to better than ±2 parts in 10<sup>39</sup>.
It is possible to make similar estimates of error bars on the ultimate knowability
of the other constants in Baez's list.
I find that the total amount of information
in all of their values seems to be safely between 30 and 125 digits per constant, i.e.
99 to 415 bits per constant – which if X=0 would be
between 2574 and 10790 bits in all.
Note here that Veltman 1994 (page 253) incorrectly claimed
that "being an unstable particle, the W-boson's mass is inherently ambiguous at
the level of 0.1%" (due to the energy-time uncertainty principle 
&#916;t·&#916;E&#8805;&#8463;/2 using 
the W's half-life as &#916;t).  This is nowhere near true because by measuring the mass 
of a <i>million</i> W-bosons we'd decrease &#916;M by a factor of a million, plus
there is nothing inherently stopping us from measuring the masses of the W's decay products.
</p><p>
It might be a profound fact that the string guesstimate and 
old-style physics bit complexity are comparable.
</p><p>
For practical purposes, the number of bits humanity is probably going
to ever be able to measure in the Baez constant list is probably at
most about 470 digits = 1561 bits.
(Since the most accurate physical measurements ever conducted got about 18
significant figures – atomic clock frequency ratios – and 18×26=468.)
If there are really 10<sup>500</sup> candidates then
1561 bits would not be enough to back-deduce which string theory universe we live in.
<!--Therefore if the estimates and string theory are both
correct we will never be able to deduce the True Theory of Physics.-->
</p></small><p>
Loop quantum gravity (and the other approaches)
are much less ambitious than string theory,
e.g. they do not claim to predict particle masses and coupling constants, instead merely aiming
to create a quantum gravity theory.  But to do that they need
new mathematical structures and concepts
which I understand very poorly.  It is not clear humans will be able to understand them
well enough for mathematical rigor, and unless and until they can, it will
be unclear both that they are right or that they even mean anything.
</p><p>
Rain of bricks was initially even less ambitious than that; its goal was merely to cook up
the simplest possible "new improved universe" in which the foundational problems of QFTs looked
soluble.  Although it was inspired by some considerations arising from 
quantum gravity, it did not intend to <i>be</i> quantum gravity.
However, since the initial goal now looks probable to succeed, rain of bricks now 
is upgrading its ambitions: the new hope is that
perhaps rain of bricks QFTs can become good enough to handle/be
quantum gravity by simply adding gravitons to the QFTs.
</p><p>
<b>Superstring Successes:</b>
The most impressive success of string theory so far has been its
microscopic explanation (by J.Maldecena, A.Strominger, G.Horowitz et al)
of Bekenstein-Hawking black hole entropy.
The good news: For certain kinds of metrics claimed to represent black holes, 
the string calculation duplicates the area-based formula exactly.
The bad news: this has only been done for "extremal and nearly extremal supersymmetric"
black holes, or in numbers of dimensions other than 1+3,
i.e. metrics of little or no physical relevance in our universe.
The simplest and most important kinds
of black hole, by Schwarzschild (and 
<a href="http://en.wikipedia.org/wiki/Kerr-Newman_metric">Kerr-Newman</a>)
in 1+3 spacetime dimensions, have not been "stringed."
Nor has string theory (yet) been able to explain 
Gibbons-Hawking entropy of cosmological horizons,
or managed to get rid of black hole and big bang singularities (as far
as is currently known).
Also, these derivations are (a) complicated and (b)
depend on certain unproven and dubious assumptions, 
e.g. extrapolating results at small string-coupling constant to large.
</p><p>
<small>
All the nonextremal Kerr-Newman holes with a given area are interconvertible by 
classical "reversible processes" where spin and charge are added/removed from the hole.
Hence if the entropy of any one member of the family is explained microscopically, 
we automatically get
the correct entropy formula for all of them.  But this only
would be a <i>thermodynamic</i>,
not a <i>microscopic</i>, explanation of black hole entropy for all the holes reached in this 
indirect manner.
</small></p><p>
In contrast, rain of bricks explains <i>every</i> physically-relevant 
kind of black hole entropy and Gibbons-Hawking too,
extremely simply, genuinely microscopically,
and with the exact same constant for all of them – but the
exact value of this constant is not predicted de novo and instead the &#961;<sub>rain</sub>
rain of bricks parameter needs to be intentionally chosen
to yield a match to Hawking's constant.
</p><p>
<b><a href="http://en.wikipedia.org/wiki/Loop_quantum_gravity">Loop quantum gravity</a></b>
is reviewed by Smolin 2004. 
There is also an amusing "dialogue" by Rovelli 2003 comparing superstrings versus LQG.
The following discussion is based on those,
albeit I warn the reader that I know Smolin made several overclaims.
Perhaps the most remarkable thing about LQG is it is "background independent" i.e.
does not live in a some-dimensional geometry, but nevertheless hopefully
"creates" that, i.e. it is a theory <i>about</i> spacetime, not set <i>within</i>
a spacetime.   This is much braver than rain of bricks, which although
it too is a theory of what spacetime is, it is constructed intentionally to
try to resemble the usual Minkowski-spacetime as closely as it can.
This bravery causes difficult problems for LQG
about conservation laws, Lorentz invariance, unitarity
(does LQG enjoy these?) whose answers are not obvious without a clear notion of "time";
nor it is obvious that the (3+1)D spacetime that "emerges" will be "asymptotically flat"...
LQG has been able to explain the entropy of a Schwarzschild black hole, 
although it only duplicates Hawking's precise constant factor by choosing a free
parameter, the "Immirzi parameter," in the theory to match Hawking's constant.
(Later note: Smolin 2004 claims an argument of Dreyer 2003 now also obtains the value of the
Immirzi parameter in a different way, now based on black hole "quasinormal mode"
quasi-frequencies. Still later note:
A least six LQG practitioners now have written papers claiming
that was all garbage and an entirely
different value of the Immirzi parameter is wanted.)
So far, LQG has not been able to explain 
Gibbons-Hawking entropy of cosmological horizons nor Kerr-Newman (rotating)
black holes, according to all papers I saw on this, albeit Smolin claims with no 
stated justification
that "all" horizons are treated in a unified way.
LQG is known in oversimplified models to get rid of black hole singularities
and mollify singularities in Bianchi IX cosmological models.
LQG plausibly causes unitarity during
black hole Hawking evaporation, i.e. no information loss (Ashtekar &amp; Bojowald 2005;
rain of bricks also predicts unitarity albeit in a randomized fashion).
How to link LQG with matter, i.e. with every nongravitational quantum field (electrons, quarks,
photons...) has not been studied much, but it is <i>hoped</i> that because in LQG "area"
and "volume" are "quantized" and cannot be arbitrarily small
(Thiemann 1998 also finds quantized "length"), <i>therefore</i> LQG should
solve the ultraviolet problems in other QFTs.  
(Seems highly plausible, but I deny that any proof has been given.)
LQG's "spin foam history" analogue of "Feynman diagrams" has been shown to
satisfy some partial convergence and Borel summability results, e.g. in unphysically-small
dimensions. [Smolin offers no opinion, but my guess would be it is extremely unlikely
we will have Borel summability in physical LQG.]
LQG predicts that "area" is <i>quantized</i>, i.e. can only assume values in
a discrete spectrum.
Re the Einstein cosmical constant, Smolin 1995 says
LQG also predicts that 6&#960;/&#923;<sub>ein</sub> is an integer – 
although to agree with experiment is evidently an extremely
large integer, about 121 digits long.
Smolin also says "There is a proposal for a mechanism to 
dynamically tune the cosmological constant to zero, analogous to the Pecci-Quinn mechanism" 
citing Alexander 2005.
Unfortunately experimentally &#923;<sub>ein</sub>&#8800;0 and 
I have trouble seeing how an <i>integer</i> can be "dynamically tuned"!
Smolin citing 2001-2 papers by M.Bojowald:
"When couplings to a scalar field are included, 
there is a natural mechanism which generates Planck scale inflation as well as a 
graceful exit from it." 
Smolin 2004 tried desperately to plausibly
link LQG to possible experiments, but in my opinion totally failed.
</p><p>
Rain of bricks also trivially
explains quantum "decoherence" and the absence of "weirdness."  The others either do not
explain this at all, 
or if they do it is in a far less clear, and I contend incorrect, way,
cf. our "<a href="https://dl.dropboxusercontent.com/u/3507527/%3Ca%20name=" delocthm1"="">delocalization theorem</a>" 
of <a href="#whatswrong">§4</a>.
</p><p>
Rain of bricks appears to be an <i>algorithmic</i> theory that will return infinity-free
predictions of whatever you want
with as much (guaranteed) accuracy as you want.  
String theory and some of the others might not be 
(and certainly aren't yet) algorithmic.
</p><p>
Rain of bricks series convergence appears to depend inherently on
setup in de Sitter, not Minkowski, space, i.e. it <i>logically forces</i>
the Einstein cosmical constant to be (a) nonzero and (b) with
absolute value considerably smaller than 1 in Planck units, and probably also
(c) with the correct (repulsive) sign.
Most other theories have a hell of a time trying to explain (b) and
then enter a deeper level of hell trying to explain (a,c).
Although string theorists used to consider this a devastating problem, they currently 
think it is soluble by use of the guesstimated 10<sup>500</sup> choices of the
"landscape" to fit virtually any data, even data with a priori probability of
order 10<sup>-121</sup>.  It then may be necessary with strings to rely on
"anthropic" considerations to see why <nobr>|&#923;|</nobr> is small.
</p><p>
Once we know we are in de Sitter space, rain of bricks <a href="#neutmass">deduces</a> that
neutrino masses (indeed the masses of any spin=1/2 fermions) must be <i>nonzero</i>
thus explaining/postdicting this remarkable fact for the first time.
However, my argument for that presumably
could be ripped out of "rain of bricks" and transplanted into (at least some) other
physical theories, so it does not necessarily uniquely favor rain of bricks.
</p><p>
Rain of bricks set in a de Sitter space background
would <a href="https://dl.dropboxusercontent.com/u/3507527/susyforbid">seem</a>
to make supersymmetry (SUSY) <i>impossible</i>,
at least forms of SUSY where superpartners of massless bosons, are massless fermions
(and/or if these then gain "effective mass" via a Higgs mechanism).
This would seem to be in direct contradiction with superstrings (which demand SUSY).
</p><p>
Theoretical extrapolations of 
"<a href="https://dl.dropboxusercontent.com/u/3507527/runcoupconstplot">running</a> coupling constants"
(<a href="#brickshape">§13</a>)
have been claimed by some to favor MSSM embedded in a GUT, thus favoring supersymmetry.
But we have seem that a twice-as-tight 4-way meet – to within ±10% –
of the 4 coupling constants (strong, weak, electromagnetic, gravity)
arises from the plain standard model <i>without</i> SUSY or GUT, at a
scale of about 5 times the Planck length.  This is quite remarkable.
Why should these couplings, which begin quite unequal at technologically accessible energies,
all become near-equal at essentially the scales postulated for "bricks"?
While is not logically necessary in rain of bricks for the fundamental forces all
to have near-equal strengths, it surely seems more "natural" if they do –
and certainly the theory is simpler if there is only <i>one</i> bare coupling consant, 
not four – hence this
seems to be some kind of evidence rain of bricks is correct physics.   And since the 
same meet is worse if we assume SUSY, this also can be taken as evidence <i>against</i>
SUSY (and hence against superstring theory).   To stand on firmer ground by
making a weaker statement: this near-meet
at near Planck length seems to favor some kind of quantum gravity as the underlying
explanation for "planck scale UV cutoffs" tgat permit
"avoiding renormalization" to rescue QFTs
from their internal problems.
</p><p>
<b>Summary of successes:</b> This, the very first paper on rain of bricks, has
achieved greater success than all
the decades of work on string theory with hundreds of
authors combined! – at least, provided we measure by the yardsticks
we just discussed.
</p>
<a name="reportcard"></a>
<table>
<caption>
<b>Table 12: ABCDF "report card" on physics.</b>
A.Strominger's "report card" on string theory was in a 2010 
<a href="http://media.physics.harvard.edu/video/index.php?id=COLLOQ_STROMINGER_091310.flv">lecture</a> video from Harvard and contains grades 
(A,A,A,A,A,B,D,D,D,F,F);
B.Greene's was table 4.2 of his 2011 book <i>The Hidden Reality</i>
and consists of six "A"s ("excellent") 
and two "I"s ("indeterminate, predictions not yet available").
Evidently Strominger and I are much tougher graders than Greene!
Greene also indicated for his goals whether each was "required" (R) or not (N)
or partly required partly not (R/N); I've added (W) to indicate something I think is or should be
required, even though Greene might disagree.
Grades for LQG are based on my (biased?)
guesses from Smolin 2004 of how Smolin would have graded it.
</caption>
<tbody><tr bgcolor="pink"><th>Goal</th>
<th>Strominger</th>
<th>Greene</th>
<th>My string grade</th>
<th>LQG</th>
<th>Rain of bricks</th>
</tr>
<tr><td>Unambiguous, falsifiable prediction(W)</td>
<td>F</td>
<td>I</td>
<td>F</td>
<td>F</td>
<td>C</td>
</tr><tr><td>Potential for Large Hadron Collider signal</td>
<td>D</td>
<td>?</td>
<td>D</td>
<td>F?</td>
<td>F</td>
</tr><tr><td>Black hole entropy puzzle(R)</td>
<td>B</td>
<td>A</td>
<td>B</td>
<td>B</td>
<td>B</td>
</tr><tr><td>Black hole info-loss puzzle(W)</td>
<td>B</td>
<td>A</td>
<td>?</td>
<td>A</td>
<td>A</td>
</tr><tr><td>Applications/inspirations for pure math(N)</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>B</td>
<td>B</td>
</tr><tr><td>Applications/inspirations for other areas physics</td>
<td>B</td>
<td>A?</td>
<td>B</td>
<td>B</td>
<td>B</td>
</tr><tr><td>Unification(R/N)</td>
<td>A</td>
<td>A</td>
<td>A</td>
<td>B-</td>
<td>B-</td>
</tr><tr><td>Uniqueness</td>
<td>D</td>
<td>?</td>
<td>D</td>
<td>C+</td>
<td>B+</td>
</tr><tr><td>Solve cosmical constant problem(W)</td>
<td>F</td>
<td>?</td>
<td>D</td>
<td>D</td>
<td>B-</td>
</tr><tr><td>Understand big bang, origin of universe</td>
<td>D</td>
<td>?</td>
<td>D</td>
<td>C</td>
<td>F</td>
</tr><tr><td>Solve &#8734;/renormalization foundation problems(R)</td>
<td>A</td>
<td>A</td>
<td>B?</td>
<td>B?</td>
<td>A</td>
</tr><tr><td>Nonperturbative convergence(R)</td>
<td>?</td>
<td>?</td>
<td>F</td>
<td>C?</td>
<td>A</td>
</tr><tr><td>Explain particle properties(N)</td>
<td>F</td>
<td>I</td>
<td>D-</td>
<td>F</td>
<td>D</td>
</tr><tr><td>Explain decoherence, lack of quantum weirdness</td>
<td>?</td>
<td>?</td>
<td>F?</td>
<td>F?</td>
<td>A</td>
</tr><tr><td>Hopes for mathematical rigor, <i>algorithmicity</i>(W)</td>
<td>?</td>
<td>?</td>
<td>F</td>
<td>D</td>
<td>B</td>
</tr><tr><td>Simplicity and Usability(W)</td>
<td>?</td>
<td>?</td>
<td>F</td>
<td>D?</td>
<td>B</td>
</tr><tr><td>Experimental confirmation(R)</td>
<td>?</td>
<td>I</td>
<td>F</td>
<td>F</td>
<td>D</td>
</tr>
<tr><td>Unitarity(R)</td>
<td>?</td>
<td>?</td>
<td>A?</td>
<td>unknown</td>
<td>A</td>
</tr>
<tr><td>Lorentz invariance</td>
<td>?</td>
<td>?</td>
<td>A</td>
<td>unknown</td>
<td>A</td>
</tr>
<tr bgcolor="pink"><th>Average (ignores omitted grades)</th>
<td>C</td>
<td>A</td>
<td>C-</td>
<td>C-</td>
<td>B-</td>
<!--
a=1; b=2; c=3; d=4; f=5;
strom=(f+d + b+b+a+b+a + d+f+d+a+f)/12;
msg=(f+d+b+a+b+ a+d+d+d+f +d+b+f+f+f+f+a+0.3+a)/18;
lqg=(f+f+b+a+b+ b+b+c+d+c +c+f+f+c+d+f+b)/17;
rb=(c+f+b+a+b+b+b+b+b+f+a+a+d+a+b+b+d+0.3+a+a)/19;
-->
</tr>
</tbody></table>

<p>
<b>"Beauty," uniqueness, radicalness, and correctness:</b>
String theory has been much praised for its "beauty" and apparent uniquely-forced structure,
the result of several "mathematical miracles."  
String theory has introduced or at least popularized a large number of radical and amazing
(crazy? useful?) ideas such as 
extended objects as particles, "branes,"
extra dimensions, 
gravity possibly propagating "outside" our universe/brane, and the "AdS/CFT
correspondence"; and it has yielded new 
"<a href="http://en.wikipedia.org/wiki/Mirror_symmetry_%28string_theory%29">mirror symmetry</a>"
mathematical ideas which have been able to solve
some previously-intractible math problems (previously thought
unconnected to string physics).
Further, in principle it does make
a large number of physical predictions, such as ruling out huge numbers of low energy
QFTs, predicting all particle types, etc; it is just that actually determining what those
predictions and ruling-outs are, to the extent we can actually get something
feasible to test, seems beyond human power and conceivably non-algorithmic i.e. beyond 
Turing-machine power too.
One <i>can</i> use "dimensional analysis" to make crude predictions
of particle masses in string theory, 
the result being that every nonzero particle mass is predicted to
be order 1 in Planck units.  This is too high, for every particle known so far, by between
18 and 30 orders of magnitude!  
String theory also predicts supersymmetry (SUSY), i.e. every particle has a "superpartner";
but no superpartners have yet been found, albeit there is some hope that "dark matter"
particles will eventually be detected/understood and perhaps are a superpartner of something.
String theory also predicts an infinite number of particle types 
but experimentally only a finite set are known. 
SUSY alone also predicts new particle types plus new
particle decay modes such as &#956;&#8594;&#947;e
forbidden by the plain (non-SUSY) standard model; but again, none of them have ever been seen.
Perhaps string theory predicts
only a <i>finite</i> set of <i>stable</i> particle types, but as far as I can tell, that
fundamental question has never been answered and
it is not obvious, at least not to me.  For example the usual classical physics 
model of small-amplitude lossless vibrations of a violin string
has an infinite spectrum, 
and no mode can "decay" into any other mode in the sense that all 
are orthogonal.
If there is a finite stable spectrum, one has to ask: 
why haven't the string theorists computed it yet?
String theory does predict gravitons – massless spin-2 particles
– thus "creating gravity" even though that had not originally been intentionally designed in,
which some have regarded as impressive.
String theory's misprediction (again,
only at the crudest level of dimensional analysis)
of the Einstein cosmical constant is even more severe: 121 orders of magnitude too high!
<!-- At least one neutrino has mass>0.04eV, 3*10^29 times that is a PLanck mass -->
It is a measure of theorists' desperation that a theory which, under normal
circumstances, would have been dismissed instantly as obvious garbage by 99% of physicists,
is leading the pack.
Some people, on the other hand, have denounced the
"ugliness" in string theory (e.g. is spacetime being
10 or 11 dimensional "beautiful" or "ugly"?);
and the postulated "landscape" of 10<sup>500</sup> flavors
somewhat diminishes its uniqueness.    
(Also,  the arguments for string theory's uniqueness are as usual in string 
theory not a proof– they essentially never prove any theorem – 
but if they were a proof then they woild depend on restrictive assumptions, i.e. 
string theory is unique <i>within</i> a certain class of models.  I strongly suspect that 
by allowing a wider and uglier class of models, string theory will stop
being unique, but still can "work."  If so, then string theory's so-called 
uniqueness really is of the same nature as rain of bricks' so-called uniqueness.)
If the crude estimate
"10<sup>500</sup>" really is "infinity" (which as far as we presently know is entirely
possible) the uniqueness arguably is gone.
Most or all these philosophical criticisms
(possible nonalgorithmicity, the "landscape," etc) do not
necessarily prevent string theory from being The True Theory, but they do suggest that if it is 
The True Theory, that's horrible.
</p><p>
The purveyors of loop quantum gravity have also got their own ways in which their theory seems 
"uniquely natural" and "beautiful" and is the beneficiary of "mathematical miracles."
</p><p>
Rain of bricks is exceedingly <i>non</i>radical, especially in contrast to string theory;
it stays with old-style QFTs to the maximum extent it can, only altering them and adding new
ideas in ways that seem logically forced.  (In doing so, it takes rigorous mathematics
and computer science desires
seriously, unlike string theory which seems to have been developed with contempt for them.)
</p><p>
Although when I began investigating rain of bricks I thought there would probably be an 
infinite number
of flavors of this picture, i.e. there would be nothing unique about it –
the investigation has revealed that there are <i>few</i> candidates for the "simplest" kinds
of rain of bricks.
There are a very large number of ways to try to do it that fail.  
We've proven theorems that
numerous alternative ideas which initally sound similar (lattice gauge theory, 
different sorts of distance formula than ours, Kaluza Klein...) either cannot be right or
would at least suffer very heavy handicaps immediately.
So only a small fraction work. 
And of those, only a <i>very</i> small number 
seem "simple and natural"...
indeed there seems to be <i>uniquely favored</i> candidate.
So there is a surprisingly large amount of uniqueness 
and a nonzero amount of "mathematical miraculousness" to rain of bricks, too.
</p>

<a name="resume"></a>
<h3>42. Resume of my currently-most-favored version rain of bricks laws  </h3>

<blockquote>
   This completes the story of [my] development of the space-time view of quantum 
electrodynamics...  It is most striking that most of the ideas developed in the course
of this research were not ultimately used in the final result. For example, the half-advanced 
and half-retarded potential was not finally used, the action expression (1) was not used, 
the idea that charges do not act on themselves was abandoned. The path-integral formulation
of quantum mechanics was useful for guessing at final expressions and at formulating the 
general theory of electrodynamics in new ways – although, strictly it was not absolutely
necessary. The same goes for the idea of the positron being a backward moving electron,
it was very convenient, but not strictly necessary for the theory...
<br> &nbsp;&nbsp;&nbsp; –
R.P.Feynman, Nobel prize lecture 1965.
</blockquote>
<p>
<small>
We list the underlying postulates of rain of bricks QFT physics in brief, omitting
possible alternate forms, motivation, and wider discussion (for which see earlier sections).
</small>
</p><p>
<b>Postulate I (Arena):</b>
The usual Minkowskian coordinate 4-tuples (t;x,y,z) are <i>not</i>
the arena on which physics happens, although it seems that way to observers with 
imperfectly-acute vision.
The true arena is something called the "rain of bricks"
which we shall now construct.
</p><p>
First, within de Sitter (1+3)-space (with some fixed repulsive value of Einstein
cosmical constant &#923;) "rain" 
<a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a>-random points 
down at some constant
number-density &#961;<sub>rain</sub>
(per Lorentz-invariant 4-volume unit).  
This 4-density is a fundamental physical constant,
presumably of order 1 point per unit 4-volume
measured in Planckian length and time units.
</p><p>
Second, associate with each raindrop point, a 1-dimensional "brick"
which is abstractly (the perimeter of) a circle with some fixed perimeter (the "brick length")
which is another fundamental physical constant of order 1 Planck length.
(Other brick dimensions besides 1, namely 2 and 3, might be better, 
e.g. the 2-dimensional surface of
a sphere of radius of Planck order; but at present I prefer 1.)
Each such brick is not embedded in de Sitter space; it exists outside it.
Distances within the brick are spatial (not time) and measured "along the circle."
<i>Only</i> points on the bricks are part of the arena of physics.
</p><p>
<b>Postulate II (Squared Distance):</b>
The (pseudo)distance between two points A and B in that arena is given by the following formula:
</p><center>
Distance(A,B)<sup>2</sup> 
=
DeSitterDistance(between the two raindrop-points respectively 
associated with A and B's bricks)<sup>2</sup>
<br>
+
GeodesicDistance(from A to B within the brick, using the fact A's and B's bricks are congruent
and hence can be mentally superimposed)<sup>2</sup>.
</center><p>
<b>Postulate IIIa (No two vertices in same brick):</b>
The vertices of any particular Feynman diagram must lie on <i>distinct</i> bricks.
In other words, no brick is allowed to contain more than one vertex of
any given Feynman diagram.
Physics occurs by means of (1) particles "propagating" between arena-points using
the usual de Sitter space propagator formula appropriate for a particle with that mass and
spin, but using
the squared-distance in postulate II within that formula instead of the plain de Sitter 
squared-distance formula (Feynman diagram edges) and (2) particles "interacting" at 
Feynman-diagram
vertices (i.e. "emission" and "absorption"); this latter occurs only on bricks.
Also note: "bare" charges and masses are to be used, see 
<a href="#crudenumer">§22</a>
for our numerical estimates.
</p><p>
<b>Postulate IV (Uniformity*):</b>
Quantum fields are uniform within each brick.
[The asterisk is because I suspect postulate IV is not really needed
because it essentially arises automatically from the others.]
</p>

<!--
Scharf

Pictures

Huang

Denef-Douglas

Is the action finite for smeared propagators? No.

Gluon attraction if chg sign alphaS?

Can we use the gauge-field lagrangian in P+S to deduce PT symmetry?

Smirnov stuff all wrong need to redo???

Can we deduce brick size from probability conservation??? No. OK, from what?

W boson is charged and will lead to original N! Dyson collapse secaenrio.
Complication: Ws decay to either quark-pairs or lepton-neutrino pairs,
the mean lifetime is 3&times;10<sup>-25</sup> sec.  
So they have to survive until the (finite time) collapse,
but this happens with some nonzero probability.

Power series ok in cavity (assuming conjectures).  
Scharfian thing ok to get finite operator product.

Gamma product ratio  lemma?

emails.

Rosen/Wright is another diml reg papr.

Action/lagr and Hamiltonian?

Smeared-Prop changes stuff and is not a Dirac PDE solution?

String/M-theory "landscape" with 10<sup>500</sup> different versions, only
a few compatible with the existence of humans? "Anthropic principle"?

PT-sym QCD using Folland and Bender-Milton like QED?

Strings and Cunningham???  Strings have no mass, right, since that would be a new physical 
parameter, which is not allowed? But ok the extra dimensions fuck up Cunningham.

Encyclopedia?  It is randomized because physics is nondeterministic.

String thy fails to explain decoherence &sect;???.

Write a Nollert/Motl program.
Kerr-Newman BHs.
QCD PT-sym version???
The Nyquist thm is for 4D Fourier but actually since E is determined by p and m,
we only have a 3-parameter set of allowed mode indices.  Modes outside
that set are nonsolutions and are not propagated.
No more: the Hamiltonian.   The S-matrix fills the gap.
Extrap down to planck scale?
Curved space?
IR infinities: nonexistence required of our QFT.  Need "locality."
Nicer simpler than superstrings???
Superstring ambitions:

Add numbers to sections thms etc??
Ugliness??
Max momentum and special relativity.
Essentialness of not using a lattice
(Tho D4 looks like a good lattice if must)
SUSY and cosmical constant?
Proof-sketch not full proof???
Can we at least place it in a computational complexity class???
Future work: more QFTs, genuine proof, and what about non-renormalizable QFTs and the 
vacuum mass density?
S
-->

<p>
Given these postulates, physics is done by the usual Feynman-diagram-based methods
(in position-space, using "bare" masses and charges,
and without "renormalization" or "regularization").  
Integrations over spacetime are of course 
<a href="#rainbasedinteg">replaced</a> by integrations over the rain of bricks arena
(i.e. summation over different bricks and integration over each brick).
Only two things happen:
(1) "propagation" and (2) "interaction."  Propagation is done using the propagator formulas 
appropriate for de Sitter spacetime, but using the rain of bricks distance formula in place
of the 
<a href="#desitdistformula">de Sitter</a>
distance formula.  Interaction consists of obsorption or emission
of a boson with an amplitude obtained from the usual formulas in terms of propagated
wave functions and/or their x,y,z,t partial derivatives.
</p>

<a name="openprobs"></a>
<h3>43. Problems for future workers </h3>

<p><b>1.</b>
QED<sub>N</sub> Feynman |integrals| should enjoy computable
upper <b>bounds</b> of the form 2<sup>O(N)</sup>N!
at least <i>provided</i> we only consider the "Euclideanized" forms of them and only allow
positive-mass particles.
<!--
This follows by combining inequalities in Zimmermann 1968 with easy lemmas such
as (a) if the "norm" of a multivariate polynomial is the sum of its |coefficients| then
the norm of the product of two polynomials is upper bounded by the product of the norms;
(b) if an N&times;N matrix has O(N) nonzero entries, each of whose absolute values is O(1),
then its |determinant| is upper bounded by 2<sup>O(N)</sup>.
Note that this kind of upper bound is tight in the sense that Lautrup 1977's lower bound
(in renormalized QED<sub>N</sub>)
is of the same form.   
[Later note: apparently this result was not known, which may mean I'm wrong or may
mean I should prove it...???]
-->
</p><p>
<b>1A:</b> 
Can tighter bounds, e.g. 2<sup>O(N)</sup>, be obtained if we forbid renormalization
and only discuss |integrals| which still are finite?
</p><p>
<b>1B:</b> Find reasonably explicit and tight upper bounds if we now also permit
zero-mass particles and if we work in Minkowski space.
It is known (e.g. Lowenstein &amp; Zimmermann 1975) that these integrals yield finite results
at least in suitable distributional senses,
but their proof techniques do not seem to yield explicit bounds.
</p><p><b>2.</b>
Find the <i>exact</i> relation between &#961;<sub>rain</sub> in rain of bricks physics,
and <b>black-hole entropy</b>, thus deducing the former.
</p>
<a name="screenscatprob"></a>
<p><b>3.</b>
Seek feasible ways in which rain of bricks physics can be 
<b>experimentally confirmed/falsified.</b>
E.g, how can the exact size and shape parameters of the bricks, be deduced from experiment?
(And once this is done one should be able, computationally, to deduce the "bare masses"
of all the particles...  could it be that only one parameter set is compatible
with the measured masses of the known particles at different energies?)
</p><p>
We have in this paper already noted a substantial number of theoretical 
predictions that rain of bricks physics makes, which differ from other physical theories.  
However, so far, all those examples have involved very large or small numbers, which
seem to render the requisite experiments infeasible.  There are also numerous predictions
rain of bricks makes which are so vastly obviously true that experiments seem superfluous,
such as the lack of quantum weirdness in human experience, nondeterministic
nature of physics, the finiteness of
the electron mass, the positivity of neutrino masses,
the positive (repulsive) sign of the Einstein cosmical constant,
and the stability of matter and the vacuum.  The trick is to find
the middle ground.  Probably the best such idea I thought of
is the electron dephasing-rate estimates in <a href="#decoherence">§32</a>.
We now examine one possible type of experiment aimed at distinguishing
old-style continuum spacetime from rain of bricks metrical stucture.
</p><p>
</p><blockquote>
The "atom" is the hypothetical smallest constituent of matter. 
Competing with the atomic hypothesis is the hypothesis of infinite divisibility of matter.
<br>&nbsp;&nbsp;<b>–</b> 
Old Encyclopedia Brittanica, "Atom."
</blockquote><p>
Historically,
early evidence for the existence of atoms, and estimates of their size (1880-1910), were got from 
observations of the behavior of gases and liquids when temperature and pressure were altered;
Millikan's oil drop experiment measuring the electron charge [1909];
oil monolayers;
X-ray diffraction by crystals [1912];
and the observation of "Brownian motion" [R.Brown 1827, A.Einstein 1905, J.Perrin 1909].
</p><p>
Of those historical ideas, only the last two seem generalizable to investigate our question of
whether <i>spacetime</i> is "atomic" (rain of bricks) or an infinitely divisible continuum
(the old model).
</p>
<a name="FIGtwoslit"></a>
<img src="WarrenSmithQED131123_files/TwoSlitDiffract.png" align="left">
<p>
To understand the difficulties confronting an experimentor, 
let us consider the problem of detecting
the granularity of spacetime via a 2-slit diffraction experiment. See 
<a href="#FIGtwoslit">figure 12</a>.
Consider a fast-moving lepton of energy M. 
</p><blockquote><small>
(One could also consider proton projectiles. Using leptons is motivated by
our earlier <a href="#decoherence">estimates</a><a> that thought electrons
since point particles should suffer more dephasing than protons.  The analysis for muons
or tauons (also leptons, i.e. point particles)
should be at first order the same as for an electron, but with the advantage for
our present purpose that they are much 
more massive hence have shorter Compton wavelengths.
Specifically 
<nobr>m<sub>&#964;</sub>/m<sub>e</sub>&#8776;3477</nobr>
and
<nobr>m<sub>&#956;</sub>/m<sub>e</sub>&#8776;207.</nobr>
</a></small></blockquote><p><a>
Our lepton starts at A, moves at relativistic speed, and diffracts around a 
cylinder of (tiny) diameter B.  
It then travels (large) distance L further before impacting viewing screen C.
</a></p><p><a>
First assume the old model – continuum spacetime.
At the central point on this screen the two shortest paths have equal length and hence we expect 
"constructive interference" and a probability peak.
There will then be a sequence of "diffraction fringes." The next probability peak will be
distance s away along the screen.  We may solve for the approximate value of s
by demanding that the two paths have path length difference of 1 de Broglie wavelength
<nobr>&#955;&#8776;h/(Mc).</nobr>
Hence
</a></p><center><a>
&#955; = ([B/2+s]<sup>2</sup> + L<sup>2</sup>)<sup>1/2</sup> - ([B/2-s]<sup>2</sup> + L<sup>2</sup>)<sup>1/2</sup>
</a></center><p><a>
Expanding the right hand side in powers of s, then truncating after the first term 
(i.e. regarding s as small), then solving for s, we find
</a></p><center><a>
s
&nbsp; &#8776; &nbsp;
(B<sup>2</sup>+4L<sup>2</sup>)<sup>1/2</sup>&#955;/(2B)
&nbsp; &#8776; &nbsp;
L&#955;/B.
</a></center><p><a>
Second, instead assume rain of bricks. 
Then our lepton as it moves along the paths will be dephased.
For an electron, as we </a><a href="#decoherence">saw</a> 
earlier, the dephasing time (in its own rest frame should be 60×10<sup>±6</sup> 
years, so an interferometer a mere 60 lightyears long should suffice to see the difference
– clear versus blurred out interference fringes.
Of course to do that you'd 
need to shield and refrigerate your apparatus enough that other sources of dephasing 
(and other) noise become comparatively neglectible.
</p><p>
The reason I did this analysis was not to show this experiment was feasible.  It
instead was merely to demonstrate that even if we can only reach
energies far smaller than Planck energy (i.e. energies we know nature actually does access)
and lengths far larger than the Planck length (which, again, nature accesses)
<i>that by itself does not prevent us from confirming or denying rain of bricks.</i>
</p><p><b>4.</b>
We have argued 
that rain of bricks should allow placing IRFrQFTs on a 
<b>rigorous</b> foundation, converting them into well-defined algorithmic theories.  
However much remains to be done before that can be claimed to have been fully accomplished.
</p><p><b>5.</b>
Can we, by putting both rain of bricks and gravitons (massless spin-2 particles)
in a pot and stirring, cook up a well behaved
<b>grand unified quantum gravity</b> theory?
How should the Einstein cosmical constant be handled?
<!--Should the background instead be de Sitter space?-->
</p><p><b>6.</b>
Does (1+3)-dimensional 
<b>de Sitter</b> spacetime <i>really</i> (<a href="#desit">§35</a>) 
rule out massless fermions and all supersymmetric
theories of physics? What happens in other dimensions?
We presented nonrigorous "proofs" that de Sitter spacetime "cures"
infrared infinities of rQFTs.
Can they be turned into genuine proofs?
</p>
<a name="opprobaction"></a>
<p><b>7.</b>
I suspect rain of bricks QFTs can be formulated in a 
<b>"Lagrange action-stationarization"</b> manner,
with an action formula that involves sums of 
integrals over brick-<i>pairs.</i>  
Do this (or show it
cannot be done).
</p><p>
Discussion:
First, I doubt there is any way to formulate rain of bricks QFTs in a "Hamiltonian" manner
– 
or at least if you can, then it would require some new interpretations/notions of
the word "Hamiltonian."
This is because in rain of bricks
there is no notion of a "universal continuum time," but
<a href="http://mathworld.wolfram.com/HamiltonsEquations.html">Hamilton's equations</a>
seem inherently to need one.  (But see <a href="#hamsand">§30</a> for some discussion of
how the "Hamiltonian" can be resuscitated in rain of bricks.)
Incidentally, this non-Hamiltonicity is probably a good thing because 
one major stumbling block for attempts to unify GR-gravity with quantum theory was
the desire by the latter for universal time as in flat Minkowski spacetime, versus
the former's refusal to provide it.  By abolishing continuum time completely, rain of bricks
smashes both these twain obstacles, hopefully/apparently now permitting
unification to proceed.
</p><p>
Second, I will now sketch an approach, 
some version of which hopefully works, for reformulating rain of
bricks QFTs in a "Lagrange action-stationarization" manner.  
Recall in the usual Lagrangian-density (LD) formulation of QFTs, the LD formula
can be mentally partitioned into two summands: 
</p><ol type="i">
<li>
The "fields only" part
</li><li>
The "interaction between particles" part
(viewing quantum "fields" and "particles" as the same thing)
</li></ol>
<p>
In the rain of bricks picture, the sole purpose of part (i) is to force/specify that the
fields should propagate using the correct propagator formula.  Rain of bricks
already demands/obeys that, so we shall not need part (i). 
The remaining action then would, in pre-rain-of-bricks QFTs,
just be a spacetime 4D integral of part (ii) of the LD.
With rain of bricks, we as usual replace that integral with an
appropriately scaled sum over all raindrop-bricks,
and also integrate within each brick.  The integrand involves certain products
of quantum fields, for example both the Maxwell photon field 
A<sub>&#956;</sub>
and the Dirac electron field
&#968;
and its derivatives
are involved in the interaction term of the QED Lagrangian density
(e.g. see EQ6.27 page 140 of Folland 2008).
We agree to handle such terms by using for the first field, its
actual value at that point on that brick,
and for the second field, its value <i>as derived from propagation</i> from source
points on <i>other</i> bricks.
If we do this, then the "action" thereby is expressed
as a formula that involves sums of 
integrals over brick-<i>pairs</i>.
This formula is analogous to the formula in classical electrostatics expressing the 
Coulombic energy 
of set of point charges as a sum over 
charge-pairs, but with the differences that
</p><ol type="a">
<li>
we do not use charge-pairs, we use brick-pairs; 
</li><li>
and while the electrostatic charges were regarded as
co-existing and interacting (via the Coulomb reciprocated-distance-based potential formula)
all at the same time, the bricks exist only at different instants
and interact across time and space via the field-propagator formulas.
</li></ol>
<p>
Note that in the classical electrostatics scenario, charges only act on <i>other</i> charges
so there is no issue about "self-force"; similarly in the rain of bricks scenario
we only propagate fields from other bricks thus enforcing 
<a href="https://dl.dropboxusercontent.com/u/3507527/post3">postulate IIIa</a>.
A closer analogy than to Coulomb electro<i>statics</i>
instead is to <a href="http://en.wikipedia.org/wiki/Adriaan_Fokker">Fokker</a>
1929's formulation of classical electro<i>dynamics</i>
using 50-50 retarded and advanced "action at a distance" 
between point charges with no fields.  
(Note: attempts in the early 1900s by W.Ritz and H.Tetrode
to get rid of the "advanced" potentials and use only the "retarded" ones
succeeded <i>but</i> only at the cost of sacrificing stationarized-action principles.)
Fokker's whole idea was redone by Wheeler &amp; Feynman 1945 &amp; 1949
as their "absorber theory" of classical electrodynamics.
It also was redone by Friedman &amp; Uryu 2006 for "post-Minkowski" 
approximate general-relativity
(classical gravity) instead of classical electrodynamics.
And Ryder 1974 gave a (better?) electromagnetic action principle inequivalent to but
highly reminiscent of Fokker's.
In Fokker's and Ryder's reformulations of Maxwell's laws, 
the trajectories of the point-charges
are the ones that extremize the Fokker (or Ryder) action functional.
</p><p>
Note that our "action" in this plan would be expressed on the rain of bricks arena alone,
i.e. the bricks only, and <i>not</i> Minkowski space which is higher dimensional.
If the bricks were &lt;2 dimensional then the action integral could
actually be lower dimensional than a Minkowski spacetime integral even though done
on brick <i>pairs</i>.
</p><p>
This plan intends to make the effect 
of "stationarizing the action" arising from
part (ii) of the LD
be equivalent to "rain of bricks QFT."
We also intend to have the effect of part (i) of the LD already be built in
because of the propogation convention.
There may be some design-freedom in this plan (one question the problem-solver will need
to analyse). 
A perhaps fruitful approach would be to write down a fairly general formula
including many undetermined coefficients, for a putative action; then
solve for the coefficients.
Again like in our <a href="#desit">§35</a>
the way to generalize to de Sitter spacetime
presumably is to demand conformal invariance in the massless limit and hence
Ryder's (not Fokker's) action presumably would be favored; I say this to indicate
that solving this problem is not going to be immediate and trivial (which is one reason
it <i>is</i> listed as a "problem," i.e. I have not already done it).
Also, note that even if/when we succeed in reformulating rain of bricks QED via an action principle,
it will not necessarily be very useful.  To see why, note that (e.g.) Fokker and Ryder's 
action principles for classical electromagnetism both seem <i>non-algorithmic</i>
(or at least, every currently known algorithm, if there are any, involves first
converting to a different formulation,
i.e <i>avoiding using</i> Fokker or Ryder variational principles).
</p><p><b>8.</b>
Understand the behavior of QED and QCD <b>perturbative series</b> asymptotically 
at <b>high order</b>, and
what happens to them with rain of bricks.
We already have quite good answers to these questions, but only if
various nonrigorous arguments are accepted.
</p><p><b>9.</b>
Understand the <b>computational complexity</b> of rain of bricks physics.
Our arguments seem to suffice to show computing whatever you want to X decimals of accuracy 
ought to be in PSPACE, i.e. solvable by a Turing machine using memory polynomially 
bounded in terms of X and
the number of input and output particles, assuming all input is available on input tapes to
an infinite number of avialable places of accuracy.
Indeed PSPACE should be improvable to P<sup>#P</sup>.  
Probably this can be further improved to BQP (cf. Smith 2???), 
although I have not attempted to do so;
if so that ought to be the best possible result in terms of the usual complexity classes.
More generally, the problem is not merely to show that rain of bricks QFTs are 
<i>algorithmic</i>, but to
get <i>efficient</i> algorithms!  This paper, essentially, has not tried.
It is somewhat galling that old style perturbative/renormalization QED methods are
better for <i>practical</i> computation that rain of bricks (at least presently).
The reasons:
</p><ol type="a">
<li> 
Rain of bricks' advantage of causing series convergence
does not terribly matter for practical QED computations because they've not yet gone
to high enough order 
to encounter the series divergence.  (In large QED computations, or in QCD, convergence really
would be a big advantage...  but it is not clear the advantages could be garnered inside
computations small enough to be practical.)
</li><li>
Rain of bricks no longer uses renormalization.  While this seems a clear theoretical/foundational
advantage (plus may enable handling gravitons), in practice renormalization is still very useful
and approximately valid in rain of bricks' view if regarded merely as an
<i>speed-and/or-accuracy-enhancing numerical method.</i>
I.e, yes, in principle rain of bricks using bare masses and charges and no renormalizing
ought to predict everything accurately (if rain of bricks is physically correct).
<i>But</i> the way in which that happens is computationally large and complex.
We can shortcut most of that computation by renormalizing.  To make an analogy
from more pedestrian physics:  organic chemistry is believed to be predicted
to very high accuracy by quantum mechanics via Schrödinger's equation.   
However, chemists have found that other computational methods usually 
give them same-accuracy results much faster, at least within the accuracy and runtime
ranges involved for most practical computations so far. These faster methods can be regarded as 
approximations to the truth.  The accuracy of these faster methods can 
be enhanced by "tuning" them by fitting
certain parameters inside the approximate models, to experimental facts
(or to data produced by truer models run for much longer compute time).
Indeed, without such tuning, most computational chemistry codes would perform far worse.
Similarly, old-style renormalized QED is an approximate model (versus rain of bricks 
being the truth, if it is) and the renormalization is simply a method
of "tuning" itself to match some experimental values, namely the 
mass and charge of the dressed electron.  This tuning causes such a large improvement 
(in fact infinity) that it for old-style QED is essential.  However, rain of bricks
itself needs no tuning (once its underlying bare charge and mass parameters 
and brick parameters are known).
Rain of bricks QED should always deliver unique arbitrarily-accurate
predictions, unlike old-style QED which has built-in accuracy limitations and when driven to 
extremes of high energy (near Planck) or particle count (millions), totally breaks down.  
But because the Planck energy is so high and since practical calculations so far have involved 
only a few particles, old-style QED for most purposes for which it is practical to apply it
today, remains remarkably effective.
</li></ol>
This suggests that a productive course 
would be to re-examine the approximate, renormalization-based,
QED algorithms invented so far, trying now to <i>prove</i>
approximation-error bounds relative to the
overarching true rain-of-bricks theory.  That way we'd enjoy the combined benefits
of the best calculation algorithms now known, plus provable meaning.
<p></p><blockquote>
[Quantum field] theory, or rather the successful part, is perturbation theory.
<br> &nbsp;&nbsp;&nbsp; – Martinus Veltman (Introduction to <i>Diagrammatica</i>).
</blockquote><p>
We can also make similar but perhaps 
more useful remarks about today's "lattice gauge theory" computations.
An obvious way to try to approximately compute rain of bricks physical predictions, is
to compute them with much smaller 
&#961;<sub>rain</sub>,
so that we have far fewer raindrops to deal with, so that the computation becomes feasible.
(For raindrop density of order the Planck density, this is equivalent to
computing with artificially larger Planck length, i.e. artificially stronger gravity.)
This can be done for various "Planck lengths" and then we can extrapolate the results down to
the actual physical value of the Planck length 1.6×10<sup>-35</sup> meters.
Further, by "renormalizing," e.g. employing different electron bare mass and charge parameters at
the different scales, this process can be made to work better.   
The lattice gauge theory computors at present
actually use a point <i>lattice</i>, which seems a bad idea because
lattices have infinitely many repeated distances (unlike random points which have none)
and are not Lorentz invariant (e.g. not rotation-invariant), and for Euclidean-space lattices
never include any distances shorter than the nearest neighbor spacing in the lattice.
Some Lorentzian lattices (i.e. in Minkowski spacetime) suffer the further 
embarrassment, which seems likely to be highly damaging, of having infinitely 
many lattice points at pseudo-distance <i>zero</i>
from the origin.  For example, 
</p><center>
(1,4,8;9)  &nbsp; and &nbsp; (2,3,6;7)  &nbsp; and &nbsp; (1,2,2;3)
 &nbsp; and &nbsp; (2,6,9;11)  &nbsp; and &nbsp; (0,0,3;3)  &nbsp; and &nbsp; (0,3,4;5)
</center><p>
all have pseudo-distance 0 from (0,0,0;0) 
if we use the integer lattice <b>Z</b><sup>3+1</sup>.
On the other hand, lattice point sets do have the advantage of being
"more uniform" than random point sets, which 
better-approximates a random point set with much greater density.
To try to gain this advantage of lattices without the disadvantages, Bill Gosper suggested 
using points equispaced along a spacefilling curve, such as a Hilbert curve,
at a fixed irrational spacing.  (I also thought of almost the same idea but 
in a different application: 
high dimensional integration can be regarded as 1-dimensional integration 
along a spacefilling curve.
This might be a better numerical method than monte carlo integration, 
especially if used "adaptively."
Gosper's additional trick of using an irrational spacing generically 
assures that no distances are repeated
and there are no zero-distances.)
Gosper supplied the following picture:
</p>
<a name="gosperhilbpic"></a>
<img src="WarrenSmithQED131123_files/Gosper.htm">
<p>
(It is also possible rain of bricks ultimately will enable completely new algorithms which
will obsolete all of old-style QED, but if so, I do not know what they are.)
</p>
<a name="curvdeqsprob"></a>
<p><b>10.</b>
<!--
The spin-dominated Kerr-Newman metric from an electron (under classical general relativity)
should substantially
differ from the Minkowski metric at length scales below 
&asymp;1.9&times;10<sup>-13</sup> meter.
Resolve the resulting mystery of
why gravitational effects do not destroy the (very high) accuracy of QED
in many ways.
I solved it!! -->
In <a href="#desit">§35</a>
we found, perhaps for the first time, the apparently <b>correct equations of physics</b>
(and "propagators")
underlying QFTs in de Sitter space.  The procedure we adopted aimed to preserve 
(1) the conformal symmetry of the massless limits of these equations, and (2) the property
that Dirac solutions yield Klein-Gordon solutions and the Klein-Gordon operator factors into two
(identical up to changing the sign of i) Dirac operators (this can also be regarded as
a "symmetry").  These requirements appear to
force unique forms –
and yield the remarkable consequence that massless spin=1/2 fermions are impossible
in de Sitter space, explaining a prominent previously unexplained experimental fact.
Now consider, not de Sitter, but rather <i>general</i>
<b>curved spacetime metrics.</b>   There are now a tremendous number of ways to write
down wrong equations thanks to an enormous variety of possible "curvature couplings"
to the Riemann curvature tensor and/or its covariant derivatives.  However, we
can winnow them with the large number of astounding new symmetries and/or conservation laws
found in the last 10 years for Maxwell, Dirac, etc equations often with the aid of
computerized tools (see the papers by Fushchych, Schwarz, Anco, and Pohjanpelto):   Demand that the
curvified forms of these equations preserve as many of these flat-space symmetries and 
conservation laws as possible.   If this is done and if it leads to a uniqueness theorem,
then for the first time, we will have a good idea what the curved-space forms
of the equations of mathematical physics ought to be.   In the past 
(e.g. see Birrell &amp; Davies 1984), authors
have simply written down one possible form selected with
essentially no justification whatever from among the infinite number of possibilities,
merely based on some vague intuition theirs was the "simplest" or
"most natural" seeming.  It now is the year 2011 and it is time to do it right.
</p>
<p><b>11.</b>
<b>A "medium-tech" idea about black hole Hawking radiation and entropy,</b> which appears to be
new, is as follows.  
Callen &amp; Welton formulated the so-called "fluctuation dissipation theorem" 
about quantum versions of what classically were linear dynamical systems with friction
(i.e. damped simple harmonic oscillators) for
example the LRC electrical circuit, which 
features exponentially-dying-and-oscillating
exact classical solutions: exp(-kt) with k complex with positive real part.
[Usual warning: when physicists call something a "theorem" for 60 years, it isn't.]
As a result of this C&amp;W found the presence of 
"Johnson noise," i.e. at any positive temperature a certain mathematically-known 
spectrum of electrical noise
was expected in LRC circuits, and the perpetual presence of that
noise can also be viewed as "entropy."
My suggestion is to apply this to the "quasimodes," aka "quasi-normal modes," of a black hole.  
That is: the usual linear
equations of physics (Maxwell, Klein Gordon, and Dirac equations;
neutrino wave equation,
linearized Einstein GR equations) can be and have been 
solved in the curved space geometries 
corresponding to black holes – e.g, see Chandrasekhar 1998 –
albeit not always in the most general (Kerr-Newman) such geometry; at present we
have greater understanding of them in the simpler
Schwarzschild geometry subcase.
These solutions always exhibit exponential decline with oscillation, just like the
solutions for an LRC circuit.  At any finite temperature, then, we expect
via the Callen-Welton theory, for there to be "thermal noise" exciting
these quasimodes with a fully known spectral distribution.
This causes any black hole to have an "atmosphere" of
thermally excited quasimodes.  This constitutes "entropy" and it constitutes
a "microscopic description" of that entropy.
Now if, via rain of bricks there is assumed to be a Planck scale "Nyquist bound"
cutting off the ultraviolet end of the quasimode spectrum, then this would
lead to arbitrarily exactly computable understanding of this kind of entropy.
I'm suggesting that it then ought to be possible with enough computing (it isn't clear how much and
it might be very large) to exactly work out rain of bricks parameters from the matching
Hawking black hole demands.   Also, our whole "medium-tech" idea ought
to be considered vis-a-vis other theories besides rain of bricks.
</p><p>
<b>12.</b>
Make a good quality experimental measurement of the 
electric <b>quadrupole moment of the electron, proton, and neutron.</b>
It is an embarrassment for experimental physics that, 
as of the year 2013, nobody has ever explicitly measured any of these,
and the geometrical shapes of the proton and neutron are not known.
But both magnetic dipole and
electric quadrupole moments have
been measured for  many atomic nuclei with spin&#8805;1 via 
"hyperfine splitting" effects on atomic spectra.
I have estimated that the electron's quadrupole moment must be in 
the millibarn×e range or below, because otherwise its effects on atomic spectra 
ought to have already been noticed.
Pure QED predicts (as a consequence of its P and T symmetries)
that all electric &#8805;dipole and magnetic &#8805;quadrupole moments
of a spin&#8804;1/2 particle (including extended particles such
as protons &amp; neutrons) must be exactly 0.  
(Hence the quadrupole moments of spin&#8804;1/2 nuclei always have essentially zero 
effect on atomic spectra.  This is the subject of the "Wigner-Eckart theorem.")
However, my point is that experimenters should not just <i>accept</i> those claims, 
they should <i>test</i> them.  Further, the "standard model," which goes beyond QED, is known to
include both P- and T-violating effects and hence should
predict small, but nonzero, electric dipole and quadrupole moments for all 
these particles.  Also, the geometrical shapes of (more precisely, charge, mass,
and angular momentum distributions inside) the proton and neutron ought
to be estimable via lattice QCD.
</p>

<a name="ackn"></a>
<h3>44. Acknowledgments </h3>
<p>
This project has heightened my appreciation for
P.A.M.Dirac, R.P.Feynman, and F.J.Dyson, three of the greatest theoretical physicists
and more than anyone else the founders of quantum field theory.   And also, note,
these three (and J.S.Schwinger) all perceived the inadequacies of QFT
and all were rightly considerably more critical of it than most later physicists.
Almost everything here rests on their ideas, and I have reason to believe that
some of the new ideas here were present in embryonic form 
in their (in some cases unpublished) investigations.
</p><p>
(I should also remark that, as we've seen, all of these four published mistakes, and since
I'm pretty sure I am not as smart as Feynman and Schwinger, that suggests I may have also...
there are many ways to go wrong in a work of this length.
Caveat lector.  Hopefully any remaining errors are repairable, and I feel as though
this has reached the "self-sealing rubber" stage where the available ideas are powerful enough
to overcome leaks.)
</p><p>
I would also like to thank G.S.Adkins, D.J.Broadhurst, and R.W.Gosper
for some more prosaic and minor – but still appreciated – help.
</p>

<a name="refs"></a>
<h3>45. References </h3>
<p>
M.Abramowitz &amp; I.Stegun (eds):
<a href="http://www.math.sfu.ca/%7Ecbm/aands/">Handbook of mathematical functions</a>,
USA National Bureau of Standards, Tenth Printing, December 1972, with corrections.
An update is the <a href="http://dlmf.nist.gov/">
NIST Digital Library of Mathematical Functions</a>.
</p><p>
R.Acharya &amp; P.Narayana Swamy:
<a href="http://arxiv.org/abs/hep-th/9910008">No Eigenvalue in Finite Quantum Electrodynamics</a>,
Int'l.J.Mod.Phys. A12 (1997) 3799-3809.
</p><p>
R.Acharya &amp; P.Narayana Swamy:
<a href="http://arxiv.org/abs/hep-th/0502110v2">
Asymptotic Freedom and Infrared slavery in PT-symmetric Quantum Electrodynamics</a>.
<tt>arXiv:hep-th/0502110v2</tt>
</p><p>
R. Acharya &amp; B.P. Nigam:
Vacuum polarization in QED: re-evaluation of fourth-order coefficient,
Phys. G: Nucl. Particle Phys. 18 (1992) L135-L136.
[Obsoleted by Nigam &amp; Acharya 1993.]
</p><p>
J.A.Adell, A.Lekuona, Y.Yu:
<a href="http://arxiv.org/abs/1001.2897v1">Sharp 
bounds on the entropy of the Poisson law and related quantities</a>,
IEEE Transactions on Information Theory 56,5 (May 2010) 2299-2306.
</p><p>
Gregory S. Adkins: 
<a href="http://authors.library.caltech.edu/6675/1/ADKprd89.pdf">Calculation of the electron 
magnetic moment in Fried-Yennie-gauge QED</a>,
Phys. Rev. D 39,12 (1989) 3798-3801.
<!-- Adkins has only 5 diagrams and mu2 = mc+mr+msv+mse+mvp
= -0.3284789658 = 0.4710010375+0.015687422-0.1875000000-1.125000000+0.4973325763
the largest diagram is mse=9/8=1.125. -->
</p><p>
Gregory S. Adkins:
Analytic evaluation of the amplitudes for orthopositronium decay to three photons to one-loop 
order, Phys. Rev. A 72,3 (2005) 032501 [11 pages].
</p><p>
Gregory S. Adkins, Richard N. Fell, Jonathan Sapirstein:
Order &#945;<sup>2</sup> <a href="http://arxiv.org/abs/hep-ph/0003028">
corrections to the Decay Rate of Orthopositronium</a>,
Phys. Rev. Lett. 84,22 (2000) 5086-5089;
including results from <a href="http://arxiv.org/abs/hep-ph/0506213">earlier</a>
Phys. Rev. Lett. 76 (1996) 4903-4906;
see also
Light-by-light scattering contributions to positronium decay rates,
Phys. Rev. A63,3 (2001) 032511 [5 pages];
but the final version, which is far superior to those, is:
Two-loop correction to the orthopositronium decay rate,
Annals of Physics 295,2 (2002) 136-193.
<!-- The 6 diagram values for 1-loop correction:
OV  -1.0289
IV  -1.8393
DV  -3.5676
SE   4.7850
L   -7.8218
A   -0.8141 
from table I page 3.
sum = âˆ’10.28661 48086 28262 24015 01692 10991.
At two-loop order, 89 diagrams split into 11 classes.  The 11 class-sums were
 -5.618, -0.705, 0.058, 2.421, 9.259(9), -20.50(26), -1.372, 9.007, 0.965, 28.860(2), 
and the 11th class uncalculated here since believed small but later paper calculated 0.350(4); 
sum=22.38(26).
The 11th class is handled in
 Phys. Rev. A63,3 (2001) 032511 [5 pages].
-->
</p><p>
Ian J. R. Aitchison:
<a href="http://arxiv.org/abs/hep-ph/0505105">
Supersymmetry and the MSSM: An Elementary Introduction</a>,
2005.
Also book:
<a href="http://catdir.loc.gov/catdir/enhancements/fy0808/2008270052-t.html">
Supersymmetry in particle physics: an elementary introduction</a>,
Cambridge University Press 2007.
QC174.17.S9 A38 
</p><p>
T. Aida, Y. Kitazawa, J. Nishimura, A. Tsuchiya:
Two-loop Renormalization in Quantum Gravity near Two Dimensions,
Nucl. Phys. B 444 (1995) 353-380 and
<a href="http://arxiv.org/abs/hep-th/9501056">hep-th/9501056</a>.
</p><p>
Toshiaki Aida &amp; Yoshihisa Kitazawa:
Two-loop prediction for scaling exponents in (2+&#949;)-dimensional quantum gravity,
Nuclear Physics B 491,1-2 (1997) 427-458 and
<a href="http://arxiv.org/abs/hep-th/9609077">hep-th/9609077</a>.
</p><p>
Gabriel Alvarez:
Coupling-constant behavior of the resonances of the cubic anharmonic oscillator,
Phys. Rev. A 37,11 (1988) 4079-4083.
<!--
H = p^2/2 + k*x^2/2 + g*x^3
En(g) = sum(N>=0) En(N) * g^N
(n=0 for ground)
En(N) = -GAMMA((N+1)/2 + n) * 60^((N+1)/2 + n)  * 8^(-N/2) * (2pi)^(-3/2) / n! * [1+O(1/N)]
E0(N) = -GAMMA((N+1)/2) * 60^((N+1)/2)  * 8^(-N/2) * (2pi)^(-3/2) * [1+O(1/N)]
He cites two papers by Caliceti about Borel summability and rigorous meaning in
an analytic continuation sense with cutoff at infinity.
-->
</p><p>
Ugo Amaldi, Wim de Boer, Hermann Fürstenau:
Comparison of grand unified theories with electroweak and strong coupling constants 
measured at LEP,
Physics Letters B 260, 3-4 (1991) 447-455.
[See also P. Langacker &amp; N. Polonsky:
The strong coupling, unification, and recent data,
Phys. Rev. D 52 (1995) 3081-3086.]
</p><p>
J. Ambjorn, J. Jurkiewicz, R. Loll:
<a href="http://arxiv.org/abs/hep-th/0105267">Dynamically 
Triangulating Lorentzian Quantum Gravity</a>,
Nucl.Phys. B610 (2001) 347-382.
</p><p>
Stephen C. Anco &amp;  Juha Pohjanpelto:
<a href="http://arxiv.org/abs/math-ph/0108017">Classification
of local conservation laws of Maxwell's equations</a>,
Acta Applicandae Math. 69,3 (2001) 285-327.
</p><p>
Stephen C. Anco &amp;  Juha Pohjanpelto:
<a href="http://arxiv.org/abs/math-ph/0202019">Conserved currents of 
massless fields of spin s&gt;0</a>,
Proc. Royal Soc. A (London) 459 (2003) 1215-1240.
</p><p>
Ian M. Anderson &amp; Charles G. Torre:
<a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.cmp/1104286113">Classification
of local generalized symmetries for the vacuum Einstein equations</a>,
Commun. Math'l. Phys. 176,3 (1996) 479-539. 
</p><p>
T.Aoyama, M.Hayakawa, T.Kinoshita, M.Nio:
Revised value of the eighth-order QED contribution to the anomalous
magnetic moment of the electron,
Phys. Rev. D 77,5 (2008) 053012 [24 pages];
see also <a href="http://arxiv.org/abs/0706.3496">Phys.Rev.Lett.99 (2007) 110406</a>.
</p><p>
Tatsumi Aoyama, Masashi Hayakawa, Toichiro Kinoshita, Makiko Nio:
<a href="http://arxiv.org/abs/1205.5368">
Tenth-Order QED Contribution to the Electron g-2 and an Improved Value of the Fine Structure Constant</a>,
Phys. Rev. Lett. 109,11 (2012) 111807 [4 pages].
</p><p>
Tatsumi Aoyama, Masashi Hayakawa, Toichiro Kinoshita, Makiko Nio:
<a href="http://ptep.oxfordjournals.org/content/2012/1/01A107.full.pdf">
<!-- also http://ptep.oxfordjournals.org/content/2012/1/01A107.full -->
Quantum electrodynamics calculation of lepton anomalous magnetic moments:
Numerical approach to the perturbation theory of QED</a>,    
Progress Theor'l &amp; Exptl. Physics (Nov. 2012) 01A107 [36 pages].
</p><p>
Huzihiro Araki:
Mathematical Theory of Quantum Fields, Oxford Univ. Press 2000.
(Int'l monographs physics #1010.)
</p><p>
Seiya Aramaki:
<a href="http://www.springerlink.com/content/u7t6817822752432/fulltext.pdf">
The Ward-Takahashi relation and Eigenvalue equation for fine-structure constant</a>,
Lettere Al Nuovo Cimento 23,18 (1978) 665-668.
</p><p>
Seiya Aramaki:
Note on the Magnitude of Renormalization Constants in Quantum Electrodynamics,
Progress of theoretical physics 69,1 (1983) 323-328.
</p><p>
Abhay Ashtekar &amp; Martin Bojowald:
<a href="http://arxiv.org/abs/grqc/0504029">Black hole evaporation: A paradigm</a>,
Classical &amp; Quantum Gravity 22 (2005) 3349-3362.
</p><p>
Chris Austin:
<a href="http://arxiv.org/abs/hep-th/0601168">A BPHZ convergence proof in Euclidean position space</a>,
http://arxiv.org/abs/hep-th/0601168,
2006 (252 pages).  See also his 42-page
<a href="http://arxiv.org/abs/hep-th/0509033">Cluster Convergence Theorem</a>,
http://arxiv.org/abs/hep-th/0509033, 2005.
</p><p>
M.A.Avila: 
<a href="http://www.ejournal.unam.mx/rmf/no472/RMF47205.pdf">Bounds 
on the gluon mass from nucleon decay</a>, 
Revista Mexicana de Fisica 47,2 (April 2001) 128-131
<!--also gives a rough antiquark-quark potential as EQ1 indicating sign of alpha[s] governs 
attraction/repulsion at short distances (+ so attract) but always attract at large dist.
-->
</p><p>
G.van Baalen, D.Kreimer, D.Uminsky, K.Yeats:
<a href="http://arxiv.org/abs/0805.0826">The 
QED &#946;-function from global solutions to Dyson-Schwinger equations</a>,
Ann.Phys 324,1 (Jan. 2009) 205-219.
</p><p>
Volker Bach,  Jürg Fröhlich, Alessandro Pizzo:
An Infrared-Finite Algorithm for Rayleigh Scattering Amplitudes, and Bohr's Frequency Condition,
Commun. Math'l. Phys. 274,2 (2007) 457-486.
<!--
In this paper, we rigorously justify Bohr's frequency condition in atomic spectroscopy. 
Moreover, we construct an algorithm enabling us to calculate the transition amplitudes for 
Rayleigh scattering of light at an atom, up to a remainder term of arbitrarily high 
order in the finestructure constant. Our algorithm is constructive and circumvents 
the infrared divergences that invalidate standard perturbation theory.
  To our knowledge, a mathematically controlled expansion, accurate to
an arbitrary
order in the finestructure constant alpha, with a finite remainder term, of
the scattering amplitudes
for Rayleigh scattering of light at an atom in nonrelativistic QED has not been
provided in the literature before. The novelty of our results is that
they turn infrared
divergences in naive perturbation theory into powers of ln( 1/alpha ).
--
Infrared-finite algorithms in QED II. The expansion of the groundstate of an atom interacting 
with the quantized radiation field, Adv. in Maths. 220,4 (2009) 1023-1074. 
--
In this paper, we present an explicit and constructive algorithm enabling us to calculate the groundstate and the groundstate energy of a non-relativistic atom minimally coupled to the quantized radiation field up to an error of arbitrary finite order in the fine structure constant. Because of infrared divergences, which invalidate a straightforward Taylor expansion, an iterative construction is employed to remove the infrared cut-off in photon momentum space and to produce a convergent algorithm.
--
Infrared-finite algorithms in QED: The groundstate of an atom interacting with the quantized 
radiation field, Commun. Math'l. Phys. 264,1 (2006) 145-165.
--
n this paper, the groundstate of a nonrelativistic atom, minimally coupled to the quantized radiation field, and its groundstate energy are constructed by an iteration scheme inspired by [10]. This scheme successively removes an infrared cutoff in momentum space and yields a convergent algorithm enabling us to calculate the groundstate and the groundstate energy, to arbitrary order in the feinstructure constant Î± ~ 1/137. In forthcoming papers, we will use our result to re-expand the groundstate and, eventually, scattering amplitudes in terms of bare quantities.
-->
</p><p>
Alain Bachelot &amp; A.Motet-Bachelot:
Les resonances d'un trou noir de Schwarzschild, 
Annales de l'Institute Henri Poincare – Physique Theorique 59,1 (1993) 3-68.
They also published a different paper on the same topic (also in French)
in the 1993 Proceedings of the 4th International Conference on Hyperbolic Problems
(ed.A.Donato &amp; F.Oliveri)
Braunschweig, Vieweg; and a shorter summary in
Comptes Rendus de l'Academie des Sciences Series I – 
Mathematique 316,8 (April 1993) 795-798.
</p><p>
P.A. Baikov, K.G. Chetyrkin, J H. Kühn:
<a href="http://arxiv.org/abs/1001.3606">
Adler Function, Bjorken Sum Rule, and the Crewther Relation to Order
&#945;<sup>4</sup> in a General Gauge Theory</a>,
Phys. Rev. Lett. 104,3 (2010) 132004  [4 pages];
also more details in <a href="http://arxiv.org/abs/0810.4048">http://arxiv.org/abs/0810.4048</a>.
<!--
EQ9 on page 3 gives the quenched QED beta function accurate to 5-loops:
beta = (4/3) * A + 4 * A^2 - 2*A^3 - 46 * A^4 + (4157/6 + 128*z3) * A^5 + O(A^6)
where
A = alpha/(4*pi)
z3 = zeta(3)
about four z5 terms cancel.
Also in the arxiv version they claimed to compute quenched QED beta function
in EQ3.6. "Thus our calculation of d4,1 has solved the problem: the rationality
ceases to exist starting from fifth loop."
  Meanwhile http://arxiv.org/pdf/1205.2810v2.pdf EQ7 gives for full beta function in
"QED with a single lepton" (which I think means, 1 electron flavor; A=alpha/pi):
beta = (1/3)*A^2 + (1/4)*A^3 - (31/288)*A^4 - (2785/31104+13*z3/36)*A^5
  + (-195067/497664 - 13*z4/96 - 25*z3/96 + 215*z5/96)*A^6 + O(A^7)
Meanwhile http://arxiv.org/pdf/hep-ph/0108197v1.pdf finds in the nf=1 flaovar case EQ9
  beta = (4/3)*A^2 + 4*A^3 - (62/9)*A^4 + ...
where now A=alpha/(4*pi)
-->
<!--
</p><p>
Marshall Baker &amp; Ken Johnson:
Applications of Conformal Symmetry in Quantum Electrodynamics, Physica A 96 (1979) 120-130.
-->
</p><p>
R.Balian, C.Itzykson, J.B.Zuber, G.Parisi: 
Asymptotic estimates in quantum electrodynamics II, Phys.Rev. D 17,4 (1978) 1041-1052.
Part I was Phys.Rev. D 16 (1977) 996-1013.
</p><p>
Thomas Banks: Modern quantum field theory, Cambridge Univ. Press 2008.
</p><p>
J.F.Barbero G, J.Lewandowski, &amp; E.J.S.Villasenor:
Flux-area operator and black hole entropy,
Phys. Rev. D80 (2009) 044016 [15 pages].
</p><p>
R.Barbieri, M.Caffo, E.Remiddi: A contribution to the 6th order electron and
muon anomalies II, Lettere al Nuovo Cimento 5,11 (Nov 1972) 769-770.
</p><p>
Valentine Bargmann &amp; Eugene P. Wigner: 
Group theoretical discussion of relativistic wave equations,
Proc.Natl.Acad.Sci USA 34,5 (1946) 211-223.
</p><p>
A.S.Barreto &amp; Maciej Zworski:
<a href="http://www.math.purdue.edu/%7Esabarre/Papers/sazw4.pdf">Distribution
of resonances for spherical black holes</a>, Math. Res. Lett. 4 (1997) 103-121.
</p><p>
Harry Bateman:
The conformal transformations of four dimensions and their applications to geometrical optics,
Proc. London Math. Soc. 7 (1909) 70-89.
</p><p>
Harry Bateman:
The transformation of the electrodynamical equations, 
Proc. London Math. Soc. 8 (1910) 223-264.
</p><p>
Harry Bateman:
The transformations of coordinates which can be 
used to transform one physical problem into another,
Proc. London Math. Soc. 8 (1910) 469-488.
</p><p>
C. Becchi, A. Rouet, R. Stora:
The Abelian Higgs-Kibble Model, Unitarity
of the S-operator, Phys. Lett. 52B (1974) 344-346.
</p><p>
Timothy C. Bell, John G. Cleary, Ian H. Witten:
Text compression,
Prentice Hall, Englewood Cliffs, N.J. 1990.
</p><p>
Carl M. Bender:
<a href="http://arxiv.org/abs/hep-th/0703096">Making 
Sense of Non-Hermitian Hamiltonians</a>,
Reports on Progress in Physics 70 (2007) 947-1018. <!-- arXiv: hep-th/0703096 -->
</p><p>
Carl M. Bender &amp; Tai Tsun Wu:
Anharmonic Oscillator. II. A Study of Perturbation Theory in Large Order,
Phys. Rev. D 7,6 (1973) 1620-1636.
C.M.Bender &amp; T.T.Wu: 
Analytic Structure of Energy Levels in a Field-Theory Model,
Phys. Rev. Lett. 21 (1968) 406-409.
C.M.Bender &amp; T.T.Wu: Large-Order Behavior of Perturbation Theory,
Phys. Rev. Lett. 27,7 (1971) 461-465. <!--has general-power AnHO:
We examine the large-order behavior of perturbation theory for the anharmonic oscillator, a 
simple quantum-field-theory model. New analytical techniques are exhibited and used to 
derive formulas giving the precise rate of divergence of perturbation theory for all energy 
levels of the x^(2N) oscillator. We compute higher-order corrections to these formulas for the 
x^4 oscillator with and without Wick ordering.
-->
C.M.Bender: Perturbation Theory in Large Order, Adv.Math. 30,3 (1978) 250-267.
C.M. Bender: Generalized Anharmonic Oscillator, J.Math'l.Phys. 11,34 (1978) 796-804.
C.M.Bender &amp; T.T.Wu: Anharmonic oscillator, Phys.Rev. 184,5 (1969) 1231-1260.
[Short version: Phys.Rev.Lett. 21 (1968) 406-409.]
C.M.Bender &amp; T.T.Wu: Statistical Analysis of Feynman Diagrams, 
Phys.Rev.Lett. 37,3 (1976) 117-120.
C.M.Bender &amp; Kuok Mee Ling:
Tight upper and lower bounds on perturbation coefficients for
anharmonic oscillators, Phys. Rev. D (3) 19,12 (1979) 3808-3810.
<!-- CMB &amp; ??: Multiple-scale PRL 77 (1996) 4114-????. -->
<!--T.I.Banks &amp; C.M.Bender:
Anharmonic Oscillator with Polynomial Self-Interaction,
Journal of Mathematical Physics 13 (1972) 1320-????.-->
<!-- coupled: T.I.Banks &amp; C.M.Bender: Phys.Rev. D 8 (1973) 3366-????.
T.I.Banks, C.M.Bender, T.T.Wu: Phys.Rev. D 8 (1973) 3346-????.
The AnHO with degree-3:
Arkady Vainshtein studied in 1964:
http://arxiv.org/pdf/hep-th/0207046  pp.2-5 p.6 eq20 is the EI series 
   eq24-26 p7 does general Borel
-->
</p><p>
C.M. Bender &amp; Stefan Boettcher:
Real spectrum in nonHermitian Hamiltonians having PT-symmetry,
Phys.Rev.Lett. 80,24 (1998) 5243-5246.
<!--
</p><p>
Carl M. Bender &amp; Gerald V. Dunne:
<a href="http://arxiv.org/abs/quant-ph/9812039">Large-order Perturbation Theory
for a Non-Hermitian PT-symmetric Hamiltonian</a>,
J. Math'l. Phys. 40 (1999) 4616-4621.
--
Cubic AnHO, conjectured to have posdef real spectrum, examined 
at high order in pert thy.   Unaware of Alvarez 11 years prior.
Alvarez seems superior to B+Dunne.
H = p^2 + x^2/4 + i*g*x^3
E(g) = sum(N>=0) E(2*N) * g^(2*N)
E(2*N) = (-1)^(N+1) * GAMMA(N+1/2) * 60^(N+1/2) * (2pi)^(-3/2) * [1-O(1/N)]
this is the same as Alvarez except for Alvarez's factor 8^(-N/2)
which I guess is due to his altered 
Halv = p^2/2 + k*x^2/2 + g*x^3
got by scaling x by sqrt(2). Yes. Agree.
-->
</p><p>
C.M. Bender &amp; K.A. Milton:
A nonunitary version of massless QED possessing a critical point,
J. Phys. A 32,7 (1999) L87-92.
</p><p>
C.M.Bender &amp; S.A.Orszag:
Advanced mathematical methods for scientists and Engineers,
Wiley 1978, reprinted by Springer.
</p><p>
C.M.Bender, Ines Cavero-Pelaez, Kimball A. Milton, K.V. Shajesh:
<a href="http://www.nhn.ou.edu/%7Emilton/papers/cmbf.pdf">
PT-Symmetric Quantum Electrodynamics</a>, 
 Physics Letters B 613 (2005) 97-104
<!-- [arXiv: hep-th/0501180],
see also
http://www.nhn.ou.edu/~milton/papers/ptproc07.pdf
=Int J Theor Phys
DOI 10.1007/s10773-010-0526-8
in which
[11] C. M. Bender and K. A. Milton, J. Phys. A: Math. Gen. 32, L87-L92 (1999).
[12] K. A. Milton, Czech. J. Phys. 54, 85 (2004), hep-th/0308035.
these 2 papers cited as preliminary

[13] C. M. Bender, K. A. Milton, and V. M. Savage:
Solution of Schwinger-Dyson equations for PT-symmetric quantum field theory,
 Phys. Rev. D 62,8 (2000)  85001
this paper is citred re asymptly free.  But it does not discuss QED per
se and refers the reader re QED to
C. M. Bender and K. A. Milton:
A nonunitary version of massless QED possessing a critical point,
J. Phys. A 32,7 (1999) L87-92.
-->
</p><p>
E.A. Bender &amp; E.R. Canfield: The asymptotic number of labeled
graphs with given degree sequence, J. Combinatorial Theory A 24 (1978) 296-307.
<!--
related:
R. C. Read and N. C. Wormald: Number of labeled 4-regular graphs,
J. Graph Theory 4,2 (1980) 203-212.
http://onlinelibrary.wiley.com/doi/10.1002/jgt.3190040208/pdf
-->
</p><p>
Alexander Berk:
Anharmonic oscillator: A study of the perturbation series for the wave function,
Phys. Rev. A 31,3 (1985) 1241-1249.
</p><p>
Frederick J. Beutler:
Error-Free Recovery of Signals from Irregularly Spaced Samples,
SIAM Review 8,3  (July 1966) 322-335.
</p><p>
L.C. Biedenharn:
Remarks on the Relativistic Kepler Problem,
Physical Review 126,2 (1962) 845-851.
</p><p>
Adel Bilal:
<a href="http://arxiv.org/abs/hep-th/0101055">Introduction to Supersymmetry</a>
(77 pages) <tt>http://arxiv.org/abs/hep-th/0101055</tt>.
</p><p>
D.J. Bird &amp; 20 others:
<a href="http://prl.aps.org/abstract/PRL/v71/i21/p3401_1">Evidence for correlated changes in
the spectrum and composition of cosmic rays at extremely high energies</a>,
Phys. Rev. Lett., 71,21 (1993) 3401-3404.
</p><p>
N.D. Birrell &amp; P.C.W. Davies:
Quantum fields in curved space,
Cambridge University Press 1984.
</p><p>
James D. Bjorken &amp; Sidney D. Drell:
Relativistic quantum mechanics (1964), and
Relativistic quantum fields (1965), McGraw Hill, 2 volumes.
</p><p>
David E. Blair:
Inversion theory and conformal mapping,
AMS 2000.
(Chapter 6 pp.95-105 covers "The Classical Proof of Liouville's Theorem" 
that the only smooth conformal maps in D-dimensional Euclidean space, D&#8805;3,
are compositions translations, scalings,
orthogonal transformations, and inversions.)
</p><p>
S. Bodenstein, C. A. Dominguez, K. Schilcher, H. Spiesberger:
<a href="http://arxiv.org/abs/1302.1735">Hadronic Contribution to the muon g-2 factor</a>,
Phys. Rev. D 88,1 (2013) 014005, 7 pages.
</p><p>
N.N. Bogoliubov &amp; O.S.Parasiuk:
On the multiplication of propagators in quantum field theory, Acta Math. 97 (1957) 227-326.
</p><p>
E.B. Bogomolny:
Calculation of instanton-anti-instanton contributions in quantum
mechanics, Phys. Lett. B 91,3-4 (1980) 431-435.
<!--
Abstract:
A general method of calculation of instanton-anti-instanton
configuration proposed. The calculation are illustrated on the example
of the anharmonic oscillator. The results agree with the direct
semi-classical approach.
-->
</p><p>
Evgeny B. Bogomolny &amp; Vladimir A. Fateyev: 
Large order calculations in gauge theories,
Physics Letters B 71,1 (1977) 93-96.
<!--
Abstract:
The perturbation series asymptotics are estimated by the coupling
constant dispersion relations in the Yang-Mills theory. It is found
that the Green functions in this theory have the imaginary parts at
physical values of the coupling constant which are connected with the
tunneling to the other vacuum states. These imaginary parts are
calculated by the steepest descent method. The saddle points are shown
to be the instanton-antiinstanton configurations of the fields.
-->
</p><p>
Evgeny B. Bogomolny &amp; Vladimir A. Fateyev: 
The Dyson instability and asymptotics of the perturbation series in QED,
Physics Letters B 76,2 (1978) 210-212.
<!-- Vladimir A Fateev's CV available online: followup never published?
Abstract:
The probability of the ground state decay at e^2 < 0 is calculated by
the steepest descent method in fermionic electrodynamics. The saddle
points are the solution of some equations which have been obtained by
calculating the asymptotic of the Dirac operator determinant in a very
strong external field. The SO(3)&times;O(2) solutions are found
explicitly. The main contribution from them to the Nth coefficient of
perturbation theory is proportional to
(-alpha)^N * S^(N/2) * GAMMA(N/2)
where
S = 4 * 3^(-3/2) * pi^3.
-->
</p><p>
Evgeny B. Bogomolny and Yu A. Kubyshin:
Asymptotic estimates for graphs with a fixed number of fermion loops
in quantum electrodynamics. 1. The choice of the form of the
steepest-descent solutions (in Russian), Yadernaya fizika 34,6 (1981) 1535-1546 =
(in English) Soviet J. Nucl. Phys. 34,6 (1981) 853-858.
Asymptotic estimates for diagrams with a fixed number of fermion loops
in quantum electrodynamics. 2. The extremal configurations with the
symmetry group O(2)×O(3),
(in Russian) Yadernaya fizika 35,1 (1982) 202? =
(in English) Soviet J. Nuclear Phys. 35,1 (1982) 114-119.
<!--
E. Bogomolny, V.A. Fateev, L.N. Lipatov:
Calculation of High Orders of Perturbation Theory in Quantum Field Theory, 
In: Physics Reviews, vol. 2 (1980) 247-393. I.M.Khalatnikov (ed).
-->
</p><p>
J.Bolmont, A.Jacholkowska, &amp; 3 others:
Study of time lags in HETE-2 Gamma-ray bursts with redshift: search for astrophysical effects 
and quantum gravity signature, Astrophysical J. 676 (2008) 532-544.
</p><p>
Rym Bouchendira, Pierre Clade, Saida Guellati-Khelifa, Francois Nez, Francois Biraben: 
<a href="http://arxiv4.library.cornell.edu/abs/1012.3627v1">
New determination of the fine structure constant and test of the quantum electrodynamics</a>
(2010)
<!-- 137.035999037(91)  http://arxiv4.library.cornell.edu/abs/1012.3627v1 
Using this determination, we obtain a theoretical value of the electron anomaly 
$a_\mathrm{e}=0.001~159~652~181~13(84)$ 
which is in agreement with the experimental measurement of Gabrielse 
$a_\mathrm{e}=0.001~159~652~180~73(28)$
diff =        (40+-89) * 10^(-14)
over 100X better than 
dehmeltdiff = (11+-6)  * 10^(-11)
-->
</p><p>
<!-- Jacob D. Bekenstein: ??? </p><p> -->
Raphael Bousso: 
<!-- Holographic entropy bound???, J. High Energy Phys. 9907 (1999) 004; -->
<a href="http://arxiv.org/pdf/hep-th/0203101">The holographic principle</a>, 
Rev. Mod. Phys. 74 (2002) 825-874.
</p><p>
John Philip Boyd:
Chebyshev and Fourier Spectral Methods,
(second ed.) Dover 2001.
</p><p>
A.J. Bracken &amp; Barry Jessup:
Local conformal-invariance of the wave equation for finite-component fields. 
<a href="http://www.maths.uq.edu.au/%7Eajb/local_conformal1.pdf">I</a>,
<!--The conditions for invariance, and fullyâ€reducible fields-->
J. Math'l. Phys. 23,10 (1982) 1925-1946;
<a href="http://www.maths.uq.edu.au/%7Eajb/local_conformal2.pdf">II</a>,
J. Math'l. Phys. 23,10 (1982) 1947-1952.
</p><p>
David J. Broadhurst:
<a href="http://arxiv.org/abs/hep-ph/9909336">Four-loop Dyson-Schwinger-Johnson anatomy</a>,
Phys.Lett. B466,2-4 (1999) 319-325.
<!-- does both gamma and beta functions, the gamma in his big table pages 9+10 does not exhibit
giant cancelation, unlike beta. -->
</p><p>
D.J. Broadhurst, R. Delbourgo, D. Kreimer:
<a href="http://arxiv.org/abs/hep-ph/9509296">Unknotting the polarized vacuum of quenched QED</a>,
Physics Letters B 366,1-4 (Jan. 1996) 421-428.
<!--explains rationality of quenched QED beta function, also explains
similar phenomena for "scalar QED"-->
</p><p>
D.J. Broadhurst, A.L. Kataev, O.V. Tarasov: 
<a href="http://arxiv.org/abs/hep-ph/9210255">Analytical on shell QED results: 3-loop vacuum
polarization, 4-loop beta-function, and the muon anomaly</a>,
Physics Letters B 298 (1993) 445-452. <!--CERN-TH-6602/92-->
</p><p>
D.J. Broadhurst &amp; D. Kreimer:
Renormalization automated by Hopf algebra,
J. Symbolic Computation 27,6 (1999) 581-600.
</p><p>
Lowell S. Brown: Quantum field theory, Cambridge Univ. Press 1992.
</p><p>
L.S. Brown &amp; G.J. Maclay: 
Vacuum Stress between Conducting Plates: An Image Solution,
Physical Review 184,5 (1969) 1272-1279.
</p><p>
A.P.Buchvostov &amp; L.N.Lipatov: 
High orders of perturbation theory in scalar electrodynamics, Phyics Lett. B 70 (1977) 48-50;
Asymptotic estimates of
high orders of perturbation theory approximations in scalar electrodynamics, 
Soviet Physics JETP 46 (1977) 871-879.
</p><p>
Curtis G. Callan Jr. &amp; David Gross: 
Bjorken Scaling in Quantum Field Theory,
Phys. Rev. D 8,12 (1973) 4383-4394.
</p><p>
Herbert B. Callen &amp; Theodore A. Welton:
<a href="http://prola.aps.org/abstract/PR/v83/i1/p34_1">Irreversibility and Generalized Noise</a>, 
Physical Review 83,1 (1951) 34-40.
<!--
A relation is obtained between the generalized resistance and the
fluctuations of the generalized forces in linear dissipative systems.
This relation forms the extension of the Nyquist relation for the
voltage fluctuations in electrical impedances. The general formalism
is illustrated by applications to several particular types of systems,
including Brownian motion, electric field fluctuations in the vacuum,
and pressure fluctuations in a gas.
-->
</p><p>
Philip Candelas: Vacuum polarization in the presence of dielectric and conducting
surfaces, Ann. Phys. 143 (1982) 241-295.
</p><p>
Steven Carlip:
<a href="http://arxiv.org/abs/1207.4503">Spontaneous Dimensional Reduction?</a>,
http://arxiv.org/abs/1207.4503.
</p><p>
<!-- Xavier Calmet:
On the precision of a length measurement,
European Physical Journal C 54,3 (April 2008) 501-505 ??
"quantum mechanics and general relativity imply the
existence of a minimal length. To be more precise, we show that no
operational device subject to quantum mechanics, general relativity
and causality could exclude the discreteness of spacetime on lengths
shorter than the Planck length." Employs "hoop conjecture."
</p><p>
Crutchfield:
Phys. Lett. B 77,1 (1978) 109-113. No horn of singularities for double well AnHO
-->
Bruno Carazza &amp; Helge Kragh: Heisenberg's lattice world, the 1930 theory sketch, 
Amer.J.Phys. 63,7 (1995) 595-605.
</p><p>
Tian Yu Cao &amp; Silvan S. Schweber:
The Conceptual Foundations and the Philosophical Aspects of
Renormalization Theory,
Synthese 97,1 (Oct 1993) 33-108 [review article].
</p><p>
S.Chandrasekhar: Mathematical theory of Black Holes,
Oxford Science Publications (monographs on physics #69) 1983. Reprinted 1998.
</p><p>
Thomas Chen, Jürg Fröhlich, Alessandro Pizzo:
Infraparticle Scattering States in NonRelativistic QED: I. The Bloch-Nordsieck Paradigm,
Communications in Mathematical Physics 294,3 (2010) 761-825.
<!--
We construct infraparticle scattering states for Compton scattering in
the standard model of non-relativistic QED. In our construction, an
infrared cutoff initially introduced to regularize the model is
removed completely. We rigorously establish the properties of
infraparticle scattering theory predicted in the classic work of Bloch
and Nordsieck from the 1930â€™s, Faddeev and Kulish, and others. Our
results represent a basic step towards solving the infrared problem in
(non-relativistic) QED.
-->
</p><p>
Konstantin G. Chetyrkin:
<a href="http://arxiv.org/abs/hep-ph/9703278">
Quark Mass Anomalous Dimension to</a> O(&amp;alpha<sub>s</sub><sup>4</sup>),
Phys.Lett. B404 (1997) 161-165.
<!-- Mainly he does QCD but EQ 11-13 does QED (set nf=1). -->
</p><p>
E.S.C.Ching, P.T.Leung, W.M.Suen, K.Young:
Wave propagation in gravitational systems: Completeness of quasinormal modes,
Phys.Rev. D 54,6 (1996) 3778-3791.
</p><p>
Sidney Coleman &amp; David J.Gross: 
Price of asymptotic freedom, 
Phys.Rev.Letters 31,13 (1973) 851-854.
<!-- THEOREM:
Asymptly free QFT must contain nonAbelian gauge fields, and cannot contain
Abelian gauge fields.
Proof:
1. cannot contain Abelian gauge fields.
2. cannot contain only scalar bosons.
3. So it must be spin=1/2 fermions and scalar bosons only.
4. Single scalar boson coupled to single fermion? They rule it out.
5. Finally consider arbitrary # of scalar bosons plus fermions with
the most general (not necessarily P-invariant) coupling.
They rule it out via one-loop analysis.
6. They argue including higher loops cannot change the conclusion.
Q.E.D.
-->
</p><p>
Sidney Coleman &amp; Erick Weinberg:
Radiative Corrections as the Origin of Spontaneous Symmetry Breaking,
Phys.Rev. D 7,6 (1973) 1888-1910.
</p><p>
John C. Collins: Renormalization, Cambridge 1986.
<!-- Edw.B.Manoukian: Renormalization, Academic Press 1983? -->
</p><p>
Robert Coquereaux:
Fermionic expansion in quantum electrodynamics,
Physical Review D 23,10 (1981) 2276-2284.
</p><p>
H.S.M. Coxeter:
A Geometrical Background for De Sitter's World,
American Mathematical Monthly 50,4 (April 1943) 217-228.
</p><p>
Ebenezer Cunningham:
The principle of relativity in electrodynamics and an extension thereof, 
Proc. London Math'l. Soc. 8 (1909) 77-98.
<!--
Each four-dimensional solution [to Maxwell's equations] could then be inverted in a 
four-dimensional hypersphere of pseudo-radius K in order to produce a new solution. 
Central to Cunningham's paper was the demonstration that Maxwell's equations retained their 
form under these transformations -->
</p><p>
Predrag Cvitanovic:
<a href="http://www.cns.gatech.edu/%7Epredrag/papers/NPB77.pdf">Asymptotic estimates
and gauge invariance</a>,
Nuclear Physics B 127 (1977) 176-188.
[Error correction: Switch the diagram values for F1 &amp; F2.]
<!--
Lautrup 1977 claims to refute Cvitanovic conjecture.
Cvitanovic predicts that the (alpha/pi)^9 term in the electron (g-2)/2 will be
0+-0.3 as opposed to the "combinatorial" prediction of 25, got by assuming all
diagrams independent values with mean=0 and stddev=+-1.
I don't know what the fuck Cvit means exactly with his prediction, the series
he gives in EQ1 actually is (alpha/(2pi)) * SUM (alpha/pi)^(2n) * (n+1)  for n>=0.
Anyhow, http://arxiv.org/abs/1205.5368 does the electron g-2 
up to 10th order in year 2012.
ae(QED) = SUM (alpha/pi)^n * a_{2n}.
And they find
a_0 = 0.5
a_2 = -0.32847896557919378
a_4 = 1.181241456587200006  (this & above known exactly)
a_8 = -1.9106(20)  here "class V" is the no-virtual-electrons class and = -2.17550(194)
a_10 = 9.16(58)    here "class V" is the no-virtual-electrons class and = +10.092(570)
which does look like it has at last embarked on rocket ride...
They remark that:
 a_8 arises from 13 gauge-invariant diagram subsets.
 a_10 arises from 32 gauge-invariant diagram subsets.
These numbers disagree with Cvit in his table 1 where he claims the sequence 1,2,4,6,9,12,16
which happens to equal floor(n/2)*ceil(n/2) at least as far as it goes.
Perhaps this is since Cvit says to ignore diagrams with electron loops.
At the end, Cvit claims Lautrup HAS REFUTED his hypothesis by showing the contribution of the
diagram with n-1 electron loops at order 2n grows like (n-1)!.   BUT Cvit still clings
to his gauge-set convergent series conjecture for diagrams lacking electron loops.
Diagrams lacking electron loops, i.e. lacking virtual electrons, still are numerous:
If one electron there are about  (2n)!/(n!*2^n) = (2n-1)!!.
-->
</p><p>
Predrag Cvitanovic &amp; T. Kinoshita:
<a href="http://www.cns.physics.gatech.edu/%7Epredrag/papers/PRD10-74-III.pdf">
Sixth-order magnetic moment of the electron</a>,
Phys. Rev. D 10,12 (1974) 4007-4031.
</p><p>
P.Cvitanovic, B.Lautrup, R.B.Pearson: 
<a href="http://www.cns.gatech.edu/%7Epredrag/papers/PRD18-78.pdf">
Number and weights of Feynman diagrams</a>, 
Phys. Rev. D 18 (1978) 1939-1949.
</p><p>
C.G.Darwin: 
The wave equations of the electron,
Proc. Royal Soc. London A 118,780 (1928) 654-680.
<!-- Darwin The Electron as a Vector Wave, A116 (1927) 227-253 
seems about Klein Gordon, Pauli,
not Dirac, and perturbative not exact. 
Dirac's own paper was 
The Quantum Theory of the Electron
Proc. R. Soc. Lond. A 117 (Feb 1928) 610-624.
a more general than Coulomb potential is
Akpan N. Ikot:
Solution of Dirac Equation with Generalized Hylleraas Potential,
Communications in Theoretical Physics Volume 59 Number 3 (2013) 268
-->
</p><p>
Paul Davies: <a href="http://books.google.com/books?id=akb2FpZSGnMC&amp;pg=PA1&amp;lpg=PA1">
The new physics</a>, Cambridge Univ. Press 1992.
</p><p>
G.C.Debney, R.P.Kerr, A.Schild:
Solutions of the Einstein and Einstein-Maxwell Equations,
J. Math'l. Phys. 10,10 (1969) 1842-1854.
See especially EQs 7.10, 7.11, 7.14.
</p><p>
Hans G. Dehmelt:
<a href="http://nobelprize.org/nobel_prizes/physics/laureates/1989/dehmelt-lecture.pdf">
Experiments with an isolated subatomic particle at rest</a>,
Nobel prize lecture 1989.
</p><p>
Bertrand Delamotte: 
<a href="http://arxiv.org/abs/hep-th/0212049">A hint of renormalization</a>, 
Amer. J. Phys. 72,2 (2004) 170-184.
</p><p>
Frederik Denef &amp; Michael R. Douglas:
<a href="http://arxiv.org/abs/hep-th/0602072">Computational complexity of the landscape I</a>,
Annals Phys. 322,5 (2007) 1096-1142.
</p><p>
Stanley Deser: Off -shell electromagnetic duality invariance,
J. Phys. A 15,3 (1982) 1053-1054.
</p><p>
Stanley Deser &amp; Claudio Teitelboim:
Duality transformations of Abelian and non-Abelian gauge fields,
Phys. Rev. D 13,6 (1976) 1592-1597.
</p><p>
David Deutsch &amp; Philip Candelas:
Boundary effects in quantum field theory
Phys. Rev. D 20,12 (1979) 3063-3080.
</p><p>
Jonathan Dimock:
Quantum electrodynamics on the 3-torus 
I:first step(2002, 70 pages, latest draft Feb 2008)
<a href="http://arxiv.org/abs/math-ph/0210020">http://arxiv.org/abs/math-ph/0210020</a>;
II: The RG flow (2004, 37 page manuscript, latest draft July 2011)
<a href="http://arxiv.org/abs/math-ph/0407063">http://arxiv.org/abs/math-ph/0407063</a>;
III: convergence (manuscript that Dimock claimed in 2011 was "to appear").
</p><p>
P.A.M. <a href="http://www.lucasianchair.org/bibliographies/dirac-bibB.html">Dirac</a>:
The electron wave equation in de Sitter space, Annals of Mathematics 36,3 (1935) 657-669.
</p><p>
P.A.M. Dirac: Wave equations in conformal space, Annals of Mathematics 37,2 (1936) 429-442.
</p><p>
P.A.M. Dirac: Does renormalization make sense?,
pp. 129-130 in D.W. Duke &amp; J. F. Owens (eds.):
Perturbative Quantum Chromodynamics, AIP (conference proceedings #74), New York, 1981.
Conf. held at Florida State University March 1981.
<!--Answer is uncompromising "no." Abstract:
The author suggests that non-irreducible representations of the Lorentz group should be 
investigated.-->
</p><p>
P.A.M. Dirac: The inadequacies of quantum field theory,
pp. 194-198 in B.M. Kursunoglu &amp; E.P. Wigner (eds.):
Paul Adrien Maurice Dirac: Reminiscences about a Great Physicist (memorial volume;
this was Dirac's last paper and published posthumously),
Cambridge University Press, Cambridge, 1990.
Another paper by A.Salam pp.263-276 in this same volume attempts to address 
Dirac's complaints.
</p><p>
A.D. Dolgov &amp; V.S. Popov: 
Modified perturbation theories for an anharmonic oscillator,
<!--Exponential anharmonic oscillator-->
Phys. Lett. B 79,4-5 (1978) 403-405.
<!-- if perturb potential is g*exp(x) then power series(g) is not Borel summable.
If exp(x^3) then power series does not exist.  Says their methods can handle
any perturb potential. -->
</p><p>
Patrick Dorey, Clare Dunning, Anna Lishman, Roberto Tateo:
<a href="http://xxx.lanl.gov/abs/0907.3673">PT symmetry 
breaking and exceptional points for a class of inhomogeneous complex potentials</a>,
J.Physics A 42 (2009) 465302.
</p><p>
P.Dorey, C.Dunning, R.Tateo:
<a href="http://xxx.lanl.gov/abs/hep-th/0103051">Spectral equivalences, 
Bethe Ansatz equation, and Reality properties
in PT-symmetric quantum mechanics</a>,
J.Physics A 34,28 (2001) 5679-5704.
</p><p>
P.Dorey, C.Dunning, R.Tateo:
<a href="http://xxx.lanl.gov/abs/hep-th/0104119">Supersymmetry and the spontaneous breakdown of PT-symmetry</a>,
J.Physics A 34,28 (2001) L391-L400.
</p><p>
P.Dorey, C.Dunning, R.Tateo:
<a href="http://arxiv.org/abs/hep-th/0309209">A reality proof in PT-symmetric 
quantum mechanics</a>,
Czechoslovak J. Phys. 54,1 (2004) 35-41.
<!--We review the proof of a conjecture concerning the reality of the spectra of certain
PT-symmetric quantum mechanical systems, obtained via a connection between the 
theories of ordinary differential equations and integrable models. 
Spectral equivalences inspired by the correspondence are also discussed.
-->
</p><p>
P.Dorey, C.Dunning, R.Tateo:
<a href="http://xxx.lanl.gov/abs/hep-th/0703066">The ODE/IM correspondence</a>,
J.Physics A 40,32 (2007) R205-R263.
</p><p>
Patrick Dorey, Adam Millican-Slater, Roberto Tateo:
<a href="http://xxx.lanl.gov/abs/hep-th/0410013">Beyond the WKB approximation 
in PT-symmetric quantum mechanics</a>,
J.Physics A 38,6 (2005) 1305-1331.
</p><p>
R.Doria, J.Frenkel, J.C.Taylor: Counterexample to nonAbelian
Bloch-Nordsieck conjecture,
Nuclear Physics B 168,1 (1980) 93-110.
</p><p>
Olaf Dreyer: <a href="arxiv:arXiv:gr-qc/0211076v1">Quasinormal Modes, the Area Spectrum, 
and Black Hole Entropy</a>, Phys. Rev. Lett. 90 (2003) 081301
</p><p>
Loyal Durand:
Addition formulas for Jacobi, Gegenbauer, Laguerre and 
hyperbolic Bessel functions of the second kind,
SIAM J. Math. Anal. 10 (1979) 425-437.
</p><p>
L.Durand, P.M.Fishbane, L.M.Simmons, Jr:
Expansion formulas and addition theorems for Gegenbauer functions, 
J. Math'l. Physics 17 (1976) 1933-1948
<!--Math. Rev. 54, 10712 (1977)-->
</p><p>
S.Dürr, Z.Fodor, &amp; 9 others:
<a href="http://arxiv.org/abs/0802.2706">Scaling study of dynamical smeared-link clover fermions</a>,
Phys.Rev.D 79 (2009) 014501.
</p><p>
S.Dürr, Z.Fodor, &amp; 10 others:
<a href="http://arxiv.org/abs/0906.3599">Ab-initio Determination of Light Hadron Masses</a>,
Science 322 (2008) 1224-1227.
</p><p>
Freeman J. Dyson:
The Radiation Theories of Tomonaga, Schwinger, and Feynman,
Phys. Rev. 75,3 (1949) 486-502.
</p><p>
Freeman J. Dyson:
The S Matrix in Quantum Electrodynamics,
Phys.Rev. 75,11 (1949) 1736-1755.
<small>Dyson's above two papers unified the works of Tomonaga, Schwinger, and Feynman
and completed the job of inventing/defining renormalized QED, except that 
lacunae remained...</small>
</p><p>
Freeman J. Dyson: Divergence of perturbation theory in quantum electrodynamics, 
Phys.Rev. 85,4 (1952) 631-632.
(Reprinted p.255-6 in <i>Selected papers of Freeman Dyson with commentary</i>, AMS 1996.)
<!--QC173.7.D972.-->
<!-- A ridiculous paper
Mofazzal Azam: Divergence of the $1/N_f$ series expansion in QED,
http://arxiv.org/abs/hep-th/0410071
claims (1) expansion of QED in the 1/NumberOfFlavors also diverges, (2)
he wanted to know if Dyson series could be "conditionally convergent" which
shows Azam is an idiot who knows nothing about complex analysis...
-->
</p><p>
Freeman J. Dyson:
<a href="http://arxiv.org/abs/quant-ph/0608140">
1951 Lectures on Advanced Quantum Mechanics Second Edition</a>,
free online textbook. Lectures by Dyson at Cornell in 1951 that
were digitized and re-issued by Michael J. Moravcsik.
</p><p>
Otto Eberhardt &amp; 6 others:
<a href="http://arxiv.org/abs/1209.1101">
Impact of a Higgs Boson at a Mass of 126 GeV on the Standard Model
with Three and Four Fermion Generations</a>,
Phys. Rev. Lett. 109,24  (2012) 241802.
</p><p>
Carl Eckart: The Application of Group Theory to the Quantum Dynamics of Monatomic Systems,
Rev. Mod. Phys. 2,3 (1930) 305-380.
</p><p>
Bradley Efron &amp; Robert Tibshirani:
An Introduction to the Bootstrap. Chapman &amp; Hall/CRC  (1994).
</p><p>
Estia J. Eichten, Kenneth D. Lane, Michael E. Peskin:
New Tests for Quark and Lepton Substructure,
Phys. Rev. Lett. 50,11 (1983) 811-814.
</p><p>
A.Einstein &amp; L.Infeld:
Gravitational equations and the problems of motion II,
Annals of Mathematics (ser. 2) 41 (1940) 455-464
</p><p>
Henri Epstein &amp; Vladimir J. Glaser:
<a href="http://archive.numdam.org/ARCHIVE/AIHPA/AIHPA_1973__19_3/AIHPA_1973__19_3_211_0/AIHPA_1973__19_3_211_0.pdf">The role of locality in perturbation theory</a>,
Annales de l'Institute Henri Poincare Sect. A (N.S.) 19,3 (1973) 211-295.
</p><p>
K.-J. Engel &amp; R. Nagel: One-parameter semigroups for linear evolution 
equations, Springer (GTM #194) 2000.  <!--See ch. III corollary 5.8.-->
</p><p>
Henri Epstein &amp; Vladimir J. Glaser:
<a href="http://cdsweb.cern.ch/record/874058/files/CM-P00058734.pdf">Adiabatic 
limit in perturbation theory</a>, pp.193-254 in Velo &amp; Wightman 1976.
Also CERN TH-1344.
<!--
</p><p>
Ronald J. Evans, J. Boersma, N.M. Blachman, A.A. Jagers:
The Entropy of a Poisson Distribution: Problem 87-6, SIAM Review 30,2 (1988) 314-317.
BETTER IS??? Adell 2010
-->
</p><p>
Charles Fefferman, Jürg Früohlich, Gian Michele Graf:
Stability of ultraviolet cutoff quantum electrodynamics with nonrelativistic matter,
Commun.Math.Phys. 190,2 (1997) 309-330.
</p><p>
Hans G.  Feichtinger &amp;  Karlheinz Gröchenig, 
Irregular sampling theorems and series expansions of band-limited
functions,
J. Math. Anal. Appl. 167,2 (1992) 530-556.
</p><p>
Hans G.  Feichtinger &amp;  Karlheinz Gröchenig, 
Iterative reconstruction of multivariate band-limited functions from
irregular sampling values,
SIAM J. Math. Anal. 23,1 (1992) 244-261.
</p><p>
Joel S. Feldman, Thomas R. Hurd, Lon Rosen, Jill D. Wright:
QED a proof of renormalizability, Springer 
(Lecture Notes in Physics #312) Berlin 1988, 176 pages.
<!--
http://books.google.com/books?id=EYa0AAAAIAAJ
The authors give a detailed and pedagogically well written proof of the renormalizability of quantum electrodynamics in four dimensions. The proof is based on the free expansion of Gallavotti and NicolA and is mathematically rigorous as well as impressively general. It applies to rather general models of quantum field theory including models with infrared or ultraviolet singularities, as shown in this monograph for the first time. Also discussed are the loop regularization for renormalized graphs and the Ward identities. The authors also establish that in QED in four dimensions only gauge invariant counterterms are required. This seems to be the first proof which will be accessible not only to the expert but also to the student.

It claims to prove on p.112ff that the coefficients in the loop 
expansion of the QED S-matrix are bounded by const*(N!)^{1/2)/R^n for 
some R>0, which would imply that it is locally Borel summable.
(But hep-ph/9701418 seems to make opposite claims?)
See also 
M. Beneke: Renormalons, http://arxiv.org/abs/hep-ph/9807443
-->
</p><p>
V.Ferrari &amp; B.Mashhoon:
Oscillations of a Black Hole,
Phys.Rev.Letters 52,16 (1984) 1361-1364.
</p><p>
V.Ferrari &amp; B.Mashhoon:
New approach to the quasinormal modes of a black hole,
Phys.Rev. D 30,2 (1984) 295-304.
</p><p>
Richard P. Feynman:
<a href="http://puhep1.princeton.edu/%7Emcdonald/examples/QED/feynman_pr_74_1430_48.pdf">Relativistic
cutoff for Quantum Electrodynamics</a>,
Phys. Rev. 74,10 (1948) 1430-1438.
Footnote 13 describes the realization that Feynman's previous Lamb
shift computation was erroneous.
The same erroneous result had also been given by Schwinger in Michigan summer school lectures,
but the correct result was computed by French &amp; Weisskopf and by Kroll &amp; Lamb.
</p><p>
Richard P. Feynman:
<a href="http://authors.library.caltech.edu/3523/1/FEYpr49c.pdf">Space-Time
Approach to Quantum Electrodynamics</a>,
Phys. Rev. 76,6 (1949) 769-789.
<!-- 
http://web.ihep.su/dbserv/compas/src/feynman48c/eng.pdf   non-relativistic QM
http://www.ffn.ub.es/luisnavarro/nuevo_maletin/Feynman_QED_1949.pdf   QED
-->
</p><p>
Richard P. Feynman:
<a href="http://www.nobelprize.org/nobel_prizes/physics/laureates/1965/feynman-lecture.html">The 
Development of the Space-Time View of Quantum Electrodynamics</a>,
Nobel Prize lecture 1965.
Available on Nobelprize.org website, as well as in book 
<i>Nobel Lectures, Physics 1963-1970</i>, Elsevier, Amsterdam, 1972.
<!--QUOTES:
With [the delta pseudo-function] replaced by f [a sharply peaked genuine function]
the calculations would give results which were not "unitary", 
that is, for which the sum of the probabilities of all alternatives was not unity. 
The deviation from unity was very small, in practice, if a was very small. 
In the limit that I took a very tiny [width of f-peak is of order a^2], 
it might not make any difference. 
  And, so the process of the renormalization could be made, you could calculate 
everything in terms of the experimental mass and then take the limit and the 
apparent difficulty that the unitarity is violated temporarily seems to disappear. 
[But] I was unable to demonstrate that, as a matter of fact, it does.
It is lucky that I did not wait to straighten out that point, for as far as I know, 
nobody has yet been able to resolve this question. 
  Experience with meson theories with stronger couplings and with strongly coupled vector photons, 
although not proving anything, convinces me that if the coupling were stronger, or if you 
went to a higher order (137th order of perturbation theory for electrodynamics), 
this difficulty would remain in the limit and there would be real trouble.
That is, I believe there is really no satisfactory quantum electrodynamics, but I'm not sure.
And, I believe, that one of the reasons for the slowness of present-day progress in 
understanding the strong interactions is that there isn't any relativistic theoretical model, 
from which you can really calculate everything. Although, it is usually said, that the 
difficulty lies in the fact that strong interactions are too hard to calculate, I believe, 
it is really because strong interactions in field theory have no solution, have no sense 
they're either infinite, or, if you try to modify them, the modification destroys the 
unitarity. [Feynman here was unaware of t'Hooft-Veltman "dimensional regulariation" which 
had not yet been thought of.] I don't think we have a completely satisfactory relativistic 
quantum-mechanical model, even one that doesn't agree with nature, but, at least, agrees 
with the logic that the sum of probability of all alternatives has to be 100%.  
Therefore, I think that the renormalization theory is simply a way to sweep the 
difficulties of the divergences of electrodynamics under the rug. I am, of course, 
not sure of that.
   This completes the story of [my] development of the space-time view of quantum 
electrodynamics...  It is most striking that most of the ideas developed in the course
of this research were not ultimately used in the final result. For example, the half-advanced 
and half-retarded potential was not finally used, the action expression (1) was not used, 
the idea that charges do not act on themselves was abandoned. The path-integral formulation
of quantum mechanics was useful for guessing at final expressions and at formulating the 
general theory of electrodynamics in new ways - although, strictly it was not absolutely
necessary. The same goes for the idea of the positron being a backward moving electron,
it was very convenient, but not strictly necessary for the theory...
-->
</p><p>
Richard P. Feynman:
QED: The Strange Theory of Light and Matter,
Princeton Univ. Press 1985, reprinted 2006. QC793.5.P422F48
</p><p>
R.P. Feynman &amp; A.R.Hibbs:
<a href="http://users.physik.fu-berlin.de/%7Ekleinert/Feynman-Hibbs/fh-typos.pdf">Quantum 
mechanics and path integrals</a>, McGraw-Hill 1965.
</p><p>
R.P. Feynman &amp; Frank L. Vernon, Jr:
<a href="http://www.physics.arizona.edu/%7Epjacquod/papers_statmech/irreversibility_relaxation/feynman_63.pdf">The theory of a general quantum mechanical system interacting with a 
linear dissipative system</a>, Annals of Physics 24 (1963) 118-173.
Reprinted Annals of Physics 281, 1-2 (April 2000) 547-607.
<small>All papers by Feynman we cite are reprinted in <i>Selected papers of Feynman</i> 
(ed.Laurie M.Brown) World Scientific 2000.</small>
</p><p>
Moshe Flato, Jacques C.H. Simon, Erik Taflin:
Asymptotic completeness, global existence and the infrared problem for the Maxwell-Dirac equations,
<a href="http://arxiv.org/abs/hep-th/9502061">308-page monograph</a>. 
Shorter earlier versions:
<a href="http://projecteuclid.org/euclid.cmp/1104159807">
On global solutions of the Maxwell-Dirac equations</a>,
Commun. Math'l. Physics 12,1 (1987) 21-49;
The Maxwell-Dirac equations: Asymptotic completeness and infrared problem,
Reviews in Math. Phys. 6,5a (1994) 1071-1083.
</p><p>
O. Fleischman:
<a href="http://www.springerlink.com/content/n6n09240q58q285n/fulltext.pdf">Gauge
Invariance, the Vertex Function,
and the Magnitude of the Renormalization Constants
of Quantum Electrodynamics</a>,
Il Nuovo Cimento 29,5 (1963) 1098-1119.
</p><p>
Adriaan Daniël Fokker: <!-- 1887-1972-->
Ein invarianter Variationssatz für die Bewegung mehrerer electrischer Massenteilshen,
Zeitschrift für Physik 58 (1929) 386-393.
Wederkeerigheid in de Werking van Geladen Deeltjes,
Physica IX (1929) 33-42.
Theorie Relativiste de l'Interaction de Deux Particules Chargees,
Physica XII (1932) 145-152.
English translations by Aloysius F.Kracklauer:
<a href="http://nonloco-physics.0catch.com/">http://nonloco-physics.0catch.com</a>.
<!-- Advanced and Retarded.
Other work on same topic by Karl Schwarzschild, Gottingen Nachricten 1903
in his section 12 he gives an acton based on an integral over all 3D space of a retarded potential
quantity which also involves a present-day velocity vector. 
I think he argues over the space of all possible point-charge-trajectories
the one extremizing this action wins.
And Hugo Martin Tetrode, Zeitschrift f&uuml;r Physik  1922.
Also later by Wheeler & Feynman. -->
Also discussed by Ryder 1974, see esp. his pages 1822-3.
</p><p>
Gerald B. Folland:
<a href="http://www.math.washington.edu/%7Efolland/Homepage/qft.pdf">Quantum field theory</a>
a tourist guide for mathematicians, Amer. Math'l. Soc. 2008, QC680.F65.
Errata <a href="http://www.math.washington.edu/%7Efolland/Homepage/qft.pdf">http://www.math.washington.edu/~folland/Homepage/qft.pdf</a>.
</p><p>
L.H. Ford &amp; N.F. Svaiter:  
<a href="http://arxiv.org/abs/quant-ph/9804056">Vacuum 
energy density near fluctuating boundaries</a>, Phys. Rev. D 58,6 (1998) 065007.
</p><p>
R.M.Frank: <a href="http://prola.aps.org/abstract/PR/v83/i6/p1189_1">The Fourth-Order 
Contribution to the Self-Energy of the Electron</a>,
Physical Review 83,6 (1951) 1189-1193.
<!--
The self-energy of a free electron is calculated to order e^4=&alpha;^2.
It is found that the result depends upon (ln&Lambda;)^2, 
but it is not the square of the second-order self-energy.
  See also: David Kaiser: Physics and Feynman's Diagrams, American Scientist (March 2005)
http://web.mit.edu/dikaiser/www/FdsAmSci.pdf
-->
</p><p>
J.B. French &amp; V.F.Weisskopf:
The electromagnetic shift of energy levels, Phys. Rev. 75,8 (1949) 1240-1248.
</p><p>
John Friedman &amp; Rafael Sorkin: Spin 1/2 from gravity, Phys. Rev.Lett. 44,17 (1980) 1100-1103.
<!--quantized spin without spin-->
</p><p>
John L. Friedman &amp; Koji Uryu:
<a href="http://arxiv.org/abs/gr-qc/0510002">
Post-Minkowski action for point particles and a helically symmetric binary solution</a>,
Phys.Rev. D73 (2006) 104039.
</p><p>
T.Fulton, F.Rohrlich, L.Witten:
Conformal Invariance in Physics,
Rev. Mod. Phys. 34,3 (1962) 442-457.
</p><p>
W.I. Fushchich &amp; A.G. Nikitin: 
<a href="http://www.imath.kiev.ua/%7Efushchych/papers/1979_2.pdf">On 
the new invariance group of Maxwell equations</a>,
Lettere al Nuovo Cimento 24,7 (1979) 220-224. 
<!--
point out there is a larger group which is not about coordinate transformations but rather 
field transformations.
This paper would seem invalidated by Bracken+Jessup 1982.
--
Wilhelm I. Fushchich &amp; A.G. Nikitin: 
<a href="http://www.imath.kiev.ua/~fushchych/papers/1978_6.pdf">Conformal invariance 
of relativistic equations for arbitrary spin particles</a>,
Letters in Mathematical Physics 2,6 (1978) 471-475.
--first established for Maxwell, then spin=1/2, then any spin, now generalize+unify all that
-->
</p><p>
S.G.Gorishny, A.L.Kataev, S.Larin, L.Surguladze: 
The analytic four-loop corrections to the QED &#946;-function in the MS scheme and the QED
&#968;-function. Total reevaluation, Physics Letters B 256,1 (1991) 81-86.
<!--
gives 4 loops, but answer is scheme dependent starting at the 3rd term.
"It is found that the zeta(3), zeta(4), and zeta(5)-terms cancel in 
the ultimate result for the four-loop coefficient of the F1-function."
-->
</p><p>
S.G. Gorishny, S.A. Larin, L.R. Surguladze, F.V. Tkachov:
MINCER: Program for multiloop calculations in quantum field theory
for the Schoonschip system,
Computer Physics Communications 55,3 (October 1989) 381-408.
<!-- goes up to 3 loops only, cannot do 4.
It seems like there is nothing that can do arb # loops, arb process? -->
</p><p>
Gilad Gour &amp; Vardarajan Suneeta:
<a href="http://arxiv.org/abs/gr-qc/0401110">Comparison of area
spectra in loop quantum gravity</a>,
Classical &amp; Quantum Gravity 21,14 (2004) 3405-3417.
</p><p>
<!--
N. Graham, R.L. Jaffe, V. Khemani, M. Quandt, O. Schr&Ìˆouml;der, H. Weige:
<a href="http://arxiv.org/abs/hep-th/0309130">The Dirichlet Casimir Problem</a>,
Nucl.Phys. B677 (2004) 379-404.
</p><p>
-->
G.Grammer &amp; D.R.Yennie:
Improved treatment for the infrared divergence problem in QED,
Phys. Rev. D 8,12 (1973) 4332-4344.
</p><p>
G.t' Hooft &amp; M.Veltman:
<a href="http://igitur-archive.library.uu.nl/phys/2005-0622-155148/13877.pdf">Regularization
and renormalization of gauge fields</a>, Nuclear Physics B 44,1 (1972) 189-213.
<!-- dimensional regularization. Good paper.Sets up analytic fn of D, get physical result 
when D=4, satisfy Ward identity, unitarity, causality for all D. -->
</p><p>
G.t' Hooft &amp; M.Veltman:
<a href="http://www.staff.science.uu.nl/%7Ehooft101/gthpub/combinatorics_1972.pdf">Combinatorics
of Gauge fields</a>,
Nucl.Phys. B 50 (1972) 318-353.
</p><p>
T.R.Hurd:
<a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.cmp/1104179080">A renormalization group proof of perturbative renormalizability</a>, 
Communications in Math'l Physics 124,1 (1989) 153-168.  <!-- lambda phi^4 theory, not QED -->
</p><p>
S.G.Gasiorowicz, D.R.Yennie, H.Suura: Magnitude of Renormalization
Constants, Phys. Rev. Lett. 2,12 (1959) 513-516.
<!-- K&auml;llen's proof here criticized, but K&auml;llen stated some similar self-criticisms.
"Five years ago Kallen published a proof that
at least one of the renormalization constants in
quantum electrodynamics is infinite. We
have re-examined this problem and arrive at the
conclusion that (a) Kallen's proof is not conclusive,
(b) the renormalization constants could be
finite under rather special circumstances, and
(c) the question of gauge invariance is quite irrelevant
to this problem."
--
</p><p>
Murray Gell-Mann &amp; Francis E. Low:
Quantum Electrodynamics at Small Distances,
Phys. Rev. 95,5 (1954) 1300-1312.
-- p1038 column2 case b raises possibility of Landau pole. -->
</p><p>
Holger Gies &amp; Joerg Jaeckel: 
<a href="http://arxiv.org/abs/hep-ph/0405183">Renormalization Flow of QED</a>,
Phys. Rev. Lett. 93,11 (2004) 110405
</p><p>
J.Glimm &amp; A.Jaffe:
Infinite renormalization of the Hamiltonian is necessary,
J. Math'l. Physics 10 (1969) 2212-2214.
<!--We show the unrenormalized hamiltonian in QFT is unbounded from below 
whenever lowest-order pertbn theory indicates that is true.-->
</p><p>
James Glimm &amp; Arthur Jaffe:
Quantum physics: a functional integral point of view,
Springer 1981. Second edition is Springer 1987.
</p><p>
James Glimm &amp; Arthur Jaffe:
Collected Papers, Vols I &amp; II, Birkhauser, Boston 1985.
QC174.46.G57.
(G+J wrote at least 50 papers and some books together, and 
dominated the field of rigorous quantum field theory models.)
</p><p>
M.Göckeler, R.Horsley, V.Linke, P.Rakow, G.Schierholz, H.Stüben:
<a href="http://arxiv.org/abs/hep-th/9712244">Is there a Landau Pole Problem in QED?</a>
Phys.Rev.Lett.80,19 (1998) 4119-4122.
<!--"...This means that spinor QED does not exist as an interacting theory,
similar to what Coleman and Weinberg found for scalar QED."
-->
</p><p>
Sheldon Goldstein:
Quantum Theory Without Observers,
Physics Today 51,3 (1998) 42-46 and 51,4 (1998) 38-42.
</p><p>
Bernard Goodman &amp; Sinisa R. Ignjatovic:
A simpler solution of the Dirac equation in a Coulomb potential,
Amer. J. Physics 65,3 (1997) 214-221;
also comments &amp; addendum  66,7 (1998) 634-638.
</p><p>
<!--
W.Gordon: Zeitschrift f&uuml;r Physik 48 (1928) 11-14.
Egil A. Hylleraas: Zeitschrift f&uuml;r Physik 140 (1955) 626-??.
English translation: Mathematical and Theoretical
Physics, Vol. 2, Wiley, New York, 1970, Part IV, Chapter 5.
Nevill F.Mott: (1929) ???
-->
</p><p>
Marc H. Goroff &amp;. Augusto Sagnotti: Ultraviolet behavior of Einstein gravity,
Nucl.Phys. B 266,3-4 (1986) 709-736;
Quantum gravity at two loops, Phys. Lett. B160 (1985) 81.
<!-- two-loop calculation showing that the S matrix of Einstein's theory of gravity
contains non-renormalizable ultraviolet divergences in four dimensions. -->
</p><p>
Sandro Graffi &amp; Vincenzo Grecchi:
Borel summability and indeterminacy of the Stieltjes moment problem: Application to the 
anharmonic oscillators,
J.Math.Phys. 19,5 (1978) 1002-1008.
</p><p>
S.Graffi, V.Grecchi, B.Simon:
<a href="http://www.math.caltech.edu/SimonPapers/10.pdf">Borel summability: application
to the anharmonic oscillator</a>,
Phys. Lett. B 32,7 (1970) 631-634.
</p><p>
P.A. Grassi, T.Hurth, M. Steinhauser:
<a href="http://arxiv.org/abs/hep-ph/0102005">The Algebraic Method</a>,
Nucl.Phys. B610 (2001) 215-250.
</p><p>
Ya I. Granovskii: 
<a href="http://iopscience.iop.org/1063-7869/47/5/L06/pdf/PHU_47_5_L06.pdf">Sommerfeld
formula and Dirac's theory</a>,
Physics Uspekhi 47,5 (2004) 523-524.
</p><p>
Vincenzo Grecchi, Marco Maioli, Andre Martinez:
<a href="http://www.ma.utexas.edu/mp_arc/c/09/09-82.pdf">Pade summability of the cubic 
oscillator</a>,
J. Physics A 42,42 (2009) 425208.
</p><p>
Leslie Greengard &amp; June-Yub Lee:
Accelerating the nonuniform fast Fourier transform,
SIAM Review 46,3 (2004) 443-454.
<!--
The NUFFT is about 4 times more expensive in one dimension than
a traditional M-point FFT for single precision accuracy. 
It is about 25 times slower in the current implementation than the traditional two-dimensional
FFT.
-->
</p><p>                                                                    
Jeff Greensite &amp; Stefan Olejnik:
<a href="http://arxiv.org/pdf/hep-lat/0302018">Coulomb Energy, Vortices, and Confinement</a>,
Phys.Rev. D67 (2003) 094503.
<!-- about quark confinement, numerical estim of effective potential -->
</p><p>
Walter Greiner &amp; Berndt Müller:
Gauge theory of weak interactions, Springer 1993.
</p><p>
Walter Greiner &amp; Joachim Reinhardt:
Quantum electrodynamics (2nd ed.), Springer 1994.
QC19.3.G7413 1989 vol. 4
</p><p>
K. Gröchenig &amp; H. Razafinjatovo:
On Landau's Necessary Density Conditions for Sampling and Interpolation of Band-Limited Functions,
J. London Math'l Soc. 54,3 (1996) 557-565.
</p><p>
D.J.Gross &amp; F.Wilczek: 
<a href="http://www.aps.org/about/pressreleases/upload/Asymptotically_Free_Gauge_Theories_I.pdf">
Asymptotically Free Gauge Theories I</a>, Phys. Rev. D8,10 (1973) 3633-3652.
<!--http://www.osti.gov/accomplishments/documents/fullText/ACC0083.pdf
p3650: [we have found] a large class of asymptly free theories. We have shown that
all semisimple gauge theories are in this class.
-->
<a href="http://www.aps.org/about/pressreleases/upload/Asymptotically_Free_Gauge_Theories_II.pdf">
Asymptotically Free Gauge Theories II</a>, Phys. Rev. D9,4 (1974) 980-993.
</p><p>
Franz Gross:
Relativistic quantum mechanics and field theory,
Wiley-VCH 1999.
</p><p>
Yukap Hahn &amp; W.Zimmermann:
<a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.cmp/1103841121">
An elementary proof of Dyson's power counting theorem</a>, 
Communications in Math'l Physics 10,4 (1968) 330-342.
</p><p>
D.Hanneke, S.Fogwell, G.Gabrielse:
<a href="http://arxiv.org/abs/0801.1134">New
Measurement of the Electron Magnetic Moment and the Fine Structure Constant</a>,
Phys. Rev. Letters 100,12 (28 March 2008) 120801 [4 pages].
</p><p>
G.H.Hardy: Divergent series,
Oxford 1949, reprinted by AMS 2000.
</p><p>
J.A. &amp; P.M. Hartigan:
<a href="http://projecteuclid.org/euclid.aos/1176346577">The dip test of unimodality</a>,
Annals of Statistics 13,1 (March 1985) 70-84.
Unfortunately incorrect FORTRAN <a href="http://lib.stat.cmu.edu/apstat/217">code</a>
for this, was published as
"Algorithm AS 217" in Applied Statistics 34,3 (1985) 320-325.
Corrected &amp; improved C code exists as an external "package" for "R"
by Martin Maechler, Dario Ringach, Ferenc Mechler, and Yong Lu.
</p><p>
G.Q.Hassoun &amp; D.R.Yennie:
Infrared divergence of the angular momentrum of Bremsstrahlung
and the physical structure of the electron,
Physical Review 134,2B (1964) B436-B444.
</p><p>
Stephen Hawking &amp; G.F.R.Ellis: Large Scale Structure of Space-Time,
Cambridge University Press 1973.
</p><p>
Peter Henrici:
Applied and computational complex analysis,
Wiley 1974. QA331.H453.
</p><p>
Klaus Hepp:
<a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.cmp/1103815087">Proof of the Bogoliubov-Parasiuk Theorem on Renormalization</a>,
Communications in Mathematical Physics 2,4 (1966) 301-326.
</p><p>
A.Houghton, J.S.Reeve, D.J.Wallace: 
High-order behavior in &#966;<sup>3</sup> field theories and the percolation problem,
Phys.Rev. B 17,7 (1978) 2956-2964.
</p><p>
Kerson Huang: Quantum Field Theory: From Operators to Path Integrals,
J. Wiley &amp; Sons 1998.   Incorrect propagator formula on p.30.
</p><p>
Charles A. Hurst: An example of a divergent perturbation expansion in field theory, 
Math'l Proceedings  Cambr. Philos. Soc. 48 (1952) 625-639;
<a href="http://rspa.royalsocietypublishing.org/content/214/1116/44.full.pdf+html">The 
enumeration of graphs in the Feynman-Dyson technique</a>, Proc. Roy. Soc. A 214 (1952) 44-61.
<!-- </p><p>
Charles A. Hurst: Perturbation expansions in quantum field theory, 
Reports on Mathematical Physics 57,1 (2006) 121-129. -->
</p><p>
C.Itzykson, G.Parisi, J.B.Zuber:
Asymptotic estimates in scalar electrodynamics, Phys.Rev.Lett. 38 (1977) 306-310.
</p><p>
John David Jackson: Classical electrodynamics, 2nd ed. Wley 1975; 3rd ed. Wiley 1999.
</p><p>
Arthur Jaffe:
<a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.cmp/1103758734">
Divergence of perturbation theory for bosons</a>,
Commun. Math'l. Physics 1,2 (1965) 127-149.
</p><p>
Arthur Jaffe:
<a href="http://www.arthurjaffe.com/Assets/pdf/CQFT.pdf">Constructive Quantum Field Theory</a>
pp. 111-127 in <i>Mathematical Physics 2000</i> (editor T.Kibble)
Imperial College Press, London 2000.
</p><p>
Arthur Jaffe:
<a href="http://www.arthurjaffe.com/Assets/pdf/Quantum-Theory_Relativity.pdf">
Quantum Theory and Relativity</a>,
pp.209-246 in <i>Contemporary Mathematics</i> 449 (2008)
Group Representations, Ergodic Theory, and Mathematical Physics: 
A Tribute to George W. Mackey (R.S. Doran, C.C. Moore, and R.J. Zimmer, editors.)
</p><p>
Josef M. Jauch &amp; Fritz Rohrlich: Theory of photons and electrons (2nd ed),
Springer-Verlag 1976.  QC680.538 <!--QC174.1.53 chem-->
</p><p>
Ulrich D. Jentschura, Andrey Surzhykov, Jean Zinn-Justin:
Unified treatment of even and odd anharmonic oscillators of arbitrary degree, 
Phys. Rev. Lett. 102,1 (2009), 011601, 4 pages.
</p><p>
George Johnson:
At Lawrence Berkeley, Physicists Say a Colleague Took Them for a Ride,
New York Times 15 October 2002.
</p><p>
Kenneth A. Johnson:
Consistency of Quantum Electrodynamics,
Physical Review 112,4 (1958) 1367-1370.
<!--
ABSTRACT:
The proof of the statement "At least one of the renormalization
constants in electrodynamics is infinite" is examined in the light of
perturbation theory and the gauge invariance of electrodynamics. The
essential result used to derive the statement is found not to
reproduce perturbation theory at least in a simple way. On the basis
of gauge considerations a conjecture is proposed which provides a
modified essential result and which is found to reproduce perturbation
theory. Even if the modified result could be rigorously established,
it would not lead to the statement that any gauge-independent quantity
is infinite. In fact, the combined results would establish only the
statement that the use of gauges where the exact electron "wave
function" relative to the "wave functions" for a free electron is a
constant, is not consistent.
-->
</p><p>
Kenneth Johnson, Marshall Baker, Raymond S. Willey:
Quantum Electrodynamics,
Phys. Rev. Lett. 11,11 (1963) 518-520.
Self-Energy of the Electron
Phys. Rev. 136,4B (1964) B1111-B1119.
Vacuum Polarization in Quantum Electrodynamics,
Phys. Rev. 163,5 (1967)  1699-1715.
K.Johnson &amp; M.Baker:
Some Speculations on High-Energy Quantum Electrodynamics,
Phys. Rev. D 8,4 (1973)  1110-1122.
Asymptotic Form of the Electron Propagator and the Self-Mass of the Electron,
Phys. Rev. D 3,10 (1971) 2516-2526.
Stephen L. Adler:
Short-Distance Behavior of Quantum Electrodynamics and an Eigenvalue Condition for &#945;,
Phys. Rev. D 5,12 (1972) 3021-3047.
J. Bernstein:
Adler's theorem in finite massless QED and possible extensions to non-Abelian gauge theories,
Nuclear Physics B 95,3 (1975) 461-476.
<!--
Stephen L. Adler:
Axial-Vector Vertex in Spinor Electrodynamics,
Phys. Rev. 177,5 (1969) 2426-2438.
S.L.Adler &amp; W.Bardeen: Absence of higher-order corrections in tge
Axial-Vector divergence equation, Phys. Rev. 182,5 (1969) 1517-1536.
C.R.Hagen:
Derivation of Adler's Divergence Condition from the Field Equations,
Phys. Rev. 177,5 (1969) 2622-2623.
-->
</p><p>
Rudolf Haag:
Local Quantum Physics: Fields, Particles, Algebras,
Springer (2nd ed.) 1996.
</p><p>
Gerard 't Hooft:
<a href="http://www.staff.science.uu.nl/%7Ehooft101/lectures/basisqft.pdf">
the conceptual basis of quantum field theory</a>,
free online textbook 2010.
</p><p>
A.Ishida, T.Namba,  &amp; 6 others:
<a href="http://arxiv.org/abs/1310.6923">
New Precision Measurement of Hyperfine Splitting of Positronium</a>,
2013, submitted to Phys.Rev.B.
</p><p>
Ulrich D. Jentschura, Svetlana Kotochigova, Eric-Olivier Le Bigot, 
Peter J. Mohr, Barry N. Taylor:
<a href="http://arxiv.org/abs/physics/0604058">Precise Calculation
of Transition Frequencies of Hydrogen and Deuterium Based on 
a Least-Squares Analysis</a>,
Phys. Rev. Letters 95,16 (2005) 163003.
<!--
Hydrogen spectrum now most precise QED prediction?
Some of the predicted transition frequencies have relative uncertainties more than an 
order of magnitude smaller than that of the g factor of the electron, which was previously 
the most accurate prediction of QED.
-->
</p><p>
Simon Judes &amp; Matt Visser:
Conservation laws in 'doubly special relativity',
Phys.Rev. D 68 (2003) 045001.
</p><p>
M.I. Kadec: The exact value of the Paley-Wiener constant,
Soviet Math. Dokl. 5 (1964) 559-561.  
</p><p>
Gunnar <a href="http://cdsweb.cern.ch/search?f=author&amp;p=K%C3%A4ll%C3%A9n%2C%20Gunnar&amp;ln=en">Källen</a>: 
On the definition of the renormalization constants of Quantum Electrodynamics, 
Helvetica Physica Acta 25 (1952) 417-434.
<a href="http://www.sdu.dk/media/bibpdf/Bind%2020-29/Bind/mfm-27-12.pdf">On the magnitude
of the renormalization constants of Quantum Electrodynamics</a>,
Kongelige Danske Videnskabernes Selskab (Matematisk-fysiske meddelelser) 27,12 (1953) 3-18,
reprinted pp.398-424 of Schwinger 1958.
These constitute parts I and II of a two-part paper, 
and part II is absolutely unreadable without part I.
See also his
Non perturbation theory approach to renormalization technique,
Physica 19,1-12 (1953) 850-858.
All this is also discussed in Källen's book
<i>Quantum electrodynamics</i>, Springer-Verlag 1972 (posthumously translated into English
by C.K.Iddings &amp; M.Mizushima; Källen died in a 1968 crash of his airplane at age 42 and
this book appears to be an almost unaltered translation of a long article he wrote in 1958
for the <i>Handbuch der Physik</i>);  
§47 pages 223-229 discusses his proof that at least one renormalization
constant must be infinite.  A footnote on page 227 corrects an error in his 1953 paper.
</p><p>
H.A.Kastrup: 
<a href="http://arxiv.org/abs/0808.2730">
On the Advancements of Conformal Transformations and their Associated 
Symmetries in Geometry and Theoretical Physics</a>,
Annalen Phys.17 (2008) 631-690.
</p><p>
A.L. Kataev &amp; S.A.Larin:
<a href="http://arxiv.org/abs/1205.2810">
Analytical five-loop expressions for the renormalization group QED &#946;-function 
in different renormalization schemes</a>,
Pisma v ZhETF 96,1 (2012) 64-67.
</p><p>
Y.Kataoka, S.Asai, T.Kobayashi:
<a href="http://arxiv.org/abs/0809.1594">
First test of O(&amp;alspha;<sup>2</sup>) correction of the orthopositronium decay rate</a>,
Phys.Lett.B 671,2 (2009) 219-223.
<!--
Positronium is an ideal system for the research of the bound state QED. 
New precise measurement of orthopositronium decay rate has been performed with an accuracy of 
150 ppm. This result is consistent with the last three results and also the 2nd order correction. 
The result combined with the last three is 7.0401&plusmn;0.0007&mu;sec^âˆ’1 (100 ppm), 
which is consistent with the 2nd order correction and differs from the 1st order calculation by 
2.6&sigma;. It is the first test to validate the 2nd order correction.
-->
</p><p>
H.Kawai, T.Kinoshita, Y.Okamoto:                                                                 
Asymptotic photon propagator and higher-order QED Callan-Symanzik beta-function, 
Physics Letters B 260,1-2 (May 1991) 193-198.
</p><p>
Hikaru Kawai,Yoshihisa Kitazawa, Masao Ninomiya:
Ultraviolet stable fixed point and scaling relations in
(2+&#949;)-dimensional quantum gravity,
Nuclear Physics B 404,3 (September 1993) 684-714
and <a href="http://arxiv.org/abs/hep-th/9303123">hep-th/9303123</a>.
</p><p>
Hikaru Kawai, Yoshihisa Kitazawa, Masao Ninomiya:
Renormalizability of quantum gravity near two dimensions,
Nuclear Physics B 467, 1-2 (1996) 313-331
and <a href="http://arxiv.org/abs/hep-th/9511217">hep-th/9511217</a>.
</p><p>
G.Keller &amp; C.Kopper:
<a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.cmp/1104285909">Renormalizability proof for QED based on flow equations</a>, 
Communications in Math'l Physics 176,1 (1996) 193-226.
</p><p>
Oliver Dimon Kellogg: Foundations of Potential Theory, Springer 1929, 
but reprinted by Dover 1953 and later.
</p><p>
Nicola N. Khuri:
Coupling-constant analyticity and the renormalization group,
Phys. Rev. D 23,10 (1981) 2285-2290.
</p><p>
N.N. Khuri &amp; H.C. Ren:
Explicit solutions for the running coupling constant and separatrix of Quantum Field Theories,
Annals of Physics 189,1 (January 1989) 142-154.
</p><p>
Toichiro Kinoshita:
<a href="http://www.riken.jp/lab-www/theory/colloquium/kinoshita.pdf">Fine 
Structure Constant, Electron Anomalous Magnetic Moment, and Quantum Electrodynamics</a>,
lecture presented at Nishina Hall, RIKEN, 17 November 2010, Japan.
</p><p>
Toichiro Kinoshita:
New Value of the &#945;<sup>3</sup> Electron Anomalous Magnetic Moment,
Phys. Rev. Lett. 75,26 (1995) 4728-4731.  
<!--  1.181259(40) numerically.
Also: numerically found an error in an analytic value which then was
recalculated by J.Sapirstein, private communication, to correct it, see EQ 20:
DELTA I[4x] = 39/8 + (ln2-5) * zeta(2) - zeta(3)/4 = -2.510003 149 = correct version.
-->
</p><p>
Toichiro Kinoshita (ed.):
<a href="http://books.google.com/books?id=bhuBDAcc2zQC&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false">Quantum 
Electrodynamics</a>,
World Scientific 1990.
[Error correction:
change the value of L2 in the table pp. 215-216 to
<nobr>L2=19&#950;(2)/27+335&#950;(3)/144-293/72</nobr>
to agree with EQ5 of Barbieri, Caffo, Remiddi 1972.]
</p><p>
T. Kinoshita &amp; W.Brent Lindquist:
Eighth-order magnetic moment of the electron. V. Diagrams containing no vacuum-polarization loop,
Phys. Rev. D 42,2 (1990) 636-655.
The abstract's "we find the complete eighth-order QED contribution to be -1.434(138)"
and "-1.934(137)"  for quenched QED (here claiming 90% confidence intervals), both are wrong.
</p><p>
Hagen Kleinert:
Path Integrals in Quantum Mechanics, Statistics, Polymer Physics, and Financial Markets,
World Scientific (5th ed.) 2009.
<!--Download up to 5 chapters free:
http://users.physik.fu-berlin.de/~kleinert/kleinert/?p=loadbook&book=8 
ps files: http://users.physik.fu-berlin.de/~kleinert/public_html/kleiner_reb3/psfiles/
-->
QC174.52.P37K54 
</p><p>
Berndt A. Kniehl, Anatoly V. Kotikov, Oleg L. Veretin:
<a href="http://arxiv.org/abs/0806.4927">
Orthopositronium lifetime at O(&#945;) and O(&#945;<sup>3</sup>ln(&#945;)) 
in closed form</a>,
Phys. Rev. Lett. 101 (2008) 193401;
also 
<a href="http://arxiv.org/abs/0909.1431">longer version</a> 
Phys. Rev. A 80,5 (2009) 052501 [11 pages].
<!-- 7 diagrams drawn in fig1 for the alpha^1 term but unfortunately 
individual diagram values are not provided.  
A = âˆ’10.28661 48086 28262 24015 01692 10991...
C =  âˆ’5.51702 74917 29858 27137 88660 98665... 
-->
</p><p>
Nikolai V. Krasnikov: 
Dispersion relation for the Gell-Mann-Low function, the spectrality condition and the 
asymptotic behaviour of the invariant charge,
Report Math'l Phys. 17,3 (1980) 309-311.
<!-- 
Murray Gell-Mann and Francis E. Low:
Quantum Electrodynamics at Small Distances
Phys. Rev. 95,5 (1954) 1300-1312
also discuss the "Landau pole" and invented the Symanzik "beta function" before it was so-named.
krasniko @ ms2.inr.ac.ru
Nikolai.Krasnikov@cern.ch
-->
</p><p>
N.V. Krasnikov:
Bounds on the renormalization group functions in QED and QCD,
Phys. Lett. B 105, 2-3 (1981) 212-214.
</p><p>
N.V. Krasnikov:
Analyticity and renormalization group,
Nuclear Physics B 192,2 (Dec. 1981) 497-508. 
</p><p>
Elisabeth Kraus: <a href="http://arxiv.org/abs/hep-th/9709154">Renormalization
of the electroweak standard model to all orders</a>,
Annals of Physics 262,2 (1998) 155-259.
<a href="http://arxiv.org/abs/hep-th/9809069">Lecture</a> 
slides also available by Kraus with Stefan G. Nibbelink; and
there is another much shorter paper 
<a href="http://arxiv.org/abs/hep-th/9807102">Algebraic 
Renormalization of the Electroweak Standard Model</a> 
by Kraus, Acta Physica Polonica B29 (1998) 2647-2654.
</p><p>
Dirk Kreimer: Knots and Feynman diagrams,
Cambridge Univ. Press 2000. QC174.52.K56 K74
</p><p>
Norman M. Kroll &amp; Willis E. Lamb, Jr:
On the self-energy of a bound electron, Phys. Rev. 75,3 (1949) 388-398.
</p><p>
Steve K. Lamoreaux: 
Demonstration of the Casimir Force in the 0.6 to 6&#956;m Range,
Phys.Rev.Lett. 78,1 (1997) 5-8.
Erratum 81,24 (1998) 5475-5476.
Comment by Astrid Lambrecht &amp; Serge Reynaud: 84,24 (2000) 5672 and
reply 5673.
<!--Casimir effect experimentally verified.-->
</p><p>
Henry J. Landau: 
Sampling, Data Transmission, and the Nyquist Rate,
Proc.IEEE 55,10 (1967) 1701-1706.
</p><p>
E.M.Lifshitz &amp; L.P.Pitaevskii: Statistical physics part 2
(vol.9 of the Landau &amp; Lifshit course of theoretical physics) Pergamon 1980.
</p><p>
Rubin H. Landau: Quantum mechanics II, Wiley-Interscience 1996 second ed.
<!--
</p><p>
O.L. de Lange:
A simple solution for the Dirac hydrogenlike atom,
Amer. J. Physics 57,10 (1989) 883-885.
</p><p>
James S. Langer: Theory of condensation point,
Annals of Physics (N.Y.) 41 (1967) 108-157.
More: 
Statistical Theory of the Decay of metastable states,
Annals of Physics (N.Y.) 54,2 (1969) 258-275; 
Theory of Spinodal Decomposition in Alloys,
Annals of Physics (N.Y.) 65 (1971) 53-86.
-->
</p><p>
Stefano Laporta:
Analytical value of some sixth-order graphs to the
electron g-2 in QED,
Phys. Rev.D 47,10 (1983) 4793-4795.
[Error correction:  replace 11/8 by 11/18 in EQ 11.]
<!--
H-graph values in
EQ 9, 10, 11; total in EQ 12 on last page.
"graph1" totally mirror sym.  -2.751 419 526
"graph4" same but external moved 1 over.  -1.206 376 517
"graph14" same but external moved further over.  +5.509 933 641
total = +1.552137597.
symmetry factors are not mentioned.
-->
</p><p>
Stefano Laporta:
<a href="http://arxiv.org/abs/hep-ph/9410248">
The analytical value of the corner-ladder graphs contribution to the
electron (g-2) in QED</a>,
Phys. Lett. B 343,1-4 (1995) 421-426.
</p><p>
Stefano Laporta:
High-precision &#949;-expansions of three-loop master integrals
contributing to the electron g-2 in QED,
Physics Letters B 523,1-2 (December 2001) 95-101.
<!-- the most divergent one has an  epsilon^(-4)  term -->
[Also: Calculation of master integrals by difference equations,
Physics Letters B 504, 1-2 (2001) 188-194.]
</p><p>
Stefano Laporta:
High-precision epsilon-expansions of massive four-loop vacuum bubbles,
Physics Letters B 549, 1-2 (November 2002) 115-122.
<!-- the most divergent one has an  epsilon^(-4)  term-->
</p><p>
Stefano Laporta &amp; Ettore Remiddi:
<a href="http://arxiv.org/abs/hep-ph/9602417">
The analytical value of the electron (g-2) at order &#945;<sup>3</sup> in QED</a>,
Physics Letters B 379,1-4 (1996) 283-291.
<!-- email: laporta@bo.infn.it , remiddi@bo.infn.it , tk42@cornell.edu -->
Also by S.Laporta and/or E.Remiddi:
The analytic value of the light-light vertex graph contributions to
the electron g-2 in QED,
Phys.Lett. B265,1-2 (1991) 182-184;
<!-- Light-light EQ1 gives the total of the 3 light-light scat graphs:  0.3710052921
agreeing with 0.370986(20) unpub numerical (T.Engelman+M.J.Levine) quoted in
Levine-Park-Roskies PRD 25,8 (1982) 2205-2207.
L+R do not give the 3 individual values.  L-P-R do not
even mention the unpub numerical value, which L+R must have
obtained privately.
Light-light total =
  5/9
+ 931*pisq/54
-24*pisq*ln2
-4*z3/3
+16*a4
+(2/3)*(ln2)^4
-(2/3)*(ln2)^2
-(41/540)*pi^4
-(5/18)*pisq*z3
+(5/6)*z5
= 0.3710052921
-->
Phys.Lett. B301,4 (1993) 440-446;
Phys.Lett. B343,1-4 (1993) 421-446;
Progress in the analytical evaluation of the electron (g-2) in QED;
the scalar part of the triple-cross graphs,
Phys.Lett. B356,2-3 (1995) 390-397;
Analytical value of some sixth-order graphs to the
electron g-2 in QED,
Phys.Rev. D47,10 (1993) 4793-4795;
Il Nuovo Cimento A 106,5 (1993) 675-683;
Acta Physica Polonica B28, 3-4 (1997) 959-977.
</p><p>
Stefano Laporta &amp; Ettore Remiddi:
Status of the QED prediction of the electron (g-2),
Nuclear Physics B (Proc. Suppl.) 181-182 (2008) 10-14.
</p><p>
Benny E. Lautrup: On high-order estimates in QED, Phys.Lett. B 69,1 (1977) 109-111.
</p><p>
T.D.Lee &amp; M.Nauenberg: Degenerate systems and mass singularities,
Physical Review 133,6B (1964) B1549-B1562.
</p><p>
George Leibbrandt:
<a href="http://einrichtungen.ph.tum.de/T30f/lec/QFT2/RevModPhys.47.849.pdf">Introduction
to the technique of dimensional regularization</a>,
Reviews in Modern Physics 47,4 (1975) 849-876.
<!--
Very cute talk is:
Fred Olness: Dimensional Regularization meets Freshman E&M,
http://www.fnal.gov/orgs/utev/talk-slides/DimReg.5.pdf .   Based on
Michel Hans:
An electrostatic example to illustrate dimensional regularization and renormalization 
group technique,
American Journal of Physics 51,8 (August 1983) 694-698.
Charles Kaufman:
An Illustration from Classical Physics of Renormalization Mathematics,
Amer. J. Phys. 37,5 (1969) 560-561.
-->
</p><p>
G. Peter Lepage:
A New Algorithm for Adaptive Multidimensional Integration,
J. Computational Physics 27,2 (1978) 192-203.
<a href="http://www-zeus.desy.de/%7Etassi/Lepage-slac-pub-1839.pdf">http://www-zeus.desy.de/~tassi/Lepage-slac-pub-1839.pdf</a>.
</p><p>
G. Peter Lepage:
<a href="http://arxiv.org/abs/hep-ph/0506330">What is renormalization?</a>
Proceedings of TASI'89: From Actions to Answers, edited by T. DeGrand and D. Toussaint, 
World Scientific (1989).
</p><p>
Michael J. Levine,
Ettore Remiddi,
Ralph Z. Roskies: 
Analytic contributions to the g factor of the electron in sixth order,
Phys. Rev. D 20,8 (1979) 2068-2076;
see also 
Phys. Rev. D 14,8 (1976) 2191-2192
and
Phys. Rev. D 13,4 (1976) 997-1002.
</p><p>
M.J. Levine &amp; Jon Wright:
Anomalous Magnetic Moment of the Electron,
Phys. Rev. D 8,9 (1973) 3171-3179.
</p><p>
M.J. Levine, H.Y. Park, R.Z. Roskies:
High-precision evaluation of contributions to g-2 of the electron in sixth order,
Phys. Rev. D 25,8 (1982) 2205-2207 &amp;
erratum
Phys. Rev. D 26,6 (1982) 1490.
[The main result 1.1765(13) claimed in their abstract is wrong.]
<!--
C6 = 1.1765(13)

graph    value
G1=7        -2.670546(30)  or  -2.267546(30)
G5=9       0.617727(121)
G2=17     0.607660(240)
G4=19     -0.334698(11)
G3=27    1.861922(240)   as corrected in erratum

other graphs: "previous work"
there are 72 graphs but only 40 if mirror images not counted.
Of these 46 (27 if no mirror recount) have been fully done analytically
plus 5 (3 if no mirror recount) have been expressed using 1D integrals
i.e. 51 (30) in all.  This leaves 21(10) remaining.

Of these 21(10), 6(2) have been evaluated to 3 decimals in ref4:
T.Engelman & MJ Levine unpub.

10(5) are evaluated here in table above.

the remaining 5(3) were evaluated in ref8 numerically:
T. Kinoshita and W. B. Lindquist, Cornell Report No.
CLNS-374, 1977 (unpublished).
-->
</p><p>
Guiyun Li &amp; 8 others:
<a href="http://www.virologyj.com/content/3/1/88">Genomic sequence and analysis 
of a vaccinia virus isolate from a patient with a smallpox vaccine-related complication</a>, 
Virology Journal 3,1 (2006) 88.
</p><p>
Elliott H. Lieb:
The stability of matter: from atoms to stars: selecta,
Springer 2001.
QC173.4.T48 L54.
Some relevant papers included and not included in this volume are:
E.H. Lieb, M. Loss, J. P. Solovej: Stability of matter in magnetic fields, 
Phys. Rev. Lett. 75 (1995) 985-989.
E.H. Lieb, H. Siedentop, J. P. Solovej: 
Stability and instability of relativistic electrons in magnetic fields, 
J. Stat'l Phys. 89 (1997) 37-59 [on page 535].
Stability of relativistic matter with magnetic fields, 
Phys. Rev. Lett. 79 (1997) 1785-1788.
E.H. Lieb, Horng-Tzer Yau: The stability and instability of relativistic matter, 
Commun. Math. Phys. 118,2 (1988) 177-213 [on page 485]. 
See also Many-body stability implies a bound on the fine structure constant, 
Phys. Rev. Lett. 61 (1988)  1695-1697.
E.H. Lieb &amp; M. Loss:
<a href="http://arxiv.org/abs/math-ph/0109002">Stability of a model
of relativistic quantum electrodynamics</a>,
Commun.Math.Phys. 228,3 (2002) 561-588.
Lieb: <a href="http://arxiv.org/abs/math-ph/0401004">
Quantum Mechanics, The Stability of Matter and Quantum Electrodynamics</a>, 2004.
</p><p>
Daniel F. Litim: 
<a href="http://arxiv.org/abs/hep-th/0606044">On fixed points of quantum gravity</a>,
AIP Conf. Proc. 841 (2006) 322-.
</p><p>
Daniel F. Litim:
<a href="http://arxiv.org/abs/1102.4624">Renormalisation group and the Planck scale</a>,
Phil. Trans. R. Soc. A 369 (2011) 2759-2778.
</p><p>
W.Liu &amp; 18 others:
High Precision Measurements of the Ground State Hyperfine Structure Interval of Muonium 
and of the Muon Magnetic Moment,
Phys.Rev.Letters 82,4 (1999) 711-714.
</p><p>
J.J.Loeffel, A.Martin, B.Simon, A.S.Wightman: 
<a href="http://www.math.caltech.edu/SimonPapers/6.pdf">Pade approximants
and the Anharmonic Oscillator</a>,
Phys. Lett. B 30,9 (1969) 656-658.
</p><p>
J.J.Loeffel &amp; Andre Martin:
Pade approximants and sextic Anharmonic Oscillators(?),
Proc. R.C. Programme Conf. 25 (May 1970) unpublished.
<!--CERN Report No. TH-1167??  cannot find.-->
Apparently was never published and unavailable to me.
</p><p>
J.S.Lomont: Conformal invariance of massless Dirac-like wave equations,
Il Nuovo Cimento 22,4 (1961) 673-679.
</p><p>
John H. Lowenstein: 
<a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.cmp/1103899686">Convergence Theorems for Renormalized Feynman Integrals with Zero-mass Propagators</a>,
Communications in Mathematical Physics 47,1 (1976) 53-68.
</p><p>
J.Lowenstein &amp; W.Zimmermann:
<a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.cmp/1103899254">The Power Counting theorem for Feynman Integrals with Massless Propagators</a>,
Communications in Mathematical Physics 44,1 (1975) 73-86.
</p><p>
J.Magueijo &amp; L.Smolin: 
Generalized Lorentz invariance with an energy scale, Phys.Rev. D 67 (2003) 044017;
see also their shorter version in Phys.Rev.Lett.
<!-- Lorentz invariance with an energy scale, PRL--> 8,19 (May 2002) 190403.
</p><p>
Franz Mandl &amp; Graham Shaw:
Quantum Field Theory,
revised ed. 1993;
there also is a "second edition," J.Wiley &amp; Sons 2010, which I have not seen.
</p><p>
Helen S. Margolis: How big is the proton?, 
Science 339,6118 (January 2013) 405-406.
</p><p>
W. Martens, L. Mihaila, J. Salomon, M. Steinhauser:
<a href="http://arxiv.org/abs/1008.3070">
Minimal Supersymmetric SU(5) and Gauge Coupling Unification at Three Loops
</a>,
Physical Review D 82,9 (2010) 095013.
</p><p>
Stephen P. Martin:
<a href="http://arxiv.org/abs/hep-ph/9709356">A Supersymmetry Primer</a>,
<tt>http://arxiv.org/abs/hep-ph/9709356</tt>.
<!--
picture fig 6.8 p.61 showing 3-way meet:
  http://zippy.physics.niu.edu/primer/sources/unification.eps
The alpha[strong] world average at MZ is
0.1183+-0.0010 as of year 2010, the picture uses 0.117-0.121 but should have
used 0.1173-0.1193.
LEP showed that the selectron mass must exceed 99 GeV at 95% confidence.
MSSM predicts the existence of muon->electron+photon decay mode, which has never been seen;
ditto for various other decays forbidden in SM but allowed under MSSM, none have yet been seen.
In many MSUGRA models, squark masses must exceed about 1 TeV due to LHC data.
Martin says if SUSY is truly the solution to the hierarchy problem then LHC
should find SUSY within the next few years after the 14 TeV upgrade.
  alpha[grav] = m*m * alpha[electromag] * G/(k*e*e)  
at mass scale m, where k=coulomb law constant=8.9875517873681764*10^9 Newt meter^2 / Coul^2,
e = 1.60217657*10^(-19) Coul, G = 6.67384 * 10^(-11) Newt**meter/kg)^2, 
alpha[Electromag] = 0.00729735257 at low energy.
  alpha[grav] = 2.1109549*10^15 * m*m   with m in Kg.
  1/alpha[grav] = 4.7371926e-16 / (m*m) with m in Kg
  1/alpha[grav] = 1.4906755e+38 / (m*m) with m in GeV  so m=1.7*10^18 GeV to get alpha=50
  alpha[grav] = (m*m) / (mPlanck*mPlanck)
  1/alpha[grav] = (mPlanck*mPlanck) / (m*m)
your 2013 article "Triumph..." figure titled "Why I [heart] SUSY" contains a red curve for gravity coupling.  I believe this red curve is simply numerically incorrect.   The correct formula
is 1/alpha[grav] = (mPlanck/m)^2
where m is the energy scale you call mu.
Your curve ends at 1/alpha=30 vertically hence horizontally should have been at m=mPlanck/squareroot(30)=2.2*10^18 GeV
instead of 5*10^16 GeV.

The red curve you did draw (which I think incorrect, but let's consider it anyhow) actually forms a practically perfect 3-way meet with alpha3 and alpha2 curve crossing point in plain (non-SUSY) standard model.  This would be another 3way miracle of the sort you are so impressed by, except you do not mention it.
Anyhow if you had, your basis for your wager would have an equal counter-basis.

Also, your alpha1 line is plotted wrong too, by
about a vertical factor of 5/3, far as I can tell.
-->
</p><p>
R.J. Martin &amp; M.J. Kearney:
An exactly solvable self-convolutive recurrence, Aequat. Math. 80 (2010) 291-318 (see p.293).
</p><p>
Yuri V. Matiyasevich: Hilbert's Tenth Problem. Cambridge, MA: MIT Press, 1993.
</p><p>
H.P. McKean &amp; I.M. Singer: Curvature and the eigenvalues of the Laplacian,
J. Diff. Geom. 1 (1967) 43-69.
</p><p>
Jagdish Mehra: 
The beat of a different drum, life and science of Richard Feynman,
Oxford Univ. Press 1994.
</p><p>
Jagdish Mehra &amp; Kimball A. Milton:
Climbing the Mountain: The Scientific Biography of Julian Schwinger,
Oxford University Press 2003.
</p><p>
J.A. Mignaco &amp; E. Remiddi,
Fourth-order vacuum polarization contribution to the sixth-order
electron magnetic moment,
Nuovo Cimento A (ser 10) 60,4  (April 1969) 519-529.
</p><p>
Luminita Mihaila:
<a href="http://arxiv.org/abs/1305.3111">
The cost of gauge coupling unification in the SU(5) model at three loops
</a>, 2013.
</p><p>
Luca Di Luzio &amp; Luminita Mihaila:
<a href="http://arxiv.org/abs/1305.2850">
Unification scale vs. electroweak-triplet mass in the SU(5) + 24_F model at three loops
</a>,
Physical Review D 87,11 (2013) 115025.
</p><p>
Luminita N. Mihaila, Jens Salomon, Matthias Steinhauser:
<a href="http://arxiv.org/abs/1201.5868">
Gauge Coupling Beta Functions in the Standard Model to Three Loops
</a>,
Phys.Rev.Lett. 108,15 (2012) 151602.
(See also their more extensive work
<a href="http://arxiv.org/abs/1208.3357">
Renormalization constants and beta functions for the gauge couplings of the Standard Model 
to three-loop order</a>.)
</p><p>
Allen P. Mills, Jr:
Line-shape effects in the measurement of the positronium hyperfine interval,
Phys. Rev. A 27,1 (1983) 262-267.
<!-- 203.3875(16) GHz -->
This corrected an earlier measurement by
Mills &amp; G.H. Bearman:
New Measurement of the Positronium Hyperfine Interval, Phys. Rev. Lett. 34,5 (1975) 246-250.
</p><p>
Charles W. Misner, Kip S. Thorne, John A. Wheeler: Gravitation, Freeman 1973.
</p><p>
Charles W. Misner &amp; John A. Wheeler: Classical physics as geometry: 
Gravitation, electromagentism, unquantized charge, and mass as properties of curved empty space, 
Ann. Phys. 2 (1957) 525-603,
reprinted in Wheeler 1962.
</p><p>
Peter J. Mohr:
Lamb Shift in a Strong Coulomb Potential,
Phys. Rev. Lett. 34,16 (1975) 1050-1052.
</p><p>
Hugh L. Montgomery &amp; Andrew M. Odlyko:
<a href="http://matwbn.icm.edu.pl/ksiazki/aa/aa49/aa4948.pdf">Large 
deviations of sums of independent random variables</a>,
Acta Arithmetica 49 (1988) 427-434.
</p><p>
Colin J. Morningstar &amp; Mike Peardon:
<a href="http://arxiv.org/abs/hep-lat/9901004">Glueball 
spectrum from an anisotropic lattice study</a>,
Physical Review D 60,3 (1999) 034509, 13 pages.
</p><p>
Lubos Motl:
<a href="http://arxiv.org/pdf/gr-qc/0212096">An analytical computation 
of asymptotic Schwarzschild quasinormal frequencies</a>,
Adv. Theor. Math. Phys. 6 (2002) 1135-1162.
</p><p>
Lubos Motl &amp; Andrew Neitzke:
<a href="http://arxiv.org/abs/hep-th/0301173">Asymptotic black hole quasinormal frequencies</a>,
Adv. Theor. Math. Phys. 7,2 (2003) 307-330.
</p><p>
Jeremy N. Munday, F.Capasso, V.A.Parsegian: 
Measured long-range repulsive Casimir-Lifshitz forces,
Nature 457 (2009) 170-173.
[Also discussed <i>Physics Today</i> (Feb. 2009) 19-22.]
<!--Casimir effect experimentally shown to also be repulsive.-->
</p><p>
Taizo Muta: Foundations of quantum chromodynamics,
introduction to perturbative methods in gauge theories,
World Scientific  (2nd ed; lecture notes in physics #57) 1998.
</p><p>
Charles Nash: Relativistic Quantum Fields, Academic Press 1978.
</p><p>
Arnold Neumaier:    
<a href="http://www.mat.univie.ac.at/%7Eneum/papers/physpapers.html#ren">Renormalization: 
An elementary tutorial</a>, manuscript 2011.    
</p><p>
G. Newton, D.A. Andrews, P.J. Unsworth:
<a href="http://rsta.royalsocietypublishing.org/content/290/1373/373">
A Precision Determination of the Lamb Shift in Hydrogen</a>,
Philo. Trans. Royal Soc. A 290,1373 (1979) 373-404.
<!-- Experiment: 1057.862(20) MHz.  They cite theory by
Erickson: 1057.910(10) and Mohr: 1057.864(14).-->
</p><p>
Max R. Niedermaier:
Gravitational Fixed Points from Perturbation Theory
Physical review letters. 103,10 (2009) 101303
</p><p>
Max R. Niedermaier:
Dimensionally reduced gravity theories are asymptotically
safe,
Nuclear Physics B 673,1-2 (November 2003) 131-169.
</p><p>
Max R. Niedermaier:
<a href="http://arxiv.org/abs/gr-qc/0610018">The Asymptotic Safety Scenario 
in Quantum Gravity -- An Introduction</a>,
Class. Quant. Grav. 24 (2007) R171-230.
</p><p>
B.P. Nigam:
Gell-Mann-Low equation: Relation between the vacuum polarization
coefficients up to the eighth order,
Phys. Rev. D 60,2 (1999) 025006.
</p><p>
B.P. Nigam &amp; R. Acharya:
Gell-Mann-Low equation: Determination of the fifth order vacuum
polarization coefficient b(5),
Phys.Rev. D47,4 (1993) 1726-1728.
</p><p>
Kazuhiko Nishijima:
BRS-invariance, asymptotic freedom and color confinement (a review),
Czechoslovak J. Phys. 46,1 (1996) 1-40.
<!--JSL 94-255v46-->
This reviews numerous papers mainly by the author in 
Int'l. J. Mod. Phys. 1994-1996,
Prog. Theor. Phys. 1985-1987,
and
Physics Letters B 116,4 (1982) 295-297.
</p><p>
P. Nogueira: Automatic Feynman graph genration,
J. Computational Phys 105,2 (1993) 279-289.
<!--graphs only, no formulas, no renorm'n-->
</p><p>
Hans-Peter Nollert:
Quasinormal modes of Schwarzschild black holes: The 
determination of quasinormal frequencies with very large imaginary parts,
Phys. Rev. D 47,12 (1993) 5253-5258.
<!-- Nollert table p5256 makes it clear numerically
that with L=fixed, re(W) behaves as power series in N^(-1/2).
Meanwhile im(W) behaves like N/2+power series in N^(-1/2).
w = (N/2)i + (0.0874247 - i/4) + (1-i)*0.97*[(L+1)*L-1] * N^(-1/2)) + G((L+1)L)/N + O(N^(-3/2))
The constant term is 0.0874247... = ln(3)/(4*Pi) where Nollert got up to the 7. 
The next term is K(L)*N^(-1/2) where
K(2)=0.485051   fit=0.4850
K(3)=1.067134   fit=1.0670
K(6)=3.9754     fit=3.9770
K(L)=0.097 * [(L+1)*L-1] approximately.
ZZ := (L) -> 0.097 * ((L+1)*L-1);
G(5)=-0.0021+0.738i
G(11)=-0.012+3.6i
G(41)=0.26+48i
fit[leastsquare[[x,y], y=a*x^2+b*x+c]]( [[5,11,41],[0.738,36,48]]);
fit[leastsquare[[x,y], y=a*x^2+b*x+c]]( [[5,11,41],[-0.0021, -0.012, 0.26]]);
G(x) seems to be growing very roughly like
G := (x) -> (0.0003+0.053*I)*x^2;
nollert@tat.physik.uni-tuebingen.de
Hans-Peter Nollert: Quasinormal modes: the characteristic `sound' of black holes and neutron stars
[Review], Classical &amp; Quantum Gravity 16,12 (1999) R159
Nollert built on work by
Edward W. Leaver 1990.
-->
</p><p>
H-P. Nollert:
Quasinormal modes: the characteristic 'sound' of black holes and neutron stars
[review],
Classical &amp; Quantum Gravity 16,12 (1999) R159-216.
</p><p>
Harry Nyquist:
<i>Certain topics in telegraph transmission theory</i>,
Trans. Amer. Inst. Elec. Engin. 47 (1928) 617-644.
</p><p>
A. Ore &amp; J.L. Powell:
Three-Photon Annihilation of an Electron-Positron Pair,
Phys. Rev. 75,11 (1949) 1696-1699.
Calculation redone (confirming it), now using Feynman diagram techniques, by
J.M.Radcliffe: Philosophical Magazine Series 7, vol. 42, issue 334 (1951) 1334,
but without publishing details.
</p><p>
Richard Otter: The Number of Trees, Annals of Maths. 49,3 (1948) 583-599.
<!-- see also http://arxiv.org/pdf/cond-mat/0501594v3.pdf -->
</p><p>
Roberto Percacci &amp; Daniele Perini:
Constraints on matter from asymptotic safety,
Phys. Rev. D 67 (2003) 081503 
and 
<a href="https://dl.dropboxusercontent.com/u/3507527/R.Percacci%20&amp;%20D.Perini:">hep-th/0207033</a>;
</p><p>
Roberto Percacci &amp; Daniele Perini:
Asymptotic safety of gravity coupled to matter, Phys. Rev. D 68 (2003) 044018
and
<a href="http://arxiv.org/abs/hep-th/0304222">hep-th/0304222</a>.
</p><p>
Roberto Percacci &amp; Daniele Perini:
<a href="http://arxiv.org/abs/hep-th/0401071">
On the Ultraviolet Behaviour of Newton's constant</a>,
Classical &amp; Quantum Gravity 21 (2004) 5035-5041.
</p><p>
Michael E. Peskin &amp; Daniel V. Schroeder: 
<a href="http://www.slac.stanford.edu/%7Empeskin/QFT.html">An introduction to 
quantum field theory</a>, 
Perseus/Westview Press 1995.
</p><p>
Andreas Petermann: Magnetic moment of the electron,
Nucl. Phys. 3 (1957) 689-690; 
<a href="http://cds.cern.ch/record/213618/files/p1.pdf">Fourth order 
magnetic moment of the electron</a>,
Helv. Phys. Acta 30,5 (1957) 407-408;
Arch. Sci. Soc. Phys. Hist. Nat. Geneve 6 (1953) 5-23.
<!-- also  Helv.Phys.Acta 26 (1953) 291 on phi^4 -->
This arose from Petermann detecting and correcting two errors in
Robert Karplus &amp; Norman M. Kroll:
Fourth-order corrections
in quantum electrodynamics and the magnetic moment of the electron,
Physical Review 77,4 (1950) 536-549.
<!-- AP's 5 diagram values are:
I:    -0.467
IIa:  +0.778
IIc:  -0.564 - ?
IId:  -0.090 + ?
IIe:  +0.016      (this is the only unquenched)
total = -0.328
all in units of (alpha/pi)^2,
where "?" is an infinite renormalization constant.
He has exact formulae for each.  
The 778 arises from "IIa" of Karplus/Kroll.  Need to see K&K PR 77 (1950) 536 for notation.
Petermann's infinite "?" is
    log(LAMBDA/m)
which is only single power of log.  This involves the "single
bubble" Lautrup diagram.
The 5 diagrams for the alpha^2 calculation are given fig 5.23 
of Greiner+R but not computed, just sum stated.

In Karplus&Kroll, 4th order diagrams in fig1 p537.
Class I(1 diagram)=quenched. Irreducible, log-infinite until renormed.
class II(6 diagrams):
 a,b,c,d=quenched
 e,f=unquenched
Class III(4 diagrams) = unquenched. All "reducible."
Class IV(5 diagrams): 4 quenched, 1 unquenched. All "reducible."
ClassV(2 diagrams) = unquenched.  Both zero due to Furry theorem.
2nd order diagrams in fig2 p538: 3 diagrams: 2 quenched, 1 unquenched.
 In the end, the only 4th order diagrams that matter are
   I, IIa, IIc, IId,  IIe
and only the last is unquenched.
-->
</p><p>
Juha Pohjanpelto:
<a href="http://arxiv.org/abs/math-ph/0109021">Classification
of generalized symmetries of the Yang-Mills fields
with a semi-simple structure group</a>,
Differential Geometry &amp; its Applics. 21,2 (2004) 147-171.
</p><p>
Juha Pohjanpelto &amp; Stephen C. Anco:
<a href="http://arxiv.org/abs/0801.1892">Generalized Symmetries
of Massless Free Fields on Minkowski Space</a>,
Symmetry, Integrability and Geometry: Methods and Applications (SIGMA) 4 (2008) 004, 17 pages.
</p><p>
Juha Pohjanpelto &amp; Stephen C. Anco:
<a href="http://arxiv.org/abs/math-ph/0306072">Symmetries and currents
of massless neutrino fields, electromagnetic and graviton fields</a>,
2003.
[This summarizes several of Anco &amp; Pohjanpelto's other works, presents
all symmetries and all currents, 
and makes them available in both tensor and spinor notation.]
</p><p>
Randolf Pohl, Ronald Gilman, Gerald A. Miller, Krzysztof Pachucki:
<a href="http://arxiv.org/abs/1301.0905">
Muonic hydrogen and the proton radius puzzle</a>,
Annu. Rev. Nucl. Part. Sci. Vol 63 (2013). 60 pages.
</p><p>
Robert T. Powers:
<a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.cmp/1103839846">Absence of interaction as a consequence of good ultraviolet behavior
in the case of a local Fermi field</a>,
Commun. Math'l Physics 4,3 (1967) 145-156.
</p><p>
Stuart A. Raby:
Desperately seeking supersymmetry (SUSY),
Reports on Progress in Physics 67 (2004) 755-811.
</p><p>
Eduardo de Rafael &amp; Jonathan L. Rosner:
Short distance behavior of quantum electrodynamics and the
Callan-Symanzik equation for the photon propagator,
Analls of Physics 82,2 (1974) 369-406.
</p><p>
George Yuri Rainich: 
Electrodynamics in general relativity,
Trans. Am. Math. Soc. 27,1 (1925) 106-136.
</p><p>
Michael Reed &amp; Barry Simon:
Methods of Modern Mathematical Physics: Functional analysis, Volume 1,
Academic Press, New York, 1980.
</p><p>
Robert J. Riddell Jr: The number of Feynman diagrams,
Phys.Rev. 91,5 (1953) 1243-1248.
</p><p>
<!--
Adam G.Riess, Lucas Macri, &amp; 10 others:
<a href="http://iopscience.iop.org/0004-637X/699/1/539/pdf/0004-637X_699_1_539.pdf">
A Redetermination of the Hubble Constant with the Hubble Space Telescope from a Differential 
Distance Ladder</a>, 
Astrophysical Journal 699,1 (2009) 539-563.
-->
Adam G.Riess, Lucas Macri, &amp; 8 others:
<a href="http://iopscience.iop.org/0004-637X/730/2/119/">
A 3% Solution: Determination of the Hubble Constant with the Hubble Space Telescope and Wide Field Camera</a>,
Astrophysical Journal 730,2 (April 2011) 119-136.
</p><p>
M.W. Ritter, P.O. Egan, V.W. Hughes, K.A. Woodle:
Precision determination of the hyperfine-structure interval in the ground state of positronium, V,
Phys. Rev. A 30,3 (1984) 1331-1338.
<!-- 203.38910(74) GHz (3.6 ppm)  -->
</p><p>
Donald Robson:
<a href="http://arxiv.org/abs/1305.4552">
Solution to the Proton Radius Problem</a>,
May-Sept. 2013.
</p><p>
F.Rohrlich:
Quantum Electrodynamics of Charged Particles without Spin, Phys. Rev. 80,2 (1950) 666-687.
</p><p>
Morris E. Rose:
Elementary Theory of Angular Momentum,
Wiley 1957.
</p><p>
M.E.Rose &amp; H.A. Bethe:
On the Absence of Polarization in Electron Scattering,
Physical Review 55,3 (1939) 277-289.
<!-- "the discrepancy between theory and experiment remains -- perhaps more glaring than before" 
(p. 278). -->
</p><p>
Lon Rosen &amp; Jill D.Wright:
<a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.cmp/1104201816">Dimensional regularization and renormalization of Quantum Electrodynamics</a>,
Communications in Math'l Physics 134,3 (1990) 433-466.
</p><p></p>
Kjell Rosquist:
<a href="http://arxiv4.library.cornell.edu/abs/gr-qc/0412064v2">Gravitationally
induced electromagnetism at the Compton scale</a>,
Classical and Quantum Gravity 23,9 (2006) 3111-3122.
This paper is wrong or perhaps a better phrase is "highly misleading."
<p></p>
Carlo Rovelli:
<a href="http://arxiv.org/abs/hep-th/0310077">A dialog on quantum gravity</a>,
Int'l.J.Modern Phys. D12 (2003) 1509-1528.
<p></p>
Carlo Rovelli: Quantum gravity, Cambridge Univ. Press 2004.
<!-- sec 8.2 does black hole entropy.
  Reason horizon has entropy, any old surface does not, is
important info gets hidden.
   area = 8*pi*immirzi*hbar*G*c^(-3) * sum sqrt(j_i * (j_i+1))   EQ8.31=EQ6.78
where j_i=1/2, 1, 3/2, ...=spins.
  8.2.2: Finds #ways area can be the right value, identifies with info-entropy.
If Immirzi=(ln2)/(pi*sqrt(3))
get the right answer.
He does not derive/mention thermal spectrum of radiation, but I guess
that follows from Hawking.
  8.2.4: If we instead postulate area=integer*A0  EQ8.47
with A0 of order 1 in Planck units,
then we count partitions, Hardy+Ramanujan+Rademacher,
and we get a way-wrong entropy proportional to sqrt(A) not to A,
and way-wrong nonthermal spectrum dominated by Planck frequency
photons.
-->
<p></p><p>
L.H.Ryder:
Conformal invariance and action-at-a-distance electrodynamics,
J. Phys. A: Math. Nucl. Gen. 7,15 (1974) 1817-1828.
</p><p>
K. Saeedi, S. Simmons, J.Z. Salvail, P. Dluhy, H. Riemann, N.V. Abrosimov, 
P. Becker, H.-J. Pohl, J.J.L. Morton, M.L.W. Thewalt:
<a href="http://www.sciencemag.org/content/342/6160/830">
Room-Temperature Quantum Bit Storage Exceeding 39 Minutes Using Ionized Donors in Silicon-28</a><a>,
Science 342,6160 (2013) 830-833.
</a></p><p><a>
Hanno Sahlmann:
Entropy calculation for a toy black hole,
Class. Quantum Grav. 25,5 (2008) 055004
[14 pages]
</a></p><p><a>
T. Sakai:  On eigenvalues of Laplacian and curvature
of Riemannian manifold, Tohoku Math. J. 23 (1971) 589-603.
</a></p><p><a>
Abdus Salam: Overlapping Divergences and the S-Matrix, Phys.Rev. 82,2 (1951) 217-227.
</a></p><p><a>
Abdus Salam: Divergent Integrals in Renormalizable Field Theories, Phys.Rev. 84,3 (1951) 426-431.
</a></p><p><a>
Tateaki Sasaki: Automatic generation of Feynman graphs in QED,
J. Computational Physics 22 (1976) 189-214.
<!--
he gives these seqs in table I p213:
vertex: 7, 72, 891, 12672     http://oeis.org/A005413
photon self energy: 3, 18, 153, 1638   http://oeis.org/A005412
vacuum: 3, 8, 39    http://oeis.org/A034892 ??
electron self energy: 3, 18, 153
photon-photon scatter: 2, 16, 195
photon-electron scatter: 8, 74, 888
electron-electron scatter: 4, 28, 303
positron-electron scatter: 10,. 94, 1136
four-electron scatter: 4, 90
-->
</a></p><p><a>
Bradley E. Schaefer: Severe limits on variation of the speed of light with frequency,
Physical Review Letters 82,25 (1999) 4964-4966.
</a></p><p><a>
Günter Scharf:
Finite Quantum Electrodynamics: The Causal Approach, 2nd edition, Springer, New York (1995)
[first ed. was 1989].  QC680.S32 <!--QC174.45.S323-->
<!-- Review: DR Grigore: Gauge invariance of QED in causal approach to renormalization,
 hep-th/9911214
Scharf has never published re QFT in Commun Math'l Physics.
 -->
</a></p><p><a>
Florian Scheck: Quantum Physics, Springer (2nd ed.) 2013.
</a></p><p><a>
B.F.Schutz &amp; C.M.Will:
Black hole normal modes: a semianalytic approach,
Astrophys. J. Lett. 291 (1985) L33-L36.
</a></p><p><a>
Fritz Schwarz: </a><a href="http://www.springerlink.com/content/h579w4801qt12280/">Symmetries 
of SU(2)-invariant Yang-Mills theories</a>,
Letters in Math'l Physics 6,5 (1982) 355-359.
</p><p>
Fritz Schwarz:
Symmetries of differential equations – from Sophus Lie to computer algebra,
SIAM Review 30,3 (1988) 450-481.
<!--
AUTOMATICALLY DETERMINING SYMMETRIES OF PARTIAL-DIFFERENTIAL EQUATIONS
 SCHWARZ F
Source: COMPUTING    Volume: 34    Issue: 2    Pages: 91-106    Published: 1985
-->
</p><p>
Silvan S Schweber: An intro to relativistic QFT, Harper Row 1961.
Reprinted by Dover 2005.
</p><p>
Julian Schwinger:
Quantum electrodynamics I, a covariant formulation,
Physical Review 74 (1948) 1439-1461.
</p><p>
Julian Schwinger:
Quantum electrodynamics II, Vacuum polarization and self-energy,
Physical Review 75 (1949) 651-679.
</p><p>
Julian Schwinger:
<!-- On Quantum Electrodynamics and the Magnetic Moment of the
Electron, Phys. Rev. 73,4 (1948) 416-417; detail-free announcement-->
Quantum
electrodynamics III: The electromagnetic properties of the electron –
radiative corrections to scattering,
76,6 (1949) 790-817.
</p><p>
Julian S. Schwinger (ed.):
Selected papers in quantum electrodynamics,
Dover reprint volume, 1958 and reprinted 2012.   
QC680.S35.
(Reprints many early QED papers
including Bloch &amp; Nordsieck 1937, Weisskopf 1939,
Dyson 1949a, Dyson 1949b, Feynman 1949, Feynman "Theory of positrons,"
and Källen 1953.)
</p><p>
Claude E. Shannon:
<a href="http://www.stanford.edu/class/ee104/shannonpaper.pdf">Communication
in the presence of noise</a>, 
Proc. Institute of Radio Engineers 37,1 (January 1949) 10-21; reprinted
as classic paper in: Proc. IEEE 86,2 (Feb.1998) 447-457.
A multidimensional version of the Shannon-Nyquist theorem is presented in
chapter 1 of D.E. Dudgeon &amp; R.M. Mersereau:
<i>Multidimensional Digital Signal Processing</i>, Prentice-Hall, Englewood Cliffs NJ 1984.
<!--SciEng QA402.3.A352v.77??-->
The one-dimensional version is discussed well in
<a href="http://en.wikipedia.org/wiki/Nyquist-Shannon_sampling_theorem">wikipedia</a>.
<!-- see also
http://marksmannet.com/RobertMarks/REPRINTS/1986_MultidimensionalSignalSampleDependency.pdf
-->
</p><p>
S.S. Shapiro &amp; M.B. Wilk:
An analysis of variance test for normality (complete samples),
Biometrika 52,3-4 (1965) 591-611.
FORTRAN <a href="http://lib.stat.cmu.edu/apstat/R94">code</a> 
for this test was published by Patrick Royston as "algorithm AS R94"
in Applied Statistics 44,4 (1995) 547-551.
<!-- The calculation of the p value is exact for n = 3, otherwise
approximations are used, separately for 4 <= n <= 11 and n >= 12.
see also:
Patrick Royston (1982) An extension of Shapiro and Wilk's W test for
normality to large samples. Applied Statistics, 31, 115â€“124.
Patrick Royston (1982) Algorithm AS 181: The W test for Normality.
Applied Statistics, 31, 176â€“180.
-->
</p><p>
Dmitrij V. Shirkov &amp; Vladimir F. Kovalev:
<a href="http://arxiv.org/abs/hep-th/0001210">Bogoliubov Renormalization
Group and Symmetry of Solution in Mathematical Physics</a>,
Phys.Rept. 352 (2001) 219-249.
<!--
Explains gamma and beta fns, Bogoliubov renormalization picture,
deriv=0 idea.
My exact solution of the 2nd order CS eqn seems new in the sense they do it
inexactly here. As is made clear on p8.
-->
</p><p>
Barry Simon:
<a href="http://www.math.caltech.edu/SimonPapers/7.pdf">
Coupling constant analyticity for the anharmonic oscillator</a>
(with appendix by Arnold Dicke),
Ann. Physics 58 (1970) 76-136.
</p><p>
Barry Simon:
<a href="http://www.math.caltech.edu/SimonPapers/R23.pdf">
Large orders and summability of eigenvalue perturbation theory: A mathematical overview</a>, 
Int'l. J. Quantum Chemistry 21 (1982) 3-25.
</p><p>
Vladimir A. Smirnov: Feynman Integral Calculus, Springer 2006.
(This is an update of his 2004 book "Evaluating Feynman Integrals.")
QC174.17.F45S65
</p><p>
Lance Smith: The asymptotics of the heat equation for a boundary value
problem, Inventiones Math. 63 (1981) 467-493.
</p><p>
Warren D. Smith:
<a href="https://dl.dropboxusercontent.com/u/3507527/DistNoise.html">Cumulative 
Noise in D Dimensions</a>: 
Solution of math problem related to both "thermoelastic noise" and quantum gravity,
2010.
</p><p>
Warren D. Smith:
<a href="http://rangevoting.org/WarrenSmithPages/homepage/masslessph.ps">
Reasons the photon is massless</a>,
paper #71 (2003) here: 
<a href="http://rangevoting.org/WarrenSmithPages/homepage/works.html">
http://rangevoting.org/WarrenSmithPages/homepage/works.html</a>.
</p><p>
Warren D. Smith:
<a href="http://rangevoting.org/WarrenSmithPages/homepage/churchq.ps">
Church's thesis meets quantum mechanics</a>,
1999
</p><p>
Warren D. Smith:
Gravity and QED fight it out. QED wins.
Unpublished manuscript 2011 resolving
<a href="#rosquistparadox">Rosquist's paradox</a>.
</p><p>
Lee Smolin:
<a href="http://arxiv.org/abs/hep-th/0408048">An invitation to loop quantum gravity</a>,
2004.
</p><p>
Rados&#321;aw Szmytkowski:
<a href="http://iopscience.iop.org/1751-8121/40/5/009/pdf/1751-8121_40_5_009.pdf">Closed
forms of the Green's function and the generalized Green's
function for the Helmholtz operator on the N-dimensional unit
sphere</a>, 
J. Phys. A 40,5 (2 Feb 2007) 995-1009.
</p><p>
Alan D. Sokal:
An improvement of Watson's theorem on Borel summability,
J. Math'l. Physics 21,2 (1980) 261-263.
</p><p>
Julian Schwinger:
On Quantum-Electrodynamics and the Magnetic Moment of the Electron,
Phys. Rev. 73,4 (1948) 416-417; also 
Quantum
electrodynamics III: The electromagnetic properties of the electron –
radiative corrections to scattering,
76 (1949) 790-817.
Yet another, 1-page, derivation of the first QED correction to Dirac's formula
for the electron magnetic moment, is given in appendix B (the last page) of
Schwinger: On Gauge Invariance and Vacuuwn Polarization,
Phys.Rev. 82,5 (1951) 664-679; and
this also is derived in Folland's book pp.248-251.
<!--
Schwinger's alpha^1 calculation is redone by Greiner+R in sec 5.4
near fig 5.22.  It involves the "no-bubble" Lautrup diagram and several others,
depending how you count, G+R count 3 diagrams beyond Dirac's 1 diagram, figs
5.22b,c,d.  (Dirac's calc is just the bare vertex, "no-photon Lautrup" diagram in
fig 5.22a.) Schwinger's calculation involves a logarithmic infinity according to
Kerson Huang book sec 12.3.  (So it appears from n=0 and n=1 cases that Lautrup is single-log
divergent for any n.)
-->
</p><p>
Charles M. Sommerfield:
Magnetic Dipole Moment of the Electron,
Ann. Phys. 5,1 (1958) 26-57.
<!-- Also Phys. Rev. 107 (1957) 328-329 but above paper gives full details.  -->
</p><p>
E.Stein &amp; G.Weiss: Introduction to Fourier analysis on Euclidean spaces,
Princeton Univ. Press 1971.
</p><p>
Hans Stephani, D.Kramer, M.Maccallum, C.Hoenselaers, E.Herlt:
Exact solutions of Einstein's field equations, Cambridge Univ. Press 2003.
(1980 edition: QC173.6E96.)
</p><p>
George Sterman:
Introduction to quantum field theory,
Cambridge Univ. Press 1993.
[Ch13 discusses infrared finiteness.
§13.3 discusses Poggio-Quinn &amp; Sterman 1976 showing finiteness of 
massless Green's functions.
§13.5 discusses Kinoshita-Lee-Nauenberg theorem on the finiteness of 
averaged transition probabilities.]
</p><p>
P.M. Stevenson: 
Optimization and the ultimate convergence of QCD perturbation theory,
Nuclear Physics B 231 (1984) 65-90.
<!--CERN version = Ref.Th. 3358-CERN (1982)-->
</p><p>
John Stillwell: Sources of hyperbolic geometry, American Math'l Soc. (History of Maths #10) 1996.
[Presents the most important early papers by Eugenio Beltrami, Felix Klein, and Henri Poincare
about hyperbolic geometry, now translated into English and with commentary added by Stillwell.]
<!--Stillwell geometry of surfaces Springer 1992 also looks interesting.-->
</p><p>
Igor M. Suslov:
<a href="http://arxiv.org/abs/0804.2650">Quantum Electrodynamics at Extremely Small Distances</a>,
<tt>http://arxiv.org/abs/0804.2650</tt> (2008).
Warning: Suslov's notation differs from ours (ours follows Peskin &amp; Schroeder).
</p><p>
Leonard Susskind: 
<a href="http://arxiv.org/abs/hep-th/9409089">The world as a hologram</a>, 
J. Math'l. Phys. 36 (1995) 6377-6396. <!-- amazingly garbagy. -->
</p><p>
Max Tegmark:
<a href="http://arxiv.org/abs/gr-qc/9310032">Apparent 
wave function collapse caused by scattering</a>,
Foundations of Physics Letters 6,6 (1993) 571-590.
</p><p>
Bernd Thaller: The Dirac Equation,
Springer (text &amp; monographs physics #357) 1992.
JSE 94-1664.
</p><p>
T. Thiemann:
<a href="http://arxiv.org/abs/gr-qc/9606092">A length operator for canonical quantum gravity</a>,
J.Math.Phys. 39 (1998) 3372-3392.
</p><p>
Walter E. Thirring: On the divergence of perturbation theory for quantized fields, 
Helv. Phys. Acta 26 (1953) 33-52.
(Reprinted pp.469-488 in 
<i>Selected Papers of Walter E. Thirring with Commentaries</i> AMS 1998.)
</p><p>
William P. Thurston with Silvio Levy:
Three-dimensional geometry and topology, 
Princeton University Press 1997.
</p><p>
Yukio Tomozawa: 
<a href="http://deepblue.lib.umich.edu/bitstream/2027.42/23159/1/0000084.pdf">Note 
on the Fried-Yennie gauge</a>, Annals of Physics 128 (1980) 491-500.
</p><p>
Hale F. Trotter:
<a href="http://www.ams.org/journals/proc/1959-010-04/S0002-9939-1959-0108732-6/">On
the product of semi-groups of operators</a>,
Proc. Amer. Math. Soc. 10 (1959) 545-551.  
</p><p>
R.S. Vallery, P.W. Zitzewitz, D.W. Gidley:
Resolution of the Orthopositronium-Lifetime Puzzle,
Phys. Rev. Lett. 90,20 (2003) 203402.
</p><p>
D.V. Vassilevich:
<a href="http://arxiv.org/abs/hep-th/0306138">Heat kernel expansion: user's manual</a>,
Physics Reports 388, 5-6 (2003) 279-360.
</p><p>
G.P.Velo &amp; A.S.Wightman: Renormalization theory, D.Reidel 1976.
Anthology, includes papers by Wightman, Speer, and Lowenstein.
</p><p>
Martinus J.G. Veltman: Diagrammatica,
the path the Feynman diagrams, Cambridge  Univ. Press (Lecture notes in Physics #4) 1994.
</p><p>
R.T.Waechter: On hearing the shape of a drum – extension to higher
dimensions, Proc. Cambridge Philos. Soc. 72 (1972) 439-447.
</p><p>
Robert M. Wald: General relativity,
University of Chicago Press 1984.
</p><p>
Robert M. Wald: Note on the stability of the Schwarzschild metric,
J. Math'l Physics 20,6 (June 1979) 1056-1058.
<!--
rigorous proof within linearized perturbation theory that the Schw.
metric is stable to small perturbations  (i.e they stay bounded for
all time).
-->
</p><p>
Sigmund Waldenstrøm:
On the Dirac equation for the hydrogen atom,
Amer. J. Physics 47,12 (1979) 1098-1100 &amp; addendum 48,8 (1980) 684.
</p><p>
Hsien-Chung Wang:
Two-point homogeneous spaces,
Annals of Maths 55,1 (1952) 177-191.
</p><p>
John Clive Ward:
On the renormalization of quantum electrodynamics,
Proc. Physical Soc. London A 64,1 (1951) 54-56.
<!--
</p><p>
J.C.Ward: 
An Identity in Quantum Electrodynamics,
Phys. Rev. 78,2 (1950) 182-182.
<small>Unreadable without Dyson 1949, and far better discussions of the Ward identity than
Ward's are available, e.g. in Peskin &amp; Schroeder.</small>
-->
</p><p>
Steven Weinberg: High-Energy Behavior in Quantum Field Theory, Phys. Rev. 118,3 (May 1960) 838-849.
</p><p>
Steven Weinberg: Infrared photons and gravitons, Phys. Rev. 140,2B (1965) B516-B524.
</p><p>
Steven Weinberg: 
Gravitation and cosmology: principles and applications of the general theory of relativity,
Wiley 1972.
<!--
It is shown that the infrared divergences arising in the quantum
theory of gravitation can be removed by the familiar methods used in
quantum electrodynamics. An additional divergence appears when
infrared photons or gravitons are emitted from noninfrared external
lines of zero mass, but it is proved that for infrared gravitons this
divergence cancels in the sum of all such diagrams. (The cancellation
does not occur in massless electrodynamics.) The formula derived for
graviton bremsstrahlung is then used to estimate the gravitational
radiation emitted during thermal collisions in the sun, and we find
this to be a stronger source of gravitational radiation (though still
very weak) than classical sources such as planetary motion. We also
verify the conjecture of Dalitz that divergences in the
Coulomb-scattering Born series may be summed to an innocuous phase
factor, and we show how this result may be extended to processes
involving arbitrary numbers of relativistic or nonrelativistic
particles with arbitrary spin.
   Weinberg says his stuff works for QED &  gravitons but not gluons & nonabelian.
-->
</p><p>
Steven Weinberg: The cosmological constant problem,
Reviews in Modern Physics 61,1 (1989) 1-23.
</p><p>
Steven Weinberg:  Ultraviolet divergencies in quantum theories of gravitation,
pp.790-831 (plus separate reference section) in: S. Hawking, W. Israel (eds.),
General Relativity, an Einstein Centenary Survey, Cambridge Univ.
Press, Cambridge, 1979.
</p><p>
Steven Weinberg: Quantum theory of fields, (3 vols.) Cambridge Univ. Press 1999.
Most important for us is vol.1 "Foundations." QC174.45.445.
</p><p>
Steven Weinberg: <a href="http://www.virologyj.com/content/3/1/88">Living with infinities</a>,
2009 lecture.
</p><p>
D.H. Weingarten &amp; J.L. Challifour:
Continuum limit of QED<sub>2</sub> on a lattice, 
I: Annals of Phys. 123,1 (1979) 61-101;
II (by Don Weingarten alone):
Annals of Phys. 126,1 (1980) 154-175.
</p><p>
R.Weiss (1990). "Interferometric Gravitational Wave Detectors". In ed. 
N. Ashby, D. Bartlett and W. Wyss. Proceedings of the Twelfth 
International Conference on General Relativity and Gravitation. Cambidge
 University Press. pp. 331-340.
</p><p>
Ulrich Weiss: Quantum dissipative systems, World Scientific 2nd ed. 
(series in modern condensed matter physics vol. 10) 1999.
QC174.12.W45
</p><p>
Victor F. Weisskopf: 
<a href="http://prola.aps.org/abstract/PR/v56/i1/p72_1">On the self-energy and 
electromagnetic field of the electron</a>, 
Physical Review 56,1 (July 1939) 72-85.
Reprinted in Schwinger 1958.
[Also briefly reviewed by Weisskopf in
<a href="http://users.physik.fu-berlin.de/%7Ekleinert/files/weisskopf.pdf">
Recent developments in the theory of the electron</a>,
Rev.Mod.Phys. 21,2 (April 1949) 305-315.]
</p><p>
John A. Wheeler: Geometrodynamics, Academic Press 1962.
</p><p>
John A. Wheeler &amp; R.P.Feynman:
<a href="http://authors.library.caltech.edu/11095/1/WHErmp45.pdf">Interaction
with the absorber as the mechanism of radiation</a>,
Reviews of Modern Physics 17,2-3 (1945) 157-181; 
<a href="ftp://www.phy.pku.edu.cn/pub/Books/%CE%EF%C0%ED/%BE%AD%B5%E4%D6%F8%D7%F7/works%20by%20feynman/Classical%20Electrodynamics%20in%20Terms%20of%20Direct%20Interparticle%20Action.pdf">Classical Electrodynamics in Terms of Direct Interparticle Action</a>,
Rev. Mod. Phys. 21,3 (1949) 425-433.
</p><p>
E.T.Whittaker: On the functions which are represented by the expansions of the 
interpolation-theory, Proc.Royal Soc. Edinburgh 35 (1915) 181-194.
</p><p>
<!-- Frank Wilczek: 
<a href="http://www.frankwilczek.com/Wilczek_Easy_Pieces/298_QCD_Made_Simple.pdf">QCD made 
simple</a>,
Physics Today (August 2000) 22-28
</p><p> -->
Ian H. Witten, Radford M. Neal, John G. Cleary:
<a http="http://www.stanford.edu/class/ee398a/handouts/papers/WittenACM87ArithmCoding.pdf">
Arithmetic Coding for Data Compression</a>,
Communications of the ACM 30,6 (June 1987) 520-540.
</p><p>
Peter Woit: Not even wrong (failure of string theory and the search for unity on physical law), 
Basic Books 2006.  QC794.6S85W65
</p><p>
Joseph A. Wolf: Spaces of constant curvature, Publish or Perish Inc. (3rd ed.) Boston 1974.
</p><p>
Hidenaga Yamagishi:
Renormalization-group analysis of supersymmetric mass hierarchies,
Nuclear Physics B 216,2 (May 1983) 508-546.
</p><p>
D.R.Yennie, S.C.Frautschi, H.Suura: The Infrared Divergence Phenomena
and High-energy Processes, Annals of Physics (NY) 13,3 (1961) 379-452.
</p><p>
Vladimir A. Yerokhin &amp; Krzysztof Pachucki:
<a href="http://arxiv.org/abs/1001.0406">Theoretical energies of low-lying states of light helium-like ions</a>
Phys. Rev. A 81 (2010) 022507.
</p><p>
F.J. Yndurain: Limits on the mass of the gluon, Phys.Lett.B345,4 (1995) 524-526.
<!-- Upper bounds on the gluon mass, mg, are discussed based on high energy experiments, 
lack of decay proton-free quarks, and scarcity of isolated quarks in matter. One gets 
bounds of the order of 1 MeV, 20 MeV or 10eâˆ’10 MeV = 10^(-4) eV, respectively.
-->
</p><p>
F.J. Yndurain: The Theory of Quark and Gluon Interactions,
Springer 2006.  QC793.5.Q2528 Y58
</p><p>
Anthony Zee:
<a href="http://www.itp.ucsb.edu/%7Ezee/QuantumFieldTh.html">Quantum Field Theory in a Nutshell</a>,
2nd ed. Princeton University Press 2003.
<!--Graviton propagator VIII EQ12 p424. EQ14 also does the massive graviton, and the two are found
to differ in the massless limit.
Discusses Ward identity Gauge invariance II.7 pp140-1.
Photon propagator EQ15, valid in a 1-parameter family of gauges, he recommends doing QED as 
a function of that parameter to verify results do not depend on it.   The "longitudinal mode" 
of a photon does not exist if massless and invariance with respect to this parameter shows that.
-->
</p><p>
Hong-Hao Zhang, Kai-Xi Feng, Si-Wei Qiu, An Zhao, Xue-Song Li:
<a href="http://arxiv.org/abs/0811.1261">On
analytic formulas of Feynman propagators in position space</a>,
Chinese Physics C 34 (2010) 1576-1582.
</p><p>
Antonino Zichichi (ed.): The Whys of Subnuclear Physics,
Plenum, New York/London 1979.
Contains lectures given at
the "Ettore Majorana" Int. School of Subnuclear Physics, Erice, July
1977 (Plenum Press 1979), in particular 
Gerard t'Hooft:
Can we make sense out of "Quantum Chromodynamics"?,
pp.943-971 
and
Arthur S. Wightman:
Should we believe in quantum field theory?, pp.983-1015.
</p><p>
Wolfhart Zimmermann: 
<a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.cmp/1103841149">The Power Counting Theorem for Minkowski Metric</a>,
Communications in Mathematical Physics 11,1 (1968) 1-8.
</p><p>
Wolfhart Zimmermann:
<a href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.cmp/1103841945">Convergence of Bogoliubov's Method of Renormalization in Momentum Space</a>, 
Communications in Mathematical Physics 15,3  (1969) 208-234.
Zimmermann also discussed this in his contribution to
S.Deser, M.Grisaru, H.Pendleton: Lectures on elementary particles and
Quantum Field Theory, MIT Press Cambridge MA 1970, vol1.
</p><p>
Wolfhart Zimmermann:
The power counting theorem for Feynman integrals with massless
propagators, pp.171-184 in
Velo &amp; Wightman 1976.
</p><p>
J.Zinn-Justin:  Perturbation series at large orders in quantum
mechanics and field theories: application to the problem of
resummation,
Physics Reports 70,2 (1981) 109-167.
</p><p>
Bruno Zumino:
The renormalization constants in quantum electrodynamics,
Il Nuovo Cimento 17,4 (1960) 547-557.
</p><p>
Wojciech H. Zurek:
Decoherence and the transition from quantum to classical, Physics Today 44,10 (1991) 36-44;
see also 46,4 (1993) 84 and a crudely-updated version of this is
<a href="http://arxiv.org/pdf/quant-ph/0306072">http://arxiv.org/pdf/quant-ph/0306072</a>.
</p>
<hr>





<div style="left: 0px; top: 0px; position: absolute; width: 29px; height: 27px; border: medium none; margin: 0px; padding: 0px; z-index: 2147483647; display: none;" class="sbi_search" id="sbi_camera_button"></div></body></html>
<!--
Gunnar Kallen (1926-1968):
Helv. Phys. Acta 25 (1952) 417   non-perturbative QED.  Shows 0<=Z3<1.
To see if there is really any problem [with infinities], it is
necessary to go beyond perturbation theory.
To explore this Kallen set out to see if the integral appearing in 1âˆ’Z3, 
and not just its expansion in powers of alpha, actually diverges in the absence of a cut-off.
Of course, he could not evaluate the integral exactly, but since every kind of 
multiparticle state makes a positive contribution to the integrand, he could 
concentrate on the contribution of the simplest states, consisting of just an 
electron and a positron --
if the integral of this contribution diverges, then the whole integral diverges.
In evaluating this contribution, he had to assume that all renormalizations 
including the renormalization of the electron mass and field were finite. 
With this assumption, and some tricky interchanges of integrations, he found that the 
integral for 1âˆ’Z3 does diverge. In this way, he reached his famous conclusion that at 
least one of the renormalization constants in quantum electrodynamics has to be infinite.
This supposedly is not rigorous (Kallen at the end of the 1953
paper disavowed any claim of mathl rigor; also says so on p1 of the paper reprinted in 
Schwinger book) but nevertheless is convincing.
Weinberg says as of 2009 the issue has not been settled.
  L.D. Landau: On the quantum theory of fields,
in Niels Bohr and the Development of Physics (Pergamon Press, NY 1955; W.Pauli ed.) 52-69.
concluded QED breaks down at very high energy, but Weinberg says LL's argument was only based 
on a 1st order approx and nobody really knows.

Gunnar K&auml;llen: On the definition of the renormalization constants of 
Quantum Electrodynamics, Helvetica Physica Acta 25 (1952) 417-434 = pt1

Gunnar K&auml;llen: On the magnitude of the renormalization constants of 
Quantum Electrodynamics, 
Math. Fys. Medd. Kongl. Dansk Vid. Selsk. Bd. 27,12 (1953);
reprinted in Schwinger.  3-???  = pt2
In pt2 he says "all we really know [from perturbative QED]
is the electron self-energy is not an analytic 
function of &alpha; at the origin."

Helvetica Physica Acta paper of 1953. 
Dan. Vid. Selsk. 1953

http://cdsweb.cern.ch/record/1241672/files/p187.pdf?version=1
Gunnar K&auml;llen: On the magnitude of the renormalization constants in
Quantum Electrodynamics, 
K.Danske vidensk. Selsk. mat-fys. Medd. 27,12 (1953) 18p???

Quantum Electrodynamics (Springer-Verlag, Berlin, 1972)

The Present Status of Quantum Electrodynamics
Annual Review of Nuclear Science
Vol. 20: 147-194 (Volume publication date December 1970)
Stanley J Brodsky, and Sidney D Drell

INDIAN JOURNAL OF PURE AND APPLIED MATHEMATICS
Volume 41, Number 1, 39-66
Notes on Euler's work on divergent factorial series and their associated continued fractions
Trond Digernes and V. S. Varadarajan

Nalini Joshi
nalini\ at\ maths.usyd.edu.au or nalini.joshi\ at\ sydney.edu.au
nalini@maths.usyd.edu.au

VI Ogievski: Dirac borel summable???,
Doklady Akad Nauk SSR 109 (1956) 919

The "Higgs mechanism" gives the W<sup>&plusmn;</sup> and Z bosons 
(responsible for the weak force)
their (large) masses.  But if the Higgs self-coupling constant 
called &lambda; by Peskin &amp; Schroeder 
&sect;20.1 were negative or zero, then both these bosons would be massless (or infinite-massed).
That masslessness would cause the weak force to be long-range, as opposed to its actual 
range (exponential e-falloff length) of order
10<sup>-18</sup>meters &ndash; drastically different physics.

http://chemistry.jhu.edu/bin/w/m/64_PhysRevA_32_1965_1985.pdf
does the quartic AnHO yet again, also summarizes other work nicely

Picard's great theorem: in any neighborhood of an essential singularity, every complex
value occurs infinitely often as a value of F(z), with at most one exceptional "missing" value.

redo of Heisenberg-Euler effective lagrangian to all orders.
 Heisenberg+Euler: one-loop nonperturbative renormalized effective action.
 V.Weisskopf: same thing in scalar QED (based on Klein-Gordon eq).
One of H&E's findings was "light-light scattering," refractive index of the vaccuum in
the presence of an imposed electric field.
The divergence of the power series in |B|e/m^2 is like N!
J. Schwinger, Phys./ Rev./ 82 (1951) 664-??
J.Schwinger: Particles sources and fields A-W 1973

S.W.Mages, M.Aicher, A.Sch"afer: http://arxiv.org/pdf/1009.1495v1
Euler-Heisenberg Lagrangian to all orders in the magnetic field and the Chiral Magnetic Effect,
2010  [2nd order in E and al orders in B]

Dunne:   http://homepages.spa.umn.edu/~vainshte/Kogan/Dunne/dunne.ps
EQ 1.19, 1.20, in pure magnetic background the series is alternating and factorially diverging
roughly like   -(-1)^N (2N)! (2eB/m^2)^(2N+4)
in pure electric background we get the same thing except without the alternating sign,
which gives us FAILURE of Borel summability.  This vacuum is unstable to pair production.
In perpendicular E & B fields, unstable if |E|>|B| and stable if |B|>|E|.  If |E|=|B|
then no quantum corrections at all.
If E,B parallel the series is divergent.  All the above Dunne stuff has been for 1-loop
effective Lagrangian.  He says the 2-loop formulae have been computed by Ritus and
seem to show the same kind of behavior.
"The leading rate of divergence is identical, up to an overall factor, to that of the one-loop 
case."
G. V. Dunne and C. Schubert:
Two-Loop Euler-Heisenberg QED Pair Production Rate
 Nucl. Phys. B 564, 591-604 (2000) [hep-th/9907190].
http://arxiv.org/abs/hep-th/9907190
G. Dunne, ``Heisenberg-Euler effective Lagrangians: Basics and extensions,'' To appear in Ian Kogan Memorial Collection, 'From Fields to Strings: Circumnavigating Theoretical Physics';  
arXiv:hep-th/0406216.
http://arxiv.org/abs/hep-th/0406216
G. Dunne, ``Perturbative-Nonperturbative Connection in Quantum Mechanics and Field Theory'', plenary talk at ArkadyFest Symposium, University of Minnesota, May 2002. Published in Continuous Advances in QCD 2002: ArkadyFest, K. Olive,  M. Shifman and M. Voloshin (Eds.), (World Scientific, Singapore 2002), pages 478 - 505.
G. Dunne and T. Hall, ``An Exact QED_3+1 Effective Action'', Physics Letters B 419 (1998) 322.
G. Dunne and C. Schubert, ``Two-loop Euler-Heisenberg Lagrangians and Borel Analysis'', Int. Journal of Mod Phys A 17 (2002), 956

In the case where E and B fields are perpendicular, the effective action is a function
of Y=(B^2-E^2)<sup>1/2</sup>e/m^2:
   L = -m^4/(8pi^2) Y<sup>2</sup>
&int; [coth(s)-s<sup>-1</sup>-s/3]s<sup>-2</sup> exp(-s/|Y|) ds
where the integral is from 0 to Y&infin;,
where if Y is positive real because B^2&gt;E^2,
then Y&infin; is just &infin;;
if Y is imaginary because B^2&lt;E^2,
then Y&infin; is i&infin;; ???
This is exact to first order in &alpha;, i.e. QED<sub>1</sub>, and to all orders
in Y.   (The QED<sub>2</sub> expression is also known; it can be expressed as a double integral
and behaves similarly.)

Craig Hogan holographic test

sum( x^n / (n/3)! , n=0..infinity )
=
exp(x^3)*(
 6*Pi*GAMMA(2/3)
-GAMMA(1/3,x^3)*3^(1/2)*GAMMA(2/3)^2
-2*GAMMA(2/3,x^3)*Pi
)/Pi/2/GAMMA(2/3)
= 
             3                                 3   1/2           2                 3
        exp(x ) (6 Pi GAMMA(2/3) - GAMMA(1/3, x ) 3    GAMMA(2/3)  - 2 GAMMA(2/3, x ) Pi)
        ---------------------------------------------------------------------------------
                                        2 Pi GAMMA(2/3)

sum( (-x)^n / (n/3)! , n=0..infinity )
=
exp(-x^3)*(
 6*Pi*GAMMA(2/3)
-GAMMA(1/3,-x^3)*3^(1/2)*GAMMA(2/3)^2
-2*GAMMA(2/3,-x^3)*Pi
)/2/Pi/GAMMA(2/3)
= 
            3                 3   1/2           2                                    3
      exp(-x ) (-GAMMA(1/3, -x ) 3    GAMMA(2/3)  + 6 Pi GAMMA(2/3) - 2 GAMMA(2/3, -x ) Pi)
      -------------------------------------------------------------------------------------
                                        2 Pi GAMMA(2/3)

sum( x^n / (n/2)! , n=0..infinity ) 
=
exp(x^2) * (1+erf(x))

sum( (-x)^n / (n/2)! , n=0..infinity ) 
=
exp(x^2) * erfc(x)


http://www-stat.wharton.upenn.edu/~steele/Publications/PDF/SEfang.pdf
J.Michael Steele:
Subadditive Euclidean functionals and non-linear growth in geometric probability, 
Annals of Probability 9,3 (1981) 365-376. 
The a.s. existence of a limit constant is proven for Poisson points.
But the variance bound is very weak.
If we want to prove tail bounds -- the probability is very low that the TSP is very short --
then what?
Can use ANN<MST<TSP.  Can argue the chance the ANN avg edge is shorter than K times
what it is supposed to be, drops like O(K)^(D*N) by the arith-geom mean inequality.
This provides factorial-like cutoff if K is of same order or shorter than N^(-1/D).

Consider N points trapped in a cubical 3-volume of order N^2.01 for a time of order N^(0.67).
The length of the Feynman diagram is almost surely of order N^(1.34) meaning hugely faster,
specifically about N^(0.67) times faster, than speed of light.   This leads to
huge amplitude cutoff of order exp(-N^(1.34)).
However, there is a small probability the length is a lot shorter, say K times the
length for 0<K<1.   You will get a cutoff factor of order    exp(-K*N^(1.34)) * O(K)^(3*N)
whose natural log is
     -K*N^(1.34) - 3*N*ln(1/K)
for 0<K<1.   If 1/K>N^(0.334) the second term is good enough for factorial-like cutoff
provided each |diagram| has an O(C^N) a priori upper bound.
If 1/K<N^(0.334)  the first term is good enough for better-than-factorial-like cutoff.
There is an N! factor to count #diagrams.
There is a <N^(2.68*N)   factor for picking N out of N^2.68 points.

Consider N points trapped in a cubical 3-volume of order N^P for a time of order N^(P/3).
The length of the Feynman diagram is almost surely of order N^(P/3 + 2/3) meaning hugely faster,
specifically about N^(2/3) times faster, than speed of light.   This leads to
huge amplitude cutoff of order exp(-N^(P/3+2/3)).
However, there is a small probability the length is a lot shorter, say K times the
length for 0<K<1.   You will get a cutoff factor of order    exp(-K*N^(P/3+2/3)) * O(K)^(3*N)
whose natural log is
     -K*N^(P/3+2/3) - 3*N*ln(1/K)
for 0<K<1.   
There is an N! factor to count #diagrams.
There is a <N^(4PN/3)   factor for picking N out of N^(4P/3) points.
That is the same factor as the 4volume^N factor for scaling.
Together that is a factor of N^((4P/3+1)*N) whose log is
   (4P/3+1)*N*lnN.
Let K=N^(-R).
We need for all R>=0 that either
    P/3+2/3-R > 1     when very large superluminal cutoff
or
   3*R > 4P/3 + 1     when small probability it is that short cutoff
or
   R > P/3-1/3        when impossible to be that short

P=1: 1-R>1, bzzz
P=1.01:  0<R<0.003 or  R>7/9  or   
P=2: 0<R<1/3 or R>11/9 or R>1/3
P=3: 0<R<2/3 or R>5/3 or R>2/3
P=4: 0<R<1 or R>2+1/9 or R>1

so that
   P+2-3 > 4P/3 + 1
   P-2 > 4P/3
   bzzz.


If P>1 and
1/K>N^(0.334) the second term is good enough for factorial-like cutoff
provided each |diagram| has an O(C^N) a priori upper bound.
If 1/K<N^(0.334)  the first term is good enough for better-than-factorial-like cutoff.

I get a small growth like C^N * N^(N/4) at most in the series coeffs... if N^(1/4)-side
hypercube and total length<N so N^(1/4) choices each new vertex at most to avoid
superluminal cutoffs.  THis is improved but still divergent unless can improve bound further.

R.P. Feynman &amp; Frank L. Vernon, Jr:
The theory of a general quantum mechanical system interacting with a 
linear dissipative system, Ann. Phys. 24 (1963) 118-173.
Reprinted Annals of Physics 281,1-2 (April 2000) 547-607.

Quasinormal modes: http://relativity.livingreviews.org/Articles/lrr-1999-2/ 
Nollert, H-P, 
Quasinormal modes of Schwarzschild black holes: The 
determination of quasinormal frequencies with very large imaginary parts,
Phys. Rev. D, 47 (1993) 5253-5258 
also same formula found by 
N.Andersson: On the asymptotic distribution of quasinormal-mode frequencies for 
Schwarzschild black holes, Class. Quantum Grav., 10 (1993) L61-L67 
and 
H.Liu: Asymptotic behaviour of quasi-normal modes of Schwarzschild black holes, 
Class. Quantum Grav. 12 (1993) 543-552. 
Ylm has two indices l and m but everything is independent of m ("degeneracy")
including the Regge Wheeler pseudopotential which depends only on l and r..
N and L are independent and do not constrain the other.
As n->infinity with L fixed the real freq approaches positive const/M
and the imag freq grows proportional to n. 
The const is exactly ln(3)*Thawking for a large class of black holes universally.
Suggesting we need a cutoff on n caused by Planck scale imag freq. cutoffn=M*const.
As L->infinity with n fixed the real freq grows prop to L and the imag freq 
fixed with L but propto n. 
Suggesting we need a cutoff on L caused by Planck scale real freq.
These cutoffs yield cubic(M) number of allowed modes.
This number is proportional to area^(3/2).
Classically kT energy stored each mode.  That true if kT much larger than hf.
But if kT small then hf/2 energy stored per mode.
However, presumably only the modes with real freq below THawking=const/M=const/sqrt(Area)
are excited much.  The others cannot get enough thermal energy to excite even a single quantum.
These modes are diddlysquat.  Almost all modes in ground state.  Hence energy approx equals
area^(3/2) in Planck units.   Which is propto mass CUBED.  Oops.
The energy of actually excited modes only, above the zeropoint energy,
propto sqrt(Area)=mass, since only modes with L and m both below const (since |m|<=L) matter,
so linear(N) mode count.
This is correct.  
Unfortunately this also yields entropy linear in N and hence M.  Oops.

Ummm,  L = some power of n, perhaps sqrt(n),
seems to be a transition point where the imag freq at fixed n rises
above constant/M.  Clearly you cannot make L too large or the real freq goes outside
the Thawking range.  The question is how large.
If L is only allowed to be as large as sqrt(N), which is my guess, and ditto for m
(hopefully these are the "excitable" modes with frequencies near to the temperature)
then the count of those modes would grow quadratic in N hence propto area.
Hence we'd get entropy proportional to area.
But we'd also get energy propto area*temperature = M^1, also correct.

Schutz, B.F., and Will, C.M., "Black hole normal modes: a semianalytic approach,"
Astrophys. J. Lett., 291, L33-L36, (1985).

Ferrari & Mashoon semianalytic method 1984 found
imagpart = +-(j+1/2)
realpart = (n+1/2)
both times const*M.

EW Leaver PRD 34,2 (1986) 384  has cf method

Barreto, A.S., and Zworski, M: 
Distribution of resonances for spherical black holes,
Math. Res. Lett. 4, (1997) 103-121. 

Lubos Motl, Andrew Neitzke., Asymptotic black hole quasinormal frequencies, 
Adv. Theor. Math. Phys. Volume 7, Number 2 (2003), 307-330.
http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&id=pdf_1&handle=euclid.atmp/1112627635
they seem to think if the imag part of the freq->infinity, indeed if |imag|>>|real|
then the real part must go to ln(3)*Thawking ? 
But maybe this is only for fixed L.. but it is independent of L and of the 
field type (Dirac, graviton, EM...).

Physics Letters B
Volume 579, Issues 1-2, 15 January 2004, Pages 25-30

Canad. J. Phys. 84(6-7): 473-479 (2006)  
Universality of highly damped quasinormal modes for single horizon black holes
R. G. Daghigh and G. Kunstatter

Greybody factors at large imaginary frequencies

Andrew Neitzke
http://arxiv.org/abs/hep-th/0304080

So it sounds like the imag freq is propto n indpt of L. 
In that case we get sum(1/n)/N as mean halflife, 
which is to say, logN/N, for the modes with n=1..N. 

Shahar Hod: CQG 21 (2004) L97 discusses area quantization for black hole, says
the quantum is O(1) in planck units regardless of mass and charge
Bohr's correspondence principle and the area spectrum of quantum black holes.
Also short version here:
Shahar Hod, (Hebrew U.) . Nov 1998. 10pp. 
Published in Phys.Rev.Lett.81:4293,1998. 
e-Print: http://arXiv.org/pdf/gr-qc/9812002
Hod says Area = 4*lplanck^2*ln(3)*integer
and says other arguments indicate it might be nonuniform spacing but still spacing
will be O(1) in Planck area units.
Hod seems to be full of shit on most stuff.
The magic constant ln3 was proven by L.Motl in 
http://arxiv.org/pdf/gr-qc/0212096
on his p1146, so long as L*(L+1) is of order well below sqrt|w|
when |w| becomes large, everything works.  |w| should be of order n.
So it suffices if L is of order n^(1/4) or less to get the ln3 asymptotic for
the real part.  I suspect n^(1/2), more weakly, suffices too.
The quadratic formula applied to EQ26 to solve for Rn=R[n-1] as in EQ30 will yield
pretty much the same result provided this is so.
The continued fraction depends only on c1 and on the product c0c2.
If L below order sqrt(N) then |c1| below order c0c2 just like if L=O(1) 
and hence (?) the continued fraction still
will be fast-converging so we can terminate it with little error without need
for his cheapo approximation.  [If L not below that order then we can make the CF 
fail to converge quick?]

http://en.wikipedia.org/wiki/Convergence_problem#Periodic_continued_fractions
see special case K=1, get convergence if (c0c2)/(c1)^2
is not a real below -1/4.  Drat, that seems to require the
L of order N^(1/4) condition? Need to figure out the signs.  Like if c1 small
and c0c2 positive, you are ok; if c1 huge you also are ok. Just stay outside
a parabola to get fast convergence rate, should be ok.   And I think that means
L = o(sqrtN) will do.

http://www.ktf.franko.lviv.ua/JPS/2004/1/pdf/93_100.pdf
6th order WKB formulae.  WKB best when large L fixed N.
The approach at 3rd order is here:
S. Iyer, C. M. Will, Phys. Rev. D 35, 3621 (1987).
The peak of the potential for large L seems to be at r of order L^(-2)
independent of N and M.   Get V=order M*L^8  there.
Get V"=order  M*L^12 there.
Suggests freq^2, M^(1/2)*L^2, and n+1/2 ought to have same order.

The pseudopotential V(r) is
  V(r) = Freq^2 - (1-2M/r) * (L*(L+1)/r^2 + sigma/r^3)
where sigma = -6 for gravl perturbs.
Saddlepoint 
  r = 1/2/(L^2+L)*(3*M*L^2+3*M*L+9+sqrt(9*M^2*L^4+18*M^2*L^3-42*M*L^2+9*M^2*L^2-42*M*L+81))
and +- on the sqrt.

   1/2/(L^2+L)*(3*M*L^2+3*M*L+9-sqrt(9*M^2*L^4+18*M^2*L^3-42*M*L^2+9*M^2*L^2-42*M*L+81))

Asymptotic freedom in QCD, excellent:
Siegfried Bethke:
Experimental Tests of Asymptotic Freedom, 
Progress in Particle & Nuclear Physics 58 (2007) 351-386.
http://arxiv.org/abs/hep-ex/0606035

Experimental QCD review:
http://pdg.lbl.gov/2007/reviews/qcdrpp.pdf

Frank Wilczek:
Asymptotic Freedom: From Paradox to Paradigm (Nobel lecture),
Proc.Nat.Acad.Sci.102:8403-8413,2005; 
Intl.J.Mod.Phys.A20:5753-5778,2005; 
Rev.Mod.Phys.77:857-870,2005
http://arxiv.org/abs/hep-ph/0502113

Original asympt freedom papers:
D.J.Gross and F.Wilczek: Ultraviolet Behavior of non-Abelian Gauge Theories, 
Phys. Rev. Lett. 30,26 (1973) 1343-1346.
http://www.aps.org/about/pressreleases/upload/Ultraviolet_Behavior_of_Non_Abelian_Gauge_Theory.pdf
All nonAbelian gauge thys based on a semisimple Lie group
enjoy UV asymptotic freedom... perhaps uniquely among renormalizable QFTs.
It is easy to incorporate fermions into such a thy without destroying
UV asymptotic freedom.  They also observe the standard model is asymptly free "if we again
ignore the complications due to Higgsons."
D.J.Gross and F.Wilczek: Asymptotically Free Gauge Theories I, Phys. Rev. D8,10 (1973) 3633-3652.
http://www.osti.gov/accomplishments/documents/fullText/ACC0083.pdf
http://www.aps.org/about/pressreleases/upload/Asymptotically_Free_Gauge_Theories_I.pdf
p3650: [we have found] a large class of asymptly freetheoreies. We have shown that
all semisimple gauge theories are in this class.
D.J.Gross and F.Wilczek:  Asymptotically Free Gauge Theories II, Phys. Rev. D9,4 (1974) 980-993.
http://www.aps.org/about/pressreleases/upload/Asymptotically_Free_Gauge_Theories_II.pdf
H.D.Politzer: Asymptotic Freedom: An Approach to Strong Interactions, 
Physics Reports 14,4 (1974) 129-180.
H.D.Politzer: Reliable Perturbative Results for Strong Interactions?
Physical Review Letters 30,26 (1973) 1346.
http://www.aps.org/about/pressreleases/upload/Politzer_Reliable_Perturbative_Results_for_Strong_Interactions.pdf
With asympt freedom, since alpha->0 perturbation theory is "arbitrarily good" at high energies.

DJ Gross: Twenty Five Years of Asymptotic Freedom
Nucl.Phys.Proc.Suppl. 74 (1999) 426-446
http://arxiv.org/abs/hep-th/9809060

Yukio Tomozawa: Note on the Fried-Yennie gauge, Annals of Physics 128 (1980) 491-500
http://deepblue.lib.umich.edu/bitstream/2027.42/23159/1/0000084.pdf
"Renormalization in QED in general introduces infrared divergences due to an expansion on the mass shell.  
While gauge invariance guarantees the cancellation of such divergences in the computation of observable 
quantities, individual diagrams are subjected to this complication and must be treated with some care.  
An exceptional case is Fried-Yennie (FY) gauge in which infreared divergences do not appear even
for individual diagrams."

H.M.Fried & D.R.Yennie: Phys Rev 112 (1958) 1391.

E.Lieb+M.Loss: Self energy of electrons in non-prerturbative QED, in Selecta, stability of matter
http://arxiv.org/abs/math-ph/9908020

physics/0608108
Jo Bovy:
The self-energy of the electron: a quintessential problem in the development of QED
Imprint10 Aug 2006. - 19 p.
To be published in:Stud. Hist. Philos. Mod. Phys.
http://arxiv.org/abs/physics/0608108
it sucks

Giorgio Velo &amp; Arthur S. Wightman (eds.): Renormalization theory. D. Reidel, Dordrecht 
1975. --(in particular pp. 95-160)




Kazuhiko Nishijima (1926-2009).
He was professor emeritus at the University of Tokyo and Kyoto University until his death in 2009
from Leukemia at age 82.
He is most well-known for the product of his collaboration with Murray Gell-Mann, the 
Gell-Mann/Nishijima formula, and the concept of strangeness, which he called the "eta-charge."

K. Nishijima:
BRS Transformation and color confinement,
Physics Letters B
Volume 116, Issue 4, 14 October 1982, Pages 295-297
Abstract
The condition of confinement of quarks and gluons in QCD is derived. 
It is shown that color confinement is realized when there exist massless scalar 
color-octet bound states of two Faddeev-Popov ghosts.

Meiun Shintani:
Extended BRS algebra and color confinement â€”-
Well-definedness of N^a charge,
ZEITSCHRIFT F. PHYSIK 
C PARTICLES AND FIELDS
Volume 28, Number 1  (1985), 87-94, DOI: 10.1007/BF01550253
We examine the color confinement scheme and its realization proposed by Kugo and Ojima. We first point out that their scheme (in its original naive form) lacks self-consistency at the level of realizations. By using the properties of the extended BRS algebra and considering the well-definedness of the charge operatorN a constituting a part of the global color operator, we then deduce the new confinement conditions in the Landau gauge which replace the K-O condition. We also clarify what kind of ghost structures are suggested by the well-or ill-definedness ofN a . It is shown that there are four possible cancellation mechanisms of ghosts. In particular, it turns out that the octet of ghosts proposed by Nishijima in his confinement theory arises from the well-definedness of theN a charge, whereas the elementary quartet arises from the ill-definedness ofN a .

Kazuhiko Nishijima:
Color Confinement and the Asymptotic Condition
Prog. Theor. Phys. Vol. 74 No. 4 (1985) pp. 889-903
Color Confinement and the Asymptotic Condition. II
Prog. Theor. Phys. Vol. 75 No. 5 (1986) pp. 1221-1230
Color confinement and a superconvergence relation, 
Prog. Theor. Phys. 77 (1987) 1035-1039.
Abstract:
It is shown on the basis of BRS algebra and renormalization group that gluons are confined 
when the number of flavors does not exceed nine. We exploit the analysis of the gluon 
propagator by Oehme and Zimmermann, but we give an interpretation of the results different 
from theirs.

K.Nishijima:
Confinement of quarks and gluons, Intern. J. Mod. Phys. A 9, 3799-3819 (1994).
K. Nishijima, Confinement of quarks and gluons II, Intern. J. Mod. Phys. A10, 3155-3167 (1995). 
K. Nishijima and N. Takase, Spectral function sum rule for gauge fields, 
Intern. J. Mod. Phys. A11, 2281-2292 (1996).

MR1388839 (97g:81074)
Nishijima, K.(J-CHUO-P)
BRS-invariance, asymptotic freedom and color confinement (a review).
Czechoslovak J. Phys. 46,1 (1996) 1-40.
In the first part of this review the author presents a proof that the
asymptotic freedom of QCD implies the lack of physical quark and gluon
states in QCD. The framework adopted is the BRST formalism (hence
indefinite metric and unphysical states in Hilbert space). The proof
is quite intricate and employs a whole array of field-theoretical
tools. It is shown in particular that one does not have to assume that
the number of flavors N_f is less than 10; N_f<17 is required for the
asymptotic freedom and is sufficient for the confinement. One should
be aware, however, that the proof yields practically no information on
the structure of physical states (which is from a physical viewpoint a
much more interesting problem). In the appendices of the paper
(entitled "Representations of BRS algebra") the metric structure of
the space of states in QCD is discussed in detail. The paper is based
on original works of the author. It is written in a concise style and
the reader is expected to be well acquainted with methods of quantum
field theory.
Abstract
It is proved that color confinement is an inevitable consequence of BRS invariance and asymptotic
freedom inherent in quantum chromodynamics. BRS invariance is used to interpret or define color
confinement and asymptotic freedom provides an essential means to prove it. It is also pointed
out that introduction of renormalization is an essential step toward this proof and that 
renormalization group serves as the most useful tool in reaching the destination.

Beta function: QED & QCD:
http://en.wikipedia.org/wiki/Beta_function_(physics)

2-loop running coupling in QED is written using lambert W fn
http://cdsweb.cern.ch/record/409154/files/9911456.pdf
x = W(x) exp W(x).
R.M. Corless, G.H. Gonnet, D.E.G. Hare, D.J. Jeffrey and D.E. Knuth, â€œOn the Lam- bert W Functionâ€. Advances in Computation Mathematics, V5, 329 (1996), available from http://pineapple.apmaths.uwo.ca/ Ìƒrmc/papers/LambertW/.
P.M. Stevenson, Phys. Rev. D23, 2916 (1981).
finds QCD running coupling constant without W, just ln's.
MAPLE dsolves 
 q * diff(al(q), q) = -b0/(4*Pi) * al(q)^2 - b1/(4*Pi)^2 * al(q)^3;
 dsolve(% , al(q));;
in terms of a root of a transcendental eqn.
 q * diff(al(q), q) = k2 * al(q)^2 + k3 * al(q)^3;
solution
k3 * al(q) = exp(X) - k2
where X solves

Stephens, C.R.; 't Hooft, G.; Whiting, B.F. (1994). "Black hole evaporation without information loss". Classical and Quantum Gravity 11: 621.

Alexander Moroz: Summability method for a horn-shaped region,
Comm. Math. Phys. Volume 133, Number 2 (1990) 369-382.
http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.cmp/1104201403&page=record
http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&id=pdf_1&handle=euclid.cmp/1104201403

Ward-Takahashi identity:
Y. Takahashi, Nuovo Cimento, Ser 10, 6 (1957) 370.
J.C. Ward: An Identity in Quantum Electrodynamics, Phys. Rev. 78,2 (1950) 182-182.

All issues of Commun Mathl Phys up to 1996:
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&page=past&handle=euclid.cmp

R. F. Streater and A. S. Wightman, PCT, Spin and Statistics and All
That, Princeton University Press, Landmarks in Mathematics and
Physics, 2000.
Wightman axioms
http://www.scholarpedia.org/article/Wightman_axioms

AXIOM I. The states of a quantum theory are normalised vectors in a
separable Hilbert space, \mathcal{H}, two such that differ by a
complex phase giving rise to the same state.
*     wave fns at the raindrops.  Bandlimiting.

AXIOM II.  Poincare/Lorentz (time-dir preserving) invariance.
Unique vacuum state with zero energy.  All other states, positive energy.
*    we don't really have an energy locally... but globally we can do via
integration of the Nyquist interpolations over all space.  Should discuss this
in the mom/energy conserv sec.  What about Lowell Brown's Hamiltonian?

AXIOM III.   QFs are operator-valued tempered distributions.
They transform in the right ways Lorentzly.
*   do they? Actually, no since we violate if push too far...

AXIOM IV. (Strong microscopic) Causality.
that's it.

http://www.oberlin.edu/physics/dstyer/StrangeQM/Klein-Gordon.pdf
useful facts about klein gordon propagator and Fourier transforms

Z Hong-Hao
On analytic formulas of Feynman propagators in position space
2010 Chinese Phys. C 34 1576

Mark Srednicki: Quantum field theory, online book draft
http://www.physics.ucsb.edu/~mark/ms-qft-DRAFT.pdf

Schutz & Will WKB estimate:
(M w)^2 = VL(r0) - i(N+1/2)*sqrt( -2 VL''(r0) )
r0=peak of potential barrier, derivative is with respect to tortoise coordinate r*
http://relativity.livingreviews.org/Articles/lrr-1999-2/
this analysis becomes valid for N large with other stuff fixed.

Lubos Motl and Andrew Neitzke:
Asymptotic black hole quasinormal frequencies,
Adv. Theor. Math. Phys. 7,2 (2003) 307-330.
p314: They claim to study in the limit Im(w)>>Re(w).
and find
   |4pi Re(w)| = ln|1+2cos(pi j)|.

Callen Welton FDT for noninteracting quasimodes
MeanSquareAmplitudeOfMode(w) = hbar Im(alpha(w)) coth( hbar w / (2 kB T) )
alpha is the "susceptibility."   Im(alpha)=Im(i*w/Z)
3.1.18
I think to translate into language I understand:
EfoldRelaxTime(w) * MeanEnergyOfMode(w) / hbar = coth( hbar w / (2 kB T) )
The function coth() includes zeropoint energy.  Replace it with 
   [coth() - hbar*w/2] = hf / [exp(hf/kT)-1]    which drops exponentially toward 0 as hf large
if you want energy above ground only.   Full energy would be infinite, but aboveground energy
sum is finite... for Johnson-Nyquist... for black hole even the aboveground sum would be infinite
if do not put a UV cutoff.  Experiments have not distinguished between the two formula versions
(hard to do since need very high freq).  Well have in sense a hot box of vacum would have
huge weight, which it experimentally does not, hence must use the above-ground formula.
Note the ExcitationProb is then about 2/[exp(hf/kT)-1]-1
which can be used to assess entropy (badly).
http://en.wikipedia.org/wiki/Bose-Einstein_statistics
Careful: The Efold time could be for energy or for sqrt(energy) which is a factor 2 change.
And there are factors of 2pi (and h vs hbar) also to confuse you.   All those can be absorbed
into the defn of RelaxTime.

Bose-Einstein entropy:
en := hf / (exp(hf*rkT)-1 );
#use dSinfo/dE = kT  defn of T to derive Sinfo(nats):
diff(en, rkT) * rkT;    #dE/d(rkT) * rkT = dSinfo
int(%, rkT);   #Sinfo + const
#result is
Sinfo := -ln(exp(hf*rkT)-1) + hf*rkT / (1-exp(-hf*rkT));
Sinfo := -ln(exp(hf/kT)-1) + (hf/kT) / (1-exp(-hf/kT))
This formula is exponentially close to 0 when kT&rarr;0+, 
although it requires a lot of near-cancellation to see it.
For example if hf=1 and kT=0.001 then Sinfo = 5*10^(-432).
Meanwhile, if kT&rarr;infinity then 
Sinfo = ln(kT/hf) + 1 + (hf/kT)^2/24 - (hf/kT)^4/960 + O(hf/kT)^6.
grows logarithmically toward infinity.

Callen-Welton entropy: Same thing as Bose-Einstein except multiply both energy & entropy
by hbar/RelaxTime.

Johnson formula  NoisePower = kB * T * bandwidth   independent of resistance.

Brownian Motion Einstein 1905.

Ryogo Kubo: Statistical-mechanical theory of irreversible processes,
J.Phys.Soc.Japan 12 (1957) 570-586.
Review: http://www-f1.ijs.si/~ramsak/km1/kubo.pdf
I want to know about relxation time

Bernard & Callen: Rev Mod Phys 31 (1959) 1017-
Callen & Greene: Phys Rev 86 (1952) 702; 88 (??) 1387.
Chandrasekhar RMP 15 (1943) 1
Green: J Chem Phys 20 (1952) 1281; J.Chem Phys 22 (1954) 398.
Takahashi: J Phys Soc Japan 7 (1952) 439
Mori: J Phys Soc Japan 11 (1956) 1029
Nakano: J Theor Phys Japan 15 (1956) 77
M.Lax:  Phys. Rev. 129 (1963) 2342
Rev. Mod. Phys. 38, 359 (1966): LAX - Classical Noise III
Rev. Mod. Phys. 38, 541 (1966): LAX - Classical Noise IV: Langevin

R.Lenk: A simple proof of the classical fluctuation dissipation theorem,
Physics Letters A 25,3 (1967) 198-199.

GH Wannier: Statl Physics, Wiley 1966 derives LCR circuit noise.
FNH Robinson: Noise & Fluctuations, Clarendon 1974, doesnt care if classical or quantum.

Lax 1963
Lax 1967
P.Candelas &amp; D.W.Sciama: The irreversible thermodynamics of black holes!!!???, 
Gen.Relat.Gravit. 9,3 (1978) 183-187.
Phys. Rev. Lett. 38,23 (1977) 1372-1375, correction 39,25 (1977) 1640-1640.
Black holes are shown to obey the principles of irreversible thermodynamics in the form of a 
fluctuation-dissipation theorem for their zero-point quantum fluctuations. Moreover Hawking 
radiation is shown to be related to the macroscopic radiation of a nonstationary black hole 
in accordance with Onsager's principle.

S.R. de Groot &amp; P.Mazur, Non-equilibrium Thermodynamics (Dover 1984) reviews FDT??

G.C.Debney, R.P.Kerr, A.Schild:
Solutions of the Einstein and Einsteinâ€Maxwell Equations,
J. Math. Phys. 10,10 (1969) 1842-1854.
See esp EQ 7.10, 7.11, 7.14.
If Q=0 the horizon is located at  DELTA = r^2 - 2Mr + a^2 = 0  (largest root)?

http://arxiv.org/pdf/0708.0276  secIII:
Under a flat-space Lorentz boost, the Kerr-Schild metric retains its special form,
and (they verify this directly) 
the area is invariant.  
Can we conclude that the area element is the same in both flat and 
curved space?  No.  The total area of any surface in flat space is
Their EQ 34 and 35 find the area element depends on the boost but 
when integrated the change cancels to 0.
If the right boost is chosen (thetaB=0) then 
dArea = (r+^2 + a^2) * sin(theta) * dtheta * dphi
which is the same as the flat space area on a sphere of radius r^2 = (r+^2+a^2).

de Sitter Kerr Newman - Plebanski Acta Phys Polonica B8 (1977) 169??

Qgravity2.tex
It should be mentioned that a quite similar analysis was performed by
G.'t Hooft (\cite{tHooft85} section 3 ``brick wall model'').
However, although 't Hooft's plan was essentially the same as here,
his paper is invalidated by two mathematical errors\footnote{
His EQ 3.5 and subsequently is missing the $(2Ms)/r^3$ term in 
our \eqr{eq:equivpoten}. Also in his EQ 3.19 at the end, he does the
integral wrong; the result should behave 
like $\sqrt{2Mh}$ for small ``$h$,'' not linearly. 
But this 3.19 error seems correctible and I think tHooft sort of meant the right thing, 
just did not write it.
Gerard 't Hooft:
On the quantum structure of a black hole,
Nucl. Phys. B256 (1985) 727-736.
Kokkotas review EQ23 gives the right ReggeWheeler wave eqn using tortoise r.
[Also, Zerilli wave eq seems more apposite, but never mind.]

Kaushik Ghosh:
A few comments on brick-wall model and the entropy of a scalar field in Schwarzschild black hole background
Nuclear Physics B 814, 1-2 (June 2009) 212-216
In this brief communication we will consider the entropy of a scalar field in Schwarzschild black hole 
background using the brick-wall model of 't Hooft. We will show that a proper counting of states lead to
an expression for the entropy of the scalar field which is different from the standard expression and is 
NOT proportional to the area of the horizon.
http://arxiv.org/pdf/0902.1601
Ghosh may be a nut.

More generally it is a fact that eigenvalue asymptotics exhibit boundary terms, such as area.
The bulk term is the same in the presence or absence of a magic mirror.  The next term
differs.   Consider a magic-mairror box with a removable magic mirror partition.
There are effectively fewer modes?  Wavenumber (no partition) is integer.
With partition, wavenumber is even integer (each side).  Same UV cutoff.
Same mode count.  Oops.  Umm, this seems to be a special case.  The mode count up to a
UV cutoff should generally alter by area terms for smooth bdy.

S.W.Hawking &amp; G.F.R.Ellis:
The Large Scale Structure of Space-Time, Cambridge University Press 1973.

Wightman reviews Scharf FQED:
Finite Quantum Electrodynamics (G. Scharf)
SIAM Rev. Volume 33, Issue 3, pp. 508-509 (September 1991)
Another review in German:
Physik in unserer Zeit
Volume 22, Issue 2, page 91, 1991
also
http://en.wikipedia.org/wiki/Causal_perturbation_theory
Causal perturbation theory is a mathematically rigorous approach to renormalization theory, which makes it possible to put the theoretical setup of perturbative quantum field theory on a sound mathematical basis. It goes back to a seminal work by Henri Epstein and Vladimir Jurko Glaser ("The role of locality in perturbation theory", published in Annales PoincarÃ© Phys. Theor. A19, p.211, 1973).
G&uuml;nter Scharf, Finite Quantum Electrodynamics: The Causal Approach, 2nd edition, Springer, New York (1995)
G&uuml;nter Scharf, Quantum Gauge Theories -- A True Ghost Story, John Wiley & Sons, New York (2001)

The causal approach to quantum field theory advocated during the last 20 years by Scharf goes
back to a classic paper by H. Epstein and V. Glaser. The method has the great advantage that
it uses mathematically well-defined objects only, namely free asymptotic fields. Therefore all mathematical operations have a precise meaning in the framework of distribution theory, in particular, there are no ultraviolet divergences. The method has been applied to abelian, massless non-abelian and to massive non-abelian gauge theories. In the latter case one obtains the complete structure of the standard electroweak theory as a consequence of (quantum) gauge invariance, without using spontaneous symmetry breaking. In the case of spin-2 gauge fields gauge invariance alone leads to the same couplings as given by Einstein's theory (see also G.Scharf, Quantum gauge theories - a true ghost story). The extension of the method to supersymmetry and Georgi-Glashow SU(5) is also under study. It turns out that supergauge theories can be constructed by the causal method in close analogy to ordinary quantum gauge theory. 

G.Scharf was born 1938.

Finite calculation of divergent self-energy diagrams 
Andeeas Aste &amp; Dirk Trautmann
Canadian Journal of Physics 81 (2003) 1433-1445
http://arxiv.org/abs/hep-th/0309052

L NUOVO CIMENTO A (1971-1996)
Volume 109, Number 11, 1605-1607, DOI: 10.1007/BF02778244
NOTE BREVI
Vacuum stability in quantum field theory
G. Scharf

the S-matrix S commutes with the Hamiltonian

A cautious evaluation of the situation is given in Weinberg's QFT book,
Vol. 2, pp.136-138

D. Espriu and R. Tarrach:
The case for triviality,
Physics Letters B 383,4 (1996) 482-486.
http://arxiv.org/pdf/hep-ph/9604431
argue that, because of the Landau pole, quantum electrodynamics is 
only an effective field theory.
Abstract
We point out that, contrary to what is believed to hold for QCD, renormalons are genuine in QED; i.e. the ambiguities which come with them do not require cancellation by hypothetical non-perturbative contributions. They are just the ambiguities characteristic of any trivial â€” and thus effective â€” theory. If QED remained an isolated theory up to an energy close to its triviality scale, these ambiguities would surely hint at new physics. This not being so, the renormalon ambiguities in QED lead to no new physics, not even to non-perturbative contributions within QED itself.
Quotes:
Nowadays one believes that the QED series is not only divergent, but also non Borel-summable.
As stressed by Parisi[2] this pathology is related to the existence of the Landau pole[3], 
or, in other words, to the fact that perturbation theory cannot be trusted at large momenta.
These ambiguities can also be seen to emerge as singularities of the Borel transform of the
perturbative series, called renormalons.
...
If, on the contrary, one treats the theory as an effective one, with an unremovable UV cutoff, 
the ambiguities in summed- up renormalized perturbation theory are the ones we expect in any 
effective theory. As we will see in what follows, using this expansion one can stay well 
within the framework of convergent perturbation theory and so the issue of non-perturbative 
contributions which cancel the ambiguities simply does not pose itself...
An effective field theory with an unremovable dimensionful UV cutoff LAMBDA may provide 
accurate predictions, but not infinitely accurate ones[9], as some dependence on LAMBDA 
remains.
   The paper is pretty incoherent for the most part.
A crux argument is on p4 where the running coupling const alpha goes infinite at
finite (tho huge, actually quite superplanckian) UV cutoff Q.
    alpha(Q) = alpha / (1+2alpha/(3pi) * ln(m/Q)) 
goes infinite at the "Landau energy"
    Q = me * exp((3pi/(2alpha)) = 2.9 * 10^280 * me = 1.2 * 10^258 * mpl
Hence Wilsonian scaling-based renormalization cannot continue forever and QED must be
an effective theory only.  "Lattice QED" approach will not work to save QED mathematically.
Any magic series-summing method will 
and should return ambiguous results
at huge energies of this order.

Karen Yeats PhD http://arxiv.org/pdf/0810.2249v1
on p22 gives an integral whose growth is exponential but not factorial.
It's Euclidean and maybe she used her power-based rernom trick at the end too.

Carnot theorem says efficiency of a heat engine between Th and Tc is at best 
1-Tc/Th.
I.e. it inputs heat Eh at temp Th, outputs work W, and outputs heat Ec at temp Tc,
where Eh=Ec+W and and the theorem is
    EfficiencyOfHeatEngine = W/Eh <= 1 - Tc/Th = (Th-Tc)/Th.

Now using fact Carnot engine is reversible we could run it as a refrigerator: 
input work W and heat Ec at temp Tc, and output heat Eh at temp Th.   Then the efficiency 
for this purpose would be    
   EfficiencyOfRefrigerator = Ec/W = (Eh-W)/W = Th/(Th-Tc)-1 = Tc/(Th-Tc).

Brilliantly doing, like, math, I conclude Ec/W <= Tc/(Th-Tc).

Note this kind of efficiency can actually exceed 1:  If Th=301K, Tc=300K, you
can by consuming 1 watt of electricity from the power co, remove 300 watts
of heat from your room  with a Carnot-type heat pump.

OK, great. So
    RefrigerationEfficiency<=1
if Th=2*Tc. And
    RefrigerationEfficiency<=1/2
if Th=3*Tc.

Removing 1 joule from 3K and outputting it to 300K costs >=99 joules.

So if I want  Tc=10^(-100) Kelvin and Th=300 Kelvin, then it is going to cost me
at least 300*10^100 joules to remove 1 joule of heat from the cold thing.

Oops.   So yeah, exponentially tiny temperatures are unreachable.
Hmm, actually (continuing the analysis) if the temp T of something 
is roughly linearly proportional to its energy E (true for a classical "ideal gas"
and also true for Fermi gas in low-T limit),
then removing infinitesimal amount dE
of energy from cold thing (temp=Tc) costs about (Th/Tc)dE
and integrating, the total cost to cool from Th down to Tc is
proportional to E*ln(Th/Tc).

So it only costs you the LOG of the temperature ratio.

That's not bad.  So to cool something down to 10^(-100) kelvin
only costs about (its energy)*log(10^100) = (its energy)*240.

That would be feasible.  So I take it back -- exponentially low
temps like 10^(-100) kelvin could be reachable for the right
kind of physical system if we could achieve Carnot-like
levels of efficiency and had perfect insulation.

Actually what really may limit you at ultralow temperatures is not Carnot,
but rather quantum uncertainty principles.   Like, the energy-time principle:
to get 10^(-100) kelvin we'd need a time of 10^100 time units, which'd
be too long, way longer than the age of the universe.

Even if you were willing to wait, then even a tiny-power heat leak would cost 
you a huge amount of refrigeration energy over that huge time, due to that Carnot factor.
So, ok, back to my old conclusion that exponentially low temperatures are not reachable.  
But 10^(-20) kelvin, maybe.

---

http://ltl.tkk.fi/wiki/LTL/World_record_in_low_temperatures
claims they got the coldest temp ever at 100 picoKelvins in a chunk of rhodium metal weighing about 0.6 grams.  This was got by
1) liquid helium to reach about 3K
2) He3-He4 "dilution fridge" to reach 3milliK
3) adiabatic demagnetization, 2 stages to reach 50 microK then 100 picoK.

P := (z) -> exp(-z*z/2) / sqrt(2*Pi);
F := (x) -> (1/2)*erf(sqrt(1/2)*x)+1/2;
H := (x) -> -(x*ln(x) + (1-x)*ln(1-x))/ln(2);
simplify(H(F(z)));
evalf(int( %, z = -infinity .. infinity ), 30);
2.606076490
2.60607648966845742760244189229

Hagen Kleinert:
Variational approach to tunneling, beyond the semiclassical approximation of Langer and 
Lipatov-perturbation coefficients to all orders,
Physics Letters B 300,3 (February 1993) 261-270.

R. Karrlein and H. Kleinert:
Precise Variational Tunneling Rates for Anharmonic Oscillator with g&lt;0,
Phys. Lett. A 187 (1994) 133-??  (14pp?)
http://arxiv.org/abs/hep-th/9504048
quartic anho.

H. Kleinert and I. Mustapic:
Decay Rates of Metastable States in Cubic Potential by Variational Perturbation Theory,
Int. J. Mod. Phys. A 11 (1996) 4383-?? (23pp) 
a different splitting is used for pert theory of anharmic osc.
Here he treats a cubic anho term.
http://arxiv.org/abs/quant-ph/9502027

R.P.Feynman and H.Kleinert:
Effective classical partition functions, Physical Review A 34,6 (1986) 5080-5084
http://users.physik.fu-berlin.de/~kleinert/159/159.pdf

H. Kleinert:
Path Integral on Spherical Surfaces in D Dimensions and on Group Spaces
with Charges and Dirac Monopoles, 
Phys. Lett. B 236 (1990) 315-?? 

http://www.asergeev.com/rsptexp/anharm1.htm
will compute stuff for you!?!

http://www.ma.utexas.edu/mp_arc/c/09/09-82.pdf

http://eproceedings.worldscinet.com/9789812777850/preserved-docs/9789812777850_0002.pdf

http://www.nhn.ou.edu/~milton/papers/benmil.ps

Ulrich D. Jentschura, Andrey Surzhykov, Jean Zinn-Justin:
Unified treatment of even and odd anharmonic oscillators of arbitrary degree, 
Phys. Rev. Lett. 102,1 (2009), 011601, 4 pages.

Another version is:
Ulrich D. JENTSCHURA , Andrey SURZHYKOV , Jean ZINN-JUSTIN:
Generalized Nonanalytic Expansions, 
PT-Symmetry and Large-Order Formulas for Odd Anharmonic Oscillators,
http://arxiv.org/pdf/0901.1858
Appendix p9 has the Bender-Wu formula now for odd M, got by citing the above PRL.
Use x^2/2 as original potential, pert=sqrt(g)*x^M, odd M>=3 (use n=0 for ground state)
Their results agree exactly with Bender+Wu PRL 27 (1971) 461-
if we change 2K to be now odd.  See these slides:
www.math.unipd.it/~luminy09/SLIDES/Jentschura.pdf
which also explain the beyond-all-order terms emplying
exp(something / g^(1/M))

H.G.Dehmelt:
A Single Atomic Particle Forever Floating at Rest in Free Space: New Value for Electron Radius,
Physica Scripta T22 (1988) 102-110.
Argues radius&lt;10<sup>-22</sup> meters.

"Single Atomic Particle at Rest in Free Space: New Value for Electron Radius", 
Hans Dehmelt, Annales de Physique (Paris) 10, 777 - 795 (1985)
Experiments with an isolated subatomic particle at rest
Rev. Mod. Phys. 62,3 (1990) 525-530

http://hussle.harvard.edu/~gabrielse/gabrielse/overviews/ElectronMagneticMoment/ElectronMagneticMoment.html
says now 3 parts in 10^13 precision for mue. This is 15X more accurate than 1987 measurement:
   old meas: R.S. Van Dyke, Jr., P.B. Schwinberg, and H.G. Dehmelt, Phys. Rev. Lett. 59, 26 (1987)
g/2 = 1.001 159 652 180 73(28)

A. Garrett Lisi:
Quantum mechanics from a universal action reservoir, 2006
http://arxiv.org/abs/physics/0605068

Solving quantum field theories via curved spacetimes.
Igor R. Klebanov, (Princeton U.) , Juan Martin Maldacena, (Princeton, Inst. Advanced Study) . 2009. 6pp.
Published in Phys.Today 62:28-33,2009. 
http://www.sns.ias.edu/~malda/Published.pdf
THis paper is worthless.

Thibault Damour:
The entropy of black holes: a primer
http://arxiv.org/abs/hep-th/0401160

Thermodynamics of black holes in anti-de Sitter space
S. W. Hawking and Don N. Page
 Comm. Math. Phys. Volume 87, Number 4 (1982), 577-588.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103922135

S.W.Hawking: Particle creation by black holes. Commun. Math. Phys. 43,3 (1975) 199-220
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103899181
this is not the nice simple derivation
Erratum: 46,2 (1976) 206.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103899590

Hartle, J.B., Hawking, S.W .: 
Path-integral derivation of black-hole radiance. Phys. Rev. D 13, 2188-2203 (1976)

GWGibbons, SWHawking:
Cosmological event horizons, thermodynamics, and particle creation. 
Phys. Rev. D15 (1977) 2738-2751.

Robert M. Wald, On particle creation by black holes, Communications in Mathematical Physics 
45, 1 (1975) 9-34
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103899393

G W Gibbons1, M J Perry1 and C N Pope2
The first law of thermodynamics for Kerr-anti-de Sitter black holes
Classical and Quantum Gravity
Volume 22, Number 9 (2005) 1503-

Claudia S. Peca, Jose P. S. Lemos:
 Thermodynamics of Reissner-NordstrÃ¶m-anti-de Sitter black holes in the grand canonical ensemble
Phys. Rev. D 59, 124007 (1999) [11 pages]

Li Huaifan, Zhang Shengli, Wu Yueqin, Zhang Lichun and Zhao Ren:
Hawking radiation of Kerr-Newman-de Sitter black hole
THE EUROPEAN PHYSICAL JOURNAL C - PARTICLES AND FIELDS
Volume 63, Number 1 (2009??) 133-138

The Entropy Function for the extremal Kerr-(anti-)de Sitter Black Holes
Jin-Ho Cho1,2, Yumi Ko1 and Soonkeon Nam1
http://arxiv.org/pdf/0804.3811

D. Astefanesei, K. Goldstein, R. P. Jena, A. Sen, and S. P. Trivedi, Rotating
attractors, JHEP 10 (2006) 058, [hep-th/0606244].

Canad. J. Phys. 81(12): 1363-1375 (2003)  
The thermodynamics of a Kerr-Newman-de Sitter black hole
M. H. Dehghani and H. KhajehAzad

Black Holes, Entropy, and Information.
Gary T. Horowitz, (UC, Santa Barbara) . Aug 2007. 8pp. 
Presented at 2007 STScI Spring Symposium on Black Holes, Baltimore, Maryland, 23-26 Apr 2007. 
e-Print: arXiv:0708.3680 [gr-qc]
http://arXiv.org/pdf/0708.3680

Strominger, A. and Vafa, C. 1996 Microscopic Origin of the Bekenstein-Hawking Entropy.
Phys. Lett. B 379, 99. [arXiv:hep-th/9601029].

Peet, A. W. 2000 TASI lectures on black holes in string theory arXiv:hep-th/0008241.
http://arXiv.org/abs/hep-th/0008241
TASI lectures on black holes in string theory
Amanda W. Peet
this is good.The periodicity in euclidean time = inverse temperature is very cool
EQ1.22 is the classical small chnage law

 R.M. Wald, â€œThe Thermodynamics of Black Holesâ€, Living Review in Relativ- ity, gr-qc/9912119.

If you dervie the ent in the Schw case, then small pert laws about chnaign area and ang mom and charge lead to entropy
formula for Kerr-Newman.

Bisognano and Wichmann [43] 
Unruh effect math'ly rigorous proof
J.J. Bisognano and E.H. Wichmann, â€œOn the Duality Condition for Quantum Fieldsâ€, J. Math. Phys. 17, 303-321 (1976).

http://arXiv.org/abs/0907.0416
The Formulation of Quantum Field Theory in Curved Spacetime
Robert M. Wald
good paper

these get black hole entropy via euclideaniation, time-period, conical singulrity
M. Banados, C. Teitelboim, and J. Zanelli, Phys. Rev. Lett. 72, 957 (1994); C. Teitelboim in
Proceedings of the Cornelius Lanczos Ineter- national Centenary Conference, 
eds. J.D. Brown et. al., SIAM, Philadel- phia (1994).

L. Susskind and John Uglum, Phys. Rev.D 50, 2700 (1994).   http://arXiv.org/abs/hep-th/9401070
good paper. This approach would seem to yield universal area*const formula.

W. Nelson: A Comment on Black hole entropy in String Theory,
UCSBTH-93-41, http://arXiv.org/abs/hep-th/9406011

Allen, Bruce; Turyn, Michael:
An evaluation of the graviton propagator in de sitter space
Nuclear Physics B 292 (1987) 813-852
http://edoc.mpg.de/get.epl?fid=61070&did=429373&ver=0
bad growing behavior at largespacelike sep

J. Math. Phys. 28,5 (1987) 1023 (7 pages)
Propagators for massive vector fields in antiâ€de Sitter spaceâ€time using Stueckelberg's Lagrangian
H. Janssen and C. Dullemond
Expressions are found for homogeneous and inhomogeneous propagators for vector fields of arbitrary mass in antiâ€de Sitter spaceâ€time using a generalization of Stueckelbergâ€™s Lagrangian for a massive vector field. The massless case (quantum electrodynamics) is also considered by taking the appropriate zeroâ€mass limit.

Jason Kumar: de Sitter 2point fns blow up logaritmically at large spacelike sep

In de Sitter space
Feynman-Dyson perturbation theory breaks down and we need to use Schwinger-Keldysh formalism
http://research.kek.jp/group/riron/workshop/theory2010/program/slides/Kitazawa.pdf
http://research.kek.jp/people/hkodama/ExDiP2010/files/KitamotoHiroyuki@ExDiP2010.pdf
propagators expresible using Legendre aka Gegenbauer functions of cosh

http://meghnad.iucaa.ernet.in/~tarun/pprnt/qftcst_EU/higuchi_dS_GWpropag.pdf
find growing to infty propagators in de Sitter space, but effectively 
not growing if only used in certain ways

www.bourbaphy.fr/moschella.pdf
de Sitter sightseeing tour

Ugo Moschella: New results on de Sitter QFT 2002
ipht.cea.fr/Docspht/articles/t95/092/public/publi.pdf

Black hole entropy is the Noether charge
    By Robert M. Wald (Chicago U., EFI & Chicago U.). 
    Published in:Phys.Rev.D48:3427-3431,1993 (arXiv: gr-qc/9307038) 

http://en.wikipedia.org/wiki/File:FeynmanPropagatorWithMass200.jpg

Helmholtz in DD: J[(D-2)/2](r)*r^((2-D)/2)  I presume? Yes up to constants.
Helmholtz in 3D: J[1/2](r)/sqrt(r)
Helmholtz in 2D: J[0](r)

makeg(gbh);
2;
[t,x,y,z];
d[x]^2+d[y]^2+d[z]^2-d[t]^2 + 
(d[t] + z*d[z]/r + (r*x*d[x]+r*y*d[y] + J*y*d[x]-J*x*d[y]) / (r^2+J^2) )^2 * 
(K + (2*M*r^3-Q^2*r^2) / (r^4+J^2*z^2));
{};
4;
[r^4 - (x^2+y^2+z^2-J^2)*r^2 - J^2*z^2=0];

de Sitter:
no such thing as energy-momentum eigenstates as we normally think of them -- a plane wave
photon with "constant momentum" will redshift unboundedly with time and thus lose momentum 
and energy.  There similarly is no such thing as a conserved angular momentum as we normally
think of it.  However, mathematically-defined conserved quantities exist.

Cunningham can be done in 5-space for de Sitter (a subgroup preserves the cosmical constant
and has the same size the the usual Cunningham grup in 4D Minkowski spacetime).


David Deutsch &amp; Philip Candelas:
Boundary effects in quantum field theory
Phys. Rev. D 20,12 (1979) 3063-3080
Electromagnetic and scalar fields are quantized in the region near an arbitrary smooth boundary, and the renormalized expectation value of the stress-energy tensor is calculated. The energy density is found to diverge as the boundary is approached. For nonconformally invariant fields it varies, to leading order, as the inverse fourth power of the distance from the boundary. For conformally invariant fields the coefficient of this leading term is zero, and the energy density varies as the inverse cube of the distance. An asymptotic series for the renormalized stress-energy tensor is developed as far as the inverse-square term in powers of the distance. Some criticisms are made of the usual approach to this problem, which is via the "renormalized mode sum energy," a quantity which is generically infinite. Green's-function methods are used in explicit calculations, and an iterative scheme is set up to generate asymptotic series for Green's functions near a smooth boundary. Contact is made with the theory of the asymptotic distribution of eigenvalues of the Laplacian operator. The method is extended to nonflat space-times and to an example with a nonsmooth boundary.

Philip Candelas: Vacuum polarization in the presence of dielectric and conducting
surfaces, Ann. Phys. 143 (1982) 241-295.

Gauge invariance implies via Noether thm the existence of conserved current and charge.

If GI ==> zero photon mass, then rain of bricks QED has zero photon mass at each order.

P.Bandyopadhyay
Gauge invariance, photon mass, and the weak interction of photons,
Il Nuovo Cimento A 55,2 (1968) 367-369.
He cites Schwinger PR 125 (1962) 397
and VIOgievetskij+IVPolubarinov conf 1962
[Y.Shimizu: Gauge invariance condition in the high virtual-photon mass limit, 
Lettere Al Nuovo Cimento 4,14 (1970) 643-646 concludes what??]
that it would be possible
to have a nonzero spin=1 boson mass and still enjoy gauge invariance.
Despite claims by Peskin+Schroeder.
PB then argues indirectly via weak interaction physics 
that GI should imply zero photon mass.
One could also argue charged black holes could not exist (charge would
be invisible "hair") if photon had mass.  Hence charge could be unconserved by
dropping it into a black hole.

Experimental evidence photons have low mass.
An upper limit to the photon mass can be inferred through satellite 
measurements of planetary magnetic fields.  The Charge Composition Explorer spacecraft 
was used to derive an upper limit of 6&times;10-16 eV with high certainty.  This was slightly 
improved in 1998 by Roderic S. Lakes in a laboratory experiment that looked for anomalous forces 
on a toroid Cavendish balance.  
  "Experimental limits on the photon mass and cosmic magnetic vector potential", Physical Review Letters 80 (1998) 1826-1829.
   The PDG says Lakes maybe should be more conservative and get 2*10^-17 eV.
Jun Luo, Liang-Cheng Tu, Zhong-Kun Hu, and En-Jie Luan: 
 "New Experimental Limit on the Photon Rest Mass with a Rotating Torsion Balance", 
Physical Review Letters 90,8 (26 February 2003) 081801
says &le;1.2&times;10<sup>-51</sup> grams.
Criticism PRL 91,14 (2003) 149101
Reply 149102.
Comment: http://arxiv.org/abs/physics/0305090

Review
AS Goldhaber & MM Nieto:
Photon and graviton mass limits,
Rev. Mod. Phys. 82,1 (2010) 939-979.

Wikipedia says if photn mas came from Higgs mechanism then the proca eq arguments
(e.g galactic magnetic field) would not be valid.

PDG says photon mass < 6&times;10<sup>-17</sup> eV  from Ryutov MHD of solar wind.
PPCF 39 (1997) A73.  Updated upper bound of 10^(-18) eV.
D.D.Ryutov: Using Plasma Physics to Weigh the Photon,
Plasma Physics Control Fusion 49,12B (2007) 429-438.
Still later update:
American Physical Society, 50th Annual Meeting of the Division of Plasma Physics, 
November 17-21, 2008, abstract #TM7.007
Solar wind data at the Pluto orbit, the same approach leads to an improved estimate, 
M<1.5x10-54kg (D.D. Ryutov. PPCF, 49, B429, 2007). 
We consider data obtained by the Voyager mission in the zone between the Pluto 
orbit and the termination shock to further improve the estimate and find that the 
upper bound can be reduced by another factor of 5, i.e. to
M<3x10-55kg.

AS Goldhaber & MM Nieto: Photon and Gravoton Mass limits,
RMP 82,1 (2010) 939-979.
http://arxiv.org/abs/0809.1003

The new limit is 7&times;10-17 eV.  Studies of galactic magnetic 
fields suggest a much better limit of less than 3&times;10-27 eV, but there is some 
doubt about the validity of this method.

Particle physics By Brian Robert Martin, Graham Shaw sec D3 writes down the maxwell and
massive-phton maxwell = proca eqns and observes their maxell gauge invariance equation
no longer holds.

Feynman derived maxwell from GI, see
FJDyson: Feynman's proof of the Maxwell equations, AmerJPhys 58 (1990) 209-211.
HWeyl also dervied maxwell as the simplest lagrangian compatible with GI.
Schwinger ditto
Also: 
DHKobe: AJP 46,4 (1978) 342-348
DHKobe: AJP 48,5 (1980) 348-353
RJHughes: AJP 60,4 (1992) 301-306
JSMarsh: AJP 61,2 (1993) 177-178
JHeras: AJP 62,10 (1994) 949-950
GI: A_u becomes A+gradient

Feynman's assumptions:
1. nonrelativistic flat space, p,x commutation relations
finds Lorentz force assuming it is acceleration independent, finds two maxwell eqns,


M. C. Land, N. Shnerb, L. P. Horwitz:
On Feynman's Approach to the Foundations of Gauge Theory
http://arxiv.org/abs/hep-th/9308003
			      AbstractIn 1948, Feynman showed Dyson how the Lorentz force and Maxwell equations could be derived from commutation relations coordinates and velocities. Several authors noted that the derived equations are not Lorentz covariant and so are not the standard Maxwell theory. In particular, Hojman and Shepley proved that the existence of commutation relations is a strong assumption, sufficient to determine the corresponding action, which for Feynman's derivation is of Newtonian form. Tanimura generalized Feynman's derivation to a Lorentz covariant form, however, this derivation does not lead to the standard Maxwell theory either. Tanimura's force equation depends on a fifth ({\it scalar}) electromagnetic potential, and the invariant evolution parameter cannot be consistently identified with the proper time of the particle motion. Moreover, the derivation cannot be made reparameterization invariant; the scalar potential causes violations of the mass-shell constraint which this invariance should guarantee. In this paper, we examine Tanimura's derivation in the framework of the proper time method in relativistic mechanics, and use the technique of Hojman and Shepley to study the unconstrained commutation relations. We show that Tanimura's result then corresponds to the five-dimensional electromagnetic theory previously derived from a Stueckelberg-type quantum theory in which one gauges the invariant parameter in the proper time method. This theory provides the final step in Feynman's program of deriving the Maxwell theory from commutation relations; the Maxwell theory emerges as the ``correlation limit'' of a more general gauge theory, in which it is properly contained.

L.D.Landau &amp; I.Ya.Pomeranchuk: On the point interaction in QED, 
Dokl. Akad. Nauk SSSR 102 (1955) 489-??; 
I.Ya.Pomeranchuk: Vanishing of the renormalized charge in QED,
Dokl. Akad. Nauk SSSR 103 (1955) 1005-??.

A.S.Kronfeld &amp; C.Quigg:
Resource Letter QCD-1: Quantum Chromodynamics
lss.fnal.gov/archive/2010/pub/fermilab-pub-10-040-t.pdf
Includes good review of running alpha[s] on pp20-21.
Hadron masses p22.

Ick-Joh Kim &amp; C.R. Hagen:
Fourth-Order Calculation of the Divergent Part of Z3 in Spin-Zero Electrodynamics,
Physical Review D 2,8 (1970) 1511-1518
D.K.Sinclair: Fourth-Order Contribution to 1/Z3 in Scalar Electrodynamics
Physical Review D 6,4 (1972) 1181-1184
    Two previous calculations of Z3-1 to fourth order in scalar electrodynamics were in 
disagreement. We show that, with slight modifications to one of these calculations, they 
are brought into agreement. A simpler calculation, in which the Ward identities are 
explicit, is presented.

Klein-Gordon lagrangian P+S EQ2.6 and ex2.2, KG equation is EQ2.7.
In view of the brick-smeared proptr regn thm,
The action for D=3-smearKG propr is logarithmically infinite.
The action for D=2-smearKG propr is like integral of s^(-2), power-law infinite.
The infinities occur on the light cone.
If we instead emply the square root of the propgtr, but product from the
two endpoints, then:
The action for D=3-smearKG propr is power law infinite (worse than logly).
The action for D=2-smearKG propr is like integral of s^(-2)/log|s|, still power-law infinite.
That made it worse!

Expansion formulas and addition theorems for Gegenbauer functions
Durand, Loyal; Fishbane, Paul M.; Simmons, L. M.
Journal of Mathematical Physics, Volume 17, Issue 11, pp. 1933-1948 (1976).
    We give a systematic summary of the properties of the Gegenbauer functions CÎ±Î»(x) and DÎ±Î»(x) for general complex degree and order, with emphasis on the functions of the second kind, DÎ±Î»(x), and on results useful in scattering theory. The results presented include Sommerfeld-Watson type expansion formulas and two reciprocal addition formulas for the functions of the second kind. 

Bobby S Acharya, Michael R Douglas:
A Finite Landscape?
http://arxiv.org/abs/hep-th/0606212
They have dubousarguments that mayeb the strign landscape, sectin compatible with 
experiment, is finite.

Conformal coupling  xi=(N-2)/(4N-4)

The classical retarded green fn solution is exactly a delta fn on the light cone (in odd spatial
dims such as 3) for wave eq and maxwell eq in flatspace.  And since de Sitter is conformal to
flat space we get ditto (up to time-dependent factors) provided we employed a conformally
invariant wave eq (conformal weights ok).
Suggests sec3 of DV Galtsov: Radiation recation, PRD 66 (2002) 025016=hep-th/0112110.

Loyal Durand:
Gegenbauer fns papers
J Mathl Phys 17 (1976) 1933-1948
SIAM J Mathl Anal 10 (1979) 425-437
these are not really aimed at full coverage just about addtion formulas.

Other side of de Sitter space, the far past isnt, the really far pst, and the far future,
as betetr behaved than Minkowski!!!

relation of conformal and static de Sitter coords???

Massimo Porrati, Rakibur Rahman, Augusto Sagnotti
String Theory and The Velo-Zwanziger Problem
http://arxiv.org/abs/1011.6411

Theoretical physics is going to hell in a handbasket.

Matter from Space
Authors: Domenico Giulini
http://arxiv.org/abs/0910.2574

The Collected Works of P. A. M. Dirac, 1924-1948 and General Theory of Relativity
Dalitz, Richard H.; Dirac, P. A. M.; Hovis, R. Corby
Physics Today, vol. 49, issue 9, p. 84

http://www.lucasianchair.org/bibliographies/dirac-bibB.html
# P.A.M. Dirac. A positive-energy relativistic wave equation. Proceedings of the Royal Society of London A, 322:435-445, 1971.
# P.A.M. Dirac. A positive-energy relativistic wave equation II. Proceedings of the Royal Society of London A, 328:1-7, 1972. 
# P.A.M. Dirac. A remarkable representation of the 3+2 de Sitter group. Journal of Mathematical Physics, 4:901-909, 1963. 
P.A.M. Dirac. Particles of finite size in the gravitational field. Proceedings of the Royal Society of London A, 270:354-356, 1966?
# P.A.M. Dirac. Generalized Hamiltonian Dynamics. Proceedings of the Royal Society of London A, 246:326-332, 1958.
P.A.M. Dirac. Generalized Hamiltonion dynamics. Canadian Journal of Mathematics, 2:129-48, 1950. 
# P.A.M. Dirac. The theory of gravitation in Hamiltonian Form. Proceedings of the Royal Society of London A, 246:333-343, 1958. 

Manfred Salmhofer:  Renormalization: an introduction, Springer 1999
It provides a mathematically rigorous, yet simple and clear introduction to that subject. It can be read by students from the third year on and it leads the reader to a level where he or she can start reading the current research literature. The book gives a thorough introduction to field-theoretic techniques such as Feynman graph expansions and renormalization. Special effort has been made to make all proofs as simple as possible by using generating function techniques throughout. Renormalization is done by using an exact renormalization group differential equation. This technique, developed during the last few years and now appearing in a textbook for the first time, provides simple but complete proofs of renormalizability theorems.
http://books.google.com/books?id=nAXncL7_KrQC

M. Salmhofer:
Improved Power Counting and Fermi Surface Renormalization
Rev.Math.Phys. 10 (1998) 553-578 

Of course, since there are many functions with the same asymptotic 
expansion (e.g., one can add arbitrary multiples of terms like 
e^{-a/x}, e^{-a/x) log x, etc.),
one has to show that the Borel summed Sf actually has the
properties that the original f was supposed to have (and from which 
the asymptotic series was derived). If, in addition, f is
uniquely determined by these properties, we know that f=Sf.
  Unfortunately, a proof for such a statement is missing in QED.

This is a good review about asymptotic series in quantum field theory:
   Jan Fischer 
   On the role of power expansions in quantum field theory
   Int.J.Mod.Phys. A12 (1997) 3625-3663
http://arxiv.org/abs/hep-ph/9704351
An example of a Borel-Sumable QED series is doscussed on p4 see EQ 6+7
nice table 1 like mine p9 about divergence rates

Questioning of Dyson divergence argument: 
PM Stevenson:
PhysRev D 23 (1981) 2916-
NuclPhys B 231 (1984) 65-

Classes of diagrams with value n!: 
DJ Gross + A Neveu: PhysRev D 10 (1974) 3235
MC Bergere + F. David: Phys Lett B 135 (1984) 412
M Beneke + VI Zakharov: Phys Lett B 312 (1993) 340-
M Beneke + VA Smirnov: Nucl Phys B 472 (1996) 529-

divergence rate of field theories without fermions:
Bukhvostov+Lipatov: PhysLett B 70 (1977) 48-, Soviet Physics JETP 46 (1977) 871-

DJ Gross+V Periwal: PRL 60 (1988) 2105 argue pertn
series in bosonic string theory diverge like h! and canot be Borel summed since all signs=+.

Borel sum(n>=0) z^n only manages for Re(z)<1 but sum is 1/(1-z) defined for all complex z!=1.

http://www.wave-scattering.com/publications.html#Summability%20Methods
Alexander Moroz: Summability method for a horn-shaped region,
CMP 133,2 (1990) 369-382  <= hep-th/9206074
about a new integral transform doubly-expnl to sum nasties
http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&id=pdf_1&handle=euclid.cmp/1104201403
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1104201403
A.Moroz: Novel summability method generalizing the Borel method, 
Czech. J. Phys. B40,7 (1990) 705-726.
A. Moroz: On the method of divergent series, Czech. J. Math. 40 (1990) 200-212.
A. Moroz: Strong asymptotic conditions or short guide to using summability methods,
Czech. J. Phys. B42,8 (1992) 753-763.

thesis THM p23
U(s) = int(0 to inf)  exp(-exp(t)) t^s dt
is meromorphic over whole coplex s-plane (if suitably extended) 
with simple poles on negative real axis and no zeroes for real s>-1
and finds its asymptotics using LambertW fn and saddlepoint method

thesis THM p29
F(t) = sum(n>=0) t^n / U(n)
is entire fn of t.
Asymptotics in open right halfplane: exp(exp(t)+t+ln(t+1))
Asymptotics in open left halfplane:  O(|t|^(-const))

THM summation method converges iff z in MittaglLeffler star of f(z)  if f is analytic at z=0.
Absolute [and uniform in interior of bounded subset of MLS].

THM 2 in CMP a particular explicit hornshaped region of analyticity of f(z) implies the summation
method will work.

Moroz says he can deal with horn shpaed and full MIttag-Leffler star, unlcealr how unique it is.
http://en.wikipedia.org/wiki/Mittag-Leffler_star

Scharf's S-matrix is about +-infinite times, not finite times.

Although many time-dependent observable consequences of QED 
can be deduced in a nonrigorous way in the Schwinger-Keldysh 
(= closed time path, CPT) formalism, 
 http://theory.gsi.de/~vanhees/publ/green.pdf 
there is at present no rigorous relativistic quantum field theory at 
finite times in 4 dimensions. 
  I haven't seen a single article that gives meaning (i.e., 
infrared and ultraviolet finite, renormalization scheme independent 
properties) to, say, quantum electrodynamics states at finite t and 
their propagation in time.
  People don't even know what an initial state should be in a 4D
relativistic QFT (i.e., from which space to take the states at 
finite t); so how can they know how to propagate it...
   Thus the standard textbook theory gives an S-matrix (or rather an 
asymptotic series for it) but not a dynamics at finite times. 
In general, the correct Hamiltonian is 
   H = lim_{Lambda to inf}  H(Lambda,g(Lambda,E,q_phys)),
where H(Lambda,g) is the canonical Hamiltonian with cutoff Lambda 
and parameter vector g (containing the so-called bare mass, charge or  
coupling constant, and field renormalization factor), and 
g(Lambda,E,q_phys) is the cutoff-dependent parameter vector as 
determined by renormalization conditions at energy scale E, which
relate g to a set of physical parameters q_phys (consisting, in case 
of QED of the physical electron mass m, the physical electron charge e, 
or, equivalently, the observed value of the fine structure constant
alpha=e^2/4pi).
  This limit probably exists, at least for renormalizable, 
asymptotically free theories, at least in 1D and 2D field theory, 
where this can be proved in certain cases. In 3D and 4D, one probably 
needs also a Lambda-dependent inner product defining the Hilbert space
to ensure that one ends up in the right representation,
and Lambda-dependent wave functions to ensure that the limiting
renormalized wave functions remain bounded in the limiting 
renormalized inner product.
   The consistency problem in a Hamiltonian approach to quantum field 
theory is precisely to show that this limit indeed exists.
The missing consistent dynamical theory in 4D relativistic QFT 
may also have consequences for the foundations of quantum mechanics. 
Clearly, measurements happen in finite time, hence cannot 
be described at present in a fundamental way (i.e., beyond the 
nonrelativistic QM approximation). Thus foundational
studies based on nonrelativistic QM are naturally incomplete.
This implies that it is quite possible that a solution of the 
unresolved issues in relativistic QFT are related to the unresolved 
issues in quantum measurement theory.

Neumaier: http://www.mat.univie.ac.at/~neum/physfaq/physics-faq.html

Arnold Neumaier:    Renormalization: An elementary tutorial
    http://www.mat.univie.ac.at/~neum/papers/physpapers.html#ren
contains an elementary treatment of many of the issues.
The nice paper hep-th/0212049 by Delamotte also discusses most of 
renormalization without ever mentioning fields (which come in quite 
late) and I think it is clearer than Neumaier:
  Bertrand Delamotte: A hint of renormalization, Amer J Phys 72,2 (2004) 170-184
http://arxiv.org/abs/hep-th/0212049

QUOTE
We again emphasize that if [the coupling constant] is believed 
to be no more than a non-measurable parameter,
useful only in intermediate calculations, it is of no consequence that this quantity is
infinite when [the energy cutoff] goes to infinity.
END QUOTE

I do not buy it.  It seems to me the bare charge IS physically meaningful and the fact it 
goes to 0 or infinity is unacceptable.  
I say it has to remain finite.
Also the whole renormalization approach only works for rQFTs with at worst logarithmic infinities.
Also the Landau pole kills this.

Neumaier claims
IR divergences occur only in theories with massless partivles and long range interactions.

The Wilson picture of running CCs can be viewed as a mere change of variables,
to make the variables be "better" (i.e. to get less sensitivity of results to them)?
Neumaier argues QFT is a limit of finite-cutoff/renormed QFTs are the cutoff tends to infinity.
In that sense it is well defined.

Curtis G. Callan, Jr., Sidney Coleman, Roman Jackiw:
A new improved energy-momentum tensor,
Annals of Physics 59,1 (1970) 42-73
Abstract
We show that the matrix elements of the conventional symmetric
energy-momentum tensor are cut-off dependent in renormalized
perturbation theory for most renormalizable field theories. However,
we argue that, for any renormalizable field theory, it is possible to
construct a new energy-momentum tensor, such that the new tensor
defines the same four-momentum and Lorentz generators as the
conventional tensor, and, further, has finite matrix elements in every
order of renormalized perturbation theory.
("Finite" means independent of the cut-off in the limit of large
cut-off.) We explicitly construct this tensor in the most general
case.
The new tensor is an improvement over the old for another reason: the
currents associated with scale transformations and conformal
transformations have very simple expressions in terms of the new
tensor, rather than the very complicated ones they have in terms of
the old.
We also show how to alter general relativity in such a way that the
new tensor becomes the source of the gravitational field, and
demonstrate that the new gravitation theory obtained in this way meets
all the experimental tests that have been applied to general
relativity.

NOTE: it is claimed the new improving terms only exist in the scalar field case.
In the other cases no change is needed ?!???
In flat spacetime the improved and old KG eqns coincide but the
energy-mom tensors do not!
In all massless field cases where we have conformal invariance the
energy-mom tensor can be improved to become traceless.

L NUOVO CIMENTO (1955-1965)
Volume 22, Number 4, 673-679, DOI: 10.1007/BF02783100
Conformal invariance of massless Dirac-like wave equations
J. S. Lomont
Abstract
It is shown that all Dirac-like wave equations with positive integral
or half-integral spin, zero rest mass, and no interaction are
conformally invariant.
The transformation ofWAVEFN under the conformai group, and the
associated conservation laws are given in a very simple form.

THE RECIPROCAL OF A BOREL SUMMABLE FUNCTION IS BOREL SUMMABLE
  AUBERSON G, MENNESSIER G
Source: COMMUNICATIONS IN MATHEMATICAL PHYSICS    Volume: 100
Issue: 3    Pages: 439-446    Published: 1985

T Fulton
F Rohrlich
L Witten
Conformal Invariance in Physics
Rev. Mod. Phys. 34,3 (1962) 442-457
C0 = conformal self transforms of flat Minkowski space  (15 param, Cunningham)
 <
Cg = conformal transforms of metric tensor.
 <
Ca = in weyl spaces (generalization of Riemannian geometry)
   Maxwell discussed sec C5 p450-.
Found to be Ca-invariant.
sec C8 redo same from variational principle.
   In C9 they show Dirac eq obeys C0-invariance in massless limit with no
interaction,
then EQ9.8 show still true even if Maxwell field also there interacting with it.
 They also note W.Pauli: Helv. Physica 13 (1940) 204-
considered general curvedspace conformal invariance of Dirac eq
and they imply he succeeded.   But this paper is known to contain at 
least one serious error... ok it seems what Pauli actually did is:
he showed the Dirac eqn in general curvedspace formulated by Schrodinger+Bargmann could 
NOT be conformally invariant with nonzero mass, and in a footnote 
at end claimed cpnformal invariance ==> trace(T)=0.

  Yang-Mills not discussed.

Yang-Mills (3+1)D conformal invariance ==> no glueball exists?

H.A.Kastrup: arxiv.org/pdf/0808.2730  discusses conformal history.
On the Advancements of Conformal Transformations and their Associated 
Symmetries in Geometry and Theoretical Physic,
Annalen Phys.17 (2008) 631-690.
Extensive review, no claim is cited of any proof of Dirac Eq
conformal invariance in general curvedspacetimes.

Saul Stahl: The Poincare Half-Plane: a gateway to modern geometry, 
Jones &amp; Bartlett Learning 1993.
QA685.S79 quite lame
http://books.google.com/books?id=TABicHVMQhMC
do not think it is a very good book

Beardon QA171.B364
Schwerdtfeger QA564.833

C.N.Yang and R.L.Mills:
Conservation of Isotopic Spin and Isotopic Gauge Invariance
Phys. Rev. 96,1 (1954) 191-195.

E. Elizalde and A. Romeo: Essentials of the Casimir effect and its computation, 
Amer. J. Phys. 59,8 (1991) 711-719

Barton 219

The Large Electron-Positron Collider (LEP) was one of the largest particle accelerators 
ever constructed. It was used from 1989 until 2000. To date, LEP is the most powerful 
accelerator of leptons ever built.
When the LEP collider started operation in August 1989 it accelerated the electrons and 
positrons to a total energy of 45 GeV each to enable production of the Z boson, which has a 
mass of approximately 91 GeV.[1] The accelerator was upgraded later to enable production of a 
pair of W bosons, each having a mass of approximately 80 GeV. LEP collider energy 
eventually topped at 209 GeV at the end in 2000.
At a Lorentz factor ( = particle energy/rest mass = [104.5 GeV/0.511 MeV]) of over 200,000, 
LEP still holds the particle accelerator speed record, extremely close to the speed of light.

Tian Yu Cao &amp; Silvan S. Schweber:
The Conceptual Foundations and the Philosophical Aspects of Renormalization Theory,
Synthese 97,1 (1993) 33-108.

Laurie M. Brown:
Renormalization : from Lorentz to Landau (and beyond) /
Published 1993
Call Number: QC174.17.R46 R46 1993

Bryce S DeWitt:
 Decoherence without Complexity and Without an Arrow of Time
pp.221-223 in
Physical Origins of Time Asymmetry
(J. Halliwell, J. Perez-Mercader and W.H. Zurek eds.)
Cambridge University Press 1996

it is massiveness, not complexity, that is the key to decoherence...
decoherence at the classical level is a natural phenomenon of the quantum cosmos...
the universe is likely to display decoherence in almost all states it may 
find itself in.... an arrow of time has no role to play in decoherence.

Bryce S DeWitt:
The global approach to QFT Oxford 2003
not at SUNYSB

Science and Ultimate Reality
 QC174.12 .S4 scieng

Stephen Adler 2003:
why decoherence has not solved the measurement problem
http://arxiv.org/abs/quant-ph/0112095
Stud.Hist.Philos.Mod.Phys.34:135-142,2003
Another alternative, which can also be formulated within the standard state vector and operator apparatus of Hilbert space, is to abandon the assumption of a deterministic unitary evolution, and to suppose instead that the evolution is stochastic unitary, in the sense that while the wave function for an individual system evolves unitarily, this evolution has a random, or stochastic component...
This approach, which has been developed in great detail, reproduces the observed fact of discrete outcomes governed by Born rule probabilities, and predicts the maintenance of coherence where that is observed (including in systems with large numbers of particles, such as recent superconductive tunneling and molecular diffraction experiments), while predicting state vector reduction when the apparatus parameters are those characterizing measurements. The stochastic approach may ultimately be falsified by experiment, but it constitutes a viable phenomenological solution to the measurement problem. Decoherence, in the absence of a detailed theory showing that it leads to stochastic outcomes with the correct properties, has yet to achieve this status.

Adler cites:
GC Ghirardi, A Rimini, and T Weber:
Unified dynamics for microscopic and macroscopic systems, Phys. Rev. D34 (1986) 470-491. 
http://en.wikipedia.org/wiki/Ghirardi-Rimini-Weber_theory

See also Ghirardi, G. C., Pearle, P. and Rimini, A. (1990) 
Markov processes in Hilbert space and continuous spontaneous localization of systems 
of identical particles, 
Phys. Rev. A42,1 (1990) 78-89.
http://pra.aps.org/abstract/PRA/v42/i1/p78_1

P.Pearle (1976, 1979), 
Reduction of the state vector by a nonlinear Schrodinger equation, Phys. Rev. D13, 857-868.
Pearle, P. (1979) Toward Explaining Why Events Occur, Int. Journ. Theor. Phys. 18, 489-518.

N.Gisin (1984) 
Quantum Measurements and Stochastic Processes, Phys. Rev. Lett. 52, 1657-1660.

Di'osi (1988)
Continuous Quantum Measurement and Ito Formalism, Phys. Lett. A 129, 419-423.

Adler, Brody, Brun and Hughston (2001) 
Martingale models for quantum state reduction, J. Phys. A: Math. Gen., 34, 8795-8820.
http://iopscience.iop.org/0305-4470/34/42/306

Adler (2002)
Environmental Influence on the Measurement Process in Stochastic Reduction Models, 
J. Phys. A: Math. Gen. 35, 841-858.
http://arxiv.org/abs/quant-ph/0109029
According to this picture, a measurement takes place when the different outcomes are 
characterized by sufficiently large environmentally induced energy fluctuations in the 
apparatus for the state vector reduction process, which is driven by the energy variance, 
to proceed rapidly to completion.
This Adler paper cites these three
LP Hughston: Geometry of Stochastic State Vector Reduction, Proc. Roy. Soc. A 452 (1996) 953-979
 [nonlinear?]
Adler S L and Horwitz L P:
Structure and properties of Hughston's stochastic extension of the Schro:dinger equation
J. Mathl. Phys. 41,5 (2000) 2485-2499, erratum 41 (2001) 976. ???
Adler S L, Brody D C, Brun T A and Hughston L P 2001 J. Phys. A: Math. Gen. 34 8795
about Planck scale as a source of the "environmental" random fluctuations.

H. D. Zeh:
The Physical Basis of The Direction of Time,
Springer
http://www.time-direction.de/
QC173.59.S65 Z413

Q. Does rain of bricks cause enough decoherence to rule out "quantum computers"?
I.e. does the rate of decoherence for a qubit grow unboundedly with its
self-separation?  
A. No.  It grows, but soon asymptotes to a constant independent of separation, at
least as far as direct effect of rain is concerned.  

Stephen L. Adler:
Quantum Theory as an Emergent Phenomenon:
The Statistical Mechanics of Matrix Models as the Precursor of Quantum Field Theory
Cambridge University Press 2004
"Quantum mechanics is not a complete theory but rather is an emergent phenomenon arising from
the statistical mechanics of matrix models that have a global unitary invariance."
not at SUNYSB

BS DeWitt+Neill Graham:
The many-worlds interpretation of quantum mechanics,
Princeton UP 
not in SUNYSB

neutrino mass: at least one flavor has mass>0.04eV, sum of 3 flavor masses is <0.3eV

In the hyperbolic plane:
Disk of radius R has area=(cosh(R)-1)*2*pi which grows ultimately exponentially.
Perimeter=sinh(R)*2*pi also grows ultimately exponentially.
Intersection of TWO such radius=R disks with different centers???
Since perimeter exponentially large for the intersection, it would seem the area
must also be.   Also, if distance between the two centers is S, then intersection
is contained in a ball of radius=R-S/2 proving its ultimate exponential growth.

In de Sitter land:
Two centers separated by S.
What area disk can they reach using a light flash after time=T?
If T>S/2 then answer is a superset of the light flash from midpoint after time T-S/2.
So: it grows exponentially.  

Heuristic convergence argument:
1. works at any fixed finite order of perturbative QED, because UV infinities now are ok.
2. in de Sitter space (this argument fails in Minkowski space!)...
3. argue from a tetrahedron lemma that there are only a finite number of raindrops
at |distance|<R from 4 given generic points.  [True for both de Sitter and Minkowski.]
4. If we restrict to a "pillbox" where all |distances|<R then we get convergence
basically because there are only a finite number of raindrops in the pillbox
so that the "infinite series" contains only a finite set of terms for any finite R>0.
5. Actually, there could be an unboundely large number of raindrops in the pillbox but
it gets factorially unlikely due to tail of Poisson distribution, and this turns
out to be enough in combination with "sqrtsum assumption" below to force the expected value of
series to converge  for any fixed finite R>0.
6. The "sqrtsum assumption" is to assume when considering certain sums of N terms
that the |sum| is going to be bounded by sqrt(N)*GeometricMean|term|, 
due to randomized phase angles.
Slightly weakened versions, such as replacing sqrt(N) by N^(1/2+o(1)),
would also be permissible if it is a small enough o(1).
7. Argue in de Sitter space using |propagator| bounds,
that the N-node Feynman diagrams considered as a sum using
the sqrtsum assumption, are going to have absolute value bounded by
O(1) if |alpha| is fixed.  Specifically we stratify the N-node diagrams into those
with GeometricMean|Feynman edge distances|<R, R/2, R/4, R/8,..., 1.  
Focus on a single such layer, say R.
The number N of raindrops in the pillbox
will grow ultimately exponentially with R 
[vs in Minkowski space the growth is power law]
and the max |diagram| will fall off 
proportionally to 1/sqrt(N)...  the number of diagrams N in the layer will
grow in a way such that the |amplitude|<const*const^N/sqrt(N).
8. Hence conclude that if |alpha| is small enough then the series will converge
ultimately geometrically.

(Polio virus: RNA=2.23*10^6 daltons.)
Foot-and-mouth disease virus, type O, strain 3 rough weight: RNA=4.18*10^6 daltons. 
RNA is single stranded, contains CHONP.  A Guanine chunk (single letter "G") in
RNA is made of P1, H11, C9, O7, N5 with mass 1*31+11*1+9*12+7*16+5*14=332.
Electrons=1*15+11*1+9*6+7*8+5*7=171.
Nuclei=1+11+9+7+5=33.
Hence Foot-and-mouth disease virus O3 RNA contains about 2.15 M electrons and 
0.42 M atomic nuclei.
But a later paper on a different strain found it had 7813 bases, which would if
all G be 7813*332=2593916=2.59*10^6 daltons.  Oops.

http://hcv.lanl.gov/content/index
is a database on hepatitis C.

Vaccinia virus, VACV-Duke strain:
http://www.virologyj.com/content/3/1/88
199,960 bp VACV-DUKE genome (double stranded DNA) encodes 
225 open reading frames, including 178 intact genes and 47 gene fragments. 
It is 66.6% AT, 33.4% GC.
6n-long (AACTTG)_n has MolecWeight=3706.4*n+const  daltons.
Hence Vaccinia DNA has MolecWeight=3706.4*199960/6=1.235*10^8 dalton
contains about 6.4*10^7 electrons and about 1.2*10^7 atomic nuclei.

7288.8-3582.4
AACTTGAACTTGAACTTG has mw=14701.6

S. O. Rice, "Distribution of the extreme values of the sum of n sine waves phased at
random," Quart. J. Appl. Math., vol. 12, pp. 375-381, 1955
Stephen O. Rice: Distribution of SUM a_n/n, a_n randomly equal to +-1,
BSTJ 52 (1973) 1097-1103.

Mirror symmetry:  
Shing-Tung Yau and Steve Nadis, The Shape of Inner Space (New York:Basic Books, 2010).

A.Strominger 2010 "string theory report card"
Not being ruled out as theory of nature A
Unambiguous, falsifiable prediction     F
Potential for Large Hadron Collider signal   D
Solving black hole puzzles              B 
Applications/inspirations for pure math A 
Applications/inspirations for other areas of physics B 
Unification                             A 
Uniqueness                              D 
Solving the cosmological constant problem F
Understanding the Big Bang / origin of Universe D 
Solving Pauli's renormalizability problem A 

Brian Greene Hiden Reality 2011 in table 4.2 gives another report card:
http://www.epubbud.com/read.php?g=9QDTX6S4&p=56
Unite gravity and QM: A
Unify all forces: A  
Incorporate key breakthoroughs from past research: A
Explain particle properties: indeterminate, no predictions
Experimental confirmation: indeterminate, no predictions
Cure singularities: A many kinds resolved, still need to address black holes and big bang
Black hole entropy: A
Generate mathematical contributions: A

A NASA webpage:
"Quarks of the same color repel each other because they have similar forces."
Multiquark assemblies must be color-neutral.

Paul Davies "The new physics":
like colors repel, different colors often attract*, opposite colors (red & antired) attract.
Asterisk:  R1*B2+B1*R2 repels (symmetric state, # denotes position, letter denotes color)
R1*B2-B1*R2 attracts, so kind of 50-50 chance?
  A 4th quark brought near a red-blu-grn trio will feel no net attract or repel.

Anyhow, if we change coupling const to imaginary, then red-red would now attract, red-antired would
now repel.

Lena Hansen:
The red, blue, and green quarks present in every particle come together to make a 
colorless particle, much as red, blue, and green light form white light when combined. 
A meson, on the other hand is composed of a red quark and an antired antiquark, whose
colors cancel each other out.
Quarks have color. Antiquarks have anticolor.

Gluons are massless, have spin 1, travel at the speed of light, 
and carry both a color and a different anticolor. 
The number of gluons (8) is determined by the number of color-anticolor combinations.

Imagine a green quark.
1. Emits green.antired gluon, now is a red quark.
2. Emits red.antigreen gluon, now back to green quark.  And so on.  
These two kinds  of gluons attract.
Also could have emitted
2'. emits red.antiblue gluon, now becomes blue quark.  
These 2 kinds of gluon red+antired attract, antiblue+green dunno but net effect is attract.
So conclude I was right.

V. Crede &amp; C.A. Meyer
The Experimental Status of Glueballs
http://arxiv.org/abs/0812.0600    [hep-ex]
Good evidence exists for a scalar glueball which is mixed with nearby mesons, 
but a full understanding is still missing.  Glueballs are difficult to detect 
(even though quite accessible energetically) because of such meson mixing.

Glueballs also reviewed here:
http://www.worldscinet.com/ijmpe/18/1801/free-access/S0218301309012124.pdf
there is a "lack of well defined observable states"
which cites Crede+Meyer

DJ Broadhurst:
Four-loop Dyson-Schwinger-Johnson anatomy
http://arxiv.org/abs/hep-ph/9909336

An early planck length unmeasuable paper 1950s is by Alden Mead.

Hidenaga Yamagishi:
Renormalization-group analysis of supersymmetric mass hierarchies,
Nuclear Physics B 216,2 (May 1983) 508-546.
Suslov claims that Krasnikov and Yamagishi both find
0 <= beta < g
from spectral representation, in Suslov's notation for beta function.
Suslov continues to claim beta=g asymptotically at large g.
At small g Suslov has beta=b2*g^2+b3*g^3+... where g=e^2.
beta[Peskin]=beta[Suslov]/e
evidently.
Hence beta[Peskin]=g/e=e asymtotically at large g,e.

David C. Brydges, J&uuml;rg Fr&ouml;hlich, and Erhard Seiler:
On the construction of quantized gauge fields,
I: General results, Ann. Phys. 121 (1979) 227-284.
II: Convergence of the lattice approximation, Commun. Math. Phys. 71,2 (1980) 159-205
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103907454
III: The two-dimensional abelian Higgs model without cutoffs,
 Commun. Math. Phys. 79,3 (1981) 353-399.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103909055

T. Balaban, D. Brydges, J.Z. Imbrie, A. Jaffe:
The Mass Gap for Higgs Models on a Unit Lattice,
Ann. Phys. 158 (1984) 281-319. 

Tadeusz Ba Lslash; aban, John Imbrie, and Arthur Jaffe:
Renormalization of the Higgs model: minimizers, propagators and the stability of mean field theory
Comm. Math. Phys. Volume 97, Number 1-2 (1985), 299-329.

J.Glimm:
Yukawa coupling of quantum fields in two dimensions,
I: Commun. Math'l. Phys. 5,5 (1967) 343â€”386
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103840123
II: Commun. Math'l. Phys. 6,1 (1967) 120â€”132
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103840171
Infinite boson mass renormalization within this model. If so Hren is positive.

Oliver A. McBryan:
Finite mass renormalizations in the Euclidean Yukawa2 field theory,
 Comm. Math. Phys. Volume 44, Number 3 (1975), 237-243.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103899345
We show that arbitrary finite boson mass renormalizations are possible in the Euclidean 
Yukawa2 theory. We work in the Matthews-Salam representation with the fermions "integrated out".

J.Glimm &amp; A.Jaffe:
A &lambda;&phi;<sup>4</sup> quantum field theory without cutoffs,
I, Phys. Rev. 176 (1968) 1945-1951.
II: the field operators and the approximate vacuum, Annals of Maths 91 (1970) 362-401.
III: the physical vacuum, Acta Math. 125 (1970) 203â€”267.
IV: perturbations of the Hamiltonian, J. Math'l. Phys. 13 (1972) 1568-1584.
bosons nonlinear self-interaction in 1+1D spacetime

J.Glimm &amp; A.Jaffe:
Infinite renormalization of the Hamiltonian is necessary,
J. Math'l. Physics 10 (1969) 2212-2214.
We show the unrenormalized hamiltonian in QFT is unbounded from below 
whenever lowest-order pertbn theory indicates that is true.

J.Glimm &amp; A.Jaffe:
Rigorous quantum field theory models,
Bull. Amer. Math. Soc. 76,2 (1970) 407-410.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.bams/1183531504

J.Glimm &amp; A.Jaffe:
Self-adjointness of the Yukawa<sub>2</sub> Hamiltonian, Annals of Physics 60 (1970) 321-383.
J.Glimm &amp; A.Jaffe:
A Yukawa interaction in infinite volume,
Commun. Math'l. Phys. 11,1 (1968) 9-18
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103841150
J.Glimm &amp; A.Jaffe:
The Yukawa$<sub>2</sub>$ quantum field theory without cutoffs, J. Funct. Anal., 7 (1971) 323-357.
J.Glimm &amp; A.Jaffe:
A model for Yukawa quantum field theory,
Phys. Rev. Letters 23 (1969) 1362-1365.

J.Glimm &amp; A.Jaffe:
Singular perturbations of self adjoint operators, 
Commun. Pure Appl. Math. 22 (1969) 401-414.

J.Glimm &amp; A.Jaffe:
The energy momentum spectrum and vacuum expectation values in quantum field theory, 
I: J. Math. Phys., 11 (1970) 3335-3338.
II: Comm. Math. Phys. 22 (1971) 1-22.

J.Glimm &amp; A.Jaffe:
Positivity of the (\Phi^4)_3 Hamiltonian, Fortschritte der Physik 21 (1973) 327-376.
J.Glimm &amp; A.Jaffe:
$\Phi^4<sub>2</sub>$ quantum field model in the single phase region: Differentiability of the 
mass and bounds on critical exponents, Phys. Rev. D, 10 (1974) 536-539.
J.Glimm &amp; A.Jaffe:
A remark on the existence of (Phi^4)_4, Phys. Rev. Lett. 33 (1974) 440-442.
J.Glimm &amp; A.Jaffe:
The coupling constant in a $\phi^4$ field theory, 
??-?? in Recent Develop. in Gauge Theories, G. t'Hooft et al., ed., Plenum Press, New York, 1980.

Oliver A. McBryan &amp; Jay Rosen:
Existence of the Critical Point in phi^4 Field Theory,
Commun. Math. Phys. 51,2 (1976) 97â€”105.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103900344

J.Feldman &amp; K.Osterwalder:
The Wightman Axioms and the Mass Gap for Weakly Coupled (phi^4)_3 Quantum Field Theories, 
Ann. Physics 97 (1976) 80-135.

J. Glimm &amp; A. Jaffe, Collected Papers, Vol I, II, Birkhauser, Boston, 1985.
G+J wrote at least 50 papers and some books together.

J.Glimm &amp; A.Jaffe:
Positivity and self adjointness of the P(phi)<sub>2</sub> Hamiltonian, 
Comm. Math. Phys. 22,4 (1971) 253-258
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103857510
   energy > -O(diameter of spatial region), H is self-adjoint
J.Glimm, A.Jaffe, and Thomas Spencer:
The Wightman axioms and particle structure in the P(phi)<sub>2</sub> quantum field model,
Annal of Maths. (2) 100 (1974) 585â€”632.
The particle structure of the weakly coupled $P(\phi)$ model and other applications of 
high temperature expansions, parts I and II, ??-?? in 
Constructive quantum field theory (Springer LNP #25, G. Velo and A. Wightman, eds.).
Phase transitions in $P(\phi )<sub>2</sub>$ quantum fields, 
Bulletin of the American Math Society 82 (1976) 713-715.
Lecture Notes in Physics, Springer-Verlag, Berlin, 1973.
Existence of phase transitions for $\Phi^4<sub>2</sub>$ quantum fields, pp. 175-184
in Colloques Internationaux C.N.R.S., Marseille, 1975.
Phase transitions for $\Phi^4<sub>2</sub>$ quantum fields, Commun. Math. Phys. 45,3 (1975) 203-216
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103899492
J.Glimm &amp; A.Jaffe: (phi^4)<sub>2</sub> quantum field model in the single phase region: differentiability
of the mass and bounds on critical exponents, 
Phys. Rev. 10 (1974) 536-539.

Francesco Guerra, Lon Rosen, Barry Simon:
Correlation inequalities and the mass gap in P(phi)<sub>2</sub>. 
I (by Simon alone): Commun. Math. Phys. 31 (1973), 127-136.
II: Uniqueness of the vacuum for a class of strongly coupled theories, 
Ann. of Math. 101 (1975) 260-267.
III: Mass gap for a class of strongly coupled theories with nonzero external field
Commun. Math'l. Phys. 41,1 (1975) 19-32.

Francesco Guerra, Lon Rosen, Barry Simon:
Nelson's symmetry and the infinite volume behavior of the vacuum in P(phi)<sub>2</sub>,
Commun. math. Physics 27,1 (1972) 10â€”22 

Francesco Guerra, Lon Rosen, Barry Simon:
The P(phi)<sub>2</sub> Euclidean quantum field theory as classical statistical mechanics, 
Ann. Math. 101 (1975) 111-259.

Francesco Guerra, Lon Rosen, Barry Simon:
The vacuum energy for P(\phi)<sub>2</sub>: Infinite volume limit and coupling constant dependence, 
Commun. Math. Phys. 29,3 (1973) 233-247.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103858550

Francesco Guerra, Lon Rosen, Barry Simon:
Statistical mechanical results in the P(\phi)<sub>2</sub> quantum field theory, 
Phys. Lett. 44B (1973) 102-104 

Francesco Guerra, Lon Rosen, Barry Simon:
Boundary conditions for the P(\phi)<sub>2</sub> Euclidean field, Ann. Inst. H. Poincare 25A (1976) 231-334. 

Francesco Guerra, Lon Rosen, Barry Simon:
The pressure is independent of the boundary conditions in P(\phi)<sub>2</sub>, 
Bull. Amer. Math. Soc. 80 (1974), 1205-1209

A.Jaffe &amp; R.Powers:
Infinite volume limit of a &lambda;&phi;<sup>4</sup> field theory,
Commun. Math'l. Phys. 7,3 (1968) 218-221.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103840418

Yusuke Kato:
Some converging examples of perturbation series in quantum field theory,
Progr. Theor. Phys. 26,1 (1961) 99-122.
it is shown that the total Hamiltonian as a self-adjoint operator can be determined in terms of
the perturbation series for every finite value of the coupling constant if the boson mass is 
not zero and that it is the unique self-adjoint extension of the symmetric operator defined 
initially. For the boson-fermion interaction with continuous spectrum, some additional 
condition on the interaction form factor is needed.

Erhard Seiler:
Schwinger functions for the Yukawa model in two dimensions with space-time cutoff,
Commun Math'l Phys  42, 2 (1975) 163-182.
It is shown that a Euclidean version of the formulae of Matthews and Salam for the Green's
functions of a two-dimensional Yukawa model with interaction in a finite space-time volume 
makes sense, if renormalized correctly.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103899004
we do not discuss the question of removing the cutoff to satisfy Osterwalder-Schrader/Wightman 
axioms.

Erhard Seiler:
On finite mass renormalizations in the twoâ€dimensional Yukawa model,
J. Math. Phys. 16,11 (1975) 2289-2293.

Erhard Seiler &amp; Barry Simon:
Bounds in the Yukawa<sub>2</sub> quantum field theory: Upper bound on the pressure, Hamiltonian bound 
and linear lower bound,
Commun Math'l Phys 45,2 (1975) 99-114.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103899448

Jacques Magnen &amp; Roland Seneor:
The Wightman axioms for the weakly coupled Yukawa model in two dimensions,
Communications in Mathematical Physics 51,3 (1976) 297-313
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103900392
We prove the convergence o f a cluster expansion fo r the weakly coupled Yukawa model in two 
dimensions.

J. Magnen &amp; R. Seneor:
Yukawa Quantum Field Theory in 3 dimensions (Y3),
Annals of the New York Academy of Sciences 337,1 (June 1980) 13-43.
proof of UV stability of Yukawa model in 3D.  A step toward constructing Y3.

Francesco Guerra:
Uniqueness of the vacuum energy density and van Hove phenomenon in the infinite volume limit 
for two-dimensional self-coupled Bose fields, 
Phys. Rev. Lett. 28 (1972) 1213-1215.

James Glimm &amp; Arthur Jaffe: A note on reflection positivity, 
Lett. Math'l. Phys. 3 (1979) 377-378. 
A.Jaffe &amp; Gordon Ritter:
Reflection Positivity and Monotonicity,
http://www.arthurjaffe.com/Assets/pdf/Reflection_Positivity.pdf
Jour. Math. Phys. 49 (2008) 052301, 1-10.

Konrad Osterwalder &amp; Robert Schrader: 
Euclidean Fermi fields and a Feynman-Kac formula for boson-fermion interactions, 
Helv. Phys. Acta 46 (1973) 227-302.

K. Osterwalder and E. Seiler:
Gauge theories on the lattice, Ann. Phys. 110 (1978) 440-471.

K. Gawedzki: Renormalization of a non-renormalizable quantum field theory, 
Nucl. Phys. B262 (1985) 33-48.

Edward Nelson:
Quantum fields and Markoff fields, pp. 413-420 in
Proc. Sympos. Pure Math. XXIII 1971, AMS, Providence, R.I., 1973, 

A.Jaffe:
Introduction to QFT,
http://www.arthurjaffe.com/Assets/pdf/IntroQFT.pdf
2005 incomplete book

J. Glimm, A. Jaffe, and T. Spencer:
A convergent expansion about mean field theory, 
I: Ann. Phys. 101 (1976) 610-630
II: Ann. Phys. 101 (1976) 631-669.

D.C. Brydges &amp; Paul Federbush: Debye screening, Comm. Math. Phys. 73,3 (1980) 197-246. 
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103907873
high temp coulomb gas has exponential falloff of correlns.

D. Brydges, J. Dimock, T. R. Hurd:
The Short Distance Behavior of (phi^4)_3,
Commun.Math.Phys. 172,1 (1995) 143-186.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1104273962

Publications of Tosio Kato:
http://www.kurims.kyoto-u.ac.jp/~kyodo/kokyuroku/contents/pdf/1234-26.pdf

Tadeusz Balaban:
Ultraviolet stability of three-dimensional lattice pure gauge field theories, 
Comm. Math. Phys. 102,2 (1985) 255-275.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1104114382
I think yang-mills in 2+1D on a lattice is meant when spacing->0?
tbalaban@math.rutgers.edu

T. Balaban:
Renormalization group approach to lattice gauge field theories. 
I: generation of effective 
actions in a small field approximation and a coupling constant renormalization in 4D, 
Comm. Math. Phys. 109 (1987) 249-301.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1104116842
II: Cluster expansions, Communications in Mathematical Physics 116,1 (1988) 1-22. 
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1104161193
"This completes the construction of the sequence of effective actions in 
the small field approximation."
he is trying to make lattice-QED rigorous. Ditto Dimock.

T. Balaban: Convergent Renormalization Expansions for Lattice Gauge Theories,
Commun. Math. Phys. 119 (1988) 243-285 
"This completes the renormalization group analysis for superrenormalizable models and yields 
convergent expansions in this case."
Whatever that meant.

Tadeusz Ba Lslash aban:
Large field renormalization. II. Localization, exponentiation, and bounds for the R operation
Commun. Math. Phys. 122,3 (1989) 355-392.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1104178467
"complete the proof of the ultraviolet stability of four-dimensional pure gauge field theories."
I.e. Yang-Mills, asymptotic freedom.

T.Balaban:
Regularity and decay of lattice Green's functions
Commun Mathl Phys  89, 4 (1983) 571-597.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103922931
"We study a class of lattice covariant Laplace operators with external gauge fields. 
We prove that these operators are positive and that their Green's functions decay exponentially. 
They also have regularity properties similar to continuous space Green's functions." 

T.Balaban: (Higgs)<sub>2,3</sub> Quantum Fields in a Finite Volume,
I: a lower bound,  Comm. Math. Phys. Volume 85,4 (1982), 603-626.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103921549
II: an upper bound, Comm. Math. Phys. Volume 86,4 (1982), 555-594.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103921845
III: Renormalization, Commun. Math. Phys. 88 (1983) 411-445.
http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&id=pdf_1&handle=euclid.cmp/1103922385
scalar QED, lower bound indenpedent of lattice spacing is proven on vacuum energy.
also upper bound.

Many publications of T.Balaban listed in
Jonathan D. Dimock:
http://arxiv.org/pdf/1108.1335v1.pdf
also can find such a list by project euclid search.

Jonathan Dimock:
http://www.math.buffalo.edu/~dimock/
 dimock@buffalo.edu
Quantum Mechanics and Quantum Field Theory: A Mathematical Primer
 by Jonathan Dimock, Cambridge University Press (2011).
Publications: http://www.nsm.buffalo.edu/~dimock/publications.pdf
Quantum electrodynamics on the 3-torus -I:first step(2002, 70 pages, latest Feb 2008)
http://arxiv.org/abs/math-ph/0210020
II:The RG flow(2004, latest July 2011, 37 pages)
http://arxiv.org/abs/math-ph/0407063
III: convergence (claimed in 2011 "to appear").

I: Quantum electrodynamics in a four dimensional space time, (QED)_4, 
is the basic theory of electrons and photons. The theory is very singular at short distances. 
Nevertheless it does have a well defined perturbation theory (expansion in the coupling constant) 
provided various renormalizations are carried out. At this level it gives a spectacularly 
precise description of nature.
  One would like to have a rigorous non-perturbative construction of the model. 
To date this has not been possible and success is not likely anytime soon. 
However the singularities are weaker in lower dimensions and there is the 
possibility of progress. In this sequence of papers we [will] give a construction of (QED)_3. 
To avoid long distance (infrared) problems, which are also present, we work on the 
three dimensional torus.
  The theory is formally defined by functional integrals. The program is to add cutoffs to 
take out the short distance (ultraviolet) singularities and make the theory well-defined. 
Then one tries to remove the cutoffs. The singularities that develop are to be cancelled by 
adjusting the bare parameters - this is renormalization. At the same time one must keep 
control throughout over the size of the functional integrals - this is the stability problem.
An effective way to deal with these difficulties is to regularize the theory by putting it on
a lattice and then study the continuum limit.
This regularization has the advantage of preserving gauge invariance. 
This helps with the treatment of the singularities- the counterterms required are minimal. 
The stability problem also seems more tractable in the lattice approximation...
   The present work will not directly generalize to (QED)4. The problem is that 
this model lacks ultraviolet asymptotic freedom, the flow of the effective 
coupling constants away zero. However the present work is progress toward a construction 
of quantum chromodynamics (QCD)3,4, the fundamental theory of the strong interactions.
It seems likely that combining Balabanâ€™s results on Yang-Mills [5],[6] with the present 
work would be sufficient, albeit still on a torus.

II: We continue the study of quantum electrodynamics on a three dimensional torus as the limit of a lattice gauge theory. In this paper we give a preliminary treatment of the renormalization group flow. We study the propagators which arise under multiple block spin averaging, both in global and local versions. We also study low order perturbation theory. However we do not control remainders. This is left to the more complete treatment of the following paper.

J. Magnen, F. Nicolo, V. Rivasseau, R. Seneor:
A Lipatov bound for phi44 Euclidean field theory,
Commun. Mathl. Phys. 108,2 (1987) 257-289.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1104116463
In dimensions below 4, where theory superrenormalizable,
rigorous analysis of pertn series has been completed:
 Harrell, E., Simon, B.: Duke Math. J. 47, 845 (1980) 
 Spencer, T.: The Lipatov argument. Commun. Math. Phys. 74, 273 (1980) 
 Breen, S.: Leading order asymptotics for (phi^4)2 perturbation theory,
   Commun. Math. Phys. 92, 179 (1983) 
 Magnen, J., Rivasseau, V.: Commun. Math. Phys. 102, 59 (1985) 
 Feldman, J., Rivasseau, V.: Ann. Inst. Henri Poincare 44, 427 (1986)

J. Magnen and R. Seneor:
Phase space cell expansion and Borel summability for the Euclidean phi43 theory,
Commun. Mathl. Phys.  56,3 (1977), 237-276.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103901188

J. Feldman, J. Magnen, V. Rivasseau, and R. Seneor:
Construction and Borel summability of infrared phi^4_4 by a phase space expansion,
Commun. Math. Phys. Volume 109, Number 3 (1987), 437-480.
in its present form our method seems to apply to any theory whose content is 
essentially perturbative (typically for which the perturbation series is Borel summable).

J.-P. Eckmann, J. Magnen, and R. Seneor:
Decay properties and Borel summability for the Schwinger functions in P(phi)2 theories,
Comm. Math. Phys. Volume 39, Number 4 (1974), 251-271.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103860232

J. Feldman, J. Magnen, V. Rivasseau, and R. Seneor:
Bounds on Completely Convergent Euclidean Feynman Graphs
Commun.Math.Phys.98,2 (1985)273-288.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103942362
Abstract. Let G be a Euclidean Feynman graph containing L(G) lines. We prove that if G
has massive propagators and does not contain any divergent subgraphs its value is bounded by 
K^L(G). We also prove the infrared analogue of this bound.
   This is for essentially any QFT where divergent subgraphs are not possible
("superrenormalizable")  and so we do not need renormalization.  
Otherwise Lautrup is counterexample.
   By "Euclidean" do they mean "there is no time"???

J. Feldman, J. Magnen, V. Rivasseau, and R. Seneor:
Bounds on Renormalized Feynman Graphs,
Commun. Math. Phys.100,1 (1985) 23-55 
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1103943336
They show upper bounds which crudely match Lautrup's lower bound, i.e.
exhibit factorial style growth (k*N)!.  These upper bounds are only valid in a certain
restricted classes of rQFTs.  One of their upper bounds matches Lautrup's k=1.  

Joel Feldman, J. Magnen, V. Rivasseau, R. Seneor:
A renormalizable field theory: the massive Gross-Neveu model in two dimensions,
Commun. Mathl. Phys. 103,1 (1986) 67-103.
http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.cmp/1104114625
The Euclidean massive Gross-Neveu model in two dimensions is just renormalizable 
and asymptotically free. Thanks to the Pauli principle, bare perturbation theory with an 
ultra-violet cut-off (and the correct ansatz for the bare mass) is convergent in a disk, 
whose radius corresponds by asymptotic freedom to a small finite renormalized coupling constant. 
Therefore, the theory can be fully constructed in a perturbative way. It satisfies the 
O.S. axioms and is the Borel sum of the renormalized perturbation expansion of the model.
    They are models of N-component fermions (with N>=2), with a quartic interaction; 
therefore their graphs are topologically the same as those of the familiar phi^4 bosonic theories.

Y.Park: Convergence of lattice approximations and infinite volume limit in the 
(lambda phi^4-sigma phi^2 - mu phi)_3
field theory, J. Math'l. Phys. 18 (1977) 354â€”366.

S. Deser, R. Jackiw, S. Templeton, Topologically massive gauge theories, 
Ann. of Phys 140,2 (1982) 372-411
heuristic discuss QED in 2+1 dimensions.

Ramzi R. Khuri, A Multimonopole Solution in String Theory, 
(hep-th/9205051) (April 1992); 
A Heterotic Multimonopole Solution, 
(hep-th/9205081) (April 1992).

 David A. Cox; Sheldon Katz (1999), Mirror Symmetry and Algebraic Geometry, Providence, 
RI: American Mathematical Society, ISBN 0-8218-1059-6.

The Mirror Formula for Quintic Threefolds
Alexander Givental
http://math.berkeley.edu/~giventh/papers/qm.pdf

P. Candelas, X. de la Ossa, P. Green and L. Parkes, A pair of Calabi-Yau manifolds as an 
exactly soluble superconformal theory. In: Essays on mirror manifolds, Int. Press, 
Hong Kong, 1992 pp. 31-95;
 Phys. Lett. B 258 (1991), 118-126; Nuclear Phys. B 359 (1991), 21-74.

D.R. Morrison: Mirror symmetry and rational curves on quintic threefolds: 
a guide for mathematicians. J. Amer. Math. Soc. 6,1 (1993) 223-247.
unreadale.

Paul S. Aspinwall and David R. Morrison:
Topological field theory and rational curves, Commun. Math. Phys. 15,2 (1993) 245-262.
http://arxiv.org/abs/arXiv:hep-th/9110048
http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.cmp/1104252136&page=record

The hyperboloid surface t^2=x^2+y^2+1 has induced by Minkowski metric, the hyperbolic plane.
The other hyperboloid surface t^2=x^2+y^2-1 has induced by Minkowski metric, de Sitter 1+1 metric.

http://www.scholarpedia.org/article/Calabi-Yau_manifold
8000 constructions found in 1989 computer search.
Then in 1990 pool grew by about 1000X.
Yau conjectured only finite # CYs exist in each dimension. Still open.
Contrast with fact infinite number of compact hyperbolic2- and 3-manifolds.
Rough count: by 2002
over 473 million toric embeddings of compact Calabi-Yau threefolds were constructed, 
with over 30,000 distinct Hodge diamonds. 

Superstring theory assumes arena is minkowski 3+1 space procucted with 6D CY compact manifold of 
complex dimension 3.  Brane theory allows fancier arenas...

Could we have a string in a quantum superpositon of "being an electron" and
"being a muon"?  Whole new kind of "quantum weirdness"?

R Foot & GC JOshi:
LETTERS IN MATHEMATICAL PHYSICS
Volume 19, Number 1, 65-71, DOI: 10.1007/BF00402262
Nonstandard signature of spacetime, superstrings, and the split composition algebras

LETTERS IN MATHEMATICAL PHYSICS
Volume 15, Number 3, 237-242, DOI: 10.1007/BF00398593
A natural framework for the minimal supersymmetric gauge theories

Rovelli:
There are several examples of quantum field theories that are well defined nonperturbatively, 
and are nevertheless nonrenormalizable if we try a perturbation expansion.

LQG background independence:
"But in GR things move on a spacetime. Fields and particles have a dynamics on a curved spacetime. 
Maybe curved, but always a spacetime."
Physics on a curved spacetime is not GR. GR is the dynamics of spacetime itself. So, quantum GR 
is the theory of a quantum spacetime, not a quantum theory on various spacetimes.
"But how can we do physics without a spacetime? You will not have energy, momenta, positions..."
Indeed.

C. Rovelli and S. Speziale: Lorentz covariance of loop
quantum gravity, arXiv:1012.1739.

F.Strocchi:
Selected topics on the general properties of quantum field theory: lecture notes,
QC174.45.S84 1993.


D. Espriu & R. Tarrach:
Ambiguities in QED: Renormalons versus triviality,
Physics Letters B 383, 4 (1996) 482-486
http://arxiv.org/pdf/hep-ph/9604431v1.pdf
Abstract
We point out that, contrary to what is believed to hold for QCD, renormalons are genuine in QED; i.e. the ambiguities which come with them do not require cancellation by hypothetical non-perturbative contributions. They are just the ambiguities characteristic of any trivial â€” and thus effective â€” theory. If QED remained an isolated theory up to an energy close to its triviality scale, these ambiguities would surely hint at new physics. This not being so, the renormalon ambiguities in QED lead to no new physics, not even to non-perturbative contributions within QED itself.

http://www.imamu.edu.sa/Scientific_selections/abstracts/Physics/THE%20ANOMALOUS%20MAGNETIC%20MOMENT%20OF%20THE%20MUON.pdf
B.Lautrup and E.de Rafael:
THE ANOMALOUS MAGNETIC MOMENT OF THE MUON AND SHORT-DISTANCE BEHAVIOUR
OF QUANTUM ELECTRODYNAMICS,
Nucl. Phys. B70 (1974) 317-350.

The basic difficulty was pointed out by Aragone and Deser [1], in the early seventies. They showed that, due to the fact that the free-field equations of motion for the spin-2 in a curved riemannian background are no longer divergence-free, as in the Minkowski case, the only ac- ceptable situation is the one in which the background spacetime curvature is constant. This rules out com- patibility with the Einstein equations, since the spin-2 field cannot be the source of a constant curvature geom- etry. When dealing with the possibility of non-minimal coupling, they pointed out that the Eintein equations themselves would be altered, and that severe restrictions would apply to the field variables because of the curva- tureâ€™s algebraic structure in each spacetime point.
C. Aragone and S. Deser: Il Nuovo Cimento 3,4 (1971) 709.

Steven Carlip:
The Small Scale Structure of Spacetime,
http://arxiv.org/abs/1009.1136
http://arxiv.org/pdf/1009.1136v1.pdf
"There is, in fact, a fairly general argument that if quantum gravity is asymptotically
safe, it must be eï¬€ectively two-dimensional at very short distances."
cites
M. Niedermaier:
<a href="http://arxiv.org/abs/gr-qc/0610018">The Asymptotic Safety Scenario 
in Quantum Gravity -- An Introduction</a>,
Class. Quant. Grav. 24 (2007) R171-230.
"it is argued that as a consequence of the scenario the self-interactions 
appear two-dimensional in the extreme ultraviolet. "
Daniel F. Litim: 
<a href="http://arxiv.org/abs/hep-th/0606044">On fixed points of quantum gravity</a>,
AIP Conf. Proc. 841 (2006) 322.
"For quantum gravity, this asymptotic safety scenario has been introduced by Weinberg
[1]. In the vicinity of two dimensions, a non-trivial fixed point has been identified within
perturbation theory, to leading [1]-[3] and subleading order [4] in epsilon=dâˆ’2<<1."
"From a renormalisation group point of view [1], the 'critical' dimension of quantumravity is 
d[crit]=2."
"In this contribution, we discuss the asymptotic safety scenario in the context of quantum 
gravity. Based on a wilsonian renormalisation group, we provide unique analytical
ï¬xed point solution in the Einstein-Hilbert truncation for any dimensions d>d[crit] [12].
The approach is related to the integrating-out of momentum modes from a path integral
representation of the theory [19], amended by an appropriate optimisation [20, 21]."
1. S. Weinberg, in General Relativity: An Einstein centenary survey, 
   Eds. S.W. Hawking and W. Israel, Cambridge University Press (1979), p. 790.
2. R. Gastmans, R. Kallosh and C. Trufï¬n, Nucl. Phys. B 133 (1978) 417.
3. S.M. Christensen and M.J. Duff, Phys. Lett. B 79 (1978) 213.
4. T. Aida and Y. Kitazawa, Nucl. Phys. B 491 (1997) 427 [hep-th/9609077]
Daniel F. Litim:
<a href="http://arxiv.org/abs/1102.4624">Renormalisation group and the Planck scale</a>,
Phil. Trans. R. Soc. A 369 (2011) 2759-2778.

Jayme De Luca:
Variational principle for the Wheeler-Feynman electrodynamics,
http://arxiv.org/abs/0901.1077

"No-interaction theorems"
Aloysius F. Kracklauer:
A geometric proof of no-interaction theorems,
J. Math. Phys. 17,5 (1976) 693-694.
The conclusion of a no-interaction theorem is that a relativistic canonical (Hamiltonian)
formulation of dynamics (such theorems available for both classical and quantum)
can only describe particles between which there is no-interaction. Of course,
this conclusion depends on the conditions in the hypothesis
of the theorem, of which there are two versions. 
The original version has been proven sequentially by stronger methods, 
first for two [3], then three [4], and finally N particles
by at least three methods [5-7]:
[3]: D. G. Currie, T. F. Jordan and E. C. G. Sudarshan, Rev. Mod. Phys. 35 (1963) 350-??.
This version is characterized by the assumption that the dynamics
of the system is governed by a scheme parameterized by a single parameter: time.
  The second version [8], which has been
proven heretofore by only one method, is characterized by the
assumption that the dynamics is governed by an N parameter
scheme.
[8]: P. Droz-Vincent, Nuovo Cimento B 12 (1972) 1-??.
   He says he does not really need special relativity and MInkowski space for his disproofs.
Kraklauer says Cartan had highly abstratc hamiltonian formulation which conflicted 
with impossibility proofs, and presents at the end
a Hamiltonian formulation leading to "delay differential equations."
Kraklauer says he understands and see 1,4,5,10,14,21.
Kraklauer also says Bell theorem, Kochen-Specker paradox, etc all are confused and he claims
to set them straight in 12: http://nonloco-physics.0catch.com/aflb252p193.pdf
  Annales de la Fondation Louis de Broglie, Volume 25, no 2 (2000) 193-207.
On a quick skim, Kraklauer12 seems ok.

Graviton (spin2) is unique. Spin3/2: at most 8 kinds of particle.
4-or-more way interactions at a point impossible, in sense would require dimensionful 
coupling consants and hence effect propto power of lengths hence negligible 
at low energies and large lengths.
Only spins 0,1/2,1,3/2,2 are possible, if tried to have others inconsistencies
would arise when considering 4-way (e.g. 2 in, 2 out) interations constructed from
3-way interactions.

Massimo Porrati:
Universal Limits on Massless High-Spin Particles,
http://arxiv.org/abs/0804.4672  [July 2008] Phys.Rev.D78:065016,2008
massless particles interacting with gravity in a Minkowski background space can have at most 
spin two. This result is proven by extending a famous theorem due to Weinberg and Witten to 
theories that do not possess a gauge-invariant stress-energy tensor.
It also cites:
[24] F. A. Berends, G. J. H. Burgers and H. van Dam, Nucl. Phys. B 260, 295 (1985).
[25] A. K. H. Bengtsson and I. Bengtsson, Class. Quant. Grav. 3, 927 (1986).
[26] X. Bekaert, N. Boulanger, S. Cnockaert and S. Leclercq, Fortsch. Phys. 54, 282
(2006) [http://arxiv.org/abs/hep-th/0602092]
the last seems to permit spin=3(?), but it seems fairly clear there will be an upper 
bound on spin...
http://arxiv.org/pdf/hep-th/0606165.pdf  seems to be another no-go theorem for high spin...
Another is  Coleman-Mandula theorem. 

Massimo Porrati:
Old and New No Go Theorems on Interacting Massless Particles in Flat Space,
http://arxiv.org/abs/1209.4876  Oct.2012   
   QUARKS 2012, Yaroslavl, Russian Federation, June 4-10, 2012
My contribution to the proceeding of this conference
shall review old and new no go theorems on interacting high-spin theories. These theorems are
surprisingly strong when combined together and rule out any interaction between particles of
spin higher than 2 and any matter that interacts with gravity. The same theorems also show
that there exists only one massless spin two particle interacting with gravity, i.e. the graviton
itself.

Massimo Porrati, Rakibur Rahman, Augusto Sagnotti:
String Theory and The Velo-Zwanziger Problem,
http://arxiv.org/abs/1011.6411  [Nov 2010] Nucl.Phys.B846:250-282,2011
[Our] equations indicate that 26D String Theory does bypass the Velo-Zwanziger problem, i.e. the 
loss of causality experienced by a massive high-spin field minimally coupled to electromagnetism.

Ron Maimon claims from 
http://physics.stackexchange.com/questions/14932/why-do-we-not-have-spin-greater-than-2:

Higher spin particles have to be coupled to conserved currents, and there are no
conserved currents of high spin in quantum field theories. The only conserved currents are 
vector currents associated with internal symmetries, the stress-energy tensor current, 
the angular momentum tensor current, and the spin-3/2 supercurrent for a supersymmetric theory.
This restriction on the currents constrains the spins to 0,1/2 (which do not need to be
coupled to currents), spin 1 (which must be coupled to the vector currents), spin 3/2 
(which must be coupled to a supercurrent, and the number of gravitinos must be less than 
or equal to the number of supercharges)  and spin 2 (which must be coupled to the 
stress-energy tensor).
Spin-1 fields can become massive by the Higgs mechanism, and all spin 3/2 gravitinos 
must become massive through spontaneous SUSY breaking because it is known unbroken SUSY is
ruled out by experimental evidence.

Of course, in string theory, there are fields of arbitrarily high spin, and unitarity is
restored by propagating all of them together... thee above impossibilities are in QFTs 
with finite sets of field/particle types, and strings escape by having infinite set, each
type is saved by some interation with still-higher-spin type.  

Also composite particles like atomic nuclei can and sometimes do have spin>2.

David Simmons-Duffin claims spin>=1 is impossible for massive particles in renormalizable QFT,
and spin>1 impossible for massless.

William H. Press, Glennys R. Farrar:
Recursive Stratified Sampling for Multidimensional Monte Carlo Integration, 
Computers in Physics 4,2 (1990) 190-195.
http://www.nr.com/webnotes?10
MISER starts out by allocating 10% of the available points for presampling the volume V. 
Then the variances of all possible subvolumes are estimated, only permeitting
exact bisection of each coordinate, so there are D choices of which we pick the best. 
When this is done MISER is 
called recursively for the pair of subvolumes yeilding the lowest possoble variance
esitmate, with the rest of the available points equally split between them.

Rudolf Sch&uuml;Ìˆer:
Adaptive Quasi-Monte Carlo Integration Based on MISER and VEGAS,
http://mint.sbg.ac.at/HIntLib/
http://mint.sbg.ac.at/rudi/projects/mcqmc2002.pdf
In H. Niederreiter, editor, Monte Carlo and Quasi-Monte Carlo Methods 2002, pages 393-406. 
Springer-Verlag, February 2004
combining VEGAS with low-discrepancy sequences seems excellent performance in
dimensions 10-20.
In H. Niederreiter, editor, Monte Carlo and Quasi-Monte Carlo Methods 2002, pages 393â€“406. Springer-Verlag, February 2004.

Rudolf Sch&uuml;Ìˆrer: 
A comparison between (quasi-)Monte Carlo and cubature rule based methods for solving 
high-dimensional integration problems,
Mathematics and Computers in Simulation 62,3-6 (March 2003) 509-517.
3rd IMACS Seminar on Monte Carlo Methods
Algorithms for estimating the integral over hyper-rectangular regions are discussed. 
Solving this problem in high dimensions is usually considered a domain of Monte Carlo 
and quasi-Monte Carlo methods, because their power degrades little with increasing dimension. 
These algorithms are compared to integration routines based on interpolatory cubature rules, 
which are usually only used in low dimensions. Adaptive as well as non-adaptive algorithms 
based on a variety of rules result in a wide range of different integration routines. 
Empirical tests performed with Genzâ€™s test function package show that cubature rule 
based algorithms can provide more accurate results than quasi-Monte Carlo routines for 
dimensions up to s=100.

http://en.wikipedia.org/wiki/Monte_Carlo_integration#MISER_Monte_Carlo
the smallest error estimate is obtained by allocating sample points in proportion 
to the standard deviation (square root of the variance) of the function in each sub-region.

R.J. Hill:
New Value of Mmu/Me from Muonium Hyperfine Splitting,
Phys. Rev. Lett. 86,15 (2001) 3280-3283.

http://arxiv.org/abs/1208.2637
Theory of the 2S-2P Lamb shift and 2S hyperfine splitting in muonic hydrogen
Aldo Antognini, Franz Kottmann, Francois Biraben, Paul Indelicato, Francois Nez, Randolf Pohl
reviews. Ends:
"The 0.3 meV (7&sigma;) discrepancy between the 
proton rms charge radii rE from muonic hydrogen [1, 2] 
and CODATA-2010 [15] persists. The 'proton radius puzzle' remains."

???
Julian Schwinger:
Renormalization theory of Quantum electrodynamics: an
individual view
pp.329-353 in "The Birth Of Particle Physics", proceedings Batavia 1980
http://www.fuw.edu.pl/~kostecki/scans/schwinger1983.pdf
page 337:
..[it] required the impetus of experiments to show that electrodynamic efects 
were neither infinite nore zero, but finite and small, and demanded understanding.

Schwinger: Theory of quantized fields,
I: Phys Rev 82 (1951) 914-927
II: Phys Rev 91 (1953) 713-728 [both reprinted in S's volume]
 about 1950-1; appendix gives short derivation
of previously known Schwinger term in anaomalous electron moment.

FJ Dyson: The S matrix in QED, Phys Rev 75 (1949) 1736-1755:  [reprinted in S's volume]
"what is to be looked for in a theory is not so much a modification of the
present theory which will make all infinite quantities finite,
but rather a turning-round of the theory so that the finite
quantities shall become primary."
Schwinger says he has accomplished that, and it is called "source theory."
This is described a bit in a paper by KA Milton on the ArXiv.
It is not a attempt to be a fundamental theory.

stability of matter
RelQM model, stab of matter if alpha<1/94 and Zalpha<=2/pi 32.
charged bosons are unstable always, good thing they do not exist.
Except they do exist (with short halflife, but exist).

J.A.M. Vermaseren, S.A. Larin, T. van Ritbergen:
<a href="http://arxiv.org/abs/hep-ph/9703284">
The 4-loop quark mass anomalous dimension and the invariant quark mass</a>,
Phys.Lett. B405 (1997) 327-333.

T. van Ritbergen, J.A.M. Vermaseren, S.A. Larin:
<a href="http://arXiv.org/abs/hep-ph/9701390">The 
four-loop beta-function in Quantum Chromodynamics</a>,
Phys.Lett. B400 (1997) 379-384.

H. Kawai, T. Kinoshita, Y. Okamoto:
Asymptotic photon propagator and higher-order QED Callan-Symanzik beta-function,
Physics Letters B 260,1-2 (May 1991) 193-198.

T. Kinoshita, H. Kawai, Y. Okamoto:
Asymptotic photon propagotor in massive QED and the muon anomalous magnetic moment,
Physics Letters B 254,1-2 (1991) 235-240.

RENORMALIZATION SCHEMES IN QED
Author(s): COQUEREAUX, R (COQUEREAUX, R)
Source:  ANNALS OF PHYSICS  Volume: 125   Issue: 2   Pages: 401-428
DOI: 10.1016/0003-4916(80)90139-6   Published: 1980
  Looks good.  EQ42 p425 relates fermion propagator and effective mass
of the fermion,.
On p427 he finds Zm and Z2 to order alpha^1 finding
1/eps divergence in the alpha^1 term, and the alpha^0 term is 1.

Tetsuya Koyama &amp; Takashi Ichinose:
<a href="http://projecteuclid.org/euclid.pja/1195516535">On the Trotter product formula</a>,
Proc. Japan Acad. Ser. A Math. Sci. 57,2 (1981), 95-100.

Takashi Ichinose &amp; Hideo Tamura:
On the Norm Convergence of the Trotter-Kato Product Formula with Error Bound,
Operator Theory Adv. &amp; Applics 126 (2001) 149-154.

Takashi Ichinose:
A product formula and its application to the Schr&ouml;dinger equation. 
Publ. Res. Inst. Math. Sci. 16 (1980) 585-600.

Steven Weinberg:
New approach to the renormalization group,
Phys.Rev. D 8,10 (1973) 3497-3509.

L.V.Ovsiannikov: ???in Russian???, Doklady Akad Nauk. SSSR 109 (1956) 1112-1115.
Or 1121???

Kurt Symanzik: 
<A href="http://projecteuclid.org/euclid.cmp/1103842537">Small distance 
behavior in field theory and power counting</a>, 
Commun. Math'l. Phys 18,3 (1970) 227-246.

F.J.Dyson: 
<a href="http://rspa.royalsocietypublishing.org/content/207/1090/395.full.pdf">The 
Renormalization Method in Quantum Electrodynamics</a>,
Proceedings of the Royal Society of LOndon A 207 (1951) 395-401.
seems vague. Not worth citing.

O. Piguet, A. Rouet:
Symmetries in perturbative quantum field theory (Review Article),
Physics Reports 76,1 (1981) 1-77.
"In pertuebation thery one can arrangeto use no regulator whatsoever" (among much else).
-->